Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown
Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root
Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root
Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root
Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root
Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root
Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root
Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root
Jun 15 02:04:59 combo sshd(pam_unix)[20896]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root
Jun 15 02:04:59 combo sshd(pam_unix)[20897]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root
Jun 15 02:04:59 combo sshd(pam_unix)[20898]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root
Jun 15 04:06:18 combo su(pam_unix)[21416]: session opened for user cyrus by (uid=0)
Jun 15 04:06:19 combo su(pam_unix)[21416]: session closed for user cyrus
Jun 15 04:06:20 combo logrotate: ALERT exited abnormally with [1]
Jun 15 04:12:42 combo su(pam_unix)[22644]: session opened for user news by (uid=0)
Jun 15 04:12:43 combo su(pam_unix)[22644]: session closed for user news
Jun 15 12:12:34 combo sshd(pam_unix)[23397]: check pass; user unknown
Jun 15 12:12:34 combo sshd(pam_unix)[23397]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:12:34 combo sshd(pam_unix)[23395]: check pass; user unknown
Jun 15 12:12:34 combo sshd(pam_unix)[23395]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:12:34 combo sshd(pam_unix)[23404]: check pass; user unknown
Jun 15 12:12:34 combo sshd(pam_unix)[23404]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:12:34 combo sshd(pam_unix)[23399]: check pass; user unknown
Jun 15 12:12:34 combo sshd(pam_unix)[23399]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:12:34 combo sshd(pam_unix)[23406]: check pass; user unknown
Jun 15 12:12:34 combo sshd(pam_unix)[23406]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:12:34 combo sshd(pam_unix)[23396]: check pass; user unknown
Jun 15 12:12:34 combo sshd(pam_unix)[23394]: check pass; user unknown
Jul  1 09:00:55 calvisitor-10-105-160-95 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  1 09:01:05 calvisitor-10-105-160-95 com.apple.CDScheduler[43]: Thermal pressure state: 1 Memory pressure state: 0
Jul  1 09:01:06 calvisitor-10-105-160-95 QQ[10018]: FA||Url||taskID[2019352994] dealloc
Jul  1 09:02:26 calvisitor-10-105-160-95 kernel[0]: ARPT: 620701.011328: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  1 09:02:26 authorMacBook-Pro kernel[0]: ARPT: 620702.879952: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  1 09:03:11 calvisitor-10-105-160-95 mDNSResponder[91]: mDNS_DeregisterInterface: Frequent transitions for interface awdl0 (FE80:0000:0000:0000:D8A5:90FF:FEF5:7FFF)
Jul  1 09:03:13 calvisitor-10-105-160-95 kernel[0]: ARPT: 620749.901374: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': Yes, 'TimeRemaining': 0,
Jul  1 09:04:33 calvisitor-10-105-160-95 kernel[0]: ARPT: 620750.434035: wl0: wl_update_tcpkeep_seq: Original Seq: 3226706533, Ack: 3871687177, Win size: 4096
Jul  1 09:04:33 authorMacBook-Pro kernel[0]: ARPT: 620752.337198: ARPT: Wake Reason: Wake on Scan offload
Jul  1 09:04:37 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  1 09:12:20 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  1 09:12:21 calvisitor-10-105-160-95 symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  1 09:18:16 calvisitor-10-105-160-95 kernel[0]: ARPT: 620896.311264: wl0: MDNS: 0 SRV Recs, 0 TXT Recs
Jul  1 09:19:03 calvisitor-10-105-160-95 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  1 09:19:03 authorMacBook-Pro configd[53]: setting hostname to "authorMacBook-Pro.local"
Jul  1 09:19:13 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 439034 seconds.  Ignoring.
Jul  1 09:21:57 authorMacBook-Pro corecaptured[31174]: CCIOReporterFormatter::addRegistryChildToChannelDictionary streams 7
Jul  1 09:21:58 calvisitor-10-105-160-95 com.apple.WebKit.WebContent[25654]: [09:21:58.929] <<<< CRABS >>>> crabsFlumeHostAvailable: [0x7f961cf08cf0] Byte flume reports host available again.
Jul  1 09:22:02 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 2450 seconds.  Ignoring.
Jul  1 09:22:25 calvisitor-10-105-160-95 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  1 09:23:26 calvisitor-10-105-160-95 kernel[0]: AirPort: Link Down on awdl0. Reason 1 (Unspecified).
Jul  1 09:23:26 calvisitor-10-105-160-95 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  1 09:24:13 calvisitor-10-105-160-95 kernel[0]: PM response took 2010 ms (54, powerd)
Jul  1 09:25:21 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 438666 seconds.  Ignoring.
Jul  1 09:25:45 calvisitor-10-105-160-95 kernel[0]: ARPT: 621131.293163: wl0: Roamed or switched channel, reason #8, bssid 5c:50:15:4c:18:13, last RSSI -64
Jul  1 09:25:59 calvisitor-10-105-160-95 kernel[0]: ARPT: 621145.554555: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': Yes, 'TimeRemaining': 0,
Jul  1 09:26:41 calvisitor-10-105-160-95 kernel[0]: ARPT: 621146.080894: wl0: wl_update_tcpkeep_seq: Original Seq: 3014995849, Ack: 2590995288, Win size: 4096
Jul  1 09:26:43 calvisitor-10-105-160-95 networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x4 to 0x8000000000000000
Jul  1 09:26:47 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 2165 seconds.  Ignoring.
Jul  1 09:27:01 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 13090 seconds.  Ignoring.
Jul  1 09:27:06 calvisitor-10-105-160-95 kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  1 09:28:41 authorMacBook-Pro netbiosd[31198]: network_reachability_changed : network is not reachable, netbiosd is shutting down
Jul  1 09:28:41 authorMacBook-Pro corecaptured[31206]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  1 09:28:50 calvisitor-10-105-160-95 com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  1 09:28:53 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 2039 seconds.  Ignoring.
Jul  1 09:29:02 calvisitor-10-105-160-95 sandboxd[129] ([31211]): com.apple.Addres(31211) deny network-outbound /private/var/run/mDNSResponder
Jul  1 09:29:14 calvisitor-10-105-160-95 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  1 09:29:25 calvisitor-10-105-160-95 kernel[0]: ARPT: 621241.634070: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:c6b3:1ff:fecd:467f
Jul  1 09:31:48 authorMacBook-Pro kernel[0]: AirPort: Link Up on en0
Jul  1 09:31:53 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 438274 seconds.  Ignoring.
Jul  1 09:32:03 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1849 seconds.  Ignoring.
Jul  1 09:32:13 calvisitor-10-105-160-95 kernel[0]: Sandbox: com.apple.Addres(31229) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  1 09:32:28 calvisitor-10-105-160-95 mDNSResponder[91]: mDNS_DeregisterInterface: Frequent transitions for interface awdl0 (FE80:0000:0000:0000:D8A5:90FF:FEF5:7FFF)
Jul  1 09:33:13 calvisitor-10-105-160-95 kernel[0]: ARPT: 621342.242614: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  1 09:33:13 calvisitor-10-105-160-95 kernel[0]: AirPort: Link Up on awdl0
Jul  1 09:33:13 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  1 09:33:58 calvisitor-10-105-160-95 kernel[0]: ARPT: 621389.379319: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:c6b3:1ff:fecd:467f
Jul  1 09:34:42 calvisitor-10-105-160-95 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 0 milliseconds
Jul  1 09:34:52 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 438095 seconds.  Ignoring.
Jul  1 09:35:27 calvisitor-10-105-160-95 mDNSResponder[91]: mDNS_DeregisterInterface: Frequent transitions for interface en0 (2607:F140:6000:0008:C6B3:01FF:FECD:467F)
Jul  1 09:36:19 calvisitor-10-105-160-95 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 7
Jul  1 09:39:57 calvisitor-10-105-160-95 kernel[0]: ARPT: 621490.858770: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq 2119064372, Ack 3325040593, Win size 278
Jul  1 09:39:57 calvisitor-10-105-160-95 kernel[0]: ARPT: 621490.890645: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  1 09:39:57 calvisitor-10-105-160-95 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  1 09:39:57 authorMacBook-Pro kernel[0]: ARPT: 621492.770239: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  1 09:41:34 calvisitor-10-105-160-95 kernel[0]: en0::IO80211Interface::postMessage bssid changed
Jul  1 09:41:34 authorMacBook-Pro kernel[0]: ARPT: 621542.378462: ARPT: Wake Reason: Wake on Scan offload
Jul  1 09:41:34 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  1 09:41:44 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1268 seconds.  Ignoring.
Jul  1 09:41:44 calvisitor-10-105-160-95 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 12119 seconds.  Ignoring.
Jul  1 09:41:54 calvisitor-10-105-160-95 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  1 09:41:54 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 437673 seconds.  Ignoring.
Jul  1 09:42:16 calvisitor-10-105-160-95 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 12087 seconds.  Ignoring.
Jul  1 09:42:23 calvisitor-10-105-160-95 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - intel_rp = 1 dlla_reporting_supported = 0
Jul  1 09:42:54 calvisitor-10-105-160-95 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  1 09:43:22 calvisitor-10-105-160-95 mDNSResponder[91]: mDNS_RegisterInterface: Frequent transitions for interface en0 (FE80:0000:0000:0000:C6B3:01FF:FECD:467F)
Jul  1 09:44:23 authorMacBook-Pro kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  1 09:44:26 authorMacBook-Pro kernel[0]: AirPort: Link Up on en0
Jul  1 09:44:32 calvisitor-10-105-160-95 com.apple.CDScheduler[43]: Thermal pressure state: 1 Memory pressure state: 0
Jul  1 09:45:08 calvisitor-10-105-160-95 kernel[0]: ARPT: 621686.164365: wl0: setup_keepalive: Local port: 62614, Remote port: 443
Jul  1 09:45:46 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  1 09:45:52 calvisitor-10-105-160-95 networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x4 to 0x8000000000000000
Jul  1 09:59:26 calvisitor-10-105-160-95 kernel[0]: ARPT: 621738.114066: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  1 10:08:20 calvisitor-10-105-160-95 Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-01 17:08:20 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  1 10:08:20 calvisitor-10-105-160-95 kernel[0]: en0: channel changed to 1
Jul  1 10:08:20 authorMacBook-Pro kernel[0]: ARPT: 621799.252673: AQM agg results 0x8001 len hi/lo: 0x0 0x26 BAbitmap(0-3) 0 0 0 0
Jul  1 10:08:21 authorMacBook-Pro corecaptured[31313]: CCFile::captureLog Received Capture notice id: 1498928900.759059, reason = AuthFail:sts:5_rsn:0
Jul  1 10:08:29 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 10602 seconds.  Ignoring.
Jul  1 10:08:49 calvisitor-10-105-160-95 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  1 10:08:55 calvisitor-10-105-160-95 AddressBookSourceSync[31318]: Unrecognized attribute value: t:AbchPersonItemType
Jul  1 10:09:58 calvisitor-10-105-160-95 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  1 10:10:24 calvisitor-10-105-160-95 kernel[0]: Sandbox: com.apple.Addres(31328) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  1 10:10:27 calvisitor-10-105-160-95 kernel[0]: Sandbox: com.apple.Addres(31328) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  1 10:13:39 calvisitor-10-105-160-95 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  1 10:13:43 calvisitor-10-105-160-95 SpotlightNetHelper[352]: CFPasteboardRef CFPasteboardCreate(CFAllocatorRef, CFStringRef) : failed to create global data
Jul  1 10:13:57 calvisitor-10-105-160-95 kernel[0]: Sandbox: com.apple.Addres(31346) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  1 10:38:53 calvisitor-10-105-160-95 sandboxd[129] ([31376]): com.apple.Addres(31376) deny network-outbound /private/var/run/mDNSResponder
Jul  1 10:46:47 calvisitor-10-105-160-95 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  1 10:46:47 calvisitor-10-105-160-95 sharingd[30299]: 10:46:47.425 : BTLE scanner Powered On
Jul  1 10:46:48 calvisitor-10-105-160-95 QQ[10018]: button report: 0x8002be0
Jul  1 10:47:08 calvisitor-10-105-160-95 sandboxd[129] ([31382]): com.apple.Addres(31382) deny network-outbound /private/var/run/mDNSResponder
Jul  1 11:20:51 calvisitor-10-105-160-95 sharingd[30299]: 11:20:51.293 : BTLE discovered device with hash <01faa200 00000000 0000>
Jul  1 11:24:45 calvisitor-10-105-160-95 secd[276]:  securityd_xpc_dictionary_handler cloudd[326] copy_matching Error Domain=NSOSStatusErrorDomain Code=-50 "query missing class name" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}
Jul  1 11:29:32 calvisitor-10-105-160-95 locationd[82]: Location icon should now be in state 'Inactive'
Jul  1 11:38:18 calvisitor-10-105-160-95 kernel[0]: ARPT: 626126.086205: wl0: setup_keepalive: interval 900, retry_interval 30, retry_count 10
Jul  1 11:38:18 calvisitor-10-105-160-95 kernel[0]: ARPT: 626126.086246: wl0: MDNS: IPV4 Addr: 10.105.160.95
Jul  1 11:39:47 calvisitor-10-105-160-95 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  1 11:39:47 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  1 11:39:48 authorMacBook-Pro kernel[0]: en0::IO80211Interface::postMessage bssid changed
Jul  1 11:39:48 calvisitor-10-105-160-95 networkd[195]: __42-[NETClientConnection evaluateCrazyIvan46]_block_invoke CI46 - Hit by torpedo! QQ.10018 tc19060 125.39.133.143:14000
Jul  1 11:39:48 calvisitor-10-105-160-95 kernel[0]: ARPT: 626132.740936: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  1 11:41:26 authorMacBook-Pro kernel[0]: Setting BTCoex Config: enable_2G:1, profile_2g:0, enable_5G:1, profile_5G:0
Jul  1 11:41:54 calvisitor-10-105-160-95 kernel[0]: Sandbox: com.apple.Addres(31432) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  1 11:41:55 calvisitor-10-105-160-95 kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  1 11:43:08 authorMacBook-Pro UserEventAgent[43]: Captive: [CNInfoNetworkActive:1748] en0: SSID 'CalVisitor' making interface primary (cache indicates network not captive)
Jul  1 11:43:23 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 16227 seconds.  Ignoring.
Jul  1 11:44:20 calvisitor-10-105-160-95 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  1 11:44:25 authorMacBook-Pro kernel[0]: en0::IO80211Interface::postMessage bssid changed
Jul  1 11:44:26 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from SUSPENDED to AUTO
Jul  1 11:46:16 calvisitor-10-105-160-95 symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  1 11:46:16 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  1 11:46:19 authorMacBook-Pro sharingd[30299]: 11:46:19.229 : Finished generating hashes
Jul  1 11:46:21 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Evaluating
Jul  1 11:46:36 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 16034 seconds.  Ignoring.
Jul  1 11:46:48 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1277 seconds.  Ignoring.
Jul  1 11:47:56 authorMacBook-Pro kernel[0]: ARPT: 626380.467130: ARPT: Wake Reason: Wake on Scan offload
Jul  1 11:48:28 calvisitor-10-105-160-95 AddressBookSourceSync[31471]: Unrecognized attribute value: t:AbchPersonItemType
Jul  1 11:48:43 calvisitor-10-105-160-95 kernel[0]: PM response took 1938 ms (54, powerd)
Jul  1 11:49:29 calvisitor-10-105-160-95 QQ[10018]: tcp_connection_destination_perform_socket_connect 19110 connectx to 183.57.48.75:80@0 failed: [50] Network is down
Jul  1 11:49:29 calvisitor-10-105-160-95 kernel[0]: AirPort: Link Down on en0. Reason 8 (Disassociated because station leaving).
Jul  1 11:49:29 authorMacBook-Pro sharingd[30299]: 11:49:29.473 : BTLE scanner Powered On
Jul  1 11:49:29 authorMacBook-Pro kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  1 11:49:29 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  1 11:49:30 authorMacBook-Pro corecaptured[31480]: CCXPCService::setStreamEventHandler Registered for notification callback.
Jul  1 11:49:30 authorMacBook-Pro Dropbox[24019]: [0701/114930:WARNING:dns_config_service_posix.cc(306)] Failed to read DnsConfig.
Jul  1 11:49:35 calvisitor-10-105-160-95 cdpd[11807]: Saw change in network reachability (isReachable=2)
Jul  1 11:51:02 authorMacBook-Pro kernel[0]: [HID] [ATC] AppleDeviceManagementHIDEventService::processWakeReason Wake reason: Host (0x01)
Jul  1 11:51:07 authorMacBook-Pro kernel[0]: en0: BSSID changed to 5c:50:15:36:bc:03
Jul  1 11:51:11 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 15759 seconds.  Ignoring.
Jul  1 11:51:12 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 4439 seconds.  Ignoring.
Jul  1 11:51:22 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 4429 seconds.  Ignoring.
Jul  1 11:51:25 calvisitor-10-105-160-95 com.apple.AddressBook.InternetAccountsBridge[31496]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  1 11:53:45 calvisitor-10-105-160-95 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 0 milliseconds
Jul  1 11:53:45 authorMacBook-Pro kernel[0]: Setting BTCoex Config: enable_2G:1, profile_2g:0, enable_5G:1, profile_5G:0
Jul  1 11:53:49 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name apsd as bundle ID (this is expected for daemons without bundle ID
Jul  1 11:55:14 calvisitor-10-105-160-95 kernel[0]: AirPort: Link Down on en0. Reason 8 (Disassociated because station leaving).
Jul  1 11:55:24 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 4187 seconds.  Ignoring.
Jul  1 11:55:24 calvisitor-10-105-160-95 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 4099 seconds.  Ignoring.
Jul  1 11:58:27 calvisitor-10-105-160-95 kernel[0]: ARPT: 626625.204595: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq 3034132215, Ack 528237229, Win size 278
Jul  1 11:58:27 authorMacBook-Pro com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 15323 seconds.  Ignoring.
Jul  1 12:12:04 calvisitor-10-105-160-95 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  1 12:12:21 calvisitor-10-105-160-95 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  1 12:26:01 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 427826 seconds.  Ignoring.
Jul  1 12:26:01 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 2350 seconds.  Ignoring.
Jul  1 12:39:29 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1542 seconds.  Ignoring.
Jul  1 12:39:55 calvisitor-10-105-160-95 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1428 seconds.  Ignoring.
Jul  1 12:52:57 calvisitor-10-105-160-95 kernel[0]: RTC: Maintenance 2017/7/1 19:52:56, sleep 2017/7/1 19:40:18
Jul  1 12:52:57 calvisitor-10-105-160-95 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  1 12:52:57 calvisitor-10-105-160-95 kernel[0]: en0: channel changed to 132,+1
Jul  1 12:53:53 calvisitor-10-105-160-95 kernel[0]: ARPT: 626908.045241: wl0: setup_keepalive: interval 900, retry_interval 30, retry_count 10
Jul  1 13:06:35 calvisitor-10-105-160-95 kernel[0]: AirPort: Link Up on awdl0
Jul  1 13:20:12 calvisitor-10-105-160-95 sharingd[30299]: 13:20:12.402 : BTLE scanner Powered On
Jul  1 13:20:12 calvisitor-10-105-160-95 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 424575 seconds.  Ignoring.
Jul  1 13:20:31 calvisitor-10-105-160-95 com.apple.AddressBook.InternetAccountsBridge[31588]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  1 13:20:33 calvisitor-10-105-160-95 sandboxd[129] ([31588]): com.apple.Addres(31588) deny network-outbound /private/var/run/mDNSResponder
Jul  1 13:34:07 calvisitor-10-105-160-95 com.apple.AddressBook.InternetAccountsBridge[31595]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  1 13:41:48 calvisitor-10-105-160-95 kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  1 13:41:58 calvisitor-10-105-160-95 com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  1 13:42:01 calvisitor-10-105-160-95 locationd[82]: Location icon should now be in state 'Active'
Jul  1 13:42:41 calvisitor-10-105-160-95 kernel[0]: ARPT: 627141.702095: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:c6b3:1ff:fecd:467f
Jul  1 13:44:07 calvisitor-10-105-160-95 com.apple.AddressBook.InternetAccountsBridge[31608]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  1 13:47:33 calvisitor-10-105-160-95 kernel[0]: en0: 802.11d country code set to 'X3'.
Jul  1 13:47:33 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  1 13:47:53 calvisitor-10-105-160-95 kernel[0]: en0: channel changed to 6
Jul  1 13:49:09 authorMacBook-Pro kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  1 14:01:24 calvisitor-10-105-160-95 kernel[0]: ARPT: 627305.613279: wl0: setup_keepalive: Seq: 1664112163, Ack: 818851215, Win size: 4096
Jul  1 14:14:51 calvisitor-10-105-160-95 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  1 14:28:27 calvisitor-10-105-160-95 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  1 14:28:55 calvisitor-10-105-160-95 kernel[0]: ARPT: 627355.597577: ARPT: Wake Reason: Wake on Scan offload
Jul  1 14:29:01 calvisitor-10-105-160-95 sandboxd[129] ([10018]): QQ(10018) deny mach-lookup com.apple.networking.captivenetworksupport
Jul  1 14:38:45 calvisitor-10-105-160-95 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  1 14:52:03 calvisitor-10-105-160-95 kernel[0]: RTC: Maintenance 2017/7/1 21:52:00, sleep 2017/7/1 21:39:51
Jul  1 14:52:03 calvisitor-10-105-160-95 kernel[0]: [HID] [MT] AppleMultitouchDevice::willTerminate entered
Jul  1 14:52:03 calvisitor-10-105-160-95 blued[85]: [BluetoothHIDDeviceController] EventServiceDisconnectedCallback
Jul  1 15:05:42 calvisitor-10-105-160-95 kernel[0]: PMStats: Hibernate read took 197 ms
Jul  1 15:05:47 calvisitor-10-105-160-95 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  1 15:05:51 calvisitor-10-105-160-95 com.apple.AddressBook.InternetAccountsBridge[31654]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  1 19:43:22 calvisitor-10-105-160-95 kernel[0]: pages 1471315, wire 491253, act 449877, inact 3, cleaned 0 spec 5, zf 23, throt 0, compr 345876, xpmapped 40000
Jul  1 19:43:22 calvisitor-10-105-160-95 VDCAssistant[213]: VDCAssistant:  Found a camera (0x1430000005ac8290) , but was not able to start it up (0x0 -- (os/kern) successful)
Jul  1 19:43:22 calvisitor-10-105-160-95 WindowServer[184]: handle_will_sleep_auth_and_shield_windows: Reordering authw 0x7fa823a04400(2004) (lock state: 3)
Jul  1 19:43:32 calvisitor-10-105-163-202 symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  1 19:43:36 calvisitor-10-105-163-202 QQ[10018]: button report: 0x8002bdf
Jul  1 19:46:26 calvisitor-10-105-163-202 GoogleSoftwareUpdateAgent[31702]: 2017-07-01 19:46:26.133 GoogleSoftwareUpdateAgent[31702/0x7000002a0000] [lvl=2] -[KSAgentApp(KeystoneThread) runKeystonesInThreadWithArg:] Checking with local engine: <KSUpdateEngine:0x100259c60 ticketStore=<KSPersistentTicketStore:0x100253770 store=<KSKeyedPersistentStore:0x100254d10 path="/Users/xpc/Library/Google/GoogleSoftwareUpdate/TicketStore/Keystone.ticketstore" lockFile=<KSLockFile:0x100254d80 path="/Users/xpc/Library/Google/GoogleSoftwareUpdate/TicketStore/Keystone.ticketstore.lock" locked=NO > >> processor=<KSActionProcessor:0x100259e70 delegate=<KSUpdateEngine:0x100259c60> isProcessing=NO actionsCompleted=0 progress=0.00 errors=0 currentActionErrors=0 events=0 currentActionEvents=0 actionQueue=( ) > delegate=(null) serverInfoStore=<KSServerPrivateInfoStore:0x1002594d0 path="/Users/xpc/Library/Google/GoogleSoftwareUpdate/Servers"> errors=0 >
Jul  1 19:46:42 calvisitor-10-105-163-202 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950712080>.
Jul  1 19:46:42 calvisitor-10-105-163-202 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9951303310>.
Jul  1 19:50:12 calvisitor-10-105-163-202 quicklookd[31687]: Error returned from iconservicesagent: (null)
Jul  1 20:13:23 calvisitor-10-105-163-202 Safari[9852]: tcp_connection_tls_session_error_callback_imp 1977 __tcp_connection_tls_session_callback_write_block_invoke.434 error 22
Jul  1 20:17:07 calvisitor-10-105-163-202 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9951105bf0>.
Jul  1 20:17:08 calvisitor-10-105-163-202 quicklookd[31687]: Error returned from iconservicesagent: (null)
Jul  1 20:23:09 calvisitor-10-105-163-202 cloudd[326]:  SecOSStatusWith error:[-50] Error Domain=NSOSStatusErrorDomain Code=-50 "query missing class name" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}
Jul  1 21:03:00 calvisitor-10-105-163-202 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  1 21:10:19 calvisitor-10-105-163-202 Preview[11512]: WARNING: Type1 font data isn't in the correct format required by the Adobe Type 1 Font Format specification.
Jul  1 21:17:32 calvisitor-10-105-163-202 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  1 21:18:10 calvisitor-10-105-163-202 quicklookd[31687]: Error returned from iconservicesagent: (null)
Jul  1 21:18:10 calvisitor-10-105-163-202 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950712080>.
Jul  1 21:19:06 calvisitor-10-105-163-202 quicklookd[31687]: Error returned from iconservicesagent: (null)
Jul  1 21:21:33 calvisitor-10-105-163-202 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950712080>.
Jul  1 21:24:38 calvisitor-10-105-163-202 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9951005740>.
Jul  1 21:33:23 calvisitor-10-105-163-202 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950606790>.
Jul  1 22:03:31 calvisitor-10-105-163-202 com.apple.cts[258]: com.apple.ical.sync.x-coredata://DB05755C-483D-44B7-B93B-ED06E57FF420/CalDAVPrincipal/p11: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 59 seconds.  Ignoring.
Jul  1 22:08:16 calvisitor-10-105-163-202 WindowServer[184]: device_generate_desktop_screenshot: authw 0x7fa823c89600(2000), shield 0x7fa8258cac00(2001)
Jul  1 22:12:41 calvisitor-10-105-163-202 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950606790>.
Jul  1 22:13:49 calvisitor-10-105-163-202 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  1 22:19:34 calvisitor-10-105-163-202 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950606790>.
Jul  1 22:20:14 calvisitor-10-105-163-202 WindowServer[184]: device_generate_desktop_screenshot: authw 0x7fa823c89600(2000), shield 0x7fa8258cac00(2001)
Jul  1 22:20:57 calvisitor-10-105-163-202 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950712080>.
Jul  1 22:20:58 calvisitor-10-105-163-202 quicklookd[31687]: Error returned from iconservicesagent: (null)
Jul  1 22:22:59 calvisitor-10-105-163-202 CalendarAgent[279]: [com.apple.calendar.store.log.caldav.coredav] [Refusing to parse response to PROPPATCH because of content-type: [text/html; charset=UTF-8].]
Jul  1 22:25:38 calvisitor-10-105-163-202 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 250, items, fQueryRetries, 0, fLastRetryTimestamp, 520665636.6
Jul  1 22:30:15 calvisitor-10-105-163-202 QQ[10018]: button report: 0x8002bdf
Jul  1 23:32:34 calvisitor-10-105-163-202 kernel[0]: ARPT: 640362.070027: wl0: wl_update_tcpkeep_seq: Original Seq: 2000710617, Ack: 2120985509, Win size: 4096
Jul  1 23:32:34 calvisitor-10-105-163-202 kernel[0]: Previous sleep cause: 5
Jul  1 23:32:34 calvisitor-10-105-163-202 kernel[0]: AirPort: Link Up on awdl0
Jul  1 23:46:06 calvisitor-10-105-163-202 kernel[0]: Wake reason: RTC (Alarm)
Jul  1 23:46:06 calvisitor-10-105-163-202 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 2 us
Jul  1 23:46:06 calvisitor-10-105-163-202 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  2 02:18:39 calvisitor-10-105-163-202 sharingd[30299]: 02:18:39.156 : BTLE scanner Powered On
Jul  2 02:18:39 calvisitor-10-105-163-202 configd[53]: network changed: v4(en0-:10.105.163.202) v6(en0:2607:f140:6000:8:c6b3:1ff:fecd:467f) DNS! Proxy SMB
Jul  2 02:19:03 calvisitor-10-105-163-202 com.apple.AddressBook.InternetAccountsBridge[31953]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  2 02:32:17 calvisitor-10-105-163-202 kernel[0]: hibernate_page_list_setall found pageCount 448015
Jul  2 02:32:17 calvisitor-10-105-163-202 kernel[0]: [HID] [ATC] [Error] AppleDeviceManagementHIDEventService::start Could not make a string from out connection notification key
Jul  2 02:32:17 calvisitor-10-105-163-202 kernel[0]: en0: BSSID changed to 5c:50:15:4c:18:1c
Jul  2 02:32:17 calvisitor-10-105-163-202 kernel[0]: [IOBluetoothFamily][staticBluetoothTransportShowsUp] -- Received Bluetooth Controller register service notification -- 0x7000
Jul  2 10:11:17 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x8000000000000000 to 0x4
Jul  2 10:20:52 calvisitor-10-105-163-202 QQ[10018]: FA||Url||taskID[2019353117] dealloc
Jul  2 10:27:44 calvisitor-10-105-163-202 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 250, items, fQueryRetries, 0, fLastRetryTimestamp, 520708962.7
Jul  2 10:40:07 calvisitor-10-105-163-202 GoogleSoftwareUpdateAgent[32012]: 2017-07-02 10:40:07.676 GoogleSoftwareUpdateAgent[32012/0x7000002a0000] [lvl=2] -[KSAgentApp updateProductWithProductID:usingEngine:] Checking for updates for "com.google.Keystone" using engine <KSUpdateEngine:0x100313e70 ticketStore=<KSPersistentTicketStore:0x100332a60 store=<KSKeyedPersistentStore:0x100315fa0 path="/Users/xpc/Library/Google/GoogleSoftwareUpdate/TicketStore/Keystone.ticketstore" lockFile=<KSLockFile:0x1003176d0 path="/Users/xpc/Library/Google/GoogleSoftwareUpdate/TicketStore/Keystone.ticketstore.lock" locked=NO > >> processor=<KSActionProcessor:0x1003143e0 delegate=<KSUpdateEngine:0x100313e70> isProcessing=NO actionsCompleted=0 progress=0.00 errors=0 currentActionErrors=0 events=0 currentActionEvents=0 actionQueue=( ) > delegate=(null) serverInfoStore=<KSServerPrivateInfoStore:0x100311a10 path="/Users/xpc/Library/Google/GoogleSoftwareUpdate/Servers"> errors=0 >
Jul  2 11:38:49 calvisitor-10-105-163-202 QQ[10018]: 2017/07/02 11:38:49.414 | I | VoipWrapper  | DAVEngineImpl.cpp:1400:Close             | close video chat. llFriendUIN = ******2341.
Jul  2 11:39:07 calvisitor-10-105-163-202 kernel[0]: ARPT: 645791.413780: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': No, 'TimeRemaining': 156,
Jul  2 11:42:54 calvisitor-10-105-163-202 kernel[0]: Previous sleep cause: 5
Jul  2 11:42:54 calvisitor-10-105-163-202 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  2 11:42:55 authorMacBook-Pro kernel[0]: ARPT: 645795.024045: ARPT: Wake Reason: Wake on Scan offload
Jul  2 11:43:53 calvisitor-10-105-163-202 kernel[0]: PM response took 28003 ms (54, powerd)
Jul  2 12:15:23 calvisitor-10-105-163-202 kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  2 12:15:23 calvisitor-10-105-163-202 sharingd[30299]: 12:15:23.005 : Discoverable mode changed to Off
Jul  2 12:15:24 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 19617 failed: 3 - No network route
Jul  2 12:15:30 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Authenticated
Jul  2 12:29:56 calvisitor-10-105-163-202 kernel[0]: ARPT: 645957.322055: wl0: setup_keepalive: Local IP: 10.105.163.202
Jul  2 12:43:24 calvisitor-10-105-163-202 locationd[82]: Location icon should now be in state 'Active'
Jul  2 12:56:19 calvisitor-10-105-163-202 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 4
Jul  2 13:00:07 calvisitor-10-105-163-202 kernel[0]: AirPort: Link Down on awdl0. Reason 1 (Unspecified).
Jul  2 13:01:36 calvisitor-10-105-163-202 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  2 13:01:37 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  2 13:02:03 calvisitor-10-105-163-202 com.apple.AddressBook.InternetAccountsBridge[32155]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  2 13:03:10 calvisitor-10-105-163-202 kernel[0]: Previous sleep cause: 5
Jul  2 13:03:10 authorMacBook-Pro kernel[0]: ARPT: 646193.687729: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  2 13:03:41 calvisitor-10-105-163-202 AddressBookSourceSync[32160]: Unrecognized attribute value: t:AbchPersonItemType
Jul  2 13:05:38 calvisitor-10-105-163-202 kernel[0]: TBT W (2): 0x0040 [x]
Jul  2 13:05:40 authorMacBook-Pro configd[53]: network changed: DNS* Proxy
Jul  2 13:06:17 calvisitor-10-105-163-202 kernel[0]: ARPT: 646292.668059: wl0: MDNS: 0 SRV Recs, 0 TXT Recs
Jul  2 13:10:47 calvisitor-10-105-163-202 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  2 13:12:08 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name apsd as bundle ID (this is expected for daemons without bundle ID
Jul  2 13:12:08 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  2 13:12:41 calvisitor-10-105-163-202 symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  2 13:13:22 authorMacBook-Pro mDNSResponder[91]: mDNS_RegisterInterface: Frequent transitions for interface awdl0 (FE80:0000:0000:0000:D8A5:90FF:FEF5:7FFF)
Jul  2 13:44:22 calvisitor-10-105-163-202 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 1 us
Jul  2 13:44:30 authorMacBook-Pro sandboxd[129] ([10018]): QQ(10018) deny mach-lookup com.apple.networking.captivenetworksupport
Jul  2 13:44:55 calvisitor-10-105-163-202 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  2 13:45:01 calvisitor-10-105-163-202 com.apple.AddressBook.InternetAccountsBridge[32208]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  2 13:56:23 calvisitor-10-105-163-202 kernel[0]: ARPT: 646509.760609: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  2 14:07:52 calvisitor-10-105-163-202 locationd[82]: Location icon should now be in state 'Active'
Jul  2 14:33:03 calvisitor-10-105-163-202 locationd[82]: Location icon should now be in state 'Inactive'
Jul  2 14:41:07 calvisitor-10-105-163-202 Safari[9852]: tcp_connection_tls_session_error_callback_imp 2044 __tcp_connection_tls_session_callback_write_block_invoke.434 error 22
Jul  2 14:44:01 calvisitor-10-105-163-202 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  2 14:52:57 calvisitor-10-105-163-202 locationd[82]: Location icon should now be in state 'Active'
Jul  2 15:06:24 calvisitor-10-105-163-202 syslogd[44]: ASL Sender Statistics
Jul  2 15:33:55 calvisitor-10-105-163-202 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000300
Jul  2 15:34:11 calvisitor-10-105-163-202 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  2 15:46:40 calvisitor-10-105-163-202 GoogleSoftwareUpdateAgent[32432]: 2017-07-02 15:46:40.516 GoogleSoftwareUpdateAgent[32432/0x7000002a0000] [lvl=2] -[KSOutOfProcessFetcher(PrivateMethods) launchedHelperTaskForToolPath:error:] KSOutOfProcessFetcher launched '/Users/xpc/Library/Google/GoogleSoftwareUpdate/GoogleSoftwareUpdate.bundle/Contents/MacOS/ksfetch' with process id: 32433
Jul  2 15:46:40 calvisitor-10-105-163-202 GoogleSoftwareUpdateAgent[32432]: 2017-07-02 15:46:40.697 GoogleSoftwareUpdateAgent[32432/0x7000002a0000] [lvl=2] -[KSUpdateEngine updateAllExceptProduct:] KSUpdateEngine updating all installed products, except:'com.google.Keystone'.
Jul  2 15:46:41 calvisitor-10-105-163-202 ksfetch[32435]: 2017-07-02 15:46:41.445 ksfetch[32435/0x7fff79824000] [lvl=2] main() ksfetch fetching URL (<NSMutableURLRequest: 0x1005110b0> { URL: https://tools.google.com/service/update2?cup2hreq=53f725cf03f511fab16f19e789ce64aa1eed72395fc246e9f1100748325002f4&cup2key=7:1132320327 }) to folder:/tmp/KSOutOfProcessFetcher.YH2CjY1tnx/download
Jul  2 16:38:07 calvisitor-10-105-163-202 locationd[82]: Location icon should now be in state 'Inactive'
Jul  2 16:51:10 calvisitor-10-105-163-202 QQ[10018]: FA||Url||taskID[2019353182] dealloc
Jul  2 16:55:53 calvisitor-10-105-163-202 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.32502): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.SearchHelper.xpc/Contents/MacOS/com.apple.Safari.SearchHelper error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  2 17:01:19 calvisitor-10-105-163-202 cloudd[326]:  SecOSStatusWith error:[-50] Error Domain=NSOSStatusErrorDomain Code=-50 "query missing class name" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}
Jul  2 17:07:56 calvisitor-10-105-163-202 locationd[82]: Location icon should now be in state 'Active'
Jul  2 17:11:18 calvisitor-10-105-163-202 Safari[9852]: tcp_connection_tls_session_error_callback_imp 2068 __tcp_connection_tls_session_callback_write_block_invoke.434 error 22
Jul  2 17:13:46 calvisitor-10-105-163-202 com.apple.WebKit.WebContent[32514]: [17:13:46.390] <<<< IQ-CA >>>> piqca_setUsePreQueue: (0x7f92413e3000) rejecting report of layer being serviced - IQ has not yet begun to update
Jul  2 17:19:15 calvisitor-10-105-163-202 com.apple.WebKit.WebContent[32514]: [17:19:15.148] itemasync_SetProperty signalled err=-12785 (kFigBaseObjectError_Invalidated) (invalidated) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/Player/FigPlayer_Async.c line 2306
Jul  2 17:34:21 calvisitor-10-105-163-202 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  2 17:36:15 calvisitor-10-105-163-202 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  2 17:39:27 calvisitor-10-105-163-202 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.32564): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.WebFeedParser.xpc/Contents/MacOS/com.apple.Safari.WebFeedParser error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  2 17:44:05 calvisitor-10-105-163-202 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  2 17:56:40 calvisitor-10-105-163-202 com.apple.ncplugin.WorldClock[32583]: host connection <NSXPCConnection: 0x7fddbbb015c0> connection from pid 30298 invalidated
Jul  2 17:56:40 calvisitor-10-105-163-202 com.apple.ncplugin.weather[32585]: Error in CoreDragRemoveReceiveHandler: -1856
Jul  2 18:08:55 calvisitor-10-105-163-202 kernel[0]: ARPT: 661549.802297: wl0: leaveModulePoweredForOffloads: Wi-Fi will stay on.
Jul  2 18:08:55 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  2 18:08:56 authorMacBook-Pro kernel[0]: ARPT: 661552.832561: IOPMPowerSource Information: onWake,  SleepType: Normal Sleep,  'ExternalConnected': Yes, 'TimeRemaining': 0,
Jul  2 18:09:15 calvisitor-10-105-163-202 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  2 18:23:33 calvisitor-10-105-163-202 locationd[82]: Location icon should now be in state 'Active'
Jul  2 18:35:12 calvisitor-10-105-163-202 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  2 18:35:12 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  2 18:35:13 authorMacBook-Pro symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  2 18:35:23 calvisitor-10-105-163-202 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 49 seconds.  Ignoring.
Jul  2 18:35:44 calvisitor-10-105-163-202 sandboxd[129] ([32626]): com.apple.Addres(32626) deny network-outbound /private/var/run/mDNSResponder
Jul  2 18:35:57 calvisitor-10-105-163-202 kernel[0]: ARPT: 661708.530713: wl0: MDNS: IPV6 Addr: fe80:0:0:0:c6b3:1ff:fecd:467f
Jul  2 18:36:01 calvisitor-10-105-163-202 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - intel_rp = 1 dlla_reporting_supported = 0
Jul  2 18:37:25 authorMacBook-Pro configd[53]: network changed: v4(en0-:10.105.163.202) v6(en0:2607:f140:6000:8:c6b3:1ff:fecd:467f) DNS! Proxy SMB
Jul  2 18:38:31 authorMacBook-Pro com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 2262 seconds.  Ignoring.
Jul  2 18:38:31 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Inactive
Jul  2 18:38:32 authorMacBook-Pro corecaptured[32639]: CCXPCService::setStreamEventHandler Registered for notification callback.
Jul  2 18:38:36 calvisitor-10-105-163-202 kernel[0]: Setting BTCoex Config: enable_2G:1, profile_2g:0, enable_5G:1, profile_5G:0
Jul  2 18:39:18 calvisitor-10-105-163-202 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  2 18:40:01 authorMacBook-Pro kernel[0]: in6_unlink_ifa: IPv6 address 0x77c9114551ab23ab has no prefix
Jul  2 18:40:08 calvisitor-10-105-163-202 configd[53]: network changed: v4(en0:10.105.163.202) v6(en0+:2607:f140:6000:8:c6b3:1ff:fecd:467f) DNS! Proxy SMB
Jul  2 18:40:21 calvisitor-10-105-163-202 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 318966 seconds.  Ignoring.
Jul  2 18:40:40 calvisitor-10-105-163-202 com.apple.AddressBook.InternetAccountsBridge[32655]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  2 18:40:46 calvisitor-10-105-163-202 kernel[0]: ARPT: 661856.313502: wl0: MDNS: 0 SRV Recs, 0 TXT Recs
Jul  2 18:53:41 calvisitor-10-105-163-202 kernel[0]: en0: BSSID changed to 5c:50:15:36:bc:03
Jul  2 18:53:41 calvisitor-10-105-163-202 kernel[0]: en0: channel changed to 6
Jul  2 18:53:51 calvisitor-10-105-163-202 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 318156 seconds.  Ignoring.
Jul  2 18:54:02 calvisitor-10-105-163-202 com.apple.AddressBook.InternetAccountsBridge[32662]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  2 18:54:36 calvisitor-10-105-163-202 kernel[0]: ARPT: 661915.168735: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:4de9:a101:c96c:f28b
Jul  2 19:21:05 calvisitor-10-105-163-202 com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  2 19:21:15 calvisitor-10-105-163-202 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 3498 seconds.  Ignoring.
Jul  2 19:34:33 calvisitor-10-105-163-202 com.apple.geod[30311]: PBRequester failed with Error Error Domain=NSURLErrorDomain Code=-1001 "The request timed out." UserInfo={NSUnderlyingError=0x7fe133460660 {Error Domain=kCFErrorDomainCFNetwork Code=-1001 "The request timed out." UserInfo={NSErrorFailingURLStringKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, NSErrorFailingURLKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, _kCFStreamErrorCodeKey=-2102, _kCFStreamErrorDomainKey=4, NSLocalizedDescription=The request timed out.}}, NSErrorFailingURLStringKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, NSErrorFailingURLKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, _kCFStreamErrorDomainKey=4, _kCFStreamErrorCodeKey=-2102, NSLocalizedDescription=The request timed out.}
Jul  2 19:35:29 calvisitor-10-105-163-202 kernel[0]: ARPT: 662096.028575: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:4de9:a101:c96c:f28b
Jul  2 19:35:32 calvisitor-10-105-163-202 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - intel_rp = 1 dlla_reporting_supported = 0
Jul  2 19:48:11 calvisitor-10-105-163-202 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 314896 seconds.  Ignoring.
Jul  2 19:48:20 calvisitor-10-105-163-202 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 106 seconds.  Ignoring.
Jul  2 19:48:30 calvisitor-10-105-163-202 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  2 20:01:48 calvisitor-10-105-163-202 kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  2 20:01:48 calvisitor-10-105-163-202 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 5
Jul  2 20:01:48 calvisitor-10-105-163-202 Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-03 03:01:48 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  2 20:01:59 calvisitor-10-105-163-202 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 61304 seconds.  Ignoring.
Jul  2 20:15:26 calvisitor-10-105-163-202 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  2 20:15:26 calvisitor-10-105-163-202 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  2 20:22:06 calvisitor-10-105-163-202 Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-03 03:22:06 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  2 20:22:38 calvisitor-10-105-163-202 WindowServer[184]: device_generate_lock_screen_screenshot: authw 0x7fa823962400(2000)[0, 0, 1440, 900] shield 0x7fa82d372000(2001), dev [1440,900]
Jul  2 20:43:36 calvisitor-10-105-163-202 locationd[82]: Location icon should now be in state 'Active'
Jul  2 20:50:47 calvisitor-10-105-163-202 ksfetch[32776]: 2017-07-02 20:50:47.457 ksfetch[32776/0x7fff79824000] [lvl=2] KSHelperReceiveAllData() KSHelperTool read 1926 bytes from stdin.
Jul  2 21:17:07 calvisitor-10-105-163-202 com.apple.WebKit.WebContent[32778]: [21:17:07.529] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  2 21:43:56 calvisitor-10-105-163-202 locationd[82]: Location icon should now be in state 'Inactive'
Jul  2 21:44:16 calvisitor-10-105-163-202 Safari[9852]: tcp_connection_tls_session_error_callback_imp 2103 __tcp_connection_tls_session_callback_write_block_invoke.434 error 22
Jul  2 21:46:49 calvisitor-10-105-163-202 syslogd[44]: ASL Sender Statistics
Jul  2 21:48:53 calvisitor-10-105-163-202 sharingd[30299]: 21:48:53.041 : BTLE scanner Powered Off
Jul  2 22:09:17 calvisitor-10-105-163-202 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  2 22:24:03 authorMacBook-Pro com.apple.geod[30311]: PBRequester failed with Error Error Domain=NSURLErrorDomain Code=-1009 "The Internet connection appears to be offline." UserInfo={NSUnderlyingError=0x7fe13512cf70 {Error Domain=kCFErrorDomainCFNetwork Code=-1009 "The Internet connection appears to be offline." UserInfo={NSErrorFailingURLStringKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, NSErrorFailingURLKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, _kCFStreamErrorCodeKey=8, _kCFStreamErrorDomainKey=12, NSLocalizedDescription=The Internet connection appears to be offline.}}, NSErrorFailingURLStringKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, NSErrorFailingURLKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, _kCFStreamErrorDomainKey=12, _kCFStreamErrorCodeKey=8, NSLocalizedDescription=The Internet connection appears to be offline.}
Jul  2 22:24:03 authorMacBook-Pro kernel[0]: ARPT: 669592.164786: AQM agg params 0xfc0 maxlen hi/lo 0x0 0xffff minlen 0x0 adjlen 0x0
Jul  2 22:24:04 authorMacBook-Pro corecaptured[32877]: Received Capture Event
Jul  2 22:24:14 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Evaluating
Jul  2 22:24:15 authorMacBook-Pro kernel[0]: Sandbox: QQ(10018) deny(1) mach-lookup com.apple.networking.captivenetworksupport
Jul  2 22:24:18 authorMacBook-Pro com.apple.AddressBook.InternetAccountsBridge[32885]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  2 22:24:25 authorMacBook-Pro kernel[0]: Sandbox: com.apple.Addres(32885) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  2 22:24:43 authorMacBook-Pro corecaptured[32877]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  2 22:32:34 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [22:32:34.846] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  2 22:44:55 authorMacBook-Pro locationd[82]: Location icon should now be in state 'Active'
Jul  2 23:21:43 authorMacBook-Pro kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  2 23:22:08 authorMacBook-Pro kernel[0]: ARPT: 671682.028482: wl0: MDNS: IPV6 Addr: fe80:0:0:0:c6b3:1ff:fecd:467f
Jul  2 23:22:10 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  2 23:35:17 authorMacBook-Pro kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 2 us
Jul  2 23:35:17 authorMacBook-Pro kernel[0]: AirPort: Link Down on awdl0. Reason 1 (Unspecified).
Jul  2 23:35:17 authorMacBook-Pro syslogd[44]: ASL Sender Statistics
Jul  2 23:35:17 authorMacBook-Pro kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  2 23:35:21 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  2 23:48:54 authorMacBook-Pro secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  3 00:02:22 authorMacBook-Pro kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  3 00:02:22 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 00:02:27 authorMacBook-Pro Safari[9852]: tcp_connection_tls_session_error_callback_imp 2115 __tcp_connection_tls_session_callback_write_block_invoke.434 error 22
Jul  3 00:27:35 authorMacBook-Pro ntpd[207]: sigio_handler: sigio_handler_active != 0
Jul  3 00:27:54 authorMacBook-Pro QQ[10018]: ############################## _getSysMsgList
Jul  3 00:41:11 authorMacBook-Pro syslogd[44]: Configuration Notice: ASL Module "com.apple.performance" claims selected messages. Those messages may not appear in standard system log files or in the ASL database.
Jul  3 00:55:12 authorMacBook-Pro kernel[0]: ARPT: 671856.784669: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': No, 'TimeRemaining': 18011,
Jul  3 01:06:37 authorMacBook-Pro kernel[0]: Wake reason: ?
Jul  3 01:06:37 authorMacBook-Pro kernel[0]: en0: channel changed to 132,+1
Jul  3 01:06:37 authorMacBook-Pro sharingd[30299]: 01:06:37.436 : Scanning mode Contacts Only
Jul  3 01:06:37 authorMacBook-Pro kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  3 01:06:48 authorMacBook-Pro com.apple.CDScheduler[43]: Thermal pressure state: 1 Memory pressure state: 0
Jul  3 01:07:08 authorMacBook-Pro kernel[0]: ARPT: 671889.268467: wl0: setup_keepalive: Seq: 2040703749, Ack: 3006590414, Win size: 4096
Jul  3 01:31:00 authorMacBook-Pro kernel[0]: ARPT: 671958.142550: wl0: setup_keepalive: Local port: 49791, Remote port: 5223
Jul  3 01:42:26 authorMacBook-Pro sharingd[30299]: 01:42:26.004 : BTLE scanning stopped
Jul  3 01:54:51 authorMacBook-Pro sandboxd[129] ([32992]): com.apple.Addres(32992) deny network-outbound /private/var/run/mDNSResponder
Jul  3 01:58:00 authorMacBook-Pro ntpd[207]: wake time set +0.270003 s
Jul  3 01:58:04 authorMacBook-Pro kernel[0]: ARPT: 672041.595629: wl0: setup_keepalive: interval 258, retry_interval 30, retry_count 10
Jul  3 02:07:59 authorMacBook-Pro kernel[0]: Previous sleep cause: 5
Jul  3 02:07:59 authorMacBook-Pro hidd[98]: [HID] [MT] MTActuatorManagement::getActuatorRef Calling MTActuatorOpen() outside of MTTrackpadHIDManager.
Jul  3 02:08:34 authorMacBook-Pro kernel[0]: ARPT: 672113.005012: wl0: setup_keepalive: interval 258, retry_interval 30, retry_count 10
Jul  3 02:08:34 authorMacBook-Pro kernel[0]: ARPT: 672113.005034: wl0: setup_keepalive: Seq: 1128495564, Ack: 3106452487, Win size: 4096
Jul  3 02:19:59 authorMacBook-Pro kernel[0]: ARPT: 672115.511090: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  3 02:20:05 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000300
Jul  3 02:32:01 authorMacBook-Pro Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-03 09:32:01 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  3 03:07:51 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 03:07:55 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 03:31:43 authorMacBook-Pro sharingd[30299]: 03:31:43.005 : BTLE scanning stopped
Jul  3 03:31:43 authorMacBook-Pro kernel[0]: in6_unlink_ifa: IPv6 address 0x77c9114551ab225b has no prefix
Jul  3 03:31:43 authorMacBook-Pro kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  3 03:43:40 authorMacBook-Pro kernel[0]: Previous sleep cause: 5
Jul  3 03:43:40 authorMacBook-Pro kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 1 us
Jul  3 03:44:10 authorMacBook-Pro kernel[0]: ARPT: 672405.912863: wl0: MDNS: IPV6 Addr: 2607:f140:400:a01b:c6b3:1ff:fecd:467f
Jul  3 03:55:39 authorMacBook-Pro locationd[82]: NETWORK: requery, 0, 0, 0, 0, 252, items, fQueryRetries, 0, fLastRetryTimestamp, 520765684.9
Jul  3 03:55:49 authorMacBook-Pro locationd[82]: Location icon should now be in state 'Inactive'
Jul  3 03:56:18 authorMacBook-Pro mDNSResponder[91]: mDNS_DeregisterInterface: Frequent transitions for interface en0 (10.142.110.44)
Jul  3 04:08:14 authorMacBook-Pro kernel[0]: ARPT: 672487.663921: wl0: MDNS: IPV6 Addr: 2607:f140:400:a01b:c6b3:1ff:fecd:467f
Jul  3 04:19:59 authorMacBook-Pro sandboxd[129] ([33047]): com.apple.Addres(33047) deny network-outbound /private/var/run/mDNSResponder
Jul  3 04:31:30 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 04:31:31 authorMacBook-Pro ntpd[207]: wake time set -0.331349 s
Jul  3 04:43:26 authorMacBook-Pro kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  3 04:43:49 authorMacBook-Pro sandboxd[129] ([33056]): com.apple.Addres(33056) deny network-outbound /private/var/run/mDNSResponder
Jul  3 04:55:22 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 04:55:22 authorMacBook-Pro kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  3 05:07:13 authorMacBook-Pro kernel[0]: en0: channel changed to 132,+1
Jul  3 05:19:07 authorMacBook-Pro kernel[0]: ARPT: 672663.206073: wl0: wl_update_tcpkeep_seq: Original Seq: 2185760336, Ack: 2085655440, Win size: 4096
Jul  3 05:30:59 authorMacBook-Pro kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 5
Jul  3 05:31:00 authorMacBook-Pro QQ[10018]: button report: 0x80039B7
Jul  3 05:31:03 authorMacBook-Pro kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  3 05:33:53 authorMacBook-Pro kernel[0]: ARPT: 672735.825491: wl0: leaveModulePoweredForOffloads: Wi-Fi will stay on.
Jul  3 05:33:53 authorMacBook-Pro kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 6
Jul  3 05:57:59 authorMacBook-Pro com.apple.CDScheduler[43]: Thermal pressure state: 1 Memory pressure state: 0
Jul  3 05:58:10 authorMacBook-Pro com.apple.AddressBook.InternetAccountsBridge[33109]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  3 06:09:46 authorMacBook-Pro ntpd[207]: sigio_handler: sigio_handler_active != 0
Jul  3 06:09:56 authorMacBook-Pro com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  3 06:22:02 authorMacBook-Pro kernel[0]: Sandbox: com.apple.Addres(33119) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  3 06:22:20 authorMacBook-Pro kernel[0]: ARPT: 672922.026642: wl0: MDNS: 0 SRV Recs, 0 TXT Recs
Jul  3 06:33:47 authorMacBook-Pro kernel[0]: ARPT: 672925.537944: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  3 06:33:47 authorMacBook-Pro kernel[0]: ARPT: 672925.539048: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  3 06:33:47 authorMacBook-Pro kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  3 06:45:42 authorMacBook-Pro kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  3 06:45:43 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 06:46:03 authorMacBook-Pro com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  3 06:46:24 authorMacBook-Pro kernel[0]: ARPT: 673002.849007: wl0: MDNS: IPV6 Addr: fe80:0:0:0:c6b3:1ff:fecd:467f
Jul  3 06:57:52 authorMacBook-Pro kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  3 06:57:52 authorMacBook-Pro sharingd[30299]: 06:57:52.002 : Purged contact hashes
Jul  3 07:09:47 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 07:10:10 authorMacBook-Pro sandboxd[129] ([33139]): com.apple.Addres(33139) deny network-outbound /private/var/run/mDNSResponder
Jul  3 07:10:13 authorMacBook-Pro sandboxd[129] ([33139]): com.apple.Addres(33139) deny network-outbound /private/var/run/mDNSResponder
Jul  3 07:21:45 authorMacBook-Pro kernel[0]: ARPT: 673078.174655: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  3 07:33:34 authorMacBook-Pro kernel[0]: ARPT: 673110.784021: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq 4039579370, Ack 2406464715, Win size 278
Jul  3 07:33:34 authorMacBook-Pro kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  3 07:33:34 authorMacBook-Pro sharingd[30299]: 07:33:34.878 : BTLE scanning started
Jul  3 07:33:34 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 07:33:35 authorMacBook-Pro kernel[0]: IOPMrootDomain: idle cancel, state 1
Jul  3 07:45:34 authorMacBook-Pro kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 1 us
Jul  3 07:45:55 authorMacBook-Pro com.apple.CDScheduler[43]: Thermal pressure state: 0 Memory pressure state: 0
Jul  3 08:21:18 authorMacBook-Pro kernel[0]: ARPT: 673255.225425: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  3 08:21:18 authorMacBook-Pro sharingd[30299]: 08:21:18.004 : Discoverable mode changed to Off
Jul  3 08:33:15 authorMacBook-Pro kernel[0]: ARPT: 673289.745639: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq 1578355965, Ack 2401645769, Win size 278
Jul  3 08:33:41 authorMacBook-Pro locationd[82]: NETWORK: requery, 0, 0, 0, 0, 252, items, fQueryRetries, 0, fLastRetryTimestamp, 520783088.4
Jul  3 08:33:45 authorMacBook-Pro kernel[0]: ARPT: 673321.718258: wl0: setup_keepalive: Local port: 50542, Remote port: 5223
Jul  3 08:33:47 authorMacBook-Pro kernel[0]: PM response took 1857 ms (54, powerd)
Jul  3 08:45:12 authorMacBook-Pro kernel[0]: ARPT: 673324.060219: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq 815278018, Ack 1982345407, Win size 278
Jul  3 08:45:12 authorMacBook-Pro kernel[0]: ARPT: 673325.798753: ARPT: Wake Reason: Wake on TCP Timeout
Jul  3 08:59:58 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 09:00:02 authorMacBook-Pro kernel[0]: ARPT: 673399.580233: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': No, 'TimeRemaining': 13027,
Jul  3 09:09:20 authorMacBook-Pro kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 8
Jul  3 09:09:50 authorMacBook-Pro kernel[0]: ARPT: 673431.714836: wl0: setup_keepalive: Local port: 50601, Remote port: 5223
Jul  3 09:09:58 authorMacBook-Pro kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  3 09:21:24 authorMacBook-Pro kernel[0]: hibernate_alloc_pages act 173850, inact 24957, anon 891, throt 0, spec 73492, wire 527143, wireinit 39927
Jul  3 09:21:24 authorMacBook-Pro kernel[0]: hibernate_teardown_pmap_structs done: last_valid_compact_indx 282563
Jul  3 09:21:59 authorMacBook-Pro kernel[0]: ARPT: 673493.721766: wl0: MDNS: 0 SRV Recs, 0 TXT Recs
Jul  3 09:22:01 authorMacBook-Pro kernel[0]: PM response took 1936 ms (54, powerd)
Jul  3 09:57:10 authorMacBook-Pro sharingd[30299]: 09:57:10.384 : Scanning mode Contacts Only
Jul  3 09:57:11 authorMacBook-Pro kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  3 09:57:29 authorMacBook-Pro com.apple.AddressBook.InternetAccountsBridge[33216]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  3 09:57:35 authorMacBook-Pro com.apple.AddressBook.InternetAccountsBridge[33216]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  3 10:09:01 authorMacBook-Pro kernel[0]: Previous sleep cause: 5
Jul  3 10:09:01 authorMacBook-Pro kernel[0]: AirPort: Link Up on awdl0
Jul  3 10:09:01 authorMacBook-Pro ntpd[207]: sigio_handler: sigio_handler_active != 1
Jul  3 10:20:32 authorMacBook-Pro QQ[10018]: button report: 0x80039B7
Jul  3 10:28:07 authorMacBook-Pro pkd[324]: enabling pid=30298 for plug-in com.apple.ncplugin.weather(1.0) 131FE7ED-87F7-471D-8797-C11107688DF7 /System/Library/CoreServices/Weather.app/Contents/PlugIns/com.apple.ncplugin.weather.appex
Jul  3 10:32:45 authorMacBook-Pro QQ[10018]: FA||Url||taskID[2019353296] dealloc
Jul  3 10:40:41 authorMacBook-Pro GoogleSoftwareUpdateAgent[33263]: 2017-07-03 10:40:41.730 GoogleSoftwareUpdateAgent[33263/0x700000323000] [lvl=2] -[KSAgentApp performSelfUpdateWithEngine:] Checking for self update with Engine: <KSUpdateEngine:0x10062de70 ticketStore=<KSPersistentTicketStore:0x1005206e0 store=<KSKeyedPersistentStore:0x1005282c0 path="/Users/xpc/Library/Google/GoogleSoftwareUpdate/TicketStore/Keystone.ticketstore" lockFile=<KSLockFile:0x100510480 path="/Users/xpc/Library/Google/GoogleSoftwareUpdate/TicketStore/Keystone.ticketstore.lock" locked=NO > >> processor=<KSActionProcessor:0x10062e060 delegate=<KSUpdateEngine:0x10062de70> isProcessing=NO actionsCompleted=0 progress=0.00 errors=0 currentActionErrors=0 events=0 currentActionEvents=0 actionQueue=( ) > delegate=(null) serverInfoStore=<KSServerPrivateInfoStore:0x10062d2d0 path="/Users/xpc/Library/Google/GoogleSoftwareUpdate/Servers"> errors=0 >
Jul  3 10:40:42 authorMacBook-Pro GoogleSoftwareUpdateAgent[33263]: 2017-07-03 10:40:42.940 GoogleSoftwareUpdateAgent[33263/0x700000323000] [lvl=2] -[KSOmahaServer updateInfosForUpdateResponse:updateRequest:infoStore:upToDateTickets:updatedTickets:events:errors:] Response passed CUP validation.
Jul  3 11:22:49 authorMacBook-Pro QQ[10018]: FA||Url||taskID[2019353306] dealloc
Jul  3 11:27:14 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:27:14.923] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  3 11:31:49 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:31:49.472] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  3 11:31:49 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:31:49.593] itemasync_CopyProperty signalled err=-12785 (kFigBaseObjectError_Invalidated) (invalidated) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/Player/FigPlayer_Async.c line 2092
Jul  3 11:34:44 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:34:44.290] itemasync_CopyProperty signalled err=-12785 (kFigBaseObjectError_Invalidated) (invalidated) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/Player/FigPlayer_Async.c line 2092
Jul  3 11:38:27 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:38:27.892] <<<< IQ-CA >>>> piqca_setUsePreQueue: (0x7fce1406d600) rejecting report of layer being serviced - IQ has not yet begun to update
Jul  3 11:39:18 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:39:18.356] <<<< IQ-CA >>>> piqca_setUsePreQueue: (0x7fce15069400) rejecting report of layer being serviced - IQ has not yet begun to update
Jul  3 11:41:18 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:41:18.041] <<<< IQ-CA >>>> piqca_setUsePreQueue: (0x7fce16267800) rejecting report of layer being serviced - IQ has not yet begun to update
Jul  3 11:48:35 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:48:35.539] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  3 11:50:02 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:50:02.531] itemasync_SetProperty signalled err=-12785 (kFigBaseObjectError_Invalidated) (invalidated) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/Player/FigPlayer_Async.c line 2306
Jul  3 11:50:13 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:50:13.362] itemasync_SetProperty signalled err=-12785 (kFigBaseObjectError_Invalidated) (invalidated) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/Player/FigPlayer_Async.c line 2306
Jul  3 11:51:29 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:51:29.334] itemasync_SetProperty signalled err=-12785 (kFigBaseObjectError_Invalidated) (invalidated) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/Player/FigPlayer_Async.c line 2306
Jul  3 11:56:05 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:56:05.232] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  3 11:58:00 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:58:00.829] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  3 11:58:36 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [11:58:36.563] itemasync_CopyProperty signalled err=-12785 (kFigBaseObjectError_Invalidated) (invalidated) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/Player/FigPlayer_Async.c line 2092
Jul  3 12:02:22 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [12:02:22.126] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  3 12:03:48 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [12:03:48.669] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  3 12:04:32 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [12:04:32.065] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  3 12:11:47 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [12:11:47.043] itemasync_CopyProperty signalled err=-12785 (kFigBaseObjectError_Invalidated) (invalidated) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/Player/FigPlayer_Async.c line 2092
Jul  3 12:22:42 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [12:22:42.202] itemasync_SetProperty signalled err=-12785 (kFigBaseObjectError_Invalidated) (invalidated) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/Player/FigPlayer_Async.c line 2306
Jul  3 12:31:16 authorMacBook-Pro ntpd[207]: wake time set -0.855670 s
Jul  3 12:31:55 authorMacBook-Pro kernel[0]: Opened file /var/log/SleepWakeStacks.bin, size 172032, extents 1, maxio 2000000 ssd 1
Jul  3 12:35:55 authorMacBook-Pro locationd[82]: NETWORK: no response from server, reachability, 2, queryRetries, 0
Jul  3 12:35:56 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  3 12:36:01 authorMacBook-Pro symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  3 12:36:01 authorMacBook-Pro kernel[0]: Unexpected payload found for message 9, dataLen 0
Jul  3 12:36:38 calvisitor-10-105-160-237 corecaptured[33373]: CCFile::captureLog
Jul  3 12:36:44 calvisitor-10-105-160-237 kernel[0]: en0: Supported channels 1 2 3 4 5 6 7 8 9 10 11 12 13 36 40 44 48 52 56 60 64 100 104 108 112 116 120 124 128 132 136 140 144 149 153 157 161
Jul  3 12:54:59 calvisitor-10-105-160-237 AddressBookSourceSync[33405]: Unrecognized attribute value: t:AbchPersonItemType
Jul  3 12:55:31 calvisitor-10-105-160-237 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 13:21:50 calvisitor-10-105-160-237 kernel[0]: ARPT: 681387.132167: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  3 13:31:32 calvisitor-10-105-160-237 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 13:31:32 calvisitor-10-105-160-237 kernel[0]: ARPT: 681446.072377: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  3 13:31:35 calvisitor-10-105-160-237 CrashReporterSupportHelper[252]: Internal name did not resolve to internal address!
Jul  3 13:31:51 calvisitor-10-105-160-237 com.apple.AddressBook.InternetAccountsBridge[33427]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  3 13:39:50 calvisitor-10-105-160-237 com.apple.xpc.launchd[1] (com.apple.WebKit.Networking.A546008E-07AF-4FFC-8FF8-D8FD260359D9[33438]): Service exited with abnormal code: 1
Jul  3 13:40:27 calvisitor-10-105-160-237 wirelessproxd[75]: Central manager is not powered on
Jul  3 13:48:22 calvisitor-10-105-160-237 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 6
Jul  3 13:48:22 authorMacBook-Pro configd[53]: setting hostname to "authorMacBook-Pro.local"
Jul  3 13:48:39 calvisitor-10-105-160-237 corecaptured[33452]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_13,48,39.362458]-CCIOReporter-002.xml, Current File [2017-07-03_13,48,39.362458]-CCIOReporter-002.xml
Jul  3 13:50:09 authorMacBook-Pro networkd[195]: __42-[NETClientConnection evaluateCrazyIvan46]_block_invoke CI46 - Hit by torpedo! QQ.10018 tc20795 14.17.42.14:14000
Jul  3 13:50:14 authorMacBook-Pro corecaptured[33452]: CCFile::copyFile fileName is [2017-07-03_13,48,39.308188]-io80211Family-002.pcapng, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/IO80211AWDLPeerManager//[2017-07-03_13,48,39.308188]-io80211Family-002.pcapng, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-03_13,50,14.954481]=AssocFail:sts:2_rsn:0/IO80211AWDLPeerManager//[2017-07-03_13,48,39.308188]-io80211Family-002.pcapng
Jul  3 13:50:15 authorMacBook-Pro kernel[0]: ARPT: 682068.402171: framerdy 0x0 bmccmd 7 framecnt 1024
Jul  3 13:51:03 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 20835 failed: 3 - No network route
Jul  3 13:51:03 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 20851 failed: 3 - No network route
Jul  3 13:51:32 calvisitor-10-105-160-237 com.apple.AddressBook.InternetAccountsBridge[33469]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  3 13:53:27 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from SUSPENDED to AUTO
Jul  3 13:53:39 calvisitor-10-105-160-237 kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  3 13:53:53 calvisitor-10-105-160-237 sandboxd[129] ([33476]): com.apple.Addres(33476) deny network-outbound /private/var/run/mDNSResponder
Jul  3 13:54:18 calvisitor-10-105-160-237 identityservicesd[272]: <IMMacNotificationCenterManager: 0x7ff1b2e00ba0>:   DND Enabled: YES
Jul  3 13:54:35 calvisitor-10-105-160-237 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  3 13:54:36 calvisitor-10-105-160-237 symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  3 13:54:53 calvisitor-10-105-160-237 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from SUSPENDED to AUTO
Jul  3 13:55:56 calvisitor-10-105-160-237 sharingd[30299]: 13:55:56.094 : Starting AirDrop server for user 501 on wake
Jul  3 14:20:59 calvisitor-10-105-160-237 kernel[0]: Sandbox: com.apple.Addres(33493) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  3 14:34:22 calvisitor-10-105-160-237 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 14:35:17 calvisitor-10-105-160-237 kernel[0]: ARPT: 682406.173418: wl0: setup_keepalive: Remote IP: 17.249.28.77
Jul  3 14:47:59 calvisitor-10-105-160-237 kernel[0]: ARPT: 682407.704265: wl0: wl_update_tcpkeep_seq: Original Seq: 1181052579, Ack: 1862377178, Win size: 4096
Jul  3 14:47:59 calvisitor-10-105-160-237 kernel[0]: RTC: Maintenance 2017/7/3 21:47:58, sleep 2017/7/3 21:35:20
Jul  3 14:48:22 calvisitor-10-105-160-237 kernel[0]: Sandbox: com.apple.Addres(33508) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  3 15:01:36 calvisitor-10-105-160-237 kernel[0]: ARPT: 682466.260119: wl0: leaveModulePoweredForOffloads: Wi-Fi will stay on.
Jul  3 15:01:36 calvisitor-10-105-160-237 Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-03 22:01:36 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  3 15:01:36 calvisitor-10-105-160-237 kernel[0]: ARPT: 682468.243050: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  3 15:01:36 calvisitor-10-105-160-237 kernel[0]: ARPT: 682468.243133: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  3 15:15:13 calvisitor-10-105-160-237 kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  3 15:15:31 calvisitor-10-105-160-237 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  3 15:15:34 calvisitor-10-105-160-237 com.apple.AddressBook.InternetAccountsBridge[33518]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  3 15:28:51 calvisitor-10-105-160-237 com.apple.WebKit.WebContent[32778]: <<<< FigByteStream >>>> FigByteStreamStatsLogOneRead: ByteStream read of 4812 bytes @ 358800 took 1.053312 secs. to complete, 5 reads >= 1 sec.
Jul  3 15:28:56 calvisitor-10-105-160-237 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - intel_rp = 1 dlla_reporting_supported = 0
Jul  3 15:42:36 calvisitor-10-105-160-237 sandboxd[129] ([33523]): com.apple.Addres(33523) deny network-outbound /private/var/run/mDNSResponder
Jul  3 15:42:59 calvisitor-10-105-160-237 kernel[0]: ARPT: 682634.998705: wl0: setup_keepalive: Remote IP: 17.249.28.93
Jul  3 15:46:27 calvisitor-10-105-160-237 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  3 15:46:27 authorMacBook-Pro kernel[0]: ARPT: 682638.445688: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  3 15:46:28 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name apsd as bundle ID (this is expected for daemons without bundle ID
Jul  3 15:46:29 authorMacBook-Pro UserEventAgent[43]: Captive: [CNInfoNetworkActive:1748] en0: SSID 'CalVisitor' making interface primary (cache indicates network not captive)
Jul  3 15:46:31 calvisitor-10-105-160-237 configd[53]: network changed: v4(en0:10.105.160.237) v6(en0!:2607:f140:6000:8:f1dc:a608:863:19ad) DNS Proxy SMB
Jul  3 15:47:12 calvisitor-10-105-160-237 corecaptured[33533]: Received Capture Event
Jul  3 15:47:13 calvisitor-10-105-160-237 corecaptured[33533]: CCFile::captureLog Received Capture notice id: 1499122032.492037, reason = AuthFail:sts:5_rsn:0
Jul  3 15:47:15 calvisitor-10-105-160-237 corecaptured[33533]: CCIOReporterFormatter::refreshSubscriptionsFromStreamRegistry clearing out any previous subscriptions
Jul  3 15:47:15 calvisitor-10-105-160-237 corecaptured[33533]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  3 15:47:15 calvisitor-10-105-160-237 corecaptured[33533]: CCFile::copyFile fileName is [2017-07-03_15,47,15.246620]-AirPortBrcm4360_Logs-007.txt, source path:/var/log/CoreCapture/com.apple.driver.AirPort.Brcm4360.0/DriverLogs//[2017-07-03_15,47,15.246620]-AirPortBrcm4360_Logs-007.txt, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.driver.AirPort.Brcm4360.0/[2017-07-03_15,47,15.349129]=AuthFail:sts:5_rsn:0/DriverLogs//[2017-07-03_15,47,15.246620]-AirPortBrcm4360_Logs-007.txt
Jul  3 15:53:05 calvisitor-10-105-160-237 kernel[0]: en0: channel changed to 1
Jul  3 15:53:38 calvisitor-10-105-160-237 kernel[0]: Sandbox: com.apple.Addres(33540) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  3 15:53:49 calvisitor-10-105-160-237 corecaptured[33533]: CCLogTap::profileRemoved, Owner: com.apple.iokit.IO80211Family, Name: OneStats
Jul  3 16:05:45 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: <<<< FigByteStream >>>> FigByteStreamStatsLogOneRead: ByteStream read of 4321 bytes @ 48 took 0.607292 sec. to complete, 7 reads >= 0.5 sec.
Jul  3 16:05:45 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  3 16:05:49 authorMacBook-Pro kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  3 16:05:51 authorMacBook-Pro corecaptured[33544]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_16,05,51.031703]-AirPortBrcm4360_Logs-003.txt, Current File [2017-07-03_16,05,51.031703]-AirPortBrcm4360_Logs-003.txt
Jul  3 16:05:51 authorMacBook-Pro corecaptured[33544]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_16,05,51.075268]-AirPortBrcm4360_Logs-005.txt, Current File [2017-07-03_16,05,51.075268]-AirPortBrcm4360_Logs-005.txt
Jul  3 16:05:51 authorMacBook-Pro corecaptured[33544]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_16,05,51.177343]-io80211Family-003.pcapng, Current File [2017-07-03_16,05,51.177343]-io80211Family-003.pcapng
Jul  3 16:05:51 authorMacBook-Pro corecaptured[33544]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  3 16:05:51 authorMacBook-Pro corecaptured[33544]: CCFile::captureLog
Jul  3 16:06:19 calvisitor-10-105-160-237 corecaptured[33544]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_16,06,19.893324]-AirPortBrcm4360_Logs-008.txt, Current File [2017-07-03_16,06,19.893324]-AirPortBrcm4360_Logs-008.txt
Jul  3 16:07:32 authorMacBook-Pro kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  3 16:07:33 authorMacBook-Pro configd[53]: network changed: v6(en0-:2607:f140:6000:8:d8d1:d506:6046:43e4) DNS- Proxy-
Jul  3 16:07:33 authorMacBook-Pro kernel[0]: ARPT: 682827.873728: AQM agg results 0x8001 len hi/lo: 0x0 0x26 BAbitmap(0-3) 0 0 0 0
Jul  3 16:07:33 authorMacBook-Pro corecaptured[33544]: CCFile::copyFile fileName is [2017-07-03_16,06,19.861073]-CCIOReporter-008.xml, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/OneStats//[2017-07-03_16,06,19.861073]-CCIOReporter-008.xml, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-03_16,07,33.914349]=AuthFail:sts:5_rsn:0/OneStats//[2017-07-03_16,06,19.861073]-CCIOReporter-008.xml
Jul  3 16:07:33 authorMacBook-Pro corecaptured[33544]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_16,07,33.934533]-AirPortBrcm4360_Logs-009.txt, Current File [2017-07-03_16,07,33.934533]-AirPortBrcm4360_Logs-009.txt
Jul  3 16:07:39 authorMacBook-Pro kernel[0]: ARPT: 682833.053879: framerdy 0x0 bmccmd 3 framecnt 1024
Jul  3 16:07:39 authorMacBook-Pro corecaptured[33544]: CCFile::copyFile fileName is [2017-07-03_16,07,38.954579]-io80211Family-016.pcapng, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/IO80211AWDLPeerManager//[2017-07-03_16,07,38.954579]-io80211Family-016.pcapng, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-03_16,07,39.094895]=AuthFail:sts:5_rsn:0/IO80211AWDLPeerManager//[2017-07-03_16,07,38.954579]-io80211Family-016.pcapng
Jul  3 16:07:39 authorMacBook-Pro corecaptured[33544]: CCIOReporterFormatter::addRegistryChildToChannelDictionary streams 7
Jul  3 16:07:39 authorMacBook-Pro corecaptured[33544]: CCFile::copyFile fileName is [2017-07-03_16,07,39.096792]-CCIOReporter-017.xml, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/OneStats//[2017-07-03_16,07,39.096792]-CCIOReporter-017.xml, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-03_16,07,39.171995]=AuthFail:sts:5_rsn:0/OneStats//[2017-07-03_16,07,39.096792]-CCIOReporter-017.xml
Jul  3 16:07:39 authorMacBook-Pro corecaptured[33544]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  3 16:07:40 authorMacBook-Pro corecaptured[33544]: CCFile::captureLog
Jul  3 16:07:40 authorMacBook-Pro kernel[0]: ARPT: 682834.192587: wlc_dump_aggfifo:
Jul  3 16:07:40 authorMacBook-Pro corecaptured[33544]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_16,07,40.650186]-CCIOReporter-027.xml, Current File [2017-07-03_16,07,40.650186]-CCIOReporter-027.xml
Jul  3 16:07:46 authorMacBook-Pro corecaptured[33544]: CCFile::captureLog
Jul  3 16:07:46 authorMacBook-Pro kernel[0]: ARPT: 682840.256116: AQM agg results 0x8001 len hi/lo: 0x0 0x30 BAbitmap(0-3) 0 0 0 0
Jul  3 16:07:46 authorMacBook-Pro corecaptured[33544]: CCFile::copyFile fileName is [2017-07-03_16,07,46.105103]-CCIOReporter-032.xml, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/OneStats//[2017-07-03_16,07,46.105103]-CCIOReporter-032.xml, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-03_16,07,46.298508]=AuthFail:sts:5_rsn:0/OneStats//[2017-07-03_16,07,46.105103]-CCIOReporter-032.xml
Jul  3 16:07:48 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x4 to 0x8000000000000000
Jul  3 16:08:13 calvisitor-10-105-160-237 kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  3 16:25:21 calvisitor-10-105-160-237 kernel[0]: Wake reason: ?
Jul  3 16:25:21 calvisitor-10-105-160-237 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  3 16:25:21 calvisitor-10-105-160-237 kernel[0]: AirPort: Link Up on awdl0
Jul  3 16:25:21 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x8000000000000000 to 0x4
Jul  3 16:25:27 authorMacBook-Pro corecaptured[33544]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_16,25,27.859687]-CCIOReporter-038.xml, Current File [2017-07-03_16,25,27.859687]-CCIOReporter-038.xml
Jul  3 16:25:45 authorMacBook-Pro com.apple.WebKit.WebContent[25654]: [16:25:45.631] <<<< CRABS >>>> crabsFlumeHostUnavailable: [0x7f961cf08cf0] Byte flume reports host unavailable.
Jul  3 16:25:54 authorMacBook-Pro sandboxd[129] ([33562]): com.apple.Addres(33562) deny network-outbound /private/var/run/mDNSResponder
Jul  3 16:27:03 authorMacBook-Pro kernel[0]: ARPT: 682969.397322: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  3 16:27:03 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x8000000000000000 to 0x4
Jul  3 16:27:05 authorMacBook-Pro corecaptured[33544]: CCFile::copyFile fileName is [2017-07-03_16,27,04.995982]-io80211Family-040.pcapng, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/IO80211AWDLPeerManager//[2017-07-03_16,27,04.995982]-io80211Family-040.pcapng, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-03_16,27,05.123034]=AuthFail:sts:5_rsn:0/IO80211AWDLPeerManager//[2017-07-03_16,27,04.995982]-io80211Family-040.pcapng
Jul  3 16:27:05 authorMacBook-Pro corecaptured[33544]: CCIOReporterFormatter::refreshSubscriptionsFromStreamRegistry clearing out any previous subscriptions
Jul  3 16:27:08 authorMacBook-Pro kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  3 16:27:09 authorMacBook-Pro kernel[0]: ARPT: 682977.654133: wlc_dump_aggfifo:
Jul  3 16:27:10 authorMacBook-Pro corecaptured[33544]: CCFile::copyFile fileName is [2017-07-03_16,27,09.910337]-AirPortBrcm4360_Logs-047.txt, source path:/var/log/CoreCapture/com.apple.driver.AirPort.Brcm4360.0/DriverLogs//[2017-07-03_16,27,09.910337]-AirPortBrcm4360_Logs-047.txt, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.driver.AirPort.Brcm4360.0/[2017-07-03_16,27,10.162319]=AuthFail:sts:5_rsn:0/DriverLogs//[2017-07-03_16,27,09.910337]-AirPortBrcm4360_Logs-047.txt
Jul  3 16:27:10 authorMacBook-Pro corecaptured[33544]: doSaveChannels@286: Will write to: /Library/Logs/CrashReporter/CoreCapture/IOReporters/[2017-07-03_16,27,09.307937] - AuthFail:sts:5_rsn:0.xml
Jul  3 16:27:11 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Authenticated
Jul  3 16:27:47 calvisitor-10-105-160-184 locationd[82]: Location icon should now be in state 'Inactive'
Jul  3 16:28:34 calvisitor-10-105-160-184 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 16:28:34 authorMacBook-Pro QQ[10018]: tcp_connection_destination_perform_socket_connect 21152 connectx to 203.205.147.206:8080@0 failed: [51] Network is unreachable
Jul  3 16:28:34 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 21158 failed: 3 - No network route
Jul  3 16:28:35 authorMacBook-Pro corecaptured[33544]: Received Capture Event
Jul  3 16:28:40 authorMacBook-Pro corecaptured[33544]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_16,28,40.259618]-AirPortBrcm4360_Logs-055.txt, Current File [2017-07-03_16,28,40.259618]-AirPortBrcm4360_Logs-055.txt
Jul  3 16:28:40 authorMacBook-Pro corecaptured[33544]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  3 16:28:40 authorMacBook-Pro corecaptured[33544]: CCFile::captureLog
Jul  3 16:28:40 authorMacBook-Pro kernel[0]: ARPT: 683047.197539: framerdy 0x0 bmccmd 3 framecnt 1024
Jul  3 16:28:55 calvisitor-10-105-160-184 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  3 16:28:56 calvisitor-10-105-160-184 QQ[10018]: button report: 0x8002bdf
Jul  3 16:29:09 calvisitor-10-105-160-184 corecaptured[33544]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_16,29,09.137954]-CCIOReporter-062.xml, Current File [2017-07-03_16,29,09.137954]-CCIOReporter-062.xml
Jul  3 16:29:09 calvisitor-10-105-160-184 corecaptured[33544]: CCIOReporterFormatter::refreshSubscriptionsFromStreamRegistry clearing out any previous subscriptions
Jul  3 16:29:30 calvisitor-10-105-160-184 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  3 16:35:52 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Evaluating
Jul  3 16:35:54 calvisitor-10-105-160-184 networkd[195]: __42-[NETClientConnection evaluateCrazyIvan46]_block_invoke CI46 - Hit by torpedo! NeteaseMusic.17988 tc9008 103.251.128.144:80
Jul  3 16:36:40 calvisitor-10-105-160-184 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  3 16:36:40 calvisitor-10-105-160-184 AddressBookSourceSync[33594]: [CardDAVPlugin-ERROR] -getPrincipalInfo:[_controller supportsRequestCompressionAtURL:https://13957525385%40163.com@p28-contacts.icloud.com/874161398/principal/] Error Domain=NSURLErrorDomain Code=-1001 "The request timed out." UserInfo={NSUnderlyingError=0x7f9af3646900 {Error Domain=kCFErrorDomainCFNetwork Code=-1001 "The request timed out." UserInfo={NSErrorFailingURLStringKey=https://13957525385%40163.com@p28-contacts.icloud.com/874161398/principal/, NSErrorFailingURLKey=https://13957525385%40163.com@p28-contacts.icloud.com/874161398/principal/, _kCFStreamErrorCodeKey=-2102, _kCFStreamErrorDomainKey=4, NSLocalizedDescription=The request timed out.}}, NSErrorFailingURLStringKey=https://13957525385%40163.com@p28-contacts.icloud.com/874161398/principal/, NSErrorFailingURLKey=https://13957525385%40163.com@p28-contacts.icloud.com/874161398/principal/, _kCFStreamErrorDomainKey=4, _kCFStreamErrorCodeKey=-2102, NSLocalizedDescription=The request timed out.}
Jul  3 16:36:49 calvisitor-10-105-160-184 corecaptured[33544]: CCFile::captureLog
Jul  3 16:36:55 calvisitor-10-105-160-184 symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  3 16:42:37 calvisitor-10-105-160-184 kernel[0]: ARPT: 683172.046921: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  3 16:42:37 calvisitor-10-105-160-184 kernel[0]: ARPT: 683173.929950: ARPT: Wake Reason: Wake on Scan offload
Jul  3 16:56:42 calvisitor-10-105-160-184 kernel[0]: ARPT: 683239.026135: wl0: MDNS: 0 SRV Recs, 0 TXT Recs
Jul  3 17:10:11 calvisitor-10-105-160-184 kernel[0]: hibernate image path: /var/vm/sleepimage
Jul  3 17:10:11 calvisitor-10-105-160-184 kernel[0]: hibernate_flush_memory: buffer_cache_gc freed up 13202 wired pages
Jul  3 17:10:11 calvisitor-10-105-160-184 kernel[0]: hibernate_machine_init pagesDone 455920 sum2 81cafc41, time: 185 ms, disk(0x20000) 847 Mb/s, comp bytes: 47288320 time: 32 ms 1369 Mb/s, crypt bytes: 158441472 time: 38 ms 3973 Mb/s
Jul  3 17:10:11 calvisitor-10-105-160-184 com.apple.CDScheduler[43]: Thermal pressure state: 1 Memory pressure state: 0
Jul  3 17:10:11 calvisitor-10-105-160-184 BezelServices 255.10[94]: ASSERTION FAILED: dvcAddrRef != ((void *)0) -[DriverServices getDeviceAddress:] line: 2789
Jul  3 17:23:55 calvisitor-10-105-160-184 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 4
Jul  3 17:23:55 calvisitor-10-105-160-184 kernel[0]: hibernate_machine_init reading
Jul  3 17:23:55 calvisitor-10-105-160-184 UserEventAgent[43]: assertion failed: 15G1510: com.apple.telemetry + 38574 [10D2E324-788C-30CC-A749-55AE67AEC7BC]: 0x7fc235807b90
Jul  3 17:25:12 calvisitor-10-105-160-184 kernel[0]: hibernate_flush_memory: buffer_cache_gc freed up 3349 wired pages
Jul  3 17:25:12 calvisitor-10-105-160-184 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  3 17:25:12 calvisitor-10-105-160-184 UserEventAgent[43]: assertion failed: 15G1510: com.apple.telemetry + 38574 [10D2E324-788C-30CC-A749-55AE67AEC7BC]: 0x7fc235807b90
Jul  3 17:25:13 calvisitor-10-105-160-184 kernel[0]: AirPort: Link Up on awdl0
Jul  3 17:25:15 calvisitor-10-105-160-184 Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-04 00:25:15 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  3 17:25:31 calvisitor-10-105-160-184 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  3 17:26:04 calvisitor-10-105-160-184 WeChat[24144]: jemmytest
Jul  3 17:37:47 calvisitor-10-105-160-184 kernel[0]: hibernate_setup(0) took 4429 ms
Jul  3 17:37:47 calvisitor-10-105-160-184 kernel[0]: **** [IOBluetoothFamily][ProcessBluetoothTransportShowsUpActionWL] -- calling IOBluetoothFamily's registerService() -- 0x5fd0 -- 0x9a00 -- 0x6800 ****
Jul  3 17:37:47 calvisitor-10-105-160-184 kernel[0]: **** [IOBluetoothFamily][ProcessBluetoothTransportShowsUpActionWL] -- Connected to the transport successfully -- 0x5fd0 -- 0x9a00 -- 0x6800 ****
Jul  3 17:37:48 calvisitor-10-105-160-184 blued[85]: hciControllerOnline; HID devices? 0
Jul  3 17:37:48 calvisitor-10-105-160-184 blued[85]: INIT -- Host controller is published
Jul  3 17:51:25 calvisitor-10-105-160-184 kernel[0]: polled file major 1, minor 0, blocksize 4096, pollers 5
Jul  3 17:51:53 calvisitor-10-105-160-184 AddressBookSourceSync[33632]: Unrecognized attribute value: t:AbchPersonItemType
Jul  3 17:52:07 calvisitor-10-105-160-184 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  3 18:06:58 calvisitor-10-105-160-184 kernel[0]: BuildActDeviceEntry exit
Jul  3 18:07:09 authorMacBook-Pro networkd[195]: __42-[NETClientConnection evaluateCrazyIvan46]_block_invoke CI46 - Hit by torpedo! QQ.10018 tc21242 119.81.102.227:80
Jul  3 18:34:20 calvisitor-10-105-162-32 kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  3 18:35:05 calvisitor-10-105-162-32 mDNSResponder[91]: mDNS_DeregisterInterface: Frequent transitions for interface en0 (10.105.162.32)
Jul  3 18:35:06 calvisitor-10-105-162-32 kernel[0]: ARPT: 683604.474196: IOPMPowerSource Information: onSleep,  SleepType: Standby,  'ExternalConnected': No, 'TimeRemaining': 578,
Jul  3 18:47:54 calvisitor-10-105-162-32 kernel[0]: ARPT: 683617.825411: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  3 18:47:59 calvisitor-10-105-162-32 kernel[0]: ARPT: 683623.036953: wl0: setup_keepalive: Remote IP: 17.249.12.155
Jul  3 18:56:23 calvisitor-10-105-162-32 kernel[0]: Wake reason: ARPT (Network)
Jul  3 18:56:23 calvisitor-10-105-162-32 kernel[0]: AppleActuatorHIDEventDriver: message service is terminated
Jul  3 18:56:23 calvisitor-10-105-162-32 BezelServices 255.10[94]: ASSERTION FAILED: dvcAddrRef != ((void *)0) -[DriverServices getDeviceAddress:] line: 2789
Jul  3 18:56:23 authorMacBook-Pro com.apple.WebKit.WebContent[25654]: [18:56:23.837] <<<< CRABS >>>> crabsFlumeHostUnavailable: [0x7f961cf08cf0] Byte flume reports host unavailable.
Jul  3 18:56:27 authorMacBook-Pro kernel[0]: en0: BSSID changed to 64:d9:89:6b:b5:33
Jul  3 18:56:33 calvisitor-10-105-162-32 QQ[10018]: button report: 0x80039B7
Jul  3 18:57:01 calvisitor-10-105-162-32 corecaptured[33660]: CCFile::captureLog Received Capture notice id: 1499133420.914478, reason = DeauthInd:sts:0_rsn:7
Jul  3 18:57:12 calvisitor-10-105-162-32 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  3 18:57:12 calvisitor-10-105-162-32 corecaptured[33660]: CCFile::captureLogRun Skipping current file Dir file [2017-07-03_18,57,12.221233]-AirPortBrcm4360_Logs-005.txt, Current File [2017-07-03_18,57,12.221233]-AirPortBrcm4360_Logs-005.txt
Jul  3 18:57:57 calvisitor-10-105-162-32 kernel[0]: payload Data 07 00
Jul  3 18:57:57 calvisitor-10-105-162-32 kernel[0]: [HID] [ATC] [Error] AppleDeviceManagementHIDEventService::start Could not make a string from out connection notification key
Jul  3 18:57:57 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  3 18:57:59 authorMacBook-Pro kernel[0]: en0: 802.11d country code set to 'US'.
Jul  3 18:58:10 authorMacBook-Pro corecaptured[33660]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  3 18:58:18 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x8000000000000000 to 0x4
Jul  3 18:58:54 calvisitor-10-105-162-32 kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  3 19:04:48 calvisitor-10-105-162-32 kernel[0]: WARNING: hibernate_page_list_setall skipped 11799 xpmapped pages
Jul  3 19:04:48 calvisitor-10-105-162-32 kernel[0]: hibernate_teardown: wired_pages 544767, free_pages 3578340, active_pages 40000, inactive_pages 0, speculative_pages 0, cleaned_pages 0, compressor_pages 112
Jul  3 19:04:48 calvisitor-10-105-162-32 kernel[0]: pages 554018, wire 418692, act 40000, inact 0, cleaned 0 spec 0, zf 0, throt 0, compr 112, xpmapped 40000
Jul  3 19:04:48 calvisitor-10-105-162-32 blued[85]: [BluetoothHIDDeviceController] EventServiceConnectedCallback
Jul  3 19:04:48 calvisitor-10-105-162-32 kernel[0]: **** [IOBluetoothFamily][ProcessBluetoothTransportShowsUpActionWL] -- calling IOBluetoothFamily's registerService() -- 0x5fd0 -- 0x9a00 -- 0xc800 ****
Jul  3 19:04:52 calvisitor-10-105-162-32 kernel[0]: ARPT: 683779.928118: framerdy 0x0 bmccmd 3 framecnt 1024
Jul  3 19:04:52 calvisitor-10-105-162-32 kernel[0]: ARPT: 683780.224800: AQM agg results 0x8001 len hi/lo: 0x0 0x26 BAbitmap(0-3) 0 0 0 0
Jul  3 19:04:52 calvisitor-10-105-162-32 corecaptured[33660]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  3 19:04:53 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [19:04:53.965] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  3 19:04:54 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 21353 failed: 3 - No network route
Jul  3 19:04:58 authorMacBook-Pro corecaptured[33660]: Received Capture Event
Jul  3 19:05:32 calvisitor-10-105-162-32 kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  3 19:05:37 calvisitor-10-105-162-32 corecaptured[33660]: CCFile::copyFile fileName is [2017-07-03_19,04,57.722196]-io80211Family-018.pcapng, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/IO80211AWDLPeerManager//[2017-07-03_19,04,57.722196]-io80211Family-018.pcapng, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-03_19,05,37.535347]=AuthFail:sts:2_rsn:0/IO80211AWDLPeerManager//[2017-07-03_19,04,57.722196]-io80211Family-018.pcapng
Jul  3 19:11:46 calvisitor-10-105-162-32 kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  3 19:11:48 authorMacBook-Pro corecaptured[33660]: Received Capture Event
Jul  3 19:11:55 calvisitor-10-105-162-32 QQ[10018]: button report: 0x8002be0
Jul  3 19:46:35 authorMacBook-Pro kernel[0]: hibernate_rebuild_pmap_structs done: last_valid_compact_indx 285862
Jul  3 19:46:35 authorMacBook-Pro kernel[0]: BuildActDeviceEntry enter
Jul  3 19:46:41 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Authenticated
Jul  3 19:55:29 calvisitor-10-105-161-225 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  3 19:55:30 calvisitor-10-105-161-225 kernel[0]: en0: channel changed to 132,+1
Jul  3 20:07:56 calvisitor-10-105-161-225 mdworker[33804]: (ImportBailout.Error:1331) Asked to exit for Diskarb
Jul  3 20:16:59 calvisitor-10-105-161-225 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.33827): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.History.xpc/Contents/MacOS/com.apple.Safari.History error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  3 20:20:37 calvisitor-10-105-161-225 com.apple.WebKit.WebContent[32778]: [20:20:37.119] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  3 20:25:13 calvisitor-10-105-161-225 GoogleSoftwareUpdateAgent[33847]: 2017-07-03 20:25:13.378 GoogleSoftwareUpdateAgent[33847/0x7000002a0000] [lvl=2] -[KSUpdateCheckAction performAction] KSUpdateCheckAction starting update check for ticket(s): {( <KSTicket:0x10053b310 productID=com.google.Keystone version=1.2.8.57 xc=<KSPathExistenceChecker:0x10053ac30 path=/Users/xpc/Library/Google/GoogleSoftwareUpdate/GoogleSoftwareUpdate.bundle> url=https://tools.google.com/service/update2 creationDate=2017-02-18 15:41:17 ticketVersion=1 > )} Using server: <KSOmahaServer:0x100248090 engine=<KSUpdateEngine:0x1007249c0> >
Jul  3 20:55:46 calvisitor-10-105-161-225 com.apple.WebKit.WebContent[32778]: [20:55:46.310] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  3 20:56:39 calvisitor-10-105-161-225 WeChat[24144]: jemmytest
Jul  3 20:56:57 calvisitor-10-105-161-225 QQ[10018]: FA||Url||taskID[2019353376] dealloc
Jul  3 21:03:03 calvisitor-10-105-161-225 com.apple.WebKit.WebContent[32778]: [21:03:03.265] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  3 21:07:43 calvisitor-10-105-161-225 com.apple.WebKit.WebContent[32778]: [21:07:43.005] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  3 21:17:04 calvisitor-10-105-161-225 Safari[9852]: KeychainGetICDPStatus: status: off
Jul  3 21:20:07 calvisitor-10-105-161-225 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  3 21:23:13 calvisitor-10-105-161-225 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.33936): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.ImageDecoder.xpc/Contents/MacOS/com.apple.Safari.ImageDecoder error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  3 21:27:07 calvisitor-10-105-161-225 com.apple.WebKit.WebContent[32778]: [21:27:07.761] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  3 21:30:39 calvisitor-10-105-161-225 ntpd[207]: sigio_handler: sigio_handler_active != 1
Jul  3 21:30:39 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 21502 failed: 3 - No network route
Jul  3 21:30:39 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 21503 failed: 3 - No network route
Jul  3 21:30:41 authorMacBook-Pro corecaptured[33951]: CCFile::captureLog
Jul  3 21:30:41 authorMacBook-Pro corecaptured[33951]: CCFile::copyFile fileName is [2017-07-03_21,30,41.859869]-CCIOReporter-002.xml, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/OneStats//[2017-07-03_21,30,41.859869]-CCIOReporter-002.xml, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-03_21,30,40.703152]=AuthFail:sts:5_rsn:0/OneStats//[2017-07-03_21,30,41.859869]-CCIOReporter-002.xml
Jul  3 21:30:44 authorMacBook-Pro kernel[0]: en0: channel changed to 1
Jul  3 21:30:50 authorMacBook-Pro kernel[0]: Sandbox: QQ(10018) deny(1) mach-lookup com.apple.networking.captivenetworksupport
Jul  3 21:30:54 authorMacBook-Pro networkd[195]: __42-[NETClientConnection evaluateCrazyIvan46]_block_invoke CI46 - Hit by torpedo! QQ.10018 tc21519 184.105.67.74:443
Jul  3 21:30:55 airbears2-10-142-110-255 kernel[0]: Sandbox: com.apple.Addres(33959) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  3 21:35:36 airbears2-10-142-110-255 locationd[82]: Location icon should now be in state 'Inactive'
Jul  3 21:41:47 airbears2-10-142-110-255 com.apple.WebKit.WebContent[32778]: [21:41:47.568] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  3 22:16:48 airbears2-10-142-110-255 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  3 22:32:26 airbears2-10-142-110-255 WeChat[24144]: jemmytest
Jul  3 23:07:12 airbears2-10-142-110-255 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520841203.7
Jul  3 23:07:40 airbears2-10-142-110-255 locationd[82]: Location icon should now be in state 'Active'
Jul  3 23:16:10 airbears2-10-142-110-255 QQ[10018]: FA||Url||taskID[2019353410] dealloc
Jul  3 23:23:34 airbears2-10-142-110-255 com.apple.AddressBook.InternetAccountsBridge[34080]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  3 23:24:32 airbears2-10-142-110-255 com.apple.WebKit.WebContent[32778]: [23:24:32.378] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  3 23:29:01 airbears2-10-142-110-255 locationd[82]: Location icon should now be in state 'Inactive'
Jul  3 23:31:42 airbears2-10-142-110-255 Safari[9852]: KeychainGetICDPStatus: status: off
Jul  3 23:44:36 airbears2-10-142-110-255 kernel[0]: ARPT: 697702.656868: AirPort_Brcm43xx::powerChange: System Sleep
Jul  3 23:55:44 airbears2-10-142-110-255 Safari[9852]: tcp_connection_tls_session_error_callback_imp 2210 tcp_connection_tls_session_handle_read_error.790 error 60
Jul  4 00:22:21 airbears2-10-142-110-255 com.apple.cts[43]: com.apple.SoftwareUpdate.Activity: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 6349 seconds.  Ignoring.
Jul  4 00:23:04 airbears2-10-142-110-255 com.apple.cts[43]: com.apple.SoftwareUpdate.Activity: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 6306 seconds.  Ignoring.
Jul  4 00:35:57 airbears2-10-142-110-255 kernel[0]: ARPT: 697853.842104: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  4 00:35:57 airbears2-10-142-110-255 com.apple.cts[43]: com.apple.SoftwareUpdate.Activity: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 5533 seconds.  Ignoring.
Jul  4 00:36:02 airbears2-10-142-110-255 kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  4 00:36:07 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 44944 seconds.  Ignoring.
Jul  4 00:48:29 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 9749 seconds.  Ignoring.
Jul  4 01:00:22 airbears2-10-142-110-255 com.apple.AddressBook.InternetAccountsBridge[646]: Checking iCDP status for DSID 874161398 (checkWithServer=0)
Jul  4 01:00:25 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1883 seconds.  Ignoring.
Jul  4 01:00:35 airbears2-10-142-110-255 com.apple.CDScheduler[43]: Thermal pressure state: 0 Memory pressure state: 0
Jul  4 01:13:14 airbears2-10-142-110-255 kernel[0]: ARPT: 698052.637142: AirPort_Brcm43xx::powerChange: System Sleep
Jul  4 01:24:39 airbears2-10-142-110-255 kernel[0]: ARPT: 698055.177765: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  4 01:37:11 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 207556 seconds.  Ignoring.
Jul  4 01:37:37 airbears2-10-142-110-255 kernel[0]: ARPT: 698150.103985: wl0: setup_keepalive: Remote IP: 17.249.12.144
Jul  4 01:37:39 airbears2-10-142-110-255 kernel[0]: ARPT: 698152.085457: AirPort_Brcm43xx::powerChange: System Sleep
Jul  4 01:48:57 airbears2-10-142-110-255 kernel[0]: ARPT: 698152.618948: wl0: leaveModulePoweredForOffloads: Wi-Fi will stay on.
Jul  4 01:48:57 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 206850 seconds.  Ignoring.
Jul  4 01:49:07 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 206840 seconds.  Ignoring.
Jul  4 02:01:03 airbears2-10-142-110-255 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 1 us
Jul  4 02:01:14 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 39837 seconds.  Ignoring.
Jul  4 02:01:30 airbears2-10-142-110-255 com.apple.AddressBook.InternetAccountsBridge[34203]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  4 02:13:35 airbears2-10-142-110-255 com.apple.CDScheduler[43]: Thermal pressure state: 0 Memory pressure state: 0
Jul  4 02:25:27 airbears2-10-142-110-255 kernel[0]: Previous sleep cause: 5
Jul  4 02:25:27 airbears2-10-142-110-255 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  4 02:25:38 airbears2-10-142-110-255 QQ[10018]: ############################## _getSysMsgList
Jul  4 02:25:47 airbears2-10-142-110-255 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 38276 seconds.  Ignoring.
Jul  4 02:26:13 airbears2-10-142-110-255 kernel[0]: ARPT: 698348.781411: wl0: MDNS: IPV6 Addr: 2607:f140:400:a01b:1c57:7ef3:8f8e:5a10
Jul  4 02:37:39 airbears2-10-142-110-255 kernel[0]: Previous sleep cause: 5
Jul  4 02:37:39 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 3199 seconds.  Ignoring.
Jul  4 02:38:25 airbears2-10-142-110-255 kernel[0]: ARPT: 698398.428268: wl0: setup_keepalive: Remote IP: 17.249.12.88
Jul  4 02:49:55 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1728 seconds.  Ignoring.
Jul  4 02:49:55 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1728 seconds.  Ignoring.
Jul  4 02:50:09 airbears2-10-142-110-255 com.apple.AddressBook.InternetAccountsBridge[34235]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  4 03:01:56 airbears2-10-142-110-255 kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  4 03:02:06 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1732 seconds.  Ignoring.
Jul  4 03:14:08 airbears2-10-142-110-255 kernel[0]: ARPT: 698501.213099: ARPT: Wake Reason: Wake on TCP Timeout
Jul  4 03:26:40 airbears2-10-142-110-255 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 34623 seconds.  Ignoring.
Jul  4 03:27:06 airbears2-10-142-110-255 kernel[0]: ARPT: 698596.169927: wl0: setup_keepalive: Local IP: 10.142.110.255
Jul  4 03:38:32 airbears2-10-142-110-255 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  4 03:38:55 airbears2-10-142-110-255 kernel[0]: Sandbox: com.apple.Addres(34268) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 03:50:54 airbears2-10-142-110-255 com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  4 03:50:58 airbears2-10-142-110-255 cloudd[326]:  SecOSStatusWith error:[-50] Error Domain=NSOSStatusErrorDomain Code=-50 "query missing class name" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}
Jul  4 03:51:31 airbears2-10-142-110-255 kernel[0]: PM response took 1999 ms (54, powerd)
Jul  4 04:15:26 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 31785 seconds.  Ignoring.
Jul  4 04:27:18 airbears2-10-142-110-255 kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  4 04:27:18 airbears2-10-142-110-255 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  4 04:27:18 airbears2-10-142-110-255 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 5 us
Jul  4 04:27:18 airbears2-10-142-110-255 kernel[0]: en0: channel changed to 6
Jul  4 04:27:39 airbears2-10-142-110-255 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  4 04:39:52 airbears2-10-142-110-255 kernel[0]: Sandbox: com.apple.Addres(34309) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 04:40:15 airbears2-10-142-110-255 kernel[0]: ARPT: 698894.284410: wl0: MDNS: IPV6 Addr: fe80:0:0:0:c6b3:1ff:fecd:467f
Jul  4 04:40:17 airbears2-10-142-110-255 kernel[0]: ARPT: 698896.274509: AirPort_Brcm43xx::powerChange: System Sleep
Jul  4 04:51:41 airbears2-10-142-110-255 kernel[0]: ARPT: 698896.831598: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  4 04:51:41 airbears2-10-142-110-255 kernel[0]: ARPT: 698898.741521: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  4 04:52:01 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 29590 seconds.  Ignoring.
Jul  4 04:52:25 airbears2-10-142-110-255 QQ[10018]: ############################## _getSysMsgList
Jul  4 05:04:19 airbears2-10-142-110-255 kernel[0]: Sandbox: com.apple.Addres(34326) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 05:16:07 airbears2-10-142-110-255 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  4 05:16:28 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 28123 seconds.  Ignoring.
Jul  4 05:28:19 airbears2-10-142-110-255 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 2 us
Jul  4 05:28:19 airbears2-10-142-110-255 kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  4 05:28:30 airbears2-10-142-110-255 com.apple.CDScheduler[43]: Thermal pressure state: 1 Memory pressure state: 0
Jul  4 05:28:30 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1077 seconds.  Ignoring.
Jul  4 05:40:41 airbears2-10-142-110-255 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 192946 seconds.  Ignoring.
Jul  4 06:03:25 airbears2-10-142-110-255 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  4 06:03:27 authorMacBook-Pro corecaptured[34369]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  4 06:12:51 calvisitor-10-105-162-105 kernel[0]: ARPT: 699244.827573: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq 2935131210, Ack 3049709979, Win size 278
Jul  4 06:12:51 calvisitor-10-105-162-105 kernel[0]: ARPT: 699246.920246: ARPT: Wake Reason: Wake on Scan offload
Jul  4 06:12:52 calvisitor-10-105-162-105 kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  4 06:13:45 calvisitor-10-105-162-105 kernel[0]: ARPT: 699300.508954: wl0: setup_keepalive: Local IP: 10.105.162.105
Jul  4 06:25:39 authorMacBook-Pro com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 23884 seconds.  Ignoring.
Jul  4 06:25:49 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 190238 seconds.  Ignoring.
Jul  4 06:26:04 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[34408]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  4 06:26:11 calvisitor-10-105-162-105 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 23852 seconds.  Ignoring.
Jul  4 06:39:19 calvisitor-10-105-162-105 kernel[0]: ARPT: 699352.804212: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq 2794574676, Ack 1746081928, Win size 278
Jul  4 06:39:19 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  4 06:26:28 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - intel_rp = 1 dlla_reporting_supported = 0
Jul  4 06:39:19 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  4 06:39:19 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 189428 seconds.  Ignoring.
Jul  4 06:39:25 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1852 seconds.  Ignoring.
Jul  4 06:39:30 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1847 seconds.  Ignoring.
Jul  4 06:40:13 calvisitor-10-105-162-105 kernel[0]: ARPT: 699408.252331: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': Yes, 'TimeRemaining': 0,
Jul  4 06:52:56 calvisitor-10-105-162-105 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  4 06:52:58 authorMacBook-Pro configd[53]: network changed: v4(en0-:10.105.162.105) v6(en0-:2607:f140:6000:8:c6b3:1ff:fecd:467f) DNS- Proxy-
Jul  4 06:53:02 authorMacBook-Pro kernel[0]: AirPort: Link Up on en0
Jul  4 06:53:12 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[34429]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  4 06:53:18 calvisitor-10-105-162-105 kernel[0]: Sandbox: com.apple.Addres(34429) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 06:53:41 calvisitor-10-105-162-105 kernel[0]: ARPT: 699456.025397: wl0: setup_keepalive: Local port: 63572, Remote port: 443
Jul  4 06:53:43 calvisitor-10-105-162-105 kernel[0]: PM response took 1999 ms (54, powerd)
Jul  4 07:06:34 authorMacBook-Pro configd[53]: setting hostname to "authorMacBook-Pro.local"
Jul  4 07:06:34 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 21875 failed: 3 - No network route
Jul  4 07:06:38 calvisitor-10-105-162-105 kernel[0]: en0: channel changed to 1
Jul  4 07:06:38 calvisitor-10-105-162-105 kernel[0]: en0::IO80211Interface::postMessage bssid changed
Jul  4 07:06:42 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 215 seconds.  Ignoring.
Jul  4 07:10:01 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  4 07:10:01 calvisitor-10-105-162-105 symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  4 07:10:06 authorMacBook-Pro kernel[0]: en0::IO80211Interface::postMessage bssid changed
Jul  4 07:10:11 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 6 seconds.  Ignoring.
Jul  4 07:10:28 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[34457]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  4 07:10:40 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 187547 seconds.  Ignoring.
Jul  4 07:23:41 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  4 07:23:41 calvisitor-10-105-162-105 kernel[0]: en0: BSSID changed to 5c:50:15:36:bc:03
Jul  4 07:23:47 authorMacBook-Pro configd[53]: network changed: DNS*
Jul  4 07:23:47 calvisitor-10-105-162-105 kernel[0]: en0: Supported channels 1 2 3 4 5 6 7 8 9 10 11 12 13 36 40 44 48 52 56 60 64 100 104 108 112 116 120 124 128 132 136 140 144 149 153 157 161 165
Jul  4 07:26:18 authorMacBook-Pro kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  4 07:26:28 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 186599 seconds.  Ignoring.
Jul  4 07:26:30 calvisitor-10-105-162-105 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 320, items, fQueryRetries, 1, fLastRetryTimestamp, 520871064.1
Jul  4 07:26:52 calvisitor-10-105-162-105 kernel[0]: en0: BSSID changed to 5c:50:15:4c:18:13
Jul  4 07:39:57 calvisitor-10-105-162-105 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  4 07:39:59 calvisitor-10-105-162-105 symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  4 07:40:17 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 185770 seconds.  Ignoring.
Jul  4 07:40:52 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 9995 seconds.  Ignoring.
Jul  4 07:54:21 calvisitor-10-105-162-105 kernel[0]: ARPT: 699815.080622: wl0: MDNS: IPV6 Addr: fe80:0:0:0:c6b3:1ff:fecd:467f
Jul  4 08:20:49 calvisitor-10-105-162-105 kernel[0]: ARPT: 699878.306468: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  4 08:20:51 calvisitor-10-105-162-105 kernel[0]: en0: 802.11d country code set to 'US'.
Jul  4 08:20:51 calvisitor-10-105-162-105 kernel[0]: en0: Supported channels 1 2 3 4 5 6 7 8 9 10 11 12 13 36 40 44 48 52 56 60 64 100 104 108 112 116 120 124 128 132 136 140 144 149 153 157 161
Jul  4 08:21:00 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 183327 seconds.  Ignoring.
Jul  4 08:21:00 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 183327 seconds.  Ignoring.
Jul  4 08:21:25 calvisitor-10-105-162-105 corecaptured[34531]: Received Capture Event
Jul  4 08:21:34 calvisitor-10-105-162-105 kernel[0]: ARPT: 699925.384446: wl0: setup_keepalive: Seq: 4253304142, Ack: 3255241453, Win size: 4096
Jul  4 08:34:26 calvisitor-10-105-162-105 kernel[0]: AirPort: Link Down on awdl0. Reason 1 (Unspecified).
Jul  4 08:34:31 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  4 08:34:36 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 182511 seconds.  Ignoring.
Jul  4 08:48:07 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 21983 failed: 3 - No network route
Jul  4 08:48:12 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Authenticated
Jul  4 08:48:35 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[34554]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  4 08:48:50 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 181657 seconds.  Ignoring.
Jul  4 09:01:41 calvisitor-10-105-162-105 kernel[0]: AirPort: Link Down on awdl0. Reason 1 (Unspecified).
Jul  4 09:01:42 calvisitor-10-105-162-105 kernel[0]: en0: channel changed to 1
Jul  4 09:01:43 authorMacBook-Pro com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 5144 seconds.  Ignoring.
Jul  4 09:01:43 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 21993 failed: 3 - No network route
Jul  4 09:01:43 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  4 09:01:47 authorMacBook-Pro kernel[0]: en0: BSSID changed to 5c:50:15:4c:18:13
Jul  4 09:01:47 authorMacBook-Pro kernel[0]: Sandbox: QQ(10018) deny(1) mach-lookup com.apple.networking.captivenetworksupport
Jul  4 09:01:48 calvisitor-10-105-162-105 configd[53]: network changed: v4(en0:10.105.162.105) v6(en0+:2607:f140:6000:8:c6b3:1ff:fecd:467f) DNS! Proxy SMB
Jul  4 09:02:25 calvisitor-10-105-162-105 QQ[10018]: FA||Url||taskID[2019353444] dealloc
Jul  4 09:15:18 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  4 09:28:56 calvisitor-10-105-162-105 symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  4 09:29:21 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[34589]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  4 09:42:32 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 138 seconds.  Ignoring.
Jul  4 09:42:52 calvisitor-10-105-162-105 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 12051 seconds.  Ignoring.
Jul  4 09:42:57 calvisitor-10-105-162-105 GoogleSoftwareUpdateAgent[34603]: 2017-07-04 09:42:57.924 GoogleSoftwareUpdateAgent[34603/0x700000323000] [lvl=2] +[KSCodeSigningVerification verifyBundle:applicationId:error:] KSCodeSigningVerification verifying code signing for '/Users/xpc/Library/Google/GoogleSoftwareUpdate/GoogleSoftwareUpdate.bundle/Contents/MacOS/GoogleSoftwareUpdateDaemon' with the requirement 'anchor apple generic and certificate 1[field.1.2.840.113635.100.6.2.6] exists and certificate leaf[field.1.2.840.113635.100.6.1.13] exists and certificate leaf[subject.OU]="EQHXZ8M8AV" and (identifier="com.google.Keystone")'
Jul  4 09:42:58 calvisitor-10-105-162-105 ksfetch[34604]: 2017-07-04 09:42:58.121 ksfetch[34604/0x7fff79824000] [lvl=2] main() ksfetch done fetching.
Jul  4 09:43:18 calvisitor-10-105-162-105 kernel[0]: ARPT: 700254.389213: wl0: setup_keepalive: Seq: 2502126767, Ack: 2906384231, Win size: 4096
Jul  4 09:56:08 calvisitor-10-105-162-105 kernel[0]: ARPT: 700256.949512: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  4 09:56:10 calvisitor-10-105-162-105 symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  4 09:56:38 calvisitor-10-105-162-105 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  4 09:57:01 calvisitor-10-105-162-105 kernel[0]: ARPT: 700311.376442: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:c6b3:1ff:fecd:467f
Jul  4 09:57:01 calvisitor-10-105-162-105 kernel[0]: ARPT: 700311.376459: wl0: MDNS: 0 SRV Recs, 0 TXT Recs
Jul  4 09:57:30 calvisitor-10-105-162-105 kernel[0]: en0: 802.11d country code set to 'X3'.
Jul  4 09:57:30 authorMacBook-Pro kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  4 09:57:30 authorMacBook-Pro com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 11173 seconds.  Ignoring.
Jul  4 09:57:30 authorMacBook-Pro symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  4 09:57:31 authorMacBook-Pro configd[53]: network changed: v4(en0!:10.105.162.105) DNS+ Proxy+ SMB
Jul  4 09:57:36 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1737 seconds.  Ignoring.
Jul  4 10:09:39 calvisitor-10-105-162-105 AddressBookSourceSync[34636]: -[SOAPParser:0x7f82b1f24e50 parser:didStartElement:namespaceURI:qualifiedName:attributes:] Type not found in EWSItemType for ExchangePersonIdGuid (t:ExchangePersonIdGuid)
Jul  4 10:13:04 calvisitor-10-105-162-105 mDNSResponder[91]: mDNS_RegisterInterface: Frequent transitions for interface en0 (FE80:0000:0000:0000:C6B3:01FF:FECD:467F)
Jul  4 10:13:05 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[25654]: [10:13:05.044] <<<< CRABS >>>> crabsFlumeHostAvailable: [0x7f961cf08cf0] Byte flume reports host available again.
Jul  4 10:13:12 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 10319 seconds.  Ignoring.
Jul  4 10:26:52 calvisitor-10-105-162-105 QQ[10018]: ############################## _getSysMsgList
Jul  4 10:29:35 calvisitor-10-105-162-105 kernel[0]: Setting BTCoex Config: enable_2G:1, profile_2g:0, enable_5G:1, profile_5G:0
Jul  4 10:29:44 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 43063 seconds.  Ignoring.
Jul  4 10:30:16 calvisitor-10-105-162-105 AddressBookSourceSync[34670]: Unrecognized attribute value: t:AbchPersonItemType
Jul  4 10:43:11 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  4 10:43:27 calvisitor-10-105-162-105 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 8416 seconds.  Ignoring.
Jul  4 10:43:31 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 8500 seconds.  Ignoring.
Jul  4 10:43:42 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[34685]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  4 10:43:46 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[34685]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  4 10:43:48 calvisitor-10-105-162-105 kernel[0]: Sandbox: com.apple.Addres(34685) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 10:43:56 calvisitor-10-105-162-105 kernel[0]: ARPT: 700626.024536: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:c6b3:1ff:fecd:467f
Jul  4 10:43:57 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  4 10:56:48 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 0 milliseconds
Jul  4 10:56:48 calvisitor-10-105-162-105 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 7615 seconds.  Ignoring.
Jul  4 10:56:57 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1609 seconds.  Ignoring.
Jul  4 10:57:46 calvisitor-10-105-162-105 kernel[0]: ARPT: 700687.112611: AirPort_Brcm43xx::powerChange: System Sleep
Jul  4 11:10:27 calvisitor-10-105-162-105 kernel[0]: en0: channel changed to 1
Jul  4 11:10:51 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[34706]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  4 11:10:58 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [11:10:58.940] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  4 11:24:04 calvisitor-10-105-162-105 kernel[0]: AirPort: Link Down on en0. Reason 8 (Disassociated because station leaving).
Jul  4 11:24:13 calvisitor-10-105-162-105 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 5970 seconds.  Ignoring.
Jul  4 11:25:03 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 172284 seconds.  Ignoring.
Jul  4 11:28:35 authorMacBook-Pro kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  4 11:28:35 authorMacBook-Pro symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  4 11:28:41 authorMacBook-Pro corecaptured[34722]: CCFile::captureLog Received Capture notice id: 1499192921.020010, reason = AssocFail:sts:5_rsn:0
Jul  4 11:28:42 authorMacBook-Pro corecaptured[34722]: CCIOReporterFormatter::addRegistryChildToChannelDictionary streams 7
Jul  4 11:28:55 calvisitor-10-105-162-105 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 5688 seconds.  Ignoring.
Jul  4 11:29:09 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[34727]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  4 11:29:09 calvisitor-10-105-162-105 corecaptured[34722]: CCFile::captureLogRun Skipping current file Dir file [2017-07-04_11,29,09.994819]-CCIOReporter-005.xml, Current File [2017-07-04_11,29,09.994819]-CCIOReporter-005.xml
Jul  4 11:29:10 calvisitor-10-105-162-105 corecaptured[34722]: CCFile::captureLogRun Skipping current file Dir file [2017-07-04_11,29,10.144779]-AirPortBrcm4360_Logs-008.txt, Current File [2017-07-04_11,29,10.144779]-AirPortBrcm4360_Logs-008.txt
Jul  4 11:29:20 calvisitor-10-105-162-105 kernel[0]: ARPT: 700863.801551: wl0: MDNS: IPV6 Addr: fe80:0:0:0:c6b3:1ff:fecd:467f
Jul  4 11:42:12 calvisitor-10-105-162-105 kernel[0]: [HID] [ATC] AppleDeviceManagementHIDEventService::processWakeReason Wake reason: Host (0x01)
Jul  4 11:42:36 calvisitor-10-105-162-105 kernel[0]: Sandbox: com.apple.Addres(34738) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 11:43:04 calvisitor-10-105-162-105 kernel[0]: ARPT: 700919.494551: wl0: MDNS: IPV6 Addr: fe80:0:0:0:c6b3:1ff:fecd:467f
Jul  4 11:55:51 calvisitor-10-105-162-105 kernel[0]: en0::IO80211Interface::postMessage bssid changed
Jul  4 11:55:52 calvisitor-10-105-162-105 corecaptured[34743]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  4 11:55:56 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1878 seconds.  Ignoring.
Jul  4 11:56:36 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 170391 seconds.  Ignoring.
Jul  4 11:56:46 calvisitor-10-105-162-105 kernel[0]: ARPT: 700980.255260: wl0: setup_keepalive: Seq: 2955519239, Ack: 2597833420, Win size: 4096
Jul  4 12:06:20 calvisitor-10-105-162-105 corecaptured[34743]: CCLogTap::profileRemoved, Owner: com.apple.iokit.IO80211Family, Name: IO80211AWDLPeerManager
Jul  4 12:06:21 calvisitor-10-105-162-105 cloudd[326]:  SecOSStatusWith error:[-50] Error Domain=NSOSStatusErrorDomain Code=-50 "query missing class name" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}
Jul  4 12:06:24 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  4 12:06:36 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[34830]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  4 12:06:39 calvisitor-10-105-162-105 kernel[0]: Sandbox: com.apple.Addres(34830) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 12:06:43 calvisitor-10-105-162-105 sandboxd[129] ([34830]): com.apple.Addres(34830) deny network-outbound /private/var/run/mDNSResponder
Jul  4 12:07:03 calvisitor-10-105-162-105 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.34835): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.History.xpc/Contents/MacOS/com.apple.Safari.History error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  4 12:08:23 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [12:08:23.866] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  4 12:13:40 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [12:13:40.079] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  4 12:14:54 calvisitor-10-105-162-105 locationd[82]: Location icon should now be in state 'Inactive'
Jul  4 12:19:03 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [12:19:03.575] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  4 12:22:18 calvisitor-10-105-162-105 UserEventAgent[43]: extension com.apple.ncplugin.WorldClock -> (null)
Jul  4 12:25:26 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [12:25:26.551] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  4 12:27:11 calvisitor-10-105-162-105 kernel[0]: ARPT: 702237.246510: wl0: setup_keepalive: Local port: 49218, Remote port: 443
Jul  4 12:34:55 calvisitor-10-105-162-105 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  4 12:34:56 authorMacBook-Pro corecaptured[34861]: CCFile::captureLog Received Capture notice id: 1499196895.670989, reason = AuthFail:sts:5_rsn:0
Jul  4 12:34:56 authorMacBook-Pro corecaptured[34861]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  4 12:35:05 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 168082 seconds.  Ignoring.
Jul  4 12:35:15 calvisitor-10-105-162-105 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  4 12:35:33 calvisitor-10-105-162-105 AddressBookSourceSync[34888]: -[SOAPParser:0x7fc7025b2810 parser:didStartElement:namespaceURI:qualifiedName:attributes:] Type not found in EWSItemType for ExchangePersonIdGuid (t:ExchangePersonIdGuid)
Jul  4 12:48:36 calvisitor-10-105-162-105 kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  4 12:49:34 calvisitor-10-105-162-105 kernel[0]: ARPT: 702351.279867: wl0: setup_keepalive: interval 900, retry_interval 30, retry_count 10
Jul  4 13:02:13 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  4 13:02:13 calvisitor-10-105-162-105 kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  4 13:02:14 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 177 seconds.  Ignoring.
Jul  4 13:02:23 calvisitor-10-105-162-105 com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  4 13:02:23 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1769 seconds.  Ignoring.
Jul  4 13:02:39 calvisitor-10-105-162-105 AddressBookSourceSync[34904]: Unrecognized attribute value: t:AbchPersonItemType
Jul  4 13:15:51 calvisitor-10-105-162-105 kernel[0]: ARPT: 702415.308381: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  4 13:29:28 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 1 us
Jul  4 13:41:20 calvisitor-10-105-162-105 QQ[10018]: button report: 0x8002be0
Jul  4 13:43:06 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [13:43:06.901] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  4 13:55:13 calvisitor-10-105-162-105 locationd[82]: Location icon should now be in state 'Inactive'
Jul  4 13:56:31 calvisitor-10-105-162-105 com.apple.AddressBook.ContactsAccountsService[289]: [Accounts] Current connection, <NSXPCConnection: 0x7fda74805bf0> connection from pid 487, doesn't have account access.
Jul  4 13:56:31 calvisitor-10-105-162-105 SCIM[487]: [Accounts] Failed to update account with identifier 76FE6715-3D27-4F21-AA35-C88C1EA820E8, error: Error Domain=ABAddressBookErrorDomain Code=1002 "(null)"
Jul  4 14:05:06 calvisitor-10-105-162-105 locationd[82]: Location icon should now be in state 'Active'
Jul  4 14:19:40 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [14:19:40.459] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  4 14:33:25 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [14:33:25.556] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  4 14:34:52 calvisitor-10-105-162-105 com.apple.SecurityServer[80]: Session 101921 created
Jul  4 14:36:08 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [14:36:08.077] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  4 14:39:45 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [14:39:45.538] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  4 14:43:39 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [14:43:39.854] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  4 15:03:53 calvisitor-10-105-162-105 GoogleSoftwareUpdateAgent[35018]: 2017-07-04 15:03:53.270 GoogleSoftwareUpdateAgent[35018/0x7000002a0000] [lvl=2] -[KSOutOfProcessFetcher(PrivateMethods) helperDidTerminate:] KSOutOfProcessFetcher fetch ended for URL: "https://tools.google.com/service/update2?cup2hreq=a70d6372a4e45a6cbba61cd7f057c79bf73c79db1b1951dc17c605e870f0419b&cup2key=7:934679018"
Jul  4 15:11:20 calvisitor-10-105-162-105 syslogd[44]: ASL Sender Statistics
Jul  4 15:20:08 calvisitor-10-105-162-105 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 300, items, fQueryRetries, 0, fLastRetryTimestamp, 520899307.1
Jul  4 15:30:20 calvisitor-10-105-162-105 locationd[82]: Location icon should now be in state 'Inactive'
Jul  4 15:35:04 calvisitor-10-105-162-105 quicklookd[35049]: Error returned from iconservicesagent: (null)
Jul  4 15:35:04 calvisitor-10-105-162-105 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f995060af50>.
Jul  4 15:35:04 calvisitor-10-105-162-105 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950411670>.
Jul  4 15:47:25 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[32778]: [15:47:25.614] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  4 16:31:38 calvisitor-10-105-162-105 com.apple.CDScheduler[43]: Thermal pressure state: 1 Memory pressure state: 0
Jul  4 16:31:50 calvisitor-10-105-162-105 kernel[0]: Sandbox: com.apple.Addres(35110) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 16:31:54 calvisitor-10-105-162-105 kernel[0]: Sandbox: com.apple.Addres(35110) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 16:32:07 calvisitor-10-105-162-105 QQ[10018]: button report: 0x8002be0
Jul  4 16:32:32 calvisitor-10-105-162-105 QQ[10018]: ############################## _getSysMsgList
Jul  4 16:58:45 calvisitor-10-105-162-105 kernel[0]: ARPT: 711599.232230: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  4 16:58:55 calvisitor-10-105-162-105 com.apple.cts[43]: com.apple.SoftwareUpdate.Activity: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 4045 seconds.  Ignoring.
Jul  4 17:00:00 calvisitor-10-105-162-105 kernel[0]: kern_open_file_for_direct_io took 6 ms
Jul  4 17:12:34 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 151433 seconds.  Ignoring.
Jul  4 17:12:34 calvisitor-10-105-162-105 com.apple.cts[43]: com.apple.SoftwareUpdate.Activity: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 3226 seconds.  Ignoring.
Jul  4 17:13:03 calvisitor-10-105-162-105 sharingd[30299]: 17:13:03.545 : Starting AirDrop server for user 501 on wake
Jul  4 17:13:38 calvisitor-10-105-162-105 kernel[0]: ARPT: 711749.913729: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': Yes, 'TimeRemaining': 0,
Jul  4 17:26:03 calvisitor-10-105-162-105 kernel[0]: ARPT: 711752.511718: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  4 17:26:44 calvisitor-10-105-162-105 sharingd[30299]: 17:26:44.228 : Scanning mode Contacts Only
Jul  4 17:40:02 calvisitor-10-105-162-105 kernel[0]: Sandbox: com.apple.Addres(35150) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 17:53:21 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  4 17:54:33 calvisitor-10-105-162-105 kernel[0]: ARPT: 711979.641310: wl0: setup_keepalive: interval 900, retry_interval 30, retry_count 10
Jul  4 18:07:00 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  4 18:07:01 calvisitor-10-105-162-105 QQ[10018]: button report: 0x80039B7
Jul  4 18:07:25 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[35165]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  4 18:20:56 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 40 seconds.  Ignoring.
Jul  4 18:21:08 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[35174]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  4 18:34:25 calvisitor-10-105-162-105 kernel[0]: ARPT: 712221.085635: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq 70582765, Ack 3488103719, Win size 358
Jul  4 18:34:25 calvisitor-10-105-162-105 kernel[0]: en0: BSSID changed to 5c:50:15:4c:18:13
Jul  4 18:34:26 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 42431 seconds.  Ignoring.
Jul  4 18:34:51 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[35181]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  4 18:35:05 calvisitor-10-105-162-105 sharingd[30299]: 18:35:05.187 : Scanning mode Contacts Only
Jul  4 18:48:22 calvisitor-10-105-162-105 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  4 18:48:27 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[35188]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  4 19:02:24 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000320
Jul  4 19:15:22 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  4 19:15:22 calvisitor-10-105-162-105 kernel[0]: Previous sleep cause: 5
Jul  4 19:15:41 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[35207]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  4 19:16:35 calvisitor-10-105-162-105 kernel[0]: ARPT: 712526.783763: wl0: setup_keepalive: interval 900, retry_interval 30, retry_count 10
Jul  4 19:29:01 calvisitor-10-105-162-105 kernel[0]: ARPT: 712530.297675: wl0: leaveModulePoweredForOffloads: Wi-Fi will stay on.
Jul  4 19:29:41 calvisitor-10-105-162-105 WindowServer[184]: CGXDisplayDidWakeNotification [712572581805245]: posting kCGSDisplayDidWake
Jul  4 19:30:13 calvisitor-10-105-162-105 kernel[0]: ARPT: 712604.204993: wl0: MDNS: IPV6 Addr: fe80:0:0:0:c6b3:1ff:fecd:467f
Jul  4 19:42:58 calvisitor-10-105-162-105 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  4 19:43:32 calvisitor-10-105-162-105 wirelessproxd[75]: Peripheral manager is not powered on
Jul  4 19:43:56 calvisitor-10-105-162-105 kernel[0]: Opened file /var/log/SleepWakeStacks.bin, size 172032, extents 1, maxio 2000000 ssd 1
Jul  4 19:56:19 calvisitor-10-105-162-105 kernel[0]: [HID] [ATC] AppleDeviceManagementHIDEventService::processWakeReason Wake reason: Host (0x01)
Jul  4 19:56:41 calvisitor-10-105-162-105 kernel[0]: Sandbox: com.apple.Addres(35229) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 19:57:00 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1867 seconds.  Ignoring.
Jul  4 20:09:58 calvisitor-10-105-162-105 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  4 20:10:18 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 36678 seconds.  Ignoring.
Jul  4 20:24:32 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 215 seconds.  Ignoring.
Jul  4 20:24:55 calvisitor-10-105-162-105 kernel[0]: Sandbox: com.apple.Addres(35250) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 20:25:22 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 139865 seconds.  Ignoring.
Jul  4 20:25:23 calvisitor-10-105-162-105 kernel[0]: IOPMrootDomain: idle cancel, state 1
Jul  4 20:25:44 calvisitor-10-105-162-105 kernel[0]: ARPT: 712915.870808: wl0: MDNS: IPV4 Addr: 10.105.162.105
Jul  4 20:25:46 calvisitor-10-105-162-105 kernel[0]: ARPT: 712918.575461: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': Yes, 'TimeRemaining': 0,
Jul  4 20:38:11 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 2 us
Jul  4 20:38:12 calvisitor-10-105-162-105 kernel[0]: ARPT: 712921.782306: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  4 20:38:12 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 139095 seconds.  Ignoring.
Jul  4 20:38:13 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 35003 seconds.  Ignoring.
Jul  4 20:38:50 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 139057 seconds.  Ignoring.
Jul  4 20:51:50 calvisitor-10-105-162-105 kernel[0]: Wake reason: RTC (Alarm)
Jul  4 20:51:50 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  4 20:51:50 calvisitor-10-105-162-105 Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-05 03:51:50 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  4 20:51:50 calvisitor-10-105-162-105 kernel[0]: ARPT: 712997.981881: IOPMPowerSource Information: onWake,  SleepType: Normal Sleep,  'ExternalConnected': Yes, 'TimeRemaining': 0,
Jul  4 20:52:10 calvisitor-10-105-162-105 com.apple.CDScheduler[43]: Thermal pressure state: 0 Memory pressure state: 0
Jul  4 20:54:03 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 0 milliseconds
Jul  4 20:54:03 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  4 20:54:03 calvisitor-10-105-162-105 sharingd[30299]: 20:54:03.455 : BTLE scanner Powered On
Jul  4 20:54:03 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 138144 seconds.  Ignoring.
Jul  4 20:54:04 authorMacBook-Pro networkd[195]: __42-[NETClientConnection evaluateCrazyIvan46]_block_invoke CI46 - Hit by torpedo! QQ.10018 tc22491 203.205.142.158:8080
Jul  4 20:54:25 calvisitor-10-105-162-105 kernel[0]: Sandbox: com.apple.Addres(35286) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  4 21:06:47 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 132 seconds.  Ignoring.
Jul  4 21:07:16 calvisitor-10-105-162-105 sharingd[30299]: 21:07:16.729 : Scanning mode Contacts Only
Jul  4 21:23:17 calvisitor-10-105-162-105 wirelessproxd[75]: Peripheral manager is not powered on
Jul  4 21:35:17 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 135670 seconds.  Ignoring.
Jul  4 21:35:54 calvisitor-10-105-162-105 kernel[0]: ARPT: 713439.104232: AirPort_Brcm43xx::powerChange: System Sleep
Jul  4 21:35:54 calvisitor-10-105-162-105 kernel[0]: ARPT: 713439.104255: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': Yes, 'TimeRemaining': 0,
Jul  4 21:49:26 calvisitor-10-105-162-105 kernel[0]: ARPT: 713493.575563: wl0: setup_keepalive: Remote IP: 17.249.28.75
Jul  4 22:02:21 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1843 seconds.  Ignoring.
Jul  4 22:02:21 calvisitor-10-105-162-105 com.apple.CDScheduler[43]: Thermal pressure state: 1 Memory pressure state: 0
Jul  4 22:15:48 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  4 22:29:25 calvisitor-10-105-162-105 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  4 22:29:26 calvisitor-10-105-162-105 sharingd[30299]: 22:29:26.099 : BTLE scanner Powered On
Jul  4 22:43:02 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  4 22:56:50 calvisitor-10-105-162-105 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 978 seconds.  Ignoring.
Jul  4 22:57:00 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[35374]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  4 22:57:01 calvisitor-10-105-162-105 locationd[82]: NETWORK: no response from server, reachability, 2, queryRetries, 1
Jul  4 23:10:17 calvisitor-10-105-162-105 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 7
Jul  4 23:10:17 calvisitor-10-105-162-105 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  4 23:10:28 calvisitor-10-105-162-105 CalendarAgent[279]: [com.apple.calendar.store.log.caldav.coredav] [Refusing to parse response to PROPPATCH because of content-type: [text/html; charset=UTF-8].]
Jul  4 23:10:29 calvisitor-10-105-162-105 kernel[0]: PM response took 113 ms (24144, WeChat)
Jul  4 23:10:31 calvisitor-10-105-162-105 WindowServer[184]: device_generate_lock_screen_screenshot: authw 0x7fa824145a00(2000)[0, 0, 1440, 900] shield 0x7fa825dafc00(2001), dev [1440,900]
Jul  4 23:10:40 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[35382]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  4 23:13:35 calvisitor-10-105-162-105 com.apple.AddressBook.InternetAccountsBridge[35394]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  4 23:14:37 calvisitor-10-105-162-105 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.35400): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.SearchHelper.xpc/Contents/MacOS/com.apple.Safari.SearchHelper error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  4 23:16:14 calvisitor-10-105-162-105 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.35412): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.SocialHelper.xpc/Contents/MacOS/com.apple.Safari.SocialHelper error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  4 23:17:17 calvisitor-10-105-162-105 Safari[9852]: KeychainGetICDPStatus: status: off
Jul  4 23:20:18 calvisitor-10-105-162-105 QQ[10018]: FA||Url||taskID[2019353517] dealloc
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00660011': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x0067000f': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00670033': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x03600009': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x0360001f': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x03f20026': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x009a0005': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00990013': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00690003': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x006a0061': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x006a00a6': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x006a00a7': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x006a00b2': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x006d0002': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00b20001': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x03ef000d': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00b8fffe': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00bb0001': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00bc0007': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00c1000a': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00c40027': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00c5000c': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00e80002': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x03eb0001': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x006f0001': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x00730020': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x007c000c': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x007c0018': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x007d000e': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x008b036a': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x008b6fb5': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x008b0360': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x008bdeaa': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x008bdeb0': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x01f60001': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x01f90004': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02080000': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02100004': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02120000': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02120002': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x023b0003': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x023d0001': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02410003': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02420002': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02420006': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02470032': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02470045': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x0247ffe9': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x0249002a': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x024a00b2': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x025c0009': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x0261fffd': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02640011': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x0268000a': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02720002': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02740004': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02cc0000': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02d10001': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02770011': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02780026': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x027b0006': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x027e0001': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02801000': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02950006': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x0295001d': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x029f0014': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x029f003a': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02a60001': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02b60001': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02b70006': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: Cocoa scripting error for '0x02cb000d': four character codes must be four characters long.
Jul  4 23:22:09 calvisitor-10-105-162-105 Microsoft Word[14463]: .sdef warning for argument '' of command 'can continue previous list' in suite 'Microsoft Word Suite': '4023' is not a valid type name.
Jul  4 23:25:29 calvisitor-10-105-162-105 WeChat[24144]: jemmytest
Jul  4 23:30:23 calvisitor-10-105-162-105 QQ[10018]: FA||Url||taskID[2019353519] dealloc
Jul  4 23:31:49 calvisitor-10-105-162-105 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  4 23:49:42 calvisitor-10-105-162-105 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950712080>.
Jul  4 23:50:59 calvisitor-10-105-162-105 quicklookd[35391]: objc[35391]: Class TSUAtomicLRUCache is implemented in both /Library/QuickLook/iWork.qlgenerator/Contents/MacOS/iWork and /System/Library/PrivateFrameworks/OfficeImport.framework/Versions/A/OfficeImport. One of the two will be used. Which one is undefined.
Jul  4 23:50:59 calvisitor-10-105-162-105 garcon[35461]: Invalidating watch set.
Jul  4 23:50:59 calvisitor-10-105-162-105 QuickLookSatellite[35448]: objc[35448]: Class TSUCustomFormatData is implemented in both /Library/QuickLook/iWork.qlgenerator/Contents/MacOS/iWork and /System/Library/PrivateFrameworks/OfficeImport.framework/Versions/A/OfficeImport. One of the two will be used. Which one is undefined.
Jul  4 23:51:29 calvisitor-10-105-162-105 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  4 23:53:29 calvisitor-10-105-162-105 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950712080>.
Jul  4 23:55:28 calvisitor-10-105-162-105 CalendarAgent[279]: [com.apple.calendar.store.log.caldav.coredav] [Refusing to parse response to PROPPATCH because of content-type: [text/html; charset=UTF-8].]
Jul  5 00:02:22 calvisitor-10-105-162-105 WeChat[24144]: jemmytest
Jul  5 00:03:06 calvisitor-10-105-162-105 sharingd[30299]: 00:03:06.178 : Started generating hashes
Jul  5 00:03:36 calvisitor-10-105-162-105 sharingd[30299]: 00:03:36.715 : BTLE scanning started
Jul  5 00:09:34 calvisitor-10-105-162-105 WeChat[24144]: jemmytest
Jul  5 00:17:32 calvisitor-10-105-162-105 com.apple.WebKit.WebContent[35435]: <<<< FigByteStream >>>> FigByteStreamStatsLogOneRead: ByteStream read of 21335 bytes @ 5481561 took 0.501237 sec. to complete, 16 reads >= 0.5 sec.
Jul  5 00:17:35 calvisitor-10-105-162-105 locationd[82]: Location icon should now be in state 'Inactive'
Jul  5 00:17:50 authorMacBook-Pro corecaptured[35613]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  5 00:17:52 authorMacBook-Pro kernel[0]: AirPort: Link Up on en0
Jul  5 00:18:04 authorMacBook-Pro QQ[10018]: button report: 0x80039B7
Jul  5 00:18:07 authorMacBook-Pro QQ[10018]: button report: 0X80076ED
Jul  5 00:18:09 authorMacBook-Pro com.apple.AddressBook.InternetAccountsBridge[35618]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  5 00:18:12 authorMacBook-Pro AddressBookSourceSync[35614]: Unrecognized attribute value: t:AbchPersonItemType
Jul  5 00:21:07 authorMacBook-Pro Google Chrome[35628]: -_continuousScroll is deprecated for NSScrollWheel. Please use -hasPreciseScrollingDeltas.
Jul  5 00:21:07 authorMacBook-Pro Google Chrome[35628]: -deviceDeltaY is deprecated for NSScrollWheel. Please use -scrollingDeltaY.
Jul  5 00:28:42 authorMacBook-Pro com.apple.WebKit.WebContent[35671]: [00:28:42.167] (Fig) signalled err=-12871
Jul  5 00:29:04 authorMacBook-Pro com.apple.WebKit.WebContent[35671]: [00:29:04.085] <<<< CRABS >>>> crabsWaitForLoad: [0x7fbc7ac683a0] Wait time out - -1001 (msRequestTimeout -1)
Jul  5 00:29:25 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 22710 failed: 3 - No network route
Jul  5 00:30:00 calvisitor-10-105-163-10 syslogd[44]: Configuration Notice: ASL Module "com.apple.corecdp.osx.asl" claims selected messages. Those messages may not appear in standard system log files or in the ASL database.
Jul  5 00:33:56 calvisitor-10-105-163-10 locationd[82]: PBRequester failed with Error Error Domain=NSURLErrorDomain Code=-1001 "The request timed out." UserInfo={NSUnderlyingError=0x7fb7ed616c80 {Error Domain=kCFErrorDomainCFNetwork Code=-1001 "The request timed out." UserInfo={NSErrorFailingURLStringKey=https://gs-loc.apple.com/clls/wloc, NSErrorFailingURLKey=https://gs-loc.apple.com/clls/wloc, _kCFStreamErrorCodeKey=-2102, _kCFStreamErrorDomainKey=4, NSLocalizedDescription=The request timed out.}}, NSErrorFailingURLStringKey=https://gs-loc.apple.com/clls/wloc, NSErrorFailingURLKey=https://gs-loc.apple.com/clls/wloc, _kCFStreamErrorDomainKey=4, _kCFStreamErrorCodeKey=-2102, NSLocalizedDescription=The request timed out.}
Jul  5 00:47:16 calvisitor-10-105-163-10 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  5 00:53:57 calvisitor-10-105-163-10 locationd[82]: NETWORK: no response from server, reachability, 2, queryRetries, 2
Jul  5 00:55:33 calvisitor-10-105-163-10 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 284, items, fQueryRetries, 0, fLastRetryTimestamp, 520934130.6
Jul  5 00:58:26 calvisitor-10-105-163-10 wirelessproxd[75]: Failed to stop a scan - central is not powered on: 4
Jul  5 00:58:27 calvisitor-10-105-163-10 WindowServer[184]: device_generate_desktop_screenshot: authw 0x7fa82407ea00(2000), shield 0x7fa82435f400(2001)
Jul  5 01:19:55 calvisitor-10-105-163-10 kernel[0]: [HID] [ATC] AppleDeviceManagementHIDEventService::processWakeReason Wake reason: Host (0x01)
Jul  5 01:33:27 calvisitor-10-105-163-10 kernel[0]: en0: channel changed to 1
Jul  5 01:47:03 calvisitor-10-105-163-10 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 7
Jul  5 02:27:53 calvisitor-10-105-163-10 com.apple.AddressBook.InternetAccountsBridge[35749]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  5 02:27:53 calvisitor-10-105-163-10 com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  5 02:27:57 calvisitor-10-105-163-10 com.apple.AddressBook.InternetAccountsBridge[35749]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  5 02:28:22 calvisitor-10-105-163-10 kernel[0]: ARPT: 720406.265686: AirPort_Brcm43xx::powerChange: System Sleep
Jul  5 02:41:24 calvisitor-10-105-163-10 kernel[0]: ARPT: 720413.304306: wl0: setup_keepalive: Remote IP: 17.249.12.81
Jul  5 02:54:51 calvisitor-10-105-163-10 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  5 03:08:26 calvisitor-10-105-163-10 kernel[0]: en0: BSSID changed to 5c:50:15:4c:18:13
Jul  5 03:08:26 calvisitor-10-105-163-10 kernel[0]: en0: BSSID changed to 5c:50:15:4c:18:13
Jul  5 03:08:49 calvisitor-10-105-163-10 sandboxd[129] ([35762]): com.apple.Addres(35762) deny network-outbound /private/var/run/mDNSResponder
Jul  5 03:35:40 calvisitor-10-105-163-10 kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  5 03:35:45 calvisitor-10-105-163-10 kernel[0]: ARPT: 720580.338311: AirPort_Brcm43xx::powerChange: System Sleep
Jul  5 03:49:12 calvisitor-10-105-163-10 kernel[0]: Wake reason: RTC (Alarm)
Jul  5 03:49:24 calvisitor-10-105-163-10 kernel[0]: Sandbox: com.apple.Addres(35773) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  5 03:49:51 calvisitor-10-105-163-10 kernel[0]: PM response took 1988 ms (54, powerd)
Jul  5 03:58:47 calvisitor-10-105-163-10 kernel[0]: Wake reason: EC.SleepTimer (SleepTimer)
Jul  5 03:58:47 calvisitor-10-105-163-10 kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  5 04:12:21 calvisitor-10-105-163-10 syslogd[44]: ASL Sender Statistics
Jul  5 04:12:22 calvisitor-10-105-163-10 kernel[0]: **** [BroadcomBluetoothHostControllerUSBTransport][start] -- Completed (matched on Device) -- 0x6000 ****
Jul  5 10:31:18 calvisitor-10-105-163-10 kernel[0]: hibernate_newruntime_map time: 0 ms, IOPolledFilePollersOpen(), ml_get_interrupts_enabled 0
Jul  5 10:31:18 calvisitor-10-105-163-10 kernel[0]: hibernate_machine_init pagesDone 451550 sum2 4886d9d9, time: 197 ms, disk(0x20000) 799 Mb/s, comp bytes: 40017920 time: 27 ms 1412 Mb/s, crypt bytes: 160550912 time: 39 ms 3904 Mb/s
Jul  5 10:31:18 calvisitor-10-105-163-10 kernel[0]: vm_compressor_fastwake_warmup (581519 - 591525) - starting
Jul  5 10:31:18 calvisitor-10-105-163-10 kernel[0]: BuildActDeviceEntry exit
Jul  5 10:31:19 calvisitor-10-105-163-10 identityservicesd[272]: <IMMacNotificationCenterManager: 0x7ff1b2e00ba0>: Updating enabled: YES   (Topics: ( "com.apple.private.alloy.icloudpairing", "com.apple.private.alloy.continuity.encryption", "com.apple.private.alloy.continuity.activity", "com.apple.ess", "com.apple.private.ids", "com.apple.private.alloy.phonecontinuity", "com.apple.madrid", "com.apple.private.ac", "com.apple.private.alloy.phone.auth", "com.apple.private.alloy.keychainsync", "com.apple.private.alloy.fmf", "com.apple.private.alloy.sms", "com.apple.private.alloy.screensharing", "com.apple.private.alloy.maps", "com.apple.private.alloy.thumper.keys", "com.apple.private.alloy.continuity.tethering" ))
Jul  5 10:31:23 calvisitor-10-105-163-10 kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  5 10:31:24 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  5 10:31:56 calvisitor-10-105-160-179 locationd[82]: Location icon should now be in state 'Active'
Jul  5 10:32:08 calvisitor-10-105-160-179 locationd[82]: Location icon should now be in state 'Inactive'
Jul  5 10:41:38 calvisitor-10-105-160-179 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  5 10:43:32 calvisitor-10-105-160-179 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  5 10:44:16 calvisitor-10-105-160-179 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  5 10:45:00 calvisitor-10-105-160-179 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.35830): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.History.xpc/Contents/MacOS/com.apple.Safari.History error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  5 10:48:06 calvisitor-10-105-160-179 quicklookd[35840]: Error returned from iconservicesagent: (null)
Jul  5 10:52:15 calvisitor-10-105-160-179 GoogleSoftwareUpdateAgent[35861]: 2017-07-05 10:52:15.982 GoogleSoftwareUpdateAgent[35861/0x700000323000] [lvl=2] -[KSOutOfProcessFetcher(PrivateMethods) helperDidTerminate:] KSOutOfProcessFetcher fetch ended for URL: "https://tools.google.com/service/update2?cup2hreq=d297a7e5b56d6bd4faa75860fff6e485c301bf4e943a561afff6c8b3707ce948&cup2key=7:2988503627"
Jul  5 10:52:16 calvisitor-10-105-160-179 ksfetch[35863]: 2017-07-05 10:52:16.779 ksfetch[35863/0x7fff79824000] [lvl=2] main() Fetcher received a request: <NSMutableURLRequest: 0x100501610> { URL: https://tools.google.com/service/update2?cup2hreq=a7873a51b1cb55518e420d20dff47d463781ed3f7aa83c3153129eefb148070b&cup2key=7:2501762722 }
Jul  5 10:52:16 calvisitor-10-105-160-179 GoogleSoftwareUpdateAgent[35861]: 2017-07-05 10:52:16.977 GoogleSoftwareUpdateAgent[35861/0x700000323000] [lvl=2] -[KSPrefetchAction performAction] KSPrefetchAction no updates to prefetch.
Jul  5 10:56:49 calvisitor-10-105-160-179 Safari[9852]: KeychainGetICDPStatus: status: off
Jul  5 10:57:01 calvisitor-10-105-160-179 QQ[10018]: 2017/07/05 10:57:01.130 | I | VoipWrapper  | DAVEngineImpl.cpp:1400:Close             | close video chat. llFriendUIN = 1742124257.
Jul  5 10:57:41 calvisitor-10-105-160-179 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  5 10:58:22 calvisitor-10-105-160-179 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.35873): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.SocialHelper.xpc/Contents/MacOS/com.apple.Safari.SocialHelper error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  5 10:58:44 calvisitor-10-105-160-179 imagent[355]: <IMMacNotificationCenterManager: 0x7fdcc9d16380>: notification observer: com.apple.FaceTime   notification: __CFNotification 0x7fdcc9d19110 {name = _NSDoNotDisturbEnabledNotification}
Jul  5 10:58:44 calvisitor-10-105-160-179 kernel[0]: Setting BTCoex Config: enable_2G:1, profile_2g:0, enable_5G:1, profile_5G:0
Jul  5 11:01:10 authorMacBook-Pro corecaptured[35877]: CCIOReporterFormatter::addRegistryChildToChannelDictionary streams 7
Jul  5 11:01:55 calvisitor-10-105-160-179 corecaptured[35877]: Received Capture Event
Jul  5 11:01:55 calvisitor-10-105-160-179 corecaptured[35877]: CCFile::captureLogRun Skipping current file Dir file [2017-07-05_11,01,55.983179]-AirPortBrcm4360_Logs-006.txt, Current File [2017-07-05_11,01,55.983179]-AirPortBrcm4360_Logs-006.txt
Jul  5 11:39:37 calvisitor-10-105-160-179 symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  5 11:39:38 authorMacBook-Pro corecaptured[35877]: CCProfileMonitor::freeResources done
Jul  5 11:39:39 authorMacBook-Pro corecaptured[35889]: CCFile::copyFile fileName is [2017-07-05_11,39,39.743225]-CCIOReporter-001.xml, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/OneStats//[2017-07-05_11,39,39.743225]-CCIOReporter-001.xml, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-05_11,39,38.739976]=AuthFail:sts:5_rsn:0/OneStats//[2017-07-05_11,39,39.743225]-CCIOReporter-001.xml
Jul  5 11:40:26 calvisitor-10-105-160-226 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  5 11:42:05 calvisitor-10-105-160-226 Mail[11203]: tcp_connection_destination_perform_socket_connect 44073 connectx to 123.125.50.30:143@0 failed: [50] Network is down
Jul  5 11:42:53 calvisitor-10-105-160-226 identityservicesd[272]: <IMMacNotificationCenterManager: 0x7ff1b2d17980>:    NC Disabled: NO
Jul  5 11:42:53 calvisitor-10-105-160-226 identityservicesd[272]: <IMMacNotificationCenterManager: 0x7ff1b2d17980>:   DND Enabled: NO
Jul  5 11:47:49 calvisitor-10-105-160-226 quicklookd[35915]: Error returned from iconservicesagent: (null)
Jul  5 12:00:35 calvisitor-10-105-160-226 WindowServer[184]: CGXDisplayDidWakeNotification [723587602857832]: posting kCGSDisplayDidWake
Jul  5 12:00:39 calvisitor-10-105-160-226 identityservicesd[272]: <IMMacNotificationCenterManager: 0x7ff1b2e00ba0>:   DND Enabled: NO
Jul  5 12:00:50 authorMacBook-Pro networkd[195]: __42-[NETClientConnection evaluateCrazyIvan46]_block_invoke CI46 - Hit by torpedo! QQ.10018 tc22930 125.39.240.34:14000
Jul  5 12:00:58 calvisitor-10-105-160-226 com.apple.AddressBook.InternetAccountsBridge[35944]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  5 12:01:10 calvisitor-10-105-160-226 QQ[10018]: button report: 0x8002bdf
Jul  5 12:02:02 calvisitor-10-105-160-226 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  5 12:05:11 calvisitor-10-105-160-226 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  5 12:35:58 calvisitor-10-105-160-226 kernel[0]: en0: BSSID changed to 88:75:56:a0:95:ed
Jul  5 12:50:34 calvisitor-10-105-160-226 kernel[0]: ARPT: 724509.898718: wl0: MDNS: IPV4 Addr: 10.105.160.226
Jul  5 13:16:51 calvisitor-10-105-160-226 kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  5 13:43:58 calvisitor-10-105-160-226 kernel[0]: en0: BSSID changed to 88:75:56:a0:95:ed
Jul  5 13:44:18 calvisitor-10-105-160-226 com.apple.AddressBook.InternetAccountsBridge[646]: Daemon connection invalidated!
Jul  5 14:03:40 calvisitor-10-105-160-226 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  5 14:03:45 authorMacBook-Pro corecaptured[36034]: CCFile::captureLog Received Capture notice id: 1499288625.645942, reason = AuthFail:sts:5_rsn:0
Jul  5 14:04:12 authorMacBook-Pro corecaptured[36034]: CCFile::captureLog Received Capture notice id: 1499288652.126295, reason = AuthFail:sts:5_rsn:0
Jul  5 14:04:16 authorMacBook-Pro corecaptured[36034]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  5 14:04:19 authorMacBook-Pro corecaptured[36034]: CCFile::copyFile fileName is [2017-07-05_14,04,19.163834]-AirPortBrcm4360_Logs-025.txt, source path:/var/log/CoreCapture/com.apple.driver.AirPort.Brcm4360.0/DriverLogs//[2017-07-05_14,04,19.163834]-AirPortBrcm4360_Logs-025.txt, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.driver.AirPort.Brcm4360.0/[2017-07-05_14,04,19.222771]=AuthFail:sts:5_rsn:0/DriverLogs//[2017-07-05_14,04,19.163834]-AirPortBrcm4360_Logs-025.txt
Jul  5 14:04:19 authorMacBook-Pro corecaptured[36034]: CCFile::captureLog
Jul  5 14:04:19 authorMacBook-Pro kernel[0]: ARPT: 724786.252727: AQM agg results 0x8001 len hi/lo: 0x0 0x26 BAbitmap(0-3) 0 0 0 0
Jul  5 14:04:19 authorMacBook-Pro corecaptured[36034]: CCFile::captureLog Received Capture notice id: 1499288659.752738, reason = AuthFail:sts:5_rsn:0
Jul  5 14:04:25 authorMacBook-Pro corecaptured[36034]: CCFile::captureLogRun Skipping current file Dir file [2017-07-05_14,04,25.118445]-AirPortBrcm4360_Logs-040.txt, Current File [2017-07-05_14,04,25.118445]-AirPortBrcm4360_Logs-040.txt
Jul  5 14:04:25 authorMacBook-Pro corecaptured[36034]: CCFile::copyFile fileName is [2017-07-05_14,04,25.705173]-CCIOReporter-044.xml, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/OneStats//[2017-07-05_14,04,25.705173]-CCIOReporter-044.xml, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-05_14,04,25.777317]=AuthFail:sts:5_rsn:0/OneStats//[2017-07-05_14,04,25.705173]-CCIOReporter-044.xml
Jul  5 14:04:26 authorMacBook-Pro corecaptured[36034]: doSaveChannels@286: Will write to: /Library/Logs/CrashReporter/CoreCapture/IOReporters/[2017-07-05_14,04,24.831957] - AssocFail:sts:5_rsn:0.xml
Jul  5 14:04:27 authorMacBook-Pro corecaptured[36034]: CCFile::captureLog Received Capture notice id: 1499288667.823332, reason = AuthFail:sts:5_rsn:0
Jul  5 14:04:27 authorMacBook-Pro corecaptured[36034]: CCFile::captureLog Received Capture notice id: 1499288667.956536, reason = AuthFail:sts:5_rsn:0
Jul  5 14:04:32 authorMacBook-Pro corecaptured[36034]: CCFile::copyFile fileName is [2017-07-05_14,04,32.840703]-io80211Family-056.pcapng, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/IO80211AWDLPeerManager//[2017-07-05_14,04,32.840703]-io80211Family-056.pcapng, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-05_14,04,32.969558]=AuthFail:sts:5_rsn:0/IO80211AWDLPeerManager//[2017-07-05_14,04,32.840703]-io80211Family-056.pcapng
Jul  5 14:04:32 authorMacBook-Pro corecaptured[36034]: CCFile::captureLog Received Capture notice id: 1499288672.969558, reason = AuthFail:sts:5_rsn:0
Jul  5 14:04:33 authorMacBook-Pro corecaptured[36034]: Received Capture Event
Jul  5 14:04:33 authorMacBook-Pro corecaptured[36034]: CCFile::copyFile fileName is [2017-07-05_14,04,33.660387]-io80211Family-063.pcapng, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/IO80211AWDLPeerManager//[2017-07-05_14,04,33.660387]-io80211Family-063.pcapng, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-05_14,04,33.736169]=AuthFail:sts:5_rsn:0/IO80211AWDLPeerManager//[2017-07-05_14,04,33.660387]-io80211Family-063.pcapng
Jul  5 14:04:35 authorMacBook-Pro corecaptured[36034]: CCFile::captureLog
Jul  5 14:04:35 authorMacBook-Pro corecaptured[36034]: CCIOReporterFormatter::refreshSubscriptionsFromStreamRegistry clearing out any previous subscriptions
Jul  5 14:15:40 authorMacBook-Pro configd[53]: network changed: v4(en0+:10.105.160.226) v6(en0:2607:f140:6000:8:c6b3:1ff:fecd:467f) DNS! Proxy SMB
Jul  5 14:56:09 authorMacBook-Pro kernel[0]: ARPT: 724869.297011: IOPMPowerSource Information: onWake,  SleepType: Normal Sleep,  'ExternalConnected': No, 'TimeRemaining': 4551,
Jul  5 14:56:13 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x8000000000000000 to 0x4
Jul  5 14:56:14 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  5 15:26:36 calvisitor-10-105-162-98 kernel[0]: IOHibernatePollerOpen(0)
Jul  5 15:26:36 calvisitor-10-105-162-98 kernel[0]: hibernate_machine_init: state 2, image pages 446175, sum was d4e6c0b2, imageSize 0x2aea6000, image1Size 0x215a4000, conflictCount 5848, nextFree 5887
Jul  5 15:26:36 calvisitor-10-105-162-98 kernel[0]: [HID] [MT] AppleMultitouchDevice::willTerminate entered
Jul  5 15:40:14 calvisitor-10-105-162-98 kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  5 15:40:14 calvisitor-10-105-162-98 BezelServices 255.10[94]: ASSERTION FAILED: dvcAddrRef != ((void *)0) -[DriverServices getDeviceAddress:] line: 2789
Jul  5 15:55:35 calvisitor-10-105-162-98 kernel[0]: hibernate_rebuild completed - took 10042450943433 msecs
Jul  5 15:55:35 calvisitor-10-105-162-98 kernel[0]: AppleActuatorDevice::stop Entered
Jul  5 15:55:35 calvisitor-10-105-162-98 kernel[0]: AppleActuatorDevice::start Entered
Jul  5 15:56:38 calvisitor-10-105-162-107 kernel[0]: ARPT: 725147.528572: AirPort_Brcm43xx::powerChange: System Sleep
Jul  5 16:04:10 calvisitor-10-105-162-107 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  5 16:04:10 calvisitor-10-105-162-107 kernel[0]: hibernate_page_list_setall found pageCount 488653
Jul  5 16:04:10 calvisitor-10-105-162-107 kernel[0]: bitmap_size 0x7f0fc, previewSize 0x4028, writing 488299 pages @ 0x97144
Jul  5 16:04:15 authorMacBook-Pro kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  5 16:04:43 calvisitor-10-105-162-107 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  5 16:05:05 calvisitor-10-105-162-107 kernel[0]: ARPT: 725209.960568: AQM agg params 0xfc0 maxlen hi/lo 0x0 0xffff minlen 0x0 adjlen 0x0
Jul  5 16:05:05 calvisitor-10-105-162-107 corecaptured[36091]: CCFile::captureLogRun Skipping current file Dir file [2017-07-05_16,05,05.078442]-io80211Family-008.pcapng, Current File [2017-07-05_16,05,05.078442]-io80211Family-008.pcapng
Jul  5 16:05:13 calvisitor-10-105-162-107 kernel[0]: ARPT: 725218.810934: AirPort_Brcm43xx::powerChange: System Sleep
Jul  5 16:05:44 calvisitor-10-105-162-107 kernel[0]: [HID] [MT] AppleMultitouchDevice::willTerminate entered
Jul  5 16:06:03 authorMacBook-Pro corecaptured[36091]: CCFile::captureLog Received Capture notice id: 1499295963.492254, reason = RoamFail:sts:1_rsn:1
Jul  5 16:12:50 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  5 16:12:52 authorMacBook-Pro cdpd[11807]: Saw change in network reachability (isReachable=0)
Jul  5 16:12:56 authorMacBook-Pro kernel[0]: en0: manual intervention required!
Jul  5 16:13:15 calvisitor-10-105-162-107 identityservicesd[272]: <IMMacNotificationCenterManager: 0x7ff1b2d17980>:    NC Disabled: NO
Jul  5 16:13:17 calvisitor-10-105-162-107 QQ[10018]: DB Error: 1 "no such table: tb_c2cMsg_2658655094"
Jul  5 16:18:06 calvisitor-10-105-162-107 GoogleSoftwareUpdateAgent[36128]: 2017-07-05 16:18:06.450 GoogleSoftwareUpdateAgent[36128/0x7000002a0000] [lvl=2] -[KSOutOfProcessFetcher beginFetchWithDelegate:] KSOutOfProcessFetcher fetching from URL: "https://tools.google.com/service/update2?cup2hreq=ac844e04cbb398fcef4cf81b4ffc44a3ebc863e89d19c0b5d39d02d78d26675b&cup2key=7:677488741"
Jul  5 16:19:21 calvisitor-10-105-162-107 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 252, items, fQueryRetries, 0, fLastRetryTimestamp, 520989480.3
Jul  5 16:24:29 authorMacBook-Pro corecaptured[36150]: CCFile::captureLog Received Capture notice id: 1499297069.333005, reason = AuthFail:sts:5_rsn:0
Jul  5 16:24:29 authorMacBook-Pro kernel[0]: ARPT: 725996.598754: framerdy 0x0 bmccmd 3 framecnt 1024
Jul  5 16:24:29 authorMacBook-Pro kernel[0]: ARPT: 725996.811165: AQM agg params 0xfc0 maxlen hi/lo 0x0 0xffff minlen 0x0 adjlen 0x0
Jul  5 16:24:30 authorMacBook-Pro corecaptured[36150]: doSaveChannels@286: Will write to: /Library/Logs/CrashReporter/CoreCapture/IOReporters/[2017-07-05_16,24,29.663045] - AuthFail:sts:5_rsn:0.xml
Jul  5 16:24:44 airbears2-10-142-108-38 QQ[10018]: button report: 0x8002bdf
Jul  5 16:26:28 airbears2-10-142-108-38 corecaptured[36150]: CCLogTap::profileRemoved, Owner: com.apple.iokit.IO80211Family, Name: IO80211AWDLPeerManager
Jul  5 16:33:10 airbears2-10-142-108-38 locationd[82]: Location icon should now be in state 'Inactive'
Jul  5 16:37:58 airbears2-10-142-108-38 syslogd[44]: ASL Sender Statistics
Jul  5 16:47:16 airbears2-10-142-108-38 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  5 16:47:16 airbears2-10-142-108-38 Safari[9852]: KeychainGetICDPStatus: status: off
Jul  5 16:47:58 airbears2-10-142-108-38 locationd[82]: Location icon should now be in state 'Active'
Jul  5 16:58:24 airbears2-10-142-108-38 imagent[355]: <IMMacNotificationCenterManager: 0x7fdcc9d16380>: Updating enabled: NO   (Topics: ( ))
Jul  5 16:58:25 airbears2-10-142-108-38 WindowServer[184]: device_generate_lock_screen_screenshot: authw 0x7fa82502bc00(2000)[0, 0, 0, 0] shield 0x7fa823930c00(2001), dev [1440,900]
Jul  5 16:58:39 airbears2-10-142-108-38 kernel[0]: ARPT: 728046.456828: wl0: setup_keepalive: Local IP: 10.142.108.38
Jul  5 17:01:33 airbears2-10-142-108-38 com.apple.AddressBook.InternetAccountsBridge[36221]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  5 17:02:52 airbears2-10-142-108-38 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  5 17:03:07 airbears2-10-142-108-38 corecaptured[36224]: CCFile::captureLog
Jul  5 17:03:12 airbears2-10-142-108-38 kernel[0]: ARPT: 728173.580097: wl0: setup_keepalive: interval 900, retry_interval 30, retry_count 10
Jul  5 17:03:12 airbears2-10-142-108-38 kernel[0]: ARPT: 728173.580149: wl0: MDNS: IPV4 Addr: 10.142.108.38
Jul  5 17:16:05 authorMacBook-Pro sandboxd[129] ([36227]): com.apple.Addres(36227) deny network-outbound /private/var/run/mDNSResponder
Jul  5 17:22:36 authorMacBook-Pro com.apple.WebKit.WebContent[25654]: [17:22:36.133] <<<< CRABS >>>> crabsFlumeHostAvailable: [0x7f961cf08cf0] Byte flume reports host available again.
Jul  5 17:22:56 calvisitor-10-105-163-9 configd[53]: setting hostname to "calvisitor-10-105-163-9.calvisitor.1918.berkeley.edu"
Jul  5 17:23:22 calvisitor-10-105-163-9 com.apple.AddressBook.InternetAccountsBridge[36248]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  5 17:25:05 calvisitor-10-105-163-9 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  5 17:25:06 authorMacBook-Pro com.apple.geod[30311]: PBRequester failed with Error Error Domain=NSURLErrorDomain Code=-1009 "The Internet connection appears to be offline." UserInfo={NSUnderlyingError=0x7fe13530fc60 {Error Domain=kCFErrorDomainCFNetwork Code=-1009 "The Internet connection appears to be offline." UserInfo={NSErrorFailingURLStringKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, NSErrorFailingURLKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, _kCFStreamErrorCodeKey=8, _kCFStreamErrorDomainKey=12, NSLocalizedDescription=The Internet connection appears to be offline.}}, NSErrorFailingURLStringKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, NSErrorFailingURLKey=https://gsp-ssl.ls.apple.com/dispatcher.arpc, _kCFStreamErrorDomainKey=12, _kCFStreamErrorCodeKey=8, NSLocalizedDescription=The Internet connection appears to be offline.}
Jul  5 17:25:08 authorMacBook-Pro kernel[0]: en0: BSSID changed to 00:a2:ee:1a:71:8c
Jul  5 17:25:08 authorMacBook-Pro kernel[0]: Unexpected payload found for message 9, dataLen 0
Jul  5 17:27:05 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  5 17:27:05 authorMacBook-Pro locationd[82]: Location icon should now be in state 'Inactive'
Jul  5 17:27:08 authorMacBook-Pro configd[53]: network changed: DNS* Proxy
Jul  5 17:27:48 calvisitor-10-105-163-9 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  5 17:39:14 authorMacBook-Pro sandboxd[129] ([10018]): QQ(10018) deny mach-lookup com.apple.networking.captivenetworksupport
Jul  5 18:19:59 authorMacBook-Pro configd[53]: arp_client_transmit(en0) failed, Network is down (50)
Jul  5 18:19:59 authorMacBook-Pro kernel[0]: en0::IO80211Interface::postMessage bssid changed
Jul  5 18:19:59 authorMacBook-Pro kernel[0]: en0: 802.11d country code set to 'X3'.
Jul  5 18:19:59 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Inactive
Jul  5 18:20:06 authorMacBook-Pro networkd[195]: -[NETClientConnection evaluateCrazyIvan46] CI46 - Perform CrazyIvan46! QQ.10018 tc23407 119.81.102.227:80
Jul  5 18:20:07 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 23411 failed: 3 - No network route
Jul  5 19:00:16 authorMacBook-Pro Dropbox[24019]: [0705/190016:WARNING:dns_config_service_posix.cc(306)] Failed to read DnsConfig.
Jul  5 19:00:34 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from SUSPENDED to AUTO
Jul  5 19:00:37 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name CalendarAgent as bundle ID (this is expected for daemons without bundle ID
Jul  5 19:00:42 authorMacBook-Pro corecaptured[36291]: CCFile::captureLog
Jul  5 19:00:43 authorMacBook-Pro corecaptured[36291]: CCFile::captureLogRun Skipping current file Dir file [2017-07-05_19,00,43.154864]-CCIOReporter-007.xml, Current File [2017-07-05_19,00,43.154864]-CCIOReporter-007.xml
Jul  5 19:00:43 authorMacBook-Pro corecaptured[36291]: CCIOReporterFormatter::refreshSubscriptionsFromStreamRegistry clearing out any previous subscriptions
Jul  5 19:00:46 authorMacBook-Pro kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  5 19:12:41 calvisitor-10-105-160-210 kernel[0]: ARPT: 728563.210777: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': No, 'TimeRemaining': 3174,
Jul  5 19:26:27 calvisitor-10-105-160-210 kernel[0]: ARPT: 728620.515604: wl0: MDNS: 0 SRV Recs, 0 TXT Recs
Jul  5 20:03:37 calvisitor-10-105-160-210 kernel[0]: Sandbox: com.apple.Addres(36325) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  5 20:16:55 calvisitor-10-105-160-210 kernel[0]: could discard act 242131 inact 107014 purgeable 254830 spec 128285 cleaned 0
Jul  5 20:16:55 calvisitor-10-105-160-210 kernel[0]: booter start at 1251 ms smc 0 ms, [18, 0, 0] total 367 ms, dsply 0, 0 ms, tramp 1080 ms
Jul  5 20:16:56 calvisitor-10-105-160-210 blued[85]: hostControllerOnline - Number of Paired devices = 1, List of Paired devices = ( "84-41-67-32-db-e1" )
Jul  5 20:17:19 calvisitor-10-105-160-210 sandboxd[129] ([36332]): com.apple.Addres(36332) deny network-outbound /private/var/run/mDNSResponder
Jul  5 20:44:17 calvisitor-10-105-160-210 kernel[0]: hibernate_newruntime_map time: 0 ms, IOPolledFilePollersOpen(), ml_get_interrupts_enabled 0
Jul  5 20:44:24 calvisitor-10-105-160-210 Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-06 03:44:24 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  5 21:08:40 calvisitor-10-105-162-81 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  5 21:48:33 calvisitor-10-105-162-81 kernel[0]: Wake reason: ARPT (Network)
Jul  5 21:48:33 calvisitor-10-105-162-81 blued[85]: [BluetoothHIDDeviceController] EventServiceConnectedCallback
Jul  5 21:48:33 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name apsd as bundle ID (this is expected for daemons without bundle ID
Jul  5 21:48:45 authorMacBook-Pro sandboxd[129] ([10018]): QQ(10018) deny mach-lookup com.apple.networking.captivenetworksupport
Jul  5 22:29:03 calvisitor-10-105-161-231 kernel[0]: pages 1401204, wire 544128, act 416065, inact 0, cleaned 0 spec 3, zf 25, throt 0, compr 266324, xpmapped 40000
Jul  5 22:29:03 calvisitor-10-105-161-231 kernel[0]: could discard act 74490 inact 9782 purgeable 34145 spec 56242 cleaned 0
Jul  5 22:29:06 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name apsd as bundle ID (this is expected for daemons without bundle ID
Jul  5 22:29:35 calvisitor-10-105-160-22 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  5 22:29:37 calvisitor-10-105-160-22 com.apple.AddressBook.InternetAccountsBridge[36395]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  5 23:09:36 calvisitor-10-105-160-22 kernel[0]: bitmap_size 0x7f0fc, previewSize 0x4028, writing 485676 pages @ 0x97144
Jul  5 23:09:36 calvisitor-10-105-160-22 kernel[0]: **** [BroadcomBluetoothHostController][SetupController] -- Delay HCI Reset by 300ms  ****
Jul  5 23:09:53 calvisitor-10-105-160-22 QQ[10018]: FA||Url||taskID[2019353593] dealloc
Jul  5 23:50:09 calvisitor-10-105-160-22 kernel[0]: AppleActuatorHIDEventDriver: stop
Jul  5 23:50:09 calvisitor-10-105-160-22 kernel[0]: **** [IOBluetoothHostControllerUSBTransport][start] -- completed -- result = TRUE -- 0xb000 ****
Jul  5 23:50:09 authorMacBook-Pro kernel[0]: AppleActuatorDeviceUserClient::start Entered
Jul  5 23:50:11 authorMacBook-Pro kernel[0]: ARPT: 729188.852474: AQM agg params 0xfc0 maxlen hi/lo 0x0 0xffff minlen 0x0 adjlen 0x0
Jul  5 23:50:51 calvisitor-10-105-162-211 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 299, items, fQueryRetries, 0, fLastRetryTimestamp, 521006902.2
Jul  6 00:30:35 calvisitor-10-105-162-211 kernel[0]: polled file major 1, minor 0, blocksize 4096, pollers 5
Jul  6 00:30:35 calvisitor-10-105-162-211 kernel[0]: IOHibernatePollerOpen(0)
Jul  6 00:30:35 authorMacBook-Pro UserEventAgent[43]: assertion failed: 15G1510: com.apple.telemetry + 38574 [10D2E324-788C-30CC-A749-55AE67AEC7BC]: 0x7fc235807b90
Jul  6 00:30:37 authorMacBook-Pro ksfetch[36439]: 2017-07-06 00:30:37.064 ksfetch[36439/0x7fff79824000] [lvl=2] main() Fetcher is exiting.
Jul  6 00:30:37 authorMacBook-Pro GoogleSoftwareUpdateAgent[36436]: 2017-07-06 00:30:37.071 GoogleSoftwareUpdateAgent[36436/0x7000002a0000] [lvl=2] -[KSUpdateEngine updateAllExceptProduct:] KSUpdateEngine updating all installed products, except:'com.google.Keystone'.
Jul  6 00:30:43 authorMacBook-Pro com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  6 01:11:06 authorMacBook-Pro Dropbox[24019]: [0706/011106:WARNING:dns_config_service_posix.cc(306)] Failed to read DnsConfig.
Jul  6 01:11:06 authorMacBook-Pro com.apple.WebKit.WebContent[32778]: [01:11:06.715] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  6 01:11:09 authorMacBook-Pro kernel[0]: Google Chrome He[36456] triggered unnest of range 0x7fff93c00000->0x7fff93e00000 of DYLD shared region in VM map 0x77c9114550458e7b. While not abnormal for debuggers, this increases system memory footprint until the target exits.
Jul  6 01:11:18 authorMacBook-Pro configd[53]: network changed: v4(en0+:10.105.162.138) v6(en0:2607:f140:6000:8:c6b3:1ff:fecd:467f) DNS! Proxy SMB
Jul  6 01:11:38 calvisitor-10-105-162-138 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  6 01:11:39 calvisitor-10-105-162-138 sandboxd[129] ([36461]): com.apple.Addres(36461) deny network-outbound /private/var/run/mDNSResponder
Jul  6 01:11:42 calvisitor-10-105-162-138 sandboxd[129] ([36461]): com.apple.Addres(36461) deny network-outbound /private/var/run/mDNSResponder
Jul  6 01:51:35 authorMacBook-Pro wirelessproxd[75]: Peripheral manager is not powered on
Jul  6 01:51:46 authorMacBook-Pro sandboxd[129] ([10018]): QQ(10018) deny mach-lookup com.apple.networking.captivenetworksupport
Jul  6 02:32:06 calvisitor-10-105-163-28 kernel[0]: WARNING: hibernate_page_list_setall skipped 19622 xpmapped pages
Jul  6 02:32:06 calvisitor-10-105-163-28 kernel[0]: BuildActDeviceEntry enter
Jul  6 02:32:06 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 23645 failed: 3 - No network route
Jul  6 02:32:10 authorMacBook-Pro Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-06 09:32:10 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  6 02:32:43 calvisitor-10-105-160-37 com.apple.AddressBook.InternetAccountsBridge[36491]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  6 02:32:44 calvisitor-10-105-160-37 sandboxd[129] ([36491]): com.apple.Addres(36491) deny network-outbound /private/var/run/mDNSResponder
Jul  6 03:12:38 calvisitor-10-105-160-37 kernel[0]: hibernate_alloc_pages act 107794, inact 10088, anon 460, throt 0, spec 58021, wire 572831, wireinit 39927
Jul  6 03:12:43 authorMacBook-Pro BezelServices 255.10[94]: ASSERTION FAILED: dvcAddrRef != ((void *)0) -[DriverServices getDeviceAddress:] line: 2789
Jul  6 03:12:49 authorMacBook-Pro networkd[195]: -[NETClientConnection evaluateCrazyIvan46] CI46 - Perform CrazyIvan46! QQ.10018 tc23677 123.151.137.101:80
Jul  6 03:13:09 calvisitor-10-105-160-37 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from SUSPENDED to AUTO
Jul  6 03:13:15 calvisitor-10-105-160-37 com.apple.AddressBook.InternetAccountsBridge[36502]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  6 03:53:08 calvisitor-10-105-160-37 kernel[0]: hibernate_page_list_setall(preflight 1) start
Jul  6 03:53:08 calvisitor-10-105-160-37 kernel[0]: hibernate_page_list_setall time: 603 ms
Jul  6 03:53:08 calvisitor-10-105-160-37 kernel[0]: IOPolledFilePollersOpen(0) 6 ms
Jul  6 03:53:08 calvisitor-10-105-160-37 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  6 03:53:18 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 23701 failed: 3 - No network route
Jul  6 04:33:35 calvisitor-10-105-160-37 kernel[0]: polled file major 1, minor 0, blocksize 4096, pollers 5
Jul  6 04:33:35 calvisitor-10-105-160-37 kernel[0]: hibernate_teardown completed - discarded 93932
Jul  6 04:33:35 calvisitor-10-105-160-37 kernel[0]: AppleActuatorDeviceUserClient::stop Entered
Jul  6 05:04:00 calvisitor-10-105-163-168 kernel[0]: pages 1418325, wire 548641, act 438090, inact 2, cleaned 0 spec 12, zf 30, throt 0, compr 254881, xpmapped 40000
Jul  6 05:04:00 calvisitor-10-105-163-168 kernel[0]: Opened file /var/vm/sleepimage, size 1073741824, extents 3, maxio 2000000 ssd 1
Jul  6 05:04:00 calvisitor-10-105-163-168 kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  6 05:04:00 calvisitor-10-105-163-168 kernel[0]: [IOBluetoothHostController::setConfigState] calling registerService
Jul  6 08:32:37 authorMacBook-Pro kernel[0]: AppleActuatorHIDEventDriver: stop
Jul  6 08:32:37 authorMacBook-Pro kernel[0]: [HID] [ATC] AppleDeviceManagementHIDEventService::processWakeReason Wake reason: Host (0x01)
Jul  6 08:32:37 authorMacBook-Pro kernel[0]: [HID] [MT] AppleMultitouchDevice::start entered
Jul  6 08:32:39 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name CalendarAgent as bundle ID (this is expected for daemons without bundle ID
Jul  6 08:32:40 authorMacBook-Pro AddressBookSourceSync[36544]: [CardDAVPlugin-ERROR] -getPrincipalInfo:[_controller supportsRequestCompressionAtURL:https://13957525385%40163.com@p28-contacts.icloud.com/874161398/principal/] Error Domain=NSURLErrorDomain Code=-1009 "The Internet connection appears to be offline." UserInfo={NSUnderlyingError=0x7f8de0c0dc70 {Error Domain=kCFErrorDomainCFNetwork Code=-1009 "The Internet connection appears to be offline." UserInfo={NSErrorFailingURLStringKey=https://13957525385%40163.com@p28-contacts.icloud.com/874161398/principal/, NSErrorFailingURLKey=https://13957525385%40163.com@p28-contacts.icloud.com/874161398/principal/, _kCFStreamErrorCodeKey=8, _kCFStreamErrorDomainKey=12, NSLocalizedDescription=The Internet connection appears to be offline.}}, NSErrorFailingURLStringKey=https://13957525385%40163.com@p28-contacts.icloud.com/874161398/principal/, NSErrorFailingURLKey=https://13957525385%40163.com@p28-contacts.icloud.com/874161398/principal/, _kCFStreamErrorDomainKey=12, _kCFStreamErrorCodeKey=8, NSLocalizedDescription=The Internet connection appears to be offline.}
Jul  6 08:32:45 authorMacBook-Pro netbiosd[36551]: Unable to start NetBIOS name service:
Jul  6 08:32:47 authorMacBook-Pro WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  6 08:33:10 calvisitor-10-105-163-253 corecaptured[36565]: doSaveChannels@286: Will write to: /Library/Logs/CrashReporter/CoreCapture/IOReporters/[2017-07-06_08,33,08.335034] - AuthFail:sts:5_rsn:0.xml
Jul  6 08:33:25 calvisitor-10-105-163-253 TCIM[30318]: [Accounts] Failed to update account with identifier 76FE6715-3D27-4F21-AA35-C88C1EA820E8, error: Error Domain=ABAddressBookErrorDomain Code=1002 "(null)"
Jul  6 08:33:53 calvisitor-10-105-163-253 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  6 08:35:08 calvisitor-10-105-163-253 corecaptured[36565]: Got an XPC error: Connection invalid
Jul  6 08:43:14 calvisitor-10-105-163-253 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 302, items, fQueryRetries, 0, fLastRetryTimestamp, 521048292.6
Jul  6 08:53:11 calvisitor-10-105-163-253 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 291, items, fQueryRetries, 0, fLastRetryTimestamp, 521048893.3
Jul  6 09:08:22 calvisitor-10-105-163-253 loginwindow[94]: -[SFLListManager(ServiceReplyProtocol) notifyChanges:toListWithIdentifier:] Notified of item changes to list with identifier com.apple.LSSharedFileList.RecentApplications
Jul  6 09:14:27 calvisitor-10-105-163-253 QQ[10018]: FA||Url||taskID[2019353614] dealloc
Jul  6 09:24:14 calvisitor-10-105-163-253 ksfetch[36728]: 2017-07-06 09:24:14.417 ksfetch[36728/0x7fff79824000] [lvl=2] main() ksfetch fetching URL (<NSMutableURLRequest: 0x100602220> { URL: https://tools.google.com/service/update2?cup2hreq=f5e83ec64ff3fc5533a3c206134a6517e274f9e1cb53df857e15049b6e4c9f8e&cup2key=7:1721929288 }) to folder:/tmp/KSOutOfProcessFetcher.aPWod5QMh1/download
Jul  6 09:24:14 calvisitor-10-105-163-253 GoogleSoftwareUpdateAgent[36726]: 2017-07-06 09:24:14.733 GoogleSoftwareUpdateAgent[36726/0x7000002a0000] [lvl=2] -[KSMultiUpdateAction performAction] KSPromptAction had no updates to apply.
Jul  6 09:31:18 calvisitor-10-105-163-253 WindowServer[184]: no sleep images for WillPowerOffWithImages
Jul  6 09:32:24 calvisitor-10-105-163-253 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  6 09:32:24 calvisitor-10-105-163-253 QQ[10018]: button report: 0x8002be0
Jul  6 09:32:37 calvisitor-10-105-163-253 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 12754 seconds.  Ignoring.
Jul  6 10:12:58 calvisitor-10-105-163-253 ChromeExistion[36773]: ChromeExistion main isUndetectWithCommand = 1
Jul  6 10:13:12 calvisitor-10-105-163-253 ChromeExistion[36775]: after trim url = https://www.google.com/_/chrome/newtab?rlz=1C5CHFA_enHK732HK732&espv=2&ie=UTF-8
Jul  6 10:13:16 calvisitor-10-105-163-253 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  6 10:17:44 calvisitor-10-105-163-253 ChromeExistion[36801]: ChromeExistion main strSendMsg = {"websitekey":false,"commandkey":true,"browserkey":true}
Jul  6 10:52:03 calvisitor-10-105-163-253 locationd[82]: Location icon should now be in state 'Active'
Jul  6 10:52:20 calvisitor-10-105-163-253 ChromeExistion[36846]: ChromeExistion main isUndetectWithCommand = 1
Jul  6 10:52:50 calvisitor-10-105-163-253 ChromeExistion[36852]: url host = www.baidu.com
Jul  6 10:53:30 calvisitor-10-105-163-253 ChromeExistion[36855]: the url = http://baike.baidu.com/item/%E8%93%9D%E9%87%87%E5%92%8C/462624?fr=aladdin
Jul  6 11:07:29 calvisitor-10-105-163-253 sharingd[30299]: 11:07:29.673 : Purged contact hashes
Jul  6 11:08:42 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  6 11:14:33 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name apsd as bundle ID (this is expected for daemons without bundle ID
Jul  6 11:21:02 calvisitor-10-105-163-253 kernel[0]: ARPT: 739017.747240: ARPT: Wake Reason: Wake on Scan offload
Jul  6 11:21:02 calvisitor-10-105-163-253 kernel[0]: en0: channel changed to 1
Jul  6 11:21:02 calvisitor-10-105-163-253 kernel[0]: AirPort: Link Up on awdl0
Jul  6 11:21:06 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name apsd as bundle ID (this is expected for daemons without bundle ID
Jul  6 11:21:06 authorMacBook-Pro configd[53]: network changed: DNS* Proxy
Jul  6 11:59:42 calvisitor-10-105-162-178 kernel[0]: en0: channel changed to 36,+1
Jul  6 12:00:10 calvisitor-10-105-162-178 sandboxd[129] ([36919]): com.apple.Addres(36919) deny network-outbound /private/var/run/mDNSResponder
Jul  6 12:00:53 authorMacBook-Pro CalendarAgent[279]: [com.apple.calendar.store.log.caldav.queue] [Adding [<CalDAVAccountRefreshQueueableOperation: 0x7fa11d67f3b0; Sequence: 0>] to failed operations.]
Jul  6 12:00:53 authorMacBook-Pro corecaptured[36918]: Received Capture Event
Jul  6 12:00:58 authorMacBook-Pro corecaptured[36918]: CCFile::captureLogRun Skipping current file Dir file [2017-07-06_12,00,58.629029]-AirPortBrcm4360_Logs-007.txt, Current File [2017-07-06_12,00,58.629029]-AirPortBrcm4360_Logs-007.txt
Jul  6 12:01:10 authorMacBook-Pro kernel[0]: ARPT: 739157.867474: wlc_dump_aggfifo:
Jul  6 12:01:16 authorMacBook-Pro corecaptured[36918]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  6 12:01:16 authorMacBook-Pro kernel[0]: ARPT: 739163.832381: AQM agg results 0x8001 len hi/lo: 0x0 0x26 BAbitmap(0-3) 0 0 0 0
Jul  6 12:01:17 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  6 12:02:25 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x4 to 0x8000000000000000
Jul  6 12:02:26 authorMacBook-Pro configd[53]: network changed: v6(en0-:2607:f140:6000:8:c6b3:1ff:fecd:467f) DNS- Proxy-
Jul  6 12:02:58 authorMacBook-Pro corecaptured[36918]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  6 12:03:00 authorMacBook-Pro corecaptured[36918]: CCFile::captureLogRun Skipping current file Dir file [2017-07-06_12,03,00.133649]-CCIOReporter-028.xml, Current File [2017-07-06_12,03,00.133649]-CCIOReporter-028.xml
Jul  6 12:03:00 authorMacBook-Pro kernel[0]: ARPT: 739241.687186: AQM agg results 0x8001 len hi/lo: 0x0 0x26 BAbitmap(0-3) 0 0 0 0
Jul  6 12:03:00 authorMacBook-Pro corecaptured[36918]: CCIOReporterFormatter::addRegistryChildToChannelDictionary streams 7
Jul  6 12:03:02 authorMacBook-Pro corecaptured[36918]: CCFile::captureLogRun Skipping current file Dir file [2017-07-06_12,03,02.449748]-AirPortBrcm4360_Logs-037.txt, Current File [2017-07-06_12,03,02.449748]-AirPortBrcm4360_Logs-037.txt
Jul  6 12:03:05 authorMacBook-Pro corecaptured[36918]: CCFile::captureLog Received Capture notice id: 1499367785.097211, reason = AuthFail:sts:5_rsn:0
Jul  6 12:03:07 authorMacBook-Pro com.apple.AddressBook.InternetAccountsBridge[36937]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  6 12:03:12 authorMacBook-Pro networkd[195]: __42-[NETClientConnection evaluateCrazyIvan46]_block_invoke CI46 - Hit by torpedo! QQ.10018 tc24039 112.90.78.169:8080
Jul  6 12:03:12 authorMacBook-Pro Dropbox[24019]: [0706/120312:WARNING:dns_config_service_posix.cc(306)] Failed to read DnsConfig.
Jul  6 12:03:12 authorMacBook-Pro kernel[0]: ARPT: 739253.582611: wlc_dump_aggfifo:
Jul  6 12:03:12 authorMacBook-Pro corecaptured[36918]: CCFile::captureLog Received Capture notice id: 1499367792.155080, reason = AuthFail:sts:5_rsn:0
Jul  6 12:03:12 authorMacBook-Pro corecaptured[36918]: CCFile::captureLogRun Skipping current file Dir file [2017-07-06_12,03,12.289973]-CCIOReporter-041.xml, Current File [2017-07-06_12,03,12.289973]-CCIOReporter-041.xml
Jul  6 12:03:37 authorMacBook-Pro kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - retries = 5
Jul  6 12:04:03 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  6 12:04:16 authorMacBook-Pro kernel[0]: ARPT: 739295.354731: wl0: Roamed or switched channel, reason #4, bssid f8:4f:57:3b:ea:b2, last RSSI -77
Jul  6 12:04:16 authorMacBook-Pro kernel[0]: en0: BSSID changed to f8:4f:57:3b:ea:b2
Jul  6 12:05:06 authorMacBook-Pro kernel[0]: in6_unlink_ifa: IPv6 address 0x77c911454f1523ab has no prefix
Jul  6 12:05:41 calvisitor-10-105-162-178 Safari[9852]: tcp_connection_tls_session_error_callback_imp 2375 __tcp_connection_tls_session_callback_write_block_invoke.434 error 22
Jul  6 12:05:46 calvisitor-10-105-162-178 kernel[0]: ARPT: 739357.156234: wl0: setup_keepalive: Seq: 1852166454, Ack: 1229910694, Win size: 4096
Jul  6 12:17:22 calvisitor-10-105-162-178 kernel[0]: ARPT: 739359.663674: wl0: leaveModulePoweredForOffloads: Wi-Fi will stay on.
Jul  6 12:17:22 calvisitor-10-105-162-178 kernel[0]: en0: BSSID changed to 88:75:56:a0:95:ed
Jul  6 12:30:59 calvisitor-10-105-162-178 kernel[0]: ARPT: 739420.595498: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  6 12:31:58 calvisitor-10-105-162-178 kernel[0]: ARPT: 739481.576701: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:c6b3:1ff:fecd:467f
Jul  6 12:58:15 calvisitor-10-105-162-178 kernel[0]: ARPT: 739549.504820: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  6 12:59:11 calvisitor-10-105-162-178 kernel[0]: ARPT: 739605.015490: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:cc53:3e31:ccd8:11d4
Jul  6 13:11:53 calvisitor-10-105-162-178 kernel[0]: Previous sleep cause: 5
Jul  6 13:12:52 calvisitor-10-105-162-178 kernel[0]: ARPT: 739668.778627: AirPort_Brcm43xx::powerChange: System Sleep
Jul  6 13:39:08 calvisitor-10-105-162-178 Mail[11203]: tcp_connection_destination_perform_socket_connect 45238 connectx to 123.125.50.30:993@0 failed: [50] Network is down
Jul  6 14:07:50 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 24150 failed: 3 - No network route
Jul  6 14:08:48 authorMacBook-Pro kernel[0]: hibernate_rebuild started
Jul  6 14:08:49 authorMacBook-Pro blued[85]: INIT -- Host controller is published
Jul  6 14:09:31 authorMacBook-Pro corecaptured[37027]: Received Capture Event
Jul  6 14:09:32 authorMacBook-Pro corecaptured[37027]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  6 14:15:54 authorMacBook-Pro CalendarAgent[279]: [com.apple.calendar.store.log.caldav.queue] [Adding [<CalDAVAccountRefreshQueueableOperation: 0x7fa11d67f3b0; Sequence: 0>] to failed operations.]
Jul  6 14:15:59 authorMacBook-Pro corecaptured[37027]: Received Capture Event
Jul  6 14:15:59 authorMacBook-Pro corecaptured[37027]: CCFile::captureLogRun Skipping current file Dir file [2017-07-06_14,15,59.644892]-CCIOReporter-007.xml, Current File [2017-07-06_14,15,59.644892]-CCIOReporter-007.xml
Jul  6 14:15:59 authorMacBook-Pro corecaptured[37027]: CCFile::captureLogRun Skipping current file Dir file [2017-07-06_14,15,59.726497]-io80211Family-008.pcapng, Current File [2017-07-06_14,15,59.726497]-io80211Family-008.pcapng
Jul  6 14:16:06 calvisitor-10-105-162-178 sharingd[30299]: 14:16:06.179 : Scanning mode Contacts Only
Jul  6 14:16:10 calvisitor-10-105-162-178 kernel[0]: Sandbox: com.apple.Addres(37034) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  6 14:17:00 calvisitor-10-105-162-178 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.user.501): Service "com.apple.xpc.launchd.unmanaged.loginwindow.94" tried to hijack endpoint "com.apple.tsm.uiserver" from owner: com.apple.SystemUIServer.agent
Jul  6 14:17:14 calvisitor-10-105-162-178 kernel[0]: ARPT: 740034.911245: wl0: setup_keepalive: interval 900, retry_interval 30, retry_count 10
Jul  6 14:29:38 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  6 14:43:10 calvisitor-10-105-162-178 kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  6 14:43:22 calvisitor-10-105-162-178 com.apple.AddressBook.InternetAccountsBridge[37051]: dnssd_clientstub ConnectToServer: connect() failed path:/var/run/mDNSResponder Socket:4 Err:-1 Errno:1 Operation not permitted
Jul  6 14:44:08 calvisitor-10-105-162-178 kernel[0]: PM response took 1999 ms (54, powerd)
Jul  6 14:56:48 calvisitor-10-105-162-178 kernel[0]: en0: channel changed to 36,+1
Jul  6 14:56:48 calvisitor-10-105-162-178 kernel[0]: RTC: Maintenance 2017/7/6 21:56:47, sleep 2017/7/6 21:44:10
Jul  6 14:56:48 calvisitor-10-105-162-178 kernel[0]: Previous sleep cause: 5
Jul  6 14:57:16 calvisitor-10-105-162-178 ksfetch[37061]: 2017-07-06 14:57:16.261 ksfetch[37061/0x7fff79824000] [lvl=2] KSHelperReceiveAllData() KSHelperTool read 1999 bytes from stdin.
Jul  6 14:57:16 calvisitor-10-105-162-178 GoogleSoftwareUpdateAgent[37059]: 2017-07-06 14:57:16.661 GoogleSoftwareUpdateAgent[37059/0x7000002a0000] [lvl=2] -[KSOutOfProcessFetcher(PrivateMethods) helperDidTerminate:] KSOutOfProcessFetcher fetch ended for URL: "https://tools.google.com/service/update2?cup2hreq=37fbcb7ab6829be04567976e3212d7a67627aef11546f8b7013d4cffaf51f739&cup2key=7:4200177539"
Jul  6 14:57:16 calvisitor-10-105-162-178 GoogleSoftwareUpdateAgent[37059]: 2017-07-06 14:57:16.667 GoogleSoftwareUpdateAgent[37059/0x7000002a0000] [lvl=2] -[KSAgentApp(KeystoneDelegate) updateEngineFinishedWithErrors:] Keystone finished: errors=0
Jul  6 15:10:30 calvisitor-10-105-162-178 kernel[0]: ARPT: 740174.102665: wl0: MDNS: 0 SRV Recs, 0 TXT Recs
Jul  6 15:24:08 calvisitor-10-105-162-178 com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  6 15:30:53 calvisitor-10-105-162-178 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 1 us
Jul  6 15:30:54 calvisitor-10-105-162-178 kernel[0]: ARPT: 740226.968934: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  6 15:45:08 calvisitor-10-105-162-178 kernel[0]: ARPT: 740345.497593: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  6 15:45:08 calvisitor-10-105-162-178 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  6 15:45:08 calvisitor-10-105-162-178 kernel[0]: in6_unlink_ifa: IPv6 address 0x77c911454f152b8b has no prefix
Jul  6 15:58:40 calvisitor-10-105-162-178 kernel[0]: ARPT: 740352.115529: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  6 16:12:15 calvisitor-10-105-162-178 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 1 us
Jul  6 16:12:15 calvisitor-10-105-162-178 kernel[0]: Previous sleep cause: 5
Jul  6 16:12:15 authorMacBook-Pro Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-06 23:12:15 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  6 16:12:20 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  6 16:15:50 calvisitor-10-105-162-178 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  6 16:16:11 calvisitor-10-105-162-178 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  6 16:17:36 authorMacBook-Pro kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  6 16:17:39 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  6 16:18:17 calvisitor-10-105-162-178 kernel[0]: ARPT: 740501.982555: wl0: MDNS: IPV6 Addr: fe80:0:0:0:c6b3:1ff:fecd:467f
Jul  6 16:29:37 calvisitor-10-105-162-178 kernel[0]: ARPT: 740504.547655: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  6 16:29:37 calvisitor-10-105-162-178 kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  6 16:29:37 authorMacBook-Pro networkd[195]: __42-[NETClientConnection evaluateCrazyIvan46]_block_invoke CI46 - Hit by torpedo! QQ.10018 tc24283 119.81.102.227:80
Jul  6 16:29:42 authorMacBook-Pro symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  6 16:29:42 authorMacBook-Pro corecaptured[37102]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  6 16:29:43 authorMacBook-Pro corecaptured[37102]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  6 16:29:48 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x8000000000000000 to 0x4
Jul  6 16:29:58 calvisitor-10-105-162-178 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  6 16:43:27 calvisitor-10-105-162-178 kernel[0]: in6_unlink_ifa: IPv6 address 0x77c911453a6db3ab has no prefix
Jul  6 16:43:37 calvisitor-10-105-162-178 com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  6 17:23:42 calvisitor-10-105-162-178 kernel[0]: ARPT: 740631.402908: IOPMPowerSource Information: onWake,  SleepType: Normal Sleep,  'ExternalConnected': No, 'TimeRemaining': 5802,
Jul  6 17:23:43 calvisitor-10-105-162-178 sharingd[30299]: 17:23:43.193 : Scanning mode Contacts Only
Jul  6 17:23:46 calvisitor-10-105-162-178 QQ[10018]: DB Error: 1 "no such table: tb_c2cMsg_2658655094"
Jul  6 17:28:48 calvisitor-10-105-162-178 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.37146): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.History.xpc/Contents/MacOS/com.apple.Safari.History error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  6 17:34:52 calvisitor-10-105-162-178 QQ[10018]: FA||Url||taskID[2019353659] dealloc
Jul  6 17:48:02 calvisitor-10-105-162-178 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 302, items, fQueryRetries, 0, fLastRetryTimestamp, 521080983.6
Jul  6 17:57:59 calvisitor-10-105-162-178 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 300, items, fQueryRetries, 0, fLastRetryTimestamp, 521081581.2
Jul  6 18:05:46 calvisitor-10-105-162-178 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  6 18:07:04 calvisitor-10-105-162-178 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f995050b170>.
Jul  6 18:09:02 calvisitor-10-105-162-178 kernel[0]: Sandbox: com.apple.WebKit(9854) deny(1) file-read-data /private/etc/hosts
Jul  6 18:09:43 calvisitor-10-105-162-178 QQ[10018]: FA||Url||taskID[2019353666] dealloc
Jul  6 18:22:57 calvisitor-10-105-162-178 AirPlayUIAgent[415]: 2017-07-06 06:22:57.163367 PM [AirPlayUIAgent] BecomingInactive: NSWorkspaceWillSleepNotification
Jul  6 18:22:57 calvisitor-10-105-162-178 QQ[10018]: 2017/07/06 18:22:57.953 | I | VoipWrapper  | DAVEngineImpl.cpp:1400:Close             | close video chat. llFriendUIN = 1742124257.
Jul  6 18:23:11 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.SoftwareUpdate.Activity: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 7518 seconds.  Ignoring.
Jul  6 18:23:43 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 258 seconds.  Ignoring.
Jul  6 18:36:51 calvisitor-10-105-162-178 kernel[0]: [HID] [ATC] AppleDeviceManagementHIDEventService::processWakeReason Wake reason: Host (0x01)
Jul  6 18:37:07 calvisitor-10-105-162-178 kernel[0]: Sandbox: com.apple.Addres(37204) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  6 18:37:20 calvisitor-10-105-162-178 kernel[0]: full wake request (reason 2) 30914 ms
Jul  6 18:37:30 calvisitor-10-105-162-178 sharingd[30299]: 18:37:30.916 : BTLE scanner Powered Off
Jul  6 18:37:37 calvisitor-10-105-162-178 QQ[10018]: ############################## _getSysMsgList
Jul  6 18:50:29 calvisitor-10-105-162-178 kernel[0]: ARPT: 744319.484045: wl0: wl_update_tcpkeep_seq: Original Seq: 1092633597, Ack: 2572586285, Win size: 4096
Jul  6 18:50:29 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 65594 seconds.  Ignoring.
Jul  6 18:50:29 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.SoftwareUpdate.Activity: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 5880 seconds.  Ignoring.
Jul  6 18:50:29 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 16680 seconds.  Ignoring.
Jul  6 18:51:42 calvisitor-10-105-162-178 kernel[0]: kern_open_file_for_direct_io(0)
Jul  6 19:04:07 calvisitor-10-105-162-178 QQ[10018]: ############################## _getSysMsgList
Jul  6 19:04:07 calvisitor-10-105-162-178 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  6 19:04:46 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 15823 seconds.  Ignoring.
Jul  6 19:17:46 calvisitor-10-105-162-178 kernel[0]: Wake reason: RTC (Alarm)
Jul  6 19:17:46 calvisitor-10-105-162-178 sharingd[30299]: 19:17:46.405 : BTLE scanner Powered Off
Jul  6 19:17:46 calvisitor-10-105-162-178 kernel[0]: ARPT: 744472.877165: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  6 19:18:27 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 15002 seconds.  Ignoring.
Jul  6 19:19:43 calvisitor-10-105-162-178 kernel[0]: ARPT: 744589.508896: wl0: setup_keepalive: Remote IP: 17.249.28.35
Jul  6 19:31:29 calvisitor-10-105-162-178 kernel[0]: ARPT: 744593.160590: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq 1529267953, Ack 4054195714, Win size 380
Jul  6 19:31:29 calvisitor-10-105-162-178 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  6 19:31:29 calvisitor-10-105-162-178 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  6 19:31:29 calvisitor-10-105-162-178 kernel[0]: ARPT: 744595.284508: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  6 19:31:39 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 524667 seconds.  Ignoring.
Jul  6 19:31:41 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 63122 seconds.  Ignoring.
Jul  6 19:31:49 calvisitor-10-105-162-178 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  6 19:31:49 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 524657 seconds.  Ignoring.
Jul  6 19:32:24 calvisitor-10-105-162-178 kernel[0]: ARPT: 744650.016431: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:f882:21d2:d1af:f093
Jul  6 19:45:06 calvisitor-10-105-162-178 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  6 19:45:16 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.SoftwareUpdate.Activity: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 2593 seconds.  Ignoring.
Jul  6 19:45:26 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 523840 seconds.  Ignoring.
Jul  6 19:45:43 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 62280 seconds.  Ignoring.
Jul  6 19:58:57 calvisitor-10-105-162-178 kernel[0]: PM response took 129 ms (10018, QQ)
Jul  6 20:03:04 calvisitor-10-105-162-178 kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  6 20:03:05 calvisitor-10-105-162-178 sharingd[30299]: 20:03:05.179 : BTLE scanner Powered Off
Jul  6 20:03:05 calvisitor-10-105-162-178 QQ[10018]: button report: 0x80039B7
Jul  6 20:03:23 calvisitor-10-105-162-178 com.apple.AddressBook.InternetAccountsBridge[37261]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  6 20:04:04 calvisitor-10-105-162-178 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  6 20:05:04 calvisitor-10-105-162-178 WeChat[24144]: jemmytest
Jul  6 20:05:07 calvisitor-10-105-162-178 Safari[9852]: KeychainGetICDPStatus: status: off
Jul  6 20:05:36 calvisitor-10-105-162-178 CalendarAgent[279]: [com.apple.calendar.store.log.caldav.queue] [Adding [<CalDAVAccountRefreshQueueableOperation: 0x7fa11d73f960; Sequence: 0>] to failed operations.]
Jul  6 20:14:33 calvisitor-10-105-162-178 QQ[10018]: FA||Url||taskID[2019353677] dealloc
Jul  6 20:30:35 calvisitor-10-105-162-178 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  6 20:35:33 calvisitor-10-105-162-178 WeChat[24144]: jemmytest
Jul  6 20:38:15 calvisitor-10-105-162-178 locationd[82]: Location icon should now be in state 'Active'
Jul  6 20:46:03 calvisitor-10-105-162-178 GoogleSoftwareUpdateAgent[37316]: 2017-07-06 20:46:03.869 GoogleSoftwareUpdateAgent[37316/0x7000002a0000] [lvl=2] -[KSOutOfProcessFetcher beginFetchWithDelegate:] KSOutOfProcessFetcher start fetch from URL: "https://tools.google.com/service/update2?cup2hreq=5e15fbe422c816bef7c133cfffdb516e16923579b9be2dfae4d7d8d211b25017&cup2key=7:780377214"
Jul  6 20:53:41 calvisitor-10-105-162-178 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 291, items, fQueryRetries, 0, fLastRetryTimestamp, 521092126.3
Jul  6 21:00:02 calvisitor-10-105-162-178 locationd[82]: NETWORK: no response from server, reachability, 2, queryRetries, 2
Jul  6 21:03:09 calvisitor-10-105-162-178 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  6 21:08:02 calvisitor-10-105-162-178 Preview[11512]: Page bounds {{0, 0}, {400, 400}}
Jul  6 21:22:31 calvisitor-10-105-162-178 WeChat[24144]: Failed to connect (titleField) outlet from (MMSessionPickerChoosenRowView) to (NSTextField): missing setter or instance variable
Jul  6 22:05:30 calvisitor-10-105-162-178 CalendarAgent[279]: [com.apple.calendar.store.log.caldav.queue] [Account xpc_ben@163.com@https://caldav.163.com/caldav/principals/users/xpc_ben%40163.com/ timed out when executing operation: <CalDAVAccountRefreshQueueableOperation: 0x7fa11f9f2290; Sequence: 0>]
Jul  6 22:26:19 calvisitor-10-105-162-178 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  6 22:33:18 calvisitor-10-105-162-178 CalendarAgent[279]: [com.apple.calendar.store.log.caldav.queue] [Account refresh failed with error: Error Domain=CoreDAVHTTPStatusErrorDomain Code=502 "(null)" UserInfo={AccountName=163, CalDAVErrFromRefresh=YES, CoreDAVHTTPHeaders=<CFBasicHash 0x7fa11fe7a810 [0x7fff7abc1440]>{type = immutable dict, count = 5, entries => 0 : Connection = <CFString 0x7fff7ab1aea0 [0x7fff7abc1440]>{contents = "keep-alive"} 3 : Content-Type = text/html 4 : Content-Length = 166 5 : Server = nginx 6 : Date = <CFString 0x7fa11ac2ab80 [0x7fff7abc1440]>{contents = "Fri, 07 Jul 2017 05:32:43 GMT"} } }]
Jul  6 22:37:40 calvisitor-10-105-162-178 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  6 23:00:45 calvisitor-10-105-162-178 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  6 23:28:39 calvisitor-10-105-162-178 locationd[82]: Location icon should now be in state 'Inactive'
Jul  6 23:33:20 calvisitor-10-105-162-178 Preview[11512]: Page bounds {{0, 0}, {400, 400}}
Jul  6 23:33:55 calvisitor-10-105-162-178 SpotlightNetHelper[352]: CFPasteboardRef CFPasteboardCreate(CFAllocatorRef, CFStringRef) : failed to create global data
Jul  7 00:01:59 calvisitor-10-105-162-178 kernel[0]: Sandbox: SpotlightNetHelp(352) deny(1) ipc-posix-shm-read-data CFPBS:186A7:
Jul  7 00:02:27 calvisitor-10-105-162-178 Safari[9852]: tcp_connection_tls_session_error_callback_imp 2438 __tcp_connection_tls_session_callback_write_block_invoke.434 error 22
Jul  7 00:09:34 calvisitor-10-105-162-178 QQ[10018]: FA||Url||taskID[2019353724] dealloc
Jul  7 00:10:01 calvisitor-10-105-162-178 taskgated[273]: no application identifier provided, can't use provisioning profiles [pid=37563]
Jul  7 00:12:09 calvisitor-10-105-162-178 SpotlightNetHelper[352]: tcp_connection_destination_handle_tls_close_notify 111 closing socket due to TLS CLOSE_NOTIFY alert
Jul  7 00:24:41 calvisitor-10-105-162-178 com.apple.WebKit.Networking[9854]: CFNetwork SSLHandshake failed (-9802)
Jul  7 00:26:36 calvisitor-10-105-162-178 Preview[11512]: Unable to simultaneously satisfy constraints: ( "<NSLayoutConstraint:0x7f8efa7db140 H:[NSImageView:0x7f8efa7db900(38)]>", "<NSLayoutConstraint:0x7f8efa7cbe00 H:|-(14)-[NSImageView:0x7f8efa7db900]   (Names: PageItemCell:0x7f8efa7cb450, '|':PageItemCell:0x7f8efa7cb450 )>", "<NSLayoutConstraint:0x7f8f17556f20 'NSView-Encapsulated-Layout-Width' H:[PageItemCell(73)]   (Names: PageItemCell:0x7f8efa7cb450 )>", "<NSLayoutConstraint:0x7f8efa7dc1c0 H:[NSImageView:0x7f8efa7db900]-(10)-[NSTextField:0x7f8efa7da9a0]>", "<NSLayoutConstraint:0x7f8efa7cbf70 H:[NSTextField:0x7f8f02f1da90]-(14)-|   (Names: PageItemCell:0x7f8efa7cb450, '|':PageItemCell:0x7f8efa7cb450 )>", "<NSLayoutConstraint:0x7f8efa7dc350 H:[NSTextField:0x7f8efa7da9a0]-(>=NSSpace(8))-[NSTextField:0x7f8f02f1da90]>" )  Will attempt to recover by breaking constraint <NSLayoutConstraint:0x7f8efa7db140 H:[NSImageView:0x7f8efa7db900(38)]>  Set the NSUserDefault NSConstraintBasedLayoutVisualizeMutuallyExclusiveConstraints to YES to have -[NSWindow visualizeConstraints:] automatically called when this happens.  And/or, break on objc_exception_throw to catch this in the debugger.
Jul  7 00:26:41 calvisitor-10-105-162-178 Preview[11512]: Unable to simultaneously satisfy constraints: ( "<NSLayoutConstraint:0x7f8efb89e920 H:[NSImageView:0x7f8efb89d5d0(38)]>", "<NSLayoutConstraint:0x7f8efb89aac0 H:|-(14)-[NSImageView:0x7f8efb89d5d0]   (Names: PageItemCell:0x7f8efb898930, '|':PageItemCell:0x7f8efb898930 )>", "<NSLayoutConstraint:0x7f8f071a6c20 'NSView-Encapsulated-Layout-Width' H:[PageItemCell(73)]   (Names: PageItemCell:0x7f8efb898930 )>", "<NSLayoutConstraint:0x7f8efb89d8a0 H:[NSImageView:0x7f8efb89d5d0]-(10)-[NSTextField:0x7f8efb89d250]>", "<NSLayoutConstraint:0x7f8efb8980e0 H:[NSTextField:0x7f8efb89e7a0]-(14)-|   (Names: PageItemCell:0x7f8efb898930, '|':PageItemCell:0x7f8efb898930 )>", "<NSLayoutConstraint:0x7f8efb89ba20 H:[NSTextField:0x7f8efb89d250]-(>=NSSpace(8))-[NSTextField:0x7f8efb89e7a0]>" )  Will attempt to recover by breaking constraint <NSLayoutConstraint:0x7f8efb89e920 H:[NSImageView:0x7f8efb89d5d0(38)]>  Set the NSUserDefault NSConstraintBasedLayoutVisualizeMutuallyExclusiveConstraints to YES to have -[NSWindow visualizeConstraints:] automatically called when this happens.  And/or, break on objc_exception_throw to catch this in the debugger.
Jul  7 00:30:06 calvisitor-10-105-162-178 syslogd[44]: Configuration Notice: ASL Module "com.apple.authkit.osx.asl" sharing output destination "/var/log/Accounts" with ASL Module "com.apple.Accounts". Output parameters from ASL Module "com.apple.Accounts" override any specified in ASL Module "com.apple.authkit.osx.asl".
Jul  7 00:43:06 calvisitor-10-105-162-178 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  7 00:43:09 calvisitor-10-105-162-178 imagent[355]: <IMMacNotificationCenterManager: 0x7fdcc9d16380>:    NC Disabled: NO
Jul  7 00:43:09 calvisitor-10-105-162-178 identityservicesd[272]: <IMMacNotificationCenterManager: 0x7ff1b2d17980>:    NC Disabled: NO
Jul  7 00:52:55 authorMacBook-Pro corecaptured[37602]: CCFile::copyFile fileName is [2017-07-07_00,52,55.450817]-CCIOReporter-001.xml, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/OneStats//[2017-07-07_00,52,55.450817]-CCIOReporter-001.xml, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-07_00,52,54.449889]=AuthFail:sts:5_rsn:0/OneStats//[2017-07-07_00,52,55.450817]-CCIOReporter-001.xml
Jul  7 00:52:58 authorMacBook-Pro symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  7 00:52:58 authorMacBook-Pro UserEventAgent[43]: Captive: [CNInfoNetworkActive:1748] en0: SSID 'CalVisitor' making interface primary (cache indicates network not captive)
Jul  7 00:53:03 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 43840 seconds.  Ignoring.
Jul  7 00:53:09 calvisitor-10-105-162-178 mds[63]: (DiskStore.Normal:2382) 20cb04f 1.000086
Jul  7 00:53:19 calvisitor-10-105-162-178 sandboxd[129] ([37617]): com.apple.Addres(37617) deny network-outbound /private/var/run/mDNSResponder
Jul  7 00:53:26 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.SoftwareUpdate.Activity: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 5703 seconds.  Ignoring.
Jul  7 01:06:34 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  7 01:33:49 calvisitor-10-105-162-178 kernel[0]: [HID] [ATC] AppleDeviceManagementHIDEventService::processWakeReason Wake reason: Host (0x01)
Jul  7 01:34:46 calvisitor-10-105-162-178 kernel[0]: ARPT: 761864.513194: AirPort_Brcm43xx::powerChange: System Sleep
Jul  7 01:47:40 calvisitor-10-105-162-178 GoogleSoftwareUpdateAgent[37644]: 2017-07-07 01:47:40.090 GoogleSoftwareUpdateAgent[37644/0x7000002a0000] [lvl=2] -[KSOutOfProcessFetcher beginFetchWithDelegate:] KSOutOfProcessFetcher start fetch from URL: "https://tools.google.com/service/update2?cup2hreq=c30943ccd5e0a03e93b6be3e2b7e2127989f08f1bde99263ffee091de8b8bc39&cup2key=7:1039900771"
Jul  7 02:01:04 calvisitor-10-105-162-178 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  7 02:14:41 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  7 02:15:01 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 38922 seconds.  Ignoring.
Jul  7 02:15:01 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 893 seconds.  Ignoring.
Jul  7 02:28:19 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  7 02:42:24 calvisitor-10-105-162-178 sandboxd[129] ([37682]): com.apple.Addres(37682) deny network-outbound /private/var/run/mDNSResponder
Jul  7 02:42:27 calvisitor-10-105-162-178 com.apple.AddressBook.InternetAccountsBridge[37682]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  7 02:55:51 calvisitor-10-105-162-178 com.apple.CDScheduler[258]: Thermal pressure state: 1 Memory pressure state: 0
Jul  7 02:56:01 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 498005 seconds.  Ignoring.
Jul  7 03:09:18 calvisitor-10-105-162-178 kernel[0]: ARPT: 762302.122693: wl0: leaveModulePoweredForOffloads: Wi-Fi will stay on.
Jul  7 03:09:38 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 35733 seconds.  Ignoring.
Jul  7 03:10:18 calvisitor-10-105-162-178 kernel[0]: ARPT: 762363.637095: AirPort_Brcm43xx::powerChange: System Sleep
Jul  7 03:22:55 calvisitor-10-105-162-178 kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  7 03:22:55 calvisitor-10-105-162-178 kernel[0]: ARPT: 762366.143164: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  7 03:23:15 calvisitor-10-105-162-178 com.apple.AddressBook.InternetAccountsBridge[37700]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 2
Jul  7 03:36:32 calvisitor-10-105-162-178 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  7 03:36:32 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  7 03:50:09 calvisitor-10-105-162-178 kernel[0]: ARPT: 762484.281874: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  7 03:50:19 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 33204 seconds.  Ignoring.
Jul  7 03:50:29 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 33282 seconds.  Ignoring.
Jul  7 03:51:08 calvisitor-10-105-162-178 kernel[0]: ARPT: 762543.171617: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': Yes, 'TimeRemaining': 0,
Jul  7 04:04:43 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.suggestions.harvest: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 106 seconds.  Ignoring.
Jul  7 04:04:43 calvisitor-10-105-162-178 kernel[0]: ARPT: 762603.017378: wl0: setup_keepalive: Local port: 62991, Remote port: 443
Jul  7 04:17:24 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  7 04:17:33 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 31570 seconds.  Ignoring.
Jul  7 04:17:37 calvisitor-10-105-162-178 locationd[82]: Location icon should now be in state 'Active'
Jul  7 04:17:52 calvisitor-10-105-162-178 sandboxd[129] ([37725]): com.apple.Addres(37725) deny network-outbound /private/var/run/mDNSResponder
Jul  7 04:31:01 calvisitor-10-105-162-178 kernel[0]: AirPort: Link Down on awdl0. Reason 1 (Unspecified).
Jul  7 04:44:38 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 491488 seconds.  Ignoring.
Jul  7 04:44:48 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1002 seconds.  Ignoring.
Jul  7 04:44:48 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1002 seconds.  Ignoring.
Jul  7 04:58:15 calvisitor-10-105-162-178 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 0 milliseconds
Jul  7 04:58:15 calvisitor-10-105-162-178 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  7 04:58:16 calvisitor-10-105-162-178 kernel[0]: AirPort: Link Up on awdl0
Jul  7 04:58:35 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 29108 seconds.  Ignoring.
Jul  7 04:58:36 calvisitor-10-105-162-178 com.apple.AddressBook.InternetAccountsBridge[37745]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  7 04:58:40 calvisitor-10-105-162-178 sandboxd[129] ([37745]): com.apple.Addres(37745) deny network-outbound /private/var/run/mDNSResponder
Jul  7 05:11:52 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  7 05:12:46 calvisitor-10-105-162-178 kernel[0]: ARPT: 762900.518967: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:3065:65eb:758e:972a
Jul  7 05:25:49 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 1065 seconds.  Ignoring.
Jul  7 05:26:27 calvisitor-10-105-162-178 kernel[0]: ARPT: 762962.511796: AirPort_Brcm43xx::powerChange: System Sleep
Jul  7 05:39:27 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 26744 seconds.  Ignoring.
Jul  7 05:39:27 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 26656 seconds.  Ignoring.
Jul  7 05:40:03 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 488163 seconds.  Ignoring.
Jul  7 05:52:44 calvisitor-10-105-162-178 kernel[0]: ARPT: 763023.568413: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  7 05:52:44 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 25947 seconds.  Ignoring.
Jul  7 06:06:21 calvisitor-10-105-162-178 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 1 us
Jul  7 06:06:21 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 25042 seconds.  Ignoring.
Jul  7 06:07:14 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 25077 seconds.  Ignoring.
Jul  7 06:48:46 authorMacBook-Pro corecaptured[37783]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  7 07:01:09 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 21754 seconds.  Ignoring.
Jul  7 07:01:14 calvisitor-10-105-162-178 corecaptured[37799]: CCFile::captureLogRun Skipping current file Dir file [2017-07-07_07,01,14.472939]-CCIOReporter-002.xml, Current File [2017-07-07_07,01,14.472939]-CCIOReporter-002.xml
Jul  7 07:15:32 calvisitor-10-105-162-178 kernel[0]: ARPT: 763490.115579: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:c6b3:1ff:fecd:467f
Jul  7 07:16:35 authorMacBook-Pro configd[53]: setting hostname to "authorMacBook-Pro.local"
Jul  7 07:16:48 calvisitor-10-105-162-178 corecaptured[37799]: CCDataTap::profileRemoved, Owner: com.apple.driver.AirPort.Brcm4360.0, Name: StateSnapshots
Jul  7 07:17:17 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 20786 seconds.  Ignoring.
Jul  7 07:30:15 calvisitor-10-105-162-178 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  7 07:30:15 calvisitor-10-105-162-178 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  7 07:30:25 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 481541 seconds.  Ignoring.
Jul  7 07:30:35 calvisitor-10-105-162-178 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  7 07:59:26 calvisitor-10-105-162-178 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 1 us
Jul  7 07:59:26 calvisitor-10-105-162-178 configd[53]: setting hostname to "authorMacBook-Pro.local"
Jul  7 07:59:26 authorMacBook-Pro kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  7 07:59:31 calvisitor-10-105-162-178 sandboxd[129] ([10018]): QQ(10018) deny mach-lookup com.apple.networking.captivenetworksupport
Jul  7 07:59:46 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 479780 seconds.  Ignoring.
Jul  7 08:28:42 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  7 08:28:48 calvisitor-10-105-162-178 networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x4 to 0x8000000000000000
Jul  7 08:42:41 calvisitor-10-105-162-178 com.apple.CDScheduler[43]: Thermal pressure state: 0 Memory pressure state: 0
Jul  7 09:09:36 calvisitor-10-105-162-178 Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-07 16:09:36 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  7 09:09:55 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 14028 seconds.  Ignoring.
Jul  7 09:10:35 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  7 09:23:13 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 474773 seconds.  Ignoring.
Jul  7 09:37:11 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 473935 seconds.  Ignoring.
Jul  7 09:50:48 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 11575 seconds.  Ignoring.
Jul  7 09:51:24 calvisitor-10-105-162-178 kernel[0]: ARPT: 764299.017125: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:9cb8:f7f2:7c03:f956
Jul  7 10:04:25 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 472301 seconds.  Ignoring.
Jul  7 10:04:25 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 845 seconds.  Ignoring.
Jul  7 10:05:03 calvisitor-10-105-162-178 kernel[0]: ARPT: 764361.057441: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:9cb8:f7f2:7c03:f956
Jul  7 10:17:44 calvisitor-10-105-162-178 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 9959 seconds.  Ignoring.
Jul  7 10:17:44 calvisitor-10-105-162-178 kernel[0]: AirPort: Link Up on awdl0
Jul  7 10:18:03 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 10028 seconds.  Ignoring.
Jul  7 10:18:45 calvisitor-10-105-162-178 kernel[0]: ARPT: 764427.640542: wl0: setup_keepalive: Seq: 204730741, Ack: 2772623181, Win size: 4096
Jul  7 10:31:22 calvisitor-10-105-162-178 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 0 milliseconds
Jul  7 10:31:22 calvisitor-10-105-162-178 kernel[0]: RTC: Maintenance 2017/7/7 17:31:21, sleep 2017/7/7 17:18:49
Jul  7 10:31:32 calvisitor-10-105-162-178 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 9219 seconds.  Ignoring.
Jul  7 10:37:43 calvisitor-10-105-162-178 kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  7 10:37:43 calvisitor-10-105-162-178 QQ[10018]: ############################## _getSysMsgList
Jul  7 10:37:43 calvisitor-10-105-162-178 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  7 10:37:43 calvisitor-10-105-162-178 kernel[0]: en0: channel changed to 1
Jul  7 10:38:06 calvisitor-10-105-162-178 Mail[11203]: Unrecognized XSSimpleTypeDefinition: OneOff
Jul  7 10:38:10 calvisitor-10-105-162-178 QQ[10018]: DB Path: /Users/xpc/Library/Containers/com.tencent.qq/Data/Documents/contents/916639562/QQ.db
Jul  7 10:38:23 calvisitor-10-105-162-178 UserEventAgent[258]: Could not get event name for stream/token: com.apple.xpc.activity/4505: 132: Request for stale data
Jul  7 10:54:41 calvisitor-10-105-162-178 ksfetch[37925]: 2017-07-07 10:54:41.296 ksfetch[37925/0x7fff79824000] [lvl=2] KSHelperReceiveAllData() KSHelperTool read 1926 bytes from stdin.
Jul  7 10:54:41 calvisitor-10-105-162-178 GoogleSoftwareUpdateAgent[37924]: 2017-07-07 10:54:41.875 GoogleSoftwareUpdateAgent[37924/0x7000002a0000] [lvl=2] -[KSUpdateCheckAction performAction] KSUpdateCheckAction starting update check for ticket(s): {( <KSTicket:0x100365950 productID=com.google.Chrome version=59.0.3071.115 xc=<KSPathExistenceChecker:0x10036e950 path=/Applications/Google Chrome.app> serverType=Omaha url=https://tools.google.com/service/update2 creationDate=2017-02-18 15:41:18 tagPath=/Applications/Google Chrome.app/Contents/Info.plist tagKey=KSChannelID brandPath=/Users/xpc/Library/Google/Google Chrome Brand.plist brandKey=KSBrandID versionPath=/Applications/Google Chrome.app/Contents/Info.plist versionKey=KSVersion cohort=1:1y5: cohortName=Stable ticketVersion=1 > )} Using server: <KSOmahaServer:0x100243f20 engine=<KSUpdateEngine:0x1007161b0> >
Jul  7 10:57:19 calvisitor-10-105-162-178 locationd[82]: Location icon should now be in state 'Inactive'
Jul  7 11:01:11 calvisitor-10-105-162-178 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  7 11:22:27 calvisitor-10-105-162-178 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 294, items, fQueryRetries, 0, fLastRetryTimestamp, 521144227.4
Jul  7 11:28:11 calvisitor-10-105-162-178 com.apple.ncplugin.weather[37956]: Error in CoreDragRemoveTrackingHandler: -1856
Jul  7 11:28:44 calvisitor-10-105-162-178 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.37963): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.History.xpc/Contents/MacOS/com.apple.Safari.History error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  7 11:48:06 calvisitor-10-105-162-178 com.apple.WebKit.WebContent[32778]: [11:48:06.869] FigAgglomeratorSetObjectForKey signalled err=-16020 (kFigStringConformerError_ParamErr) (NULL key) at /Library/Caches/com.apple.xbs/Sources/CoreMedia/CoreMedia-1731.15.207/Prototypes/LegibleOutput/FigAgglomerator.c line 92
Jul  7 11:57:06 calvisitor-10-105-162-178 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.37999): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.ImageDecoder.xpc/Contents/MacOS/com.apple.Safari.ImageDecoder error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  7 11:57:23 calvisitor-10-105-162-178 locationd[82]: Location icon should now be in state 'Active'
Jul  7 12:12:06 calvisitor-10-105-162-178 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.user.501): Service "com.apple.xpc.launchd.unmanaged.loginwindow.94" tried to hijack endpoint "com.apple.tsm.uiserver" from owner: com.apple.SystemUIServer.agent
Jul  7 12:12:27 calvisitor-10-105-162-178 locationd[82]: Location icon should now be in state 'Active'
Jul  7 12:14:35 calvisitor-10-105-162-178 kernel[0]: ARPT: 770226.223664: IOPMPowerSource Information: onWake,  SleepType: Normal Sleep,  'ExternalConnected': Yes, 'TimeRemaining': 0,
Jul  7 12:14:38 calvisitor-10-105-162-178 com.apple.SecurityServer[80]: Session 101800 destroyed
Jul  7 12:15:44 calvisitor-10-105-162-178 quicklookd[38023]: Error returned from iconservicesagent: (null)
Jul  7 12:15:44 calvisitor-10-105-162-178 quicklookd[38023]: Error returned from iconservicesagent: (null)
Jul  7 12:15:59 calvisitor-10-105-162-178 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  7 12:16:28 calvisitor-10-105-162-178 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f995070be80>.
Jul  7 12:16:28 calvisitor-10-105-162-178 quicklookd[38023]: Error returned from iconservicesagent: (null)
Jul  7 12:16:28 calvisitor-10-105-162-178 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f995060a270>.
Jul  7 12:17:49 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  7 12:20:19 calvisitor-10-105-162-178 kernel[0]: TBT W (2): 0x0040 [x]
Jul  7 12:20:25 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  7 12:20:41 calvisitor-10-105-162-178 AddressBookSourceSync[38055]: -[SOAPParser:0x7f84bad89660 parser:didStartElement:namespaceURI:qualifiedName:attributes:] Type not found in EWSItemType for ExchangePersonIdGuid (t:ExchangePersonIdGuid)
Jul  7 12:21:15 calvisitor-10-105-162-178 imagent[355]: <IMMacNotificationCenterManager: 0x7fdcc9d16380>:    NC Disabled: NO
Jul  7 12:23:11 calvisitor-10-105-162-178 kernel[0]: in6_unlink_ifa: IPv6 address 0x77c911453a6dbcdb has no prefix
Jul  7 12:23:12 authorMacBook-Pro kernel[0]: ARPT: 770518.944345: framerdy 0x0 bmccmd 3 framecnt 1024
Jul  7 12:24:25 authorMacBook-Pro corecaptured[38064]: Received Capture Event
Jul  7 12:24:29 authorMacBook-Pro corecaptured[38064]: CCFile::copyFile fileName is [2017-07-07_12,24,25.108710]-io80211Family-007.pcapng, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/IO80211AWDLPeerManager//[2017-07-07_12,24,25.108710]-io80211Family-007.pcapng, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-07_12,24,29.298901]=AuthFail:sts:5_rsn:0/IO80211AWDLPeerManager//[2017-07-07_12,24,25.108710]-io80211Family-007.pcapng
Jul  7 12:24:31 authorMacBook-Pro kernel[0]: ARPT: 770597.394609: AQM agg params 0xfc0 maxlen hi/lo 0x0 0xffff minlen 0x0 adjlen 0x0
Jul  7 12:24:31 authorMacBook-Pro corecaptured[38064]: CCFile::captureLog
Jul  7 12:24:31 authorMacBook-Pro corecaptured[38064]: CCFile::captureLogRun Skipping current file Dir file [2017-07-07_12,24,31.376032]-AirPortBrcm4360_Logs-011.txt, Current File [2017-07-07_12,24,31.376032]-AirPortBrcm4360_Logs-011.txt
Jul  7 12:24:31 authorMacBook-Pro corecaptured[38064]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  7 12:24:31 authorMacBook-Pro corecaptured[38064]: CCFile::captureLog
Jul  7 12:30:08 authorMacBook-Pro kernel[0]: ARPT: 770602.528852: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  7 12:30:08 authorMacBook-Pro kernel[0]: TBT W (2): 0x0040 [x]
Jul  7 12:30:09 authorMacBook-Pro kernel[0]: ARPT: 770605.092581: ARPT: Wake Reason: Wake on Scan offload
Jul  7 12:30:14 calvisitor-10-105-162-178 kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  7 12:30:26 calvisitor-10-105-162-178 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  7 12:30:29 calvisitor-10-105-162-178 kernel[0]: IO80211AWDLPeerManager::setAwdlAutoMode Resuming AWDL
Jul  7 13:38:22 authorMacBook-Pro corecaptured[38124]: CCIOReporterFormatter::addRegistryChildToChannelDictionary streams 7
Jul  7 13:38:22 authorMacBook-Pro corecaptured[38124]: doSaveChannels@286: Will write to: /Library/Logs/CrashReporter/CoreCapture/IOReporters/[2017-07-07_13,38,22.072050] - AssocFail:sts:2_rsn:0.xml
Jul  7 13:38:23 authorMacBook-Pro corecaptured[38124]: CCFile::captureLogRun Skipping current file Dir file [2017-07-07_13,38,23.338868]-AirPortBrcm4360_Logs-013.txt, Current File [2017-07-07_13,38,23.338868]-AirPortBrcm4360_Logs-013.txt
Jul  7 13:38:23 authorMacBook-Pro corecaptured[38124]: CCIOReporterFormatter::addRegistryChildToChannelDictionary streams 7
Jul  7 13:38:24 authorMacBook-Pro corecaptured[38124]: CCFile::captureLog Received Capture notice id: 1499459904.126805, reason = AuthFail:sts:5_rsn:0
Jul  7 13:38:24 authorMacBook-Pro corecaptured[38124]: CCFile::captureLogRun Skipping current file Dir file [2017-07-07_13,38,24.371339]-CCIOReporter-019.xml, Current File [2017-07-07_13,38,24.371339]-CCIOReporter-019.xml
Jul  7 13:38:29 authorMacBook-Pro networkd[195]: -[NETClientConnection evaluateCrazyIvan46] CI46 - Perform CrazyIvan46! NeteaseMusic.17988 tc12042 103.251.128.144:80
Jul  7 13:38:50 calvisitor-10-105-160-205 sandboxd[129] ([38132]): com.apple.Addres(38132) deny network-outbound /private/var/run/mDNSResponder
Jul  7 13:40:30 calvisitor-10-105-160-205 kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  7 13:40:31 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name CalendarAgent as bundle ID (this is expected for daemons without bundle ID
Jul  7 13:40:35 authorMacBook-Pro corecaptured[38124]: CCFile::captureLog
Jul  7 13:40:35 authorMacBook-Pro corecaptured[38124]: Received Capture Event
Jul  7 13:40:35 authorMacBook-Pro corecaptured[38124]: CCFile::captureLog
Jul  7 13:40:35 authorMacBook-Pro corecaptured[38124]: CCFile::captureLogRun Skipping current file Dir file [2017-07-07_13,40,35.857095]-AirPortBrcm4360_Logs-022.txt, Current File [2017-07-07_13,40,35.857095]-AirPortBrcm4360_Logs-022.txt
Jul  7 13:40:35 authorMacBook-Pro corecaptured[38124]: CCFile::captureLogRun Skipping current file Dir file [2017-07-07_13,40,35.961495]-io80211Family-023.pcapng, Current File [2017-07-07_13,40,35.961495]-io80211Family-023.pcapng
Jul  7 13:40:36 authorMacBook-Pro kernel[0]: ARPT: 770712.519828: framerdy 0x0 bmccmd 3 framecnt 1024
Jul  7 13:40:36 authorMacBook-Pro corecaptured[38124]: CCFile::captureLog Received Capture notice id: 1499460036.519390, reason = AuthFail:sts:5_rsn:0
Jul  7 13:40:37 authorMacBook-Pro corecaptured[38124]: CCFile::copyFile fileName is [2017-07-07_13,40,37.052249]-io80211Family-031.pcapng, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/IO80211AWDLPeerManager//[2017-07-07_13,40,37.052249]-io80211Family-031.pcapng, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-07_13,40,37.192896]=AuthFail:sts:5_rsn:0/IO80211AWDLPeerManager//[2017-07-07_13,40,37.052249]-io80211Family-031.pcapng
Jul  7 13:40:37 authorMacBook-Pro corecaptured[38124]: CCIOReporterFormatter::refreshSubscriptionsFromStreamRegistry clearing out any previous subscriptions
Jul  7 13:40:37 authorMacBook-Pro corecaptured[38124]: doSaveChannels@286: Will write to: /Library/Logs/CrashReporter/CoreCapture/IOReporters/[2017-07-07_13,40,35.959475] - AuthFail:sts:5_rsn:0.xml
Jul  7 13:40:39 calvisitor-10-105-160-205 networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x4 to 0x8000000000000000
Jul  7 13:40:52 calvisitor-10-105-160-205 QQ[10018]: ############################## _getSysMsgList
Jul  7 13:42:10 calvisitor-10-105-160-205 kernel[0]: TBT W (2): 0x0040 [x]
Jul  7 13:42:10 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x8000000000000000 to 0x4
Jul  7 13:42:15 authorMacBook-Pro kernel[0]: ARPT: 770772.533370: AQM agg params 0xfc0 maxlen hi/lo 0x0 0xffff minlen 0x0 adjlen 0x0
Jul  7 13:42:17 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name apsd as bundle ID (this is expected for daemons without bundle ID
Jul  7 13:42:17 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Authenticated
Jul  7 13:42:17 authorMacBook-Pro configd[53]: network changed: v4(en0+:10.105.160.205) v6(en0:2607:f140:6000:8:c6b3:1ff:fecd:467f) DNS! Proxy SMB
Jul  7 13:42:30 authorMacBook-Pro kernel[0]: ARPT: 770786.961436: AQM agg params 0xfc0 maxlen hi/lo 0x0 0xffff minlen 0x0 adjlen 0x0
Jul  7 13:42:34 authorMacBook-Pro corecaptured[38124]: CCFile::captureLog
Jul  7 13:42:34 authorMacBook-Pro corecaptured[38124]: CCFile::captureLogRun Skipping current file Dir file [2017-07-07_13,42,34.831310]-AirPortBrcm4360_Logs-041.txt, Current File [2017-07-07_13,42,34.831310]-AirPortBrcm4360_Logs-041.txt
Jul  7 13:42:41 authorMacBook-Pro kernel[0]: ARPT: 770798.478026: framerdy 0x0 bmccmd 3 framecnt 1024
Jul  7 13:42:46 authorMacBook-Pro kernel[0]: ARPT: 770802.879782: wlc_dump_aggfifo:
Jul  7 13:42:46 authorMacBook-Pro corecaptured[38124]: Received Capture Event
Jul  7 13:42:48 authorMacBook-Pro corecaptured[38124]: CCFile::captureLog
Jul  7 13:48:27 authorMacBook-Pro kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 0 milliseconds
Jul  7 13:48:27 authorMacBook-Pro kernel[0]: ARPT: 770812.035971: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  7 13:48:29 authorMacBook-Pro cdpd[11807]: Saw change in network reachability (isReachable=2)
Jul  7 13:48:52 calvisitor-10-105-160-205 com.apple.AddressBook.InternetAccountsBridge[38158]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  7 13:58:13 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  7 13:58:22 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name CalendarAgent as bundle ID (this is expected for daemons without bundle ID
Jul  7 14:03:25 calvisitor-10-105-160-205 kernel[0]: AppleThunderboltNHIType2::prePCIWake - power up complete - took 1 us
Jul  7 14:03:30 calvisitor-10-105-160-205 kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  7 14:03:45 calvisitor-10-105-160-205 sandboxd[129] ([38179]): com.apple.Addres(38179) deny network-outbound /private/var/run/mDNSResponder
Jul  7 14:07:21 calvisitor-10-105-160-205 com.apple.WebKit.WebContent[32778]: [14:07:21.190] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  7 14:09:44 calvisitor-10-105-160-205 cloudd[326]:  SecOSStatusWith error:[-50] Error Domain=NSOSStatusErrorDomain Code=-50 "query missing class name" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}
Jul  7 14:09:45 calvisitor-10-105-160-205 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.user.501): Service "com.apple.xpc.launchd.unmanaged.loginwindow.94" tried to hijack endpoint "com.apple.tsm.uiserver" from owner: com.apple.SystemUIServer.agent
Jul  7 14:19:56 calvisitor-10-105-160-205 syslogd[44]: ASL Sender Statistics
Jul  7 14:19:57 calvisitor-10-105-160-205 kernel[0]: in6_unlink_ifa: IPv6 address 0x77c911454f15210b has no prefix
Jul  7 14:20:52 calvisitor-10-105-160-205 kernel[0]: ARPT: 771388.849884: wl0: setup_keepalive: interval 900, retry_interval 30, retry_count 10
Jul  7 14:33:34 calvisitor-10-105-160-205 syslogd[44]: ASL Sender Statistics
Jul  7 14:33:34 calvisitor-10-105-160-205 kernel[0]: Previous sleep cause: 5
Jul  7 14:33:34 calvisitor-10-105-160-205 kernel[0]: ARPT: 771393.363637: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  7 14:47:12 calvisitor-10-105-160-205 kernel[0]: ARPT: 771456.438849: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  7 15:00:50 calvisitor-10-105-160-205 kernel[0]: ARPT: 771516.615152: AirPort_Brcm43xx::platformWoWEnable: WWEN[disable]
Jul  7 15:01:07 calvisitor-10-105-160-205 sandboxd[129] ([38210]): com.apple.Addres(38210) deny network-outbound /private/var/run/mDNSResponder
Jul  7 15:01:12 calvisitor-10-105-160-205 sandboxd[129] ([38210]): com.apple.Addres(38210) deny network-outbound /private/var/run/mDNSResponder
Jul  7 15:14:27 calvisitor-10-105-160-205 kernel[0]: AirPort: Link Down on awdl0. Reason 1 (Unspecified).
Jul  7 15:14:27 calvisitor-10-105-160-205 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  7 15:28:00 calvisitor-10-105-160-205 syslogd[44]: ASL Sender Statistics
Jul  7 15:28:49 calvisitor-10-105-160-205 kernel[0]: ARPT: 771631.938256: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': No, 'TimeRemaining': 9235,
Jul  7 16:04:15 calvisitor-10-105-160-205 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jul  7 16:04:15 calvisitor-10-105-160-205 kernel[0]: en0: channel changed to 1
Jul  7 16:04:15 calvisitor-10-105-160-205 symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  7 16:04:15 calvisitor-10-105-160-205 kernel[0]: Setting BTCoex Config: enable_2G:1, profile_2g:0, enable_5G:1, profile_5G:0
Jul  7 16:04:16 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 25340 failed: 3 - No network route
Jul  7 16:04:22 authorMacBook-Pro netbiosd[38222]: Unable to start NetBIOS name service:
Jul  7 16:04:28 authorMacBook-Pro networkd[195]: -[NETClientConnection evaluateCrazyIvan46] CI46 - Perform CrazyIvan46! QQ.10018 tc25354 123.151.10.190:80
Jul  7 16:04:44 calvisitor-10-105-161-77 kernel[0]: ARPT: 771663.085756: wl0: Roamed or switched channel, reason #8, bssid 00:a6:ca:db:93:cc, last RSSI -60
Jul  7 16:05:37 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Inactive
Jul  7 16:05:38 authorMacBook-Pro kernel[0]: en0: BSSID changed to 0c:68:03:d6:c5:1c
Jul  7 16:05:42 calvisitor-10-105-161-77 kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  7 16:24:10 authorMacBook-Pro locationd[82]: PBRequester failed with Error Error Domain=NSURLErrorDomain Code=-1009 "The Internet connection appears to be offline." UserInfo={NSUnderlyingError=0x7fb7f0035dc0 {Error Domain=kCFErrorDomainCFNetwork Code=-1009 "The Internet connection appears to be offline." UserInfo={NSErrorFailingURLStringKey=https://gs-loc.apple.com/clls/wloc, NSErrorFailingURLKey=https://gs-loc.apple.com/clls/wloc, _kCFStreamErrorCodeKey=8, _kCFStreamErrorDomainKey=12, NSLocalizedDescription=The Internet connection appears to be offline.}}, NSErrorFailingURLStringKey=https://gs-loc.apple.com/clls/wloc, NSErrorFailingURLKey=https://gs-loc.apple.com/clls/wloc, _kCFStreamErrorDomainKey=12, _kCFStreamErrorCodeKey=8, NSLocalizedDescription=The Internet connection appears to be offline.}
Jul  7 16:24:11 authorMacBook-Pro corecaptured[38241]: CCFile::copyFile fileName is [2017-07-07_16,24,11.442212]-AirPortBrcm4360_Logs-001.txt, source path:/var/log/CoreCapture/com.apple.driver.AirPort.Brcm4360.0/DriverLogs//[2017-07-07_16,24,11.442212]-AirPortBrcm4360_Logs-001.txt, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.driver.AirPort.Brcm4360.0/[2017-07-07_16,24,11.124183]=AssocFail:sts:2_rsn:0/DriverLogs//[2017-07-07_16,24,11.442212]-AirPortBrcm4360_Logs-001.txt
Jul  7 16:24:11 authorMacBook-Pro corecaptured[38241]: CCIOReporterFormatter::refreshSubscriptionsFromStreamRegistry clearing out any previous subscriptions
Jul  7 16:24:16 authorMacBook-Pro corecaptured[38241]: CCFile::captureLog Received Capture notice id: 1499469856.145137, reason = AssocFail:sts:2_rsn:0
Jul  7 16:24:16 authorMacBook-Pro corecaptured[38241]: CCIOReporterFormatter::refreshSubscriptionsFromStreamRegistry clearing out any previous subscriptions
Jul  7 16:24:18 authorMacBook-Pro corecaptured[38241]: Received Capture Event
Jul  7 16:24:23 authorMacBook-Pro networkd[195]: -[NETClientConnection effectiveBundleID] using process name apsd as bundle ID (this is expected for daemons without bundle ID
Jul  7 16:24:43 authorMacBook-Pro QQ[10018]: ############################## _getSysMsgList
Jul  7 16:24:49 authorMacBook-Pro com.apple.AddressBook.InternetAccountsBridge[38247]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  7 16:45:57 authorMacBook-Pro WindowServer[184]: CGXDisplayDidWakeNotification [771797848593539]: posting kCGSDisplayDidWake
Jul  7 16:46:23 calvisitor-10-105-160-85 com.apple.AddressBook.InternetAccountsBridge[38259]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 3
Jul  7 16:49:57 calvisitor-10-105-160-85 locationd[82]: Location icon should now be in state 'Inactive'
Jul  7 16:53:53 calvisitor-10-105-160-85 com.apple.SecurityServer[80]: Session 102106 destroyed
Jul  7 16:57:09 calvisitor-10-105-160-85 com.apple.WebKit.WebContent[32778]: [16:57:09.235] <<<< Boss >>>> figPlaybackBossPrerollCompleted: unexpected preroll-complete notification
Jul  7 16:59:44 calvisitor-10-105-160-85 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  7 17:15:46 calvisitor-10-105-160-85 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.38405): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.SearchHelper.xpc/Contents/MacOS/com.apple.Safari.SearchHelper error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  7 17:20:30 calvisitor-10-105-160-85 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  7 17:30:00 calvisitor-10-105-160-85 kernel[0]: Sandbox: QuickLookSatelli(38418) deny(1) mach-lookup com.apple.networkd
Jul  7 17:30:00 calvisitor-10-105-160-85 QuickLookSatellite[38418]: nw_path_evaluator_start_helper_connection net_helper_path_evaluation_start failed, dumping backtrace: [x86_64] libnetcore-583.50.1 0   libsystem_network.dylib             0x00007fff92fabde9 __nw_create_backtrace_string + 123 1   libsystem_network.dylib             0x00007fff92fc289f nw_path_evaluator_start_helper_connection + 196 2   libdispatch.dylib                   0x00007fff980fa93d _dispatch_call_block_and_release + 12 3   libdispatch.dylib                   0x00007fff980ef40b _dispatch_client_callout + 8 4   libdispatch.dylib                   0x00007fff980f403b _dispatch_queue_drain + 754 5   libdispatch.dylib                   0x00007fff980fa707 _dispatch_queue_invoke + 549 6   libdispatch.dylib                   0x00007fff980f2d53 _dispatch_root_queue_drain + 538 7   libdispatch.dylib                   0x00007fff980f2b00 _dispatch_worker_thread3 + 91 8   libsystem_pthread.dylib             0x00007fff8ebc44de _pthread_wqthread + 1129 9   libsystem_pthread.dylib             0x00007fff8ebc2341 start_wqthread + 13
Jul  7 17:34:51 calvisitor-10-105-160-85 Safari[9852]: KeychainGetICDPStatus: status: off
Jul  7 17:45:26 calvisitor-10-105-160-85 loginwindow[94]: CoreAnimation: warning, deleted thread with uncommitted CATransaction; set CA_DEBUG_TRANSACTIONS=1 in environment to log backtraces.
Jul  7 18:02:24 calvisitor-10-105-160-85 kernel[0]: RTC: PowerByCalendarDate setting ignored
Jul  7 18:02:24 calvisitor-10-105-160-85 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 12 unplug = 0
Jul  7 18:02:49 calvisitor-10-105-160-85 kernel[0]: Sandbox: com.apple.Addres(38449) deny(1) network-outbound /private/var/run/mDNSResponder
Jul  7 18:09:40 calvisitor-10-105-160-85 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  7 18:09:40 calvisitor-10-105-160-85 kernel[0]: en0: channel changed to 1
Jul  7 18:09:40 authorMacBook-Pro kernel[0]: [HID] [ATC] AppleDeviceManagementHIDEventService::processWakeReason Wake reason: Host (0x01)
Jul  7 18:09:41 authorMacBook-Pro cdpd[11807]: Saw change in network reachability (isReachable=0)
Jul  7 18:09:46 authorMacBook-Pro corecaptured[38453]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  7 18:10:00 calvisitor-10-105-160-85 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 40 seconds.  Ignoring.
Jul  7 18:10:10 calvisitor-10-105-160-85 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  7 18:10:14 calvisitor-10-105-160-85 kernel[0]: en0::IO80211Interface::postMessage bssid changed
Jul  7 18:11:11 calvisitor-10-105-160-85 kernel[0]: Previous sleep cause: 5
Jul  7 18:11:20 calvisitor-10-105-160-85 com.apple.cts[258]: com.apple.EscrowSecurityAlert.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 68031 seconds.  Ignoring.
Jul  7 18:11:21 calvisitor-10-105-160-85 com.apple.cts[258]: com.apple.Safari.SafeBrowsing.Update: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 2889 seconds.  Ignoring.
Jul  7 18:11:40 calvisitor-10-105-160-85 kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  7 18:23:30 calvisitor-10-105-160-85 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 0 milliseconds
Jul  7 18:23:32 calvisitor-10-105-160-85 kernel[0]: IO80211AWDLPeerManager::setAwdlSuspendedMode() Suspending AWDL, enterQuietMode(true)
Jul  7 18:23:40 calvisitor-10-105-160-85 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 67203 seconds.  Ignoring.
Jul  7 18:24:25 calvisitor-10-105-160-85 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 442301 seconds.  Ignoring.
Jul  7 18:37:36 calvisitor-10-105-160-85 AddressBookSourceSync[38490]: -[SOAPParser:0x7fca6040cb50 parser:didStartElement:namespaceURI:qualifiedName:attributes:] Type not found in EWSItemType for ExchangePersonIdGuid (t:ExchangePersonIdGuid)
Jul  7 18:38:03 calvisitor-10-105-160-85 com.apple.cts[258]: com.apple.icloud.fmfd.heartbeat: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 441483 seconds.  Ignoring.
Jul  7 18:50:45 calvisitor-10-105-160-85 kernel[0]: ARPT: 775899.188954: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq 2863091569, Ack 159598625, Win size 278
Jul  7 18:50:46 calvisitor-10-105-160-85 sharingd[30299]: 18:50:46.109 : BTLE scanner Powered On
Jul  7 18:50:46 calvisitor-10-105-160-85 com.apple.cts[43]: com.apple.SoftwareUpdate.Activity: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 4223 seconds.  Ignoring.
Jul  7 18:51:05 calvisitor-10-105-160-85 com.apple.cts[43]: com.apple.CacheDelete.daily: scheduler_evaluate_activity told me to run this job; however, but the start time isn't for 65558 seconds.  Ignoring.
Jul  7 19:04:23 calvisitor-10-105-160-85 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  7 19:04:45 calvisitor-10-105-160-85 com.apple.AddressBook.InternetAccountsBridge[38507]: dnssd_clientstub ConnectToServer: connect()-> No of tries: 1
Jul  7 19:07:53 authorMacBook-Pro kernel[0]: AppleCamIn::handleWakeEvent_gated
Jul  7 19:21:28 calvisitor-10-105-160-85 Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-08 02:21:28 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  7 19:43:07 calvisitor-10-105-160-85 kernel[0]: full wake promotion (reason 1) 374 ms
Jul  7 19:43:09 calvisitor-10-105-160-85 CalendarAgent[279]: [com.apple.calendar.store.log.caldav.coredav] [Refusing to parse response to PROPPATCH because of content-type: [text/html; charset=UTF-8].]
Jul  7 20:08:13 calvisitor-10-105-160-85 secd[276]:  securityd_xpc_dictionary_handler cloudd[326] copy_matching Error Domain=NSOSStatusErrorDomain Code=-50 "query missing class name" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}
Jul  7 20:32:00 calvisitor-10-105-160-85 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9951217bb0>.
Jul  7 20:32:00 calvisitor-10-105-160-85 quicklookd[38603]: Error returned from iconservicesagent: (null)
Jul  7 20:32:02 calvisitor-10-105-160-85 secd[276]:  securityd_xpc_dictionary_handler cloudd[326] copy_matching Error Domain=NSOSStatusErrorDomain Code=-50 "query missing class name" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}
Jul  7 20:32:12 calvisitor-10-105-160-85 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9950712080>.
Jul  7 20:32:28 calvisitor-10-105-160-85 iconservicesagent[328]: -[ISGenerateImageOp generateImageWithCompletion:] Failed to composit image for descriptor <ISBindingImageDescriptor: 0x7f9951214ca0>.
Jul  7 20:34:14 calvisitor-10-105-160-85 locationd[82]: NETWORK: requery, 0, 0, 0, 0, 328, items, fQueryRetries, 0, fLastRetryTimestamp, 521177334.4
Jul  7 20:39:28 calvisitor-10-105-160-85 com.apple.WebKit.Networking[9854]: NSURLSession/NSURLConnection HTTP load failed (kCFStreamErrorDomainSSL, -9806)
Jul  7 20:43:37 calvisitor-10-105-160-85 QuickLookSatellite[38624]: [QL] No sandbox token for request <QLThumbnailRequest vmware-usbarb-25037.log>, it will probably fail
Jul  7 20:44:13 calvisitor-10-105-160-85 GPUToolsAgent[38749]: schedule invalidation <DYTransport 0x7f89e8c00520, error: lost transport connection (31)>
Jul  7 20:50:32 calvisitor-10-105-160-85 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  7 20:51:27 calvisitor-10-105-160-85 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  7 20:52:49 calvisitor-10-105-160-85 com.apple.CDScheduler[258]: Thermal pressure state: 0 Memory pressure state: 0
Jul  7 20:57:37 calvisitor-10-105-160-85 Safari[9852]: KeychainGetICDPStatus: status: off
Jul  7 21:01:22 calvisitor-10-105-160-85 quicklookd[38603]: Error returned from iconservicesagent: (null)
Jul  7 21:08:26 calvisitor-10-105-160-85 WeChat[24144]: jemmytest
Jul  7 21:12:46 calvisitor-10-105-160-85 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  7 21:18:04 calvisitor-10-105-160-85 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.38838): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.SocialHelper.xpc/Contents/MacOS/com.apple.Safari.SocialHelper error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  7 21:22:43 calvisitor-10-105-160-85 com.apple.WebKit.WebContent[38826]: [21:22:43.147] mv_LowLevelCheckIfVideoPlayableUsingDecoder signalled err=-12956 (kFigMediaValidatorError_VideoCodecNotSupported) (video codec 1) at  line 1921
Jul  7 21:23:10 calvisitor-10-105-160-85 com.apple.WebKit.WebContent[38826]: <<<< MediaValidator >>>> mv_LookupCodecSupport: Unrecognized codec 1
Jul  7 21:23:11 calvisitor-10-105-160-85 com.apple.WebKit.WebContent[38826]: <<<< MediaValidator >>>> mv_ValidateRFC4281CodecId: Unrecognized codec 1.(null). Failed codec specific check.
Jul  7 21:24:19 calvisitor-10-105-160-85 WindowServer[184]: CoreAnimation: timed out fence 5fe83
Jul  7 21:26:38 calvisitor-10-105-160-85 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  7 21:29:30 calvisitor-10-105-160-85 Safari[9852]: tcp_connection_tls_session_error_callback_imp 2515 __tcp_connection_tls_session_callback_write_block_invoke.434 error 22
Jul  7 21:30:12 calvisitor-10-105-160-85 Safari[9852]: KeychainGetICDPStatus: keychain: -25300
Jul  7 21:31:11 calvisitor-10-105-160-85 garcon[38866]: Failed to connect (view) outlet from (NSApplication) to (NSColorPickerGridView): missing setter or instance variable
Jul  7 21:37:56 calvisitor-10-105-160-85 locationd[82]: Location icon should now be in state 'Inactive'
Jul  7 21:47:20 calvisitor-10-105-160-85 QQ[10018]: 2017/07/07 21:47:20.392 | I | VoipWrapper  | DAVEngineImpl.cpp:1400:Close             | close video chat. llFriendUIN = 515629905.
Jul  7 21:47:38 calvisitor-10-105-160-85 kernel[0]: ARPT: 783667.697957: AirPort_Brcm43xx::powerChange: System Sleep
Jul  7 21:57:08 calvisitor-10-105-160-85 kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  7 21:58:03 calvisitor-10-105-160-85 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  7 21:58:14 calvisitor-10-105-160-85 kernel[0]: IOPMrootDomain: idle cancel, state 1
Jul  7 21:58:33 calvisitor-10-105-160-85 NeteaseMusic[17988]: 21:58:33.765 ERROR:    177: timed out after 15.000s (0 0); mMajorChangePending=0
Jul  7 21:59:08 calvisitor-10-105-160-85 kernel[0]: ARPT: 783790.553857: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:c6b3:1ff:fecd:467f
Jul  7 22:10:52 calvisitor-10-105-160-85 kernel[0]: ARPT: 783795.288172: AirPort_Brcm43xx::powerChange: System Wake - Full Wake/ Dark Wake / Maintenance wake
Jul  7 22:11:54 calvisitor-10-105-160-85 QQ[10018]: button report: 0x8002be0
Jul  7 22:24:31 calvisitor-10-105-160-85 sharingd[30299]: 22:24:31.135 : BTLE scanner Powered On
Jul  7 22:32:31 calvisitor-10-105-160-85 Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-08 05:32:31 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  7 22:46:29 calvisitor-10-105-160-85 QQ[10018]: button report: 0x8002be0
Jul  7 22:47:06 calvisitor-10-105-160-85 kernel[0]: ARPT: 783991.995027: wl0: setup_keepalive: interval 900, retry_interval 30, retry_count 10
Jul  7 23:00:44 calvisitor-10-105-160-85 kernel[0]: ARPT: 784053.579480: wl0: setup_keepalive: Local port: 59927, Remote port: 443
Jul  7 23:00:46 calvisitor-10-105-160-85 kernel[0]: ARPT: 784055.560270: IOPMPowerSource Information: onSleep,  SleepType: Normal Sleep,  'ExternalConnected': No, 'TimeRemaining': 5600,
Jul  7 23:14:23 calvisitor-10-105-160-85 kernel[0]: ARPT: 784117.089800: AirPort_Brcm43xx::powerChange: System Sleep
Jul  7 23:27:02 calvisitor-10-105-160-85 kernel[0]: ARPT: 784117.625615: wl0: leaveModulePoweredForOffloads: Wi-Fi will stay on.
Jul  7 23:38:27 calvisitor-10-105-160-85 kernel[0]: ARPT: 784178.158614: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  7 23:38:32 authorMacBook-Pro kernel[0]: en0::IO80211Interface::postMessage bssid changed
Jul  8 00:30:39 authorMacBook-Pro kernel[0]: in6_unlink_ifa: IPv6 address 0x77c911454f1523ab has no prefix
Jul  8 00:30:47 authorMacBook-Pro networkd[195]: -[NETClientConnection evaluateCrazyIvan46] CI46 - Perform CrazyIvan46! QQ.10018 tc25805 123.151.137.106:80
Jul  8 00:30:48 calvisitor-10-105-160-47 symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2
Jul  8 00:44:20 calvisitor-10-105-160-47 kernel[0]: ARPT: 784287.966851: AirPort_Brcm43xx::platformWoWEnable: WWEN[enable]
Jul  8 00:45:21 calvisitor-10-105-160-47 kernel[0]: AppleThunderboltNHIType2::waitForOk2Go2Sx - intel_rp = 1 dlla_reporting_supported = 0
Jul  8 00:47:42 calvisitor-10-105-160-47 kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  8 00:58:14 calvisitor-10-105-160-47 kernel[0]: efi pagecount 72
Jul  8 01:51:46 calvisitor-10-105-160-47 kernel[0]: AirPort: Link Down on en0. Reason 8 (Disassociated because station leaving).
Jul  8 01:51:50 authorMacBook-Pro mDNSResponder[91]: mDNS_RegisterInterface: Frequent transitions for interface en0 (FE80:0000:0000:0000:C6B3:01FF:FECD:467F)
Jul  8 01:52:19 calvisitor-10-105-163-147 kernel[0]: ARPT: 784486.498066: wlc_dump_aggfifo:
Jul  8 01:52:19 calvisitor-10-105-163-147 corecaptured[38984]: CCFile::captureLogRun() Exiting CCFile::captureLogRun
Jul  8 01:52:20 calvisitor-10-105-163-147 corecaptured[38984]: CCIOReporterFormatter::refreshSubscriptionsFromStreamRegistry clearing out any previous subscriptions
Jul  8 02:32:14 calvisitor-10-105-163-147 kernel[0]: Previous sleep cause: 5
Jul  8 02:32:14 calvisitor-10-105-163-147 kernel[0]: **** [BroadcomBluetoothHostControllerUSBTransport][start] -- Completed (matched on Device) -- 0x3800 ****
Jul  8 02:32:14 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x4 to 0x8000000000000000
Jul  8 02:32:17 authorMacBook-Pro kernel[0]: en0: Supported channels 1 2 3 4 5 6 7 8 9 10 11 12 13 36 40 44 48 52 56 60 64 100 104 108 112 116 120 124 128 132 136 140 144 149 153 157 161 165
Jul  8 02:32:23 authorMacBook-Pro networkd[195]: nw_nat64_post_new_ifstate successfully changed NAT64 ifstate from 0x8000000000000000 to 0x4
Jul  8 02:32:46 calvisitor-10-105-162-175 corecaptured[38992]: CCFile::captureLog Received Capture notice id: 1499506366.010075, reason = AuthFail:sts:5_rsn:0
Jul  8 02:32:46 calvisitor-10-105-162-175 corecaptured[38992]: CCFile::captureLogRun Skipping current file Dir file [2017-07-08_02,32,46.787931]-AirPortBrcm4360_Logs-004.txt, Current File [2017-07-08_02,32,46.787931]-AirPortBrcm4360_Logs-004.txt
Jul  8 02:32:47 calvisitor-10-105-162-175 corecaptured[38992]: CCFile::copyFile fileName is [2017-07-08_02,32,47.528554]-CCIOReporter-003.xml, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/OneStats//[2017-07-08_02,32,47.528554]-CCIOReporter-003.xml, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-08_02,32,46.517498]=AssocFail:sts:2_rsn:0/OneStats//[2017-07-08_02,32,47.528554]-CCIOReporter-003.xml
Jul  8 03:12:46 calvisitor-10-105-162-175 kernel[0]: did discard act 12083 inact 17640 purgeable 20264 spec 28635 cleaned 0
Jul  8 03:12:46 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 25902 failed: 3 - No network route
Jul  8 03:12:50 authorMacBook-Pro kernel[0]: en0::IO80211Interface::postMessage bssid changed
Jul  8 03:13:06 calvisitor-10-105-162-108 corecaptured[38992]: CCDataTap::profileRemoved, Owner: com.apple.iokit.IO80211Family, Name: AssociationEventHistory
Jul  8 03:13:21 calvisitor-10-105-162-108 kernel[0]: ARPT: 784632.788065: wl0: Roamed or switched channel, reason #8, bssid 5c:50:15:36:bc:03, last RSSI -69
Jul  8 03:13:24 calvisitor-10-105-162-108 QQ[10018]: DB Error: 1 "no such table: tb_c2cMsg_2909288299"
Jul  8 03:29:10 calvisitor-10-105-162-108 kernel[0]: AirPort: Link Down on en0. Reason 8 (Disassociated because station leaving).
Jul  8 03:32:43 calvisitor-10-105-162-228 kernel[0]: IOThunderboltSwitch<0>(0x0)::listenerCallback - Thunderbolt HPD packet for route = 0x0 port = 11 unplug = 0
Jul  8 03:32:43 calvisitor-10-105-162-228 kernel[0]: **** [BroadcomBluetoothHostControllerUSBTransport][start] -- Completed (matched on Device) -- 0x8800 ****
Jul  8 03:32:43 authorMacBook-Pro symptomsd[215]: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.
Jul  8 03:32:46 authorMacBook-Pro ntpd[207]: sigio_handler: sigio_handler_active != 1
Jul  8 04:13:32 calvisitor-10-105-162-228 kernel[0]: Wake reason: ARPT (Network)
Jul  8 04:13:32 calvisitor-10-105-162-228 blued[85]: Host controller terminated
Jul  8 04:13:32 calvisitor-10-105-162-228 kernel[0]: [HID] [MT] AppleActuatorHIDEventDriver::start entered
Jul  8 04:13:32 authorMacBook-Pro kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  8 04:13:39 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Evaluating
Jul  8 04:13:39 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Authenticated
Jul  8 04:16:14 calvisitor-10-105-161-176 kernel[0]: hibernate_setup(0) took 8471 ms
Jul  8 04:16:14 authorMacBook-Pro kernel[0]: in6_unlink_ifa: IPv6 address 0x77c911454f1528eb has no prefix
Jul  8 04:16:14 authorMacBook-Pro networkd[195]: __42-[NETClientConnection evaluateCrazyIvan46]_block_invoke CI46 - Hit by torpedo! QQ.10018 tc25995 121.51.36.148:443
Jul  8 04:16:15 authorMacBook-Pro kernel[0]: Setting BTCoex Config: enable_2G:1, profile_2g:0, enable_5G:1, profile_5G:0
Jul  8 04:16:21 calvisitor-10-105-161-176 QQ[10018]: button report: 0x8002bdf
Jul  8 04:16:21 calvisitor-10-105-161-176 QQ[10018]: FA||Url||taskID[2019353853] dealloc
Jul  8 04:18:45 calvisitor-10-105-161-176 blued[85]: [BluetoothHIDDeviceController]ERROR: Could not find the disconnected object
Jul  8 04:18:45 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Inactive
Jul  8 04:19:18 calvisitor-10-105-161-176 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from SUSPENDED to AUTO
Jul  8 04:54:03 calvisitor-10-105-161-176 kernel[0]: **** [IOBluetoothHostControllerUSBTransport][start] -- completed -- result = TRUE -- 0x2800 ****
Jul  8 04:54:12 authorMacBook-Pro QQ[10018]: tcp_connection_handle_connect_conditions_bad 26042 failed: 3 - No network route
Jul  8 05:34:31 calvisitor-10-105-160-181 kernel[0]: ARPT: 785104.988856: AirPort_Brcm43xx::syncPowerState: WWEN[enabled]
Jul  8 05:34:31 calvisitor-10-105-160-181 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  8 05:34:31 calvisitor-10-105-160-181 kernel[0]: hibernate_machine_init reading
Jul  8 05:34:31 calvisitor-10-105-160-181 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  8 05:34:31 calvisitor-10-105-160-181 kernel[0]: AppleThunderboltGenericHAL::earlyWake - complete - took 1 milliseconds
Jul  8 05:34:31 calvisitor-10-105-160-181 blued[85]: [BluetoothHIDDeviceController]ERROR: Could not find the disconnected object
Jul  8 05:34:44 calvisitor-10-105-162-124 configd[53]: setting hostname to "calvisitor-10-105-162-124.calvisitor.1918.berkeley.edu"
Jul  8 05:51:55 calvisitor-10-105-162-124 kernel[0]: hibernate image path: /var/vm/sleepimage
Jul  8 05:51:55 calvisitor-10-105-162-124 kernel[0]: efi pagecount 72
Jul  8 05:51:55 calvisitor-10-105-162-124 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  8 05:51:55 calvisitor-10-105-162-124 kernel[0]: hibernate_teardown_pmap_structs done: last_valid_compact_indx 315096
Jul  8 05:52:04 authorMacBook-Pro Mail[11203]: Unrecognized XSSimpleTypeDefinition: OneOff
Jul  8 05:52:07 calvisitor-10-105-162-124 kernel[0]: vm_compressor_fastwake_warmup completed - took 12461 msecs
Jul  8 05:55:23 authorMacBook-Pro corecaptured[39090]: CCFile::captureLog Received Capture notice id: 1499518522.558304, reason = AssocFail:sts:2_rsn:0
Jul  8 05:56:40 authorMacBook-Pro corecaptured[39090]: CCFile::copyFile fileName is [2017-07-08_05,55,23.694116]-io80211Family-002.pcapng, source path:/var/log/CoreCapture/com.apple.iokit.IO80211Family/IO80211AWDLPeerManager//[2017-07-08_05,55,23.694116]-io80211Family-002.pcapng, dest path:/Library/Logs/CrashReporter/CoreCapture/com.apple.iokit.IO80211Family/[2017-07-08_05,56,40.163377]=AssocFail:sts:2_rsn:0/IO80211AWDLPeerManager//[2017-07-08_05,55,23.694116]-io80211Family-002.pcapng
Jul  8 05:56:47 authorMacBook-Pro UserEventAgent[43]: Captive: CNPluginHandler en0: Authenticated
Jul  8 05:56:53 calvisitor-10-105-162-124 WeChat[24144]: jemmytest
Jul  8 06:02:25 calvisitor-10-105-162-124 com.apple.AddressBook.ContactsAccountsService[289]: [Accounts] Current connection, <NSXPCConnection: 0x7fda72614240> connection from pid 30318, doesn't have account access.
Jul  8 06:11:14 calvisitor-10-105-162-124 secd[276]:  securityd_xpc_dictionary_handler cloudd[326] copy_matching Error Domain=NSOSStatusErrorDomain Code=-50 "query missing class name" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}
Jul  8 06:11:46 calvisitor-10-105-162-124 WindowServer[184]: send_datagram_available_ping: pid 445 failed to act on a ping it dequeued before timing out.
Jul  8 06:22:47 calvisitor-10-105-162-124 WeChat[24144]: Unable to simultaneously satisfy constraints: ( "<NSAutoresizingMaskLayoutConstraint:0x7f9d574e5370 h=-&- v=-&- V:[NSScrollView:0x7f9d64abe480]-(11)-|   (Names: '|':NSView:0x7f9d6a9cc340 )>", "<NSAutoresizingMaskLayoutConstraint:0x7f9d7561a270 h=-&- v=-&- V:|-(45)-[NSScrollView:0x7f9d64abe480]   (Names: '|':NSView:0x7f9d6a9cc340 )>", "<NSAutoresizingMaskLayoutConstraint:0x7f9d6aa2cf10 h=-&- v=--& V:[NSView:0x7f9d53160e30(0)]>", "<NSAutoresizingMaskLayoutConstraint:0x7f9d75652870 h=-&- v=-&- V:|-(0)-[NSView:0x7f9d6a9cc340]   (Names: '|':NSView:0x7f9d53160e30 )>", "<NSAutoresizingMaskLayoutConstraint:0x7f9d77ef8f50 h=-&- v=-&- V:[NSView:0x7f9d6a9cc340]-(0)-|   (Names: '|':NSView:0x7f9d53160e30 )>" )  Will attempt to recover by breaking constraint <NSAutoresizingMaskLayoutConstraint:0x7f9d7561a270 h=-&- v=-&- V:|-(45)-[NSScrollView:0x7f9d64abe480]   (Names: '|':NSView:0x7f9d6a9cc340 )>  Set the NSUserDefault NSConstraintBasedLayoutVisualizeMutuallyExclusiveConstraints to YES to have -[NSWindow visualizeConstraints:] automatically called when this happens.  And/or, break on objc_exception_throw to catch this in the debugger.
Jul  8 06:22:52 calvisitor-10-105-162-124 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.WebContent.39146): Path not allowed in target domain: type = pid, path = /System/Library/StagedFrameworks/Safari/SafariShared.framework/Versions/A/XPCServices/com.apple.Safari.ImageDecoder.xpc/Contents/MacOS/com.apple.Safari.ImageDecoder error = 147: The specified service did not ship in the requestor's bundle, origin = /System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/XPCServices/com.apple.WebKit.WebContent.xpc
Jul  8 06:26:32 calvisitor-10-105-162-124 GoogleSoftwareUpdateAgent[39159]: 2017-07-08 06:26:32.364 GoogleSoftwareUpdateAgent[39159/0x7000002a0000] [lvl=2] -[KSAgentApp(KeystoneDelegate) updateEngineFinishedWithErrors:] Keystone finished: errors=0
Jul  8 06:26:32 calvisitor-10-105-162-124 GoogleSoftwareUpdateAgent[39159]: 2017-07-08 06:26:32.365 GoogleSoftwareUpdateAgent[39159/0x7000002a0000] [lvl=2] -[KSAgentApp(KeystoneThread) runKeystonesInThreadWithArg:] About to run checks for any other apps.
Jul  8 06:45:24 calvisitor-10-105-162-124 kernel[0]: ARPT: 787923.793193: wl0: leaveModulePoweredForOffloads: Wi-Fi will stay on.
Jul  8 06:46:42 calvisitor-10-105-162-124 WeChat[24144]:     Arranged view frame: {{0, 0}, {260, 877}}
Jul  8 07:01:03 calvisitor-10-105-162-124 CalendarAgent[279]: [com.apple.calendar.store.log.caldav.coredav] [Refusing to parse response to PROPPATCH because of content-type: [text/html; charset=UTF-8].]
Jul  8 07:12:41 calvisitor-10-105-162-124 kernel[0]: IO80211AWDLPeerManager::setAwdlOperatingMode Setting the AWDL operation mode from AUTO to SUSPENDED
Jul  8 07:12:45 calvisitor-10-105-162-124 corecaptured[39203]: CCProfileMonitor::setStreamEventHandler
Jul  8 07:25:32 calvisitor-10-105-162-124 locationd[82]: Location icon should now be in state 'Inactive'
Jul  8 07:27:17 calvisitor-10-105-162-124 CommCenter[263]: Telling CSI to go low power.
Jul  8 07:27:36 calvisitor-10-105-162-124 kernel[0]: ARPT: 790457.609414: AirPort_Brcm43xx::powerChange: System Sleep
Jul  8 07:29:50 authorMacBook-Pro Dock[307]: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=0 (null)/(null) opts=(null) when=2017-07-08 14:29:50 +0000 confidence=1 from=(null)/(null) (UABestAppSuggestionManager.m #319)
Jul  8 07:30:15 calvisitor-10-105-162-124 secd[276]:  SOSAccountThisDeviceCanSyncWithCircle sync with device failure: Error Domain=com.apple.security.sos.error Code=1035 "Account identity not set" UserInfo={NSDescription=Account identity not set}
Jul  8 07:31:20 calvisitor-10-105-162-124 kernel[0]: Bluetooth -- LE is supported - Disable LE meta event
Jul  8 07:32:03 calvisitor-10-105-162-124 kernel[0]: ARPT: 790564.863081: wl0: MDNS: IPV6 Addr: 2607:f140:6000:8:c6b3:1ff:fecd:467f
Jul  8 07:43:38 calvisitor-10-105-162-124 kernel[0]: USBMSC Identifier (non-unique): 000000000820 0x5ac 0x8406 0x820, 3
Jul  8 07:57:11 calvisitor-10-105-162-124 kernel[0]: AppleCamIn::systemWakeCall - messageType = 0xE0000340
Jul  8 08:10:46 calvisitor-10-105-162-124 kernel[0]: Wake reason: RTC (Alarm)
Jul  8 08:10:46 calvisitor-10-105-162-124 kernel[0]: AppleCamIn::wakeEventHandlerThread
Jun 15 12:12:34 combo sshd(pam_unix)[23407]: check pass; user unknown
Jun 15 12:12:34 combo sshd(pam_unix)[23394]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:12:34 combo sshd(pam_unix)[23403]: check pass; user unknown
Jun 15 12:12:34 combo sshd(pam_unix)[23396]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:12:34 combo sshd(pam_unix)[23407]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:12:34 combo sshd(pam_unix)[23403]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:12:34 combo sshd(pam_unix)[23412]: check pass; user unknown
Jun 15 12:12:34 combo sshd(pam_unix)[23412]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:13:19 combo sshd(pam_unix)[23414]: check pass; user unknown
Jun 15 12:13:19 combo sshd(pam_unix)[23414]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 12:13:20 combo sshd(pam_unix)[23416]: check pass; user unknown
Jun 15 12:13:20 combo sshd(pam_unix)[23416]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 
Jun 15 14:53:32 combo sshd(pam_unix)[23661]: check pass; user unknown
Jun 15 14:53:32 combo sshd(pam_unix)[23661]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com 
Jun 15 14:53:32 combo sshd(pam_unix)[23663]: check pass; user unknown
Jun 15 14:53:32 combo sshd(pam_unix)[23663]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com 
Jun 15 14:53:32 combo sshd(pam_unix)[23664]: check pass; user unknown
Jun 15 14:53:32 combo sshd(pam_unix)[23664]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com 
Jun 15 14:53:33 combo sshd(pam_unix)[23665]: check pass; user unknown
Jun 15 14:53:33 combo sshd(pam_unix)[23665]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com 
Jun 15 14:53:34 combo sshd(pam_unix)[23669]: check pass; user unknown
Jun 15 14:53:34 combo sshd(pam_unix)[23669]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com 
Jun 15 14:53:35 combo sshd(pam_unix)[23671]: check pass; user unknown
Jun 15 14:53:35 combo sshd(pam_unix)[23671]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com 
Jun 15 14:53:35 combo sshd(pam_unix)[23673]: check pass; user unknown
Jun 15 14:53:35 combo sshd(pam_unix)[23673]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com 
Jun 15 14:53:35 combo sshd(pam_unix)[23674]: check pass; user unknown
Jun 15 14:53:35 combo sshd(pam_unix)[23674]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com 
Jun 15 14:53:36 combo sshd(pam_unix)[23678]: check pass; user unknown
Jun 15 14:53:36 combo sshd(pam_unix)[23678]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com 
Jun 15 14:53:36 combo sshd(pam_unix)[23677]: check pass; user unknown
Jun 15 14:53:36 combo sshd(pam_unix)[23677]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com 
Jun 15 20:05:31 combo sshd(pam_unix)[24138]: check pass; user unknown
Jun 15 20:05:31 combo sshd(pam_unix)[24138]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net 
Jun 15 20:05:31 combo sshd(pam_unix)[24137]: check pass; user unknown
Jun 15 20:05:31 combo sshd(pam_unix)[24137]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net 
Jun 15 20:05:31 combo sshd(pam_unix)[24141]: check pass; user unknown
Jun 15 20:05:31 combo sshd(pam_unix)[24141]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net 
Jun 15 20:05:31 combo sshd(pam_unix)[24140]: check pass; user unknown
Jun 15 20:05:31 combo sshd(pam_unix)[24140]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net 
Jun 15 20:05:31 combo sshd(pam_unix)[24139]: check pass; user unknown
Jun 15 20:05:31 combo sshd(pam_unix)[24139]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net 
Jun 16 04:10:22 combo su(pam_unix)[25178]: session opened for user cyrus by (uid=0)
Jun 16 04:10:23 combo su(pam_unix)[25178]: session closed for user cyrus
Jun 16 04:10:24 combo logrotate: ALERT exited abnormally with [1]
Jun 16 04:16:17 combo su(pam_unix)[25548]: session opened for user news by (uid=0)
Jun 16 04:16:18 combo su(pam_unix)[25548]: session closed for user news
Jun 17 04:03:33 combo su(pam_unix)[27953]: session opened for user cyrus by (uid=0)
Jun 17 04:03:34 combo su(pam_unix)[27953]: session closed for user cyrus
Jun 17 04:03:36 combo logrotate: ALERT exited abnormally with [1]

Jun 17 04:09:43 combo su(pam_unix)[29190]: session opened for user news by (uid=0)
Jun 17 04:09:45 combo su(pam_unix)[29190]: session closed for user news
Jun 17 07:07:00 combo ftpd[29504]: connection from 24.54.76.216 (24-54-76-216.bflony.adelphia.net) at Fri Jun 17 07:07:00 2005 
Jun 17 07:07:00 combo ftpd[29508]: connection from 24.54.76.216 (24-54-76-216.bflony.adelphia.net) at Fri Jun 17 07:07:00 2005 
Jun 17 07:07:00 combo ftpd[29507]: connection from 24.54.76.216 (24-54-76-216.bflony.adelphia.net) at Fri Jun 17 07:07:00 2005 
Jun 17 07:07:00 combo ftpd[29505]: connection from 24.54.76.216 (24-54-76-216.bflony.adelphia.net) at Fri Jun 17 07:07:00 2005 
Jun 17 07:07:00 combo ftpd[29506]: connection from 24.54.76.216 (24-54-76-216.bflony.adelphia.net) at Fri Jun 17 07:07:00 2005 
Jun 17 07:07:00 combo ftpd[29509]: connection from 24.54.76.216 (24-54-76-216.bflony.adelphia.net) at Fri Jun 17 07:07:00 2005 
Jun 17 07:07:02 combo ftpd[29510]: connection from 24.54.76.216 (24-54-76-216.bflony.adelphia.net) at Fri Jun 17 07:07:02 2005 
Jun 17 07:07:04 combo ftpd[29511]: connection from 24.54.76.216 (24-54-76-216.bflony.adelphia.net) at Fri Jun 17 07:07:04 2005 
Jun 17 19:43:13 combo sshd(pam_unix)[30565]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.46.224.253  user=guest
Jun 17 20:29:26 combo sshd(pam_unix)[30631]: session opened for user test by (uid=509)
Jun 17 20:34:57 combo sshd(pam_unix)[30631]: session closed for user test
Jun 17 20:55:06 combo ftpd[30755]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Fri Jun 17 20:55:06 2005 
Jun 17 20:55:06 combo ftpd[30754]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Fri Jun 17 20:55:06 2005 
Jun 17 20:55:06 combo ftpd[30753]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Fri Jun 17 20:55:06 2005 
Jun 17 20:55:06 combo ftpd[30756]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Fri Jun 17 20:55:06 2005 
Jun 17 20:55:06 combo ftpd[30757]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Fri Jun 17 20:55:06 2005 
Jun 17 20:55:07 combo ftpd[30758]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Fri Jun 17 20:55:07 2005 
Jun 17 20:55:07 combo ftpd[30759]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Fri Jun 17 20:55:07 2005 
Jun 18 01:30:59 combo sshd(pam_unix)[31201]: check pass; user unknown
Jun 18 01:30:59 combo sshd(pam_unix)[31201]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=adsl-70-242-75-179.dsl.ksc2mo.swbell.net 
Jun 18 01:30:59 combo sshd(pam_unix)[31199]: check pass; user unknown
Jun 18 01:30:59 combo sshd(pam_unix)[31199]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=adsl-70-242-75-179.dsl.ksc2mo.swbell.net 
Jun 18 01:30:59 combo sshd(pam_unix)[31198]: check pass; user unknown
Jun 18 01:30:59 combo sshd(pam_unix)[31198]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=adsl-70-242-75-179.dsl.ksc2mo.swbell.net 
Jun 18 01:30:59 combo sshd(pam_unix)[31202]: check pass; user unknown
Jun 18 01:30:59 combo sshd(pam_unix)[31202]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=adsl-70-242-75-179.dsl.ksc2mo.swbell.net 
Jun 18 01:30:59 combo sshd(pam_unix)[31205]: check pass; user unknown
Jun 18 01:30:59 combo sshd(pam_unix)[31205]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=adsl-70-242-75-179.dsl.ksc2mo.swbell.net 
Jun 18 01:30:59 combo sshd(pam_unix)[31200]: check pass; user unknown
Jun 18 01:30:59 combo sshd(pam_unix)[31200]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=adsl-70-242-75-179.dsl.ksc2mo.swbell.net 
Jun 18 01:30:59 combo sshd(pam_unix)[31206]: check pass; user unknown
Jun 18 01:30:59 combo sshd(pam_unix)[31206]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=adsl-70-242-75-179.dsl.ksc2mo.swbell.net 
Jun 18 01:30:59 combo sshd(pam_unix)[31204]: check pass; user unknown
Jun 18 01:30:59 combo sshd(pam_unix)[31204]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=adsl-70-242-75-179.dsl.ksc2mo.swbell.net 
Jun 18 01:30:59 combo sshd(pam_unix)[31203]: check pass; user unknown
Jun 18 01:30:59 combo sshd(pam_unix)[31203]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=adsl-70-242-75-179.dsl.ksc2mo.swbell.net 
Jun 18 01:30:59 combo sshd(pam_unix)[31207]: check pass; user unknown
Jun 18 01:30:59 combo sshd(pam_unix)[31207]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=adsl-70-242-75-179.dsl.ksc2mo.swbell.net 
Jun 18 02:08:10 combo ftpd[31272]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:10 2005 
Jun 18 02:08:10 combo ftpd[31273]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:10 2005 
Jun 18 02:08:10 combo ftpd[31274]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:10 2005 
Jun 18 02:08:10 combo ftpd[31275]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:10 2005 
Jun 18 02:08:10 combo ftpd[31276]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:10 2005 
Jun 18 02:08:10 combo ftpd[31277]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:10 2005 
Jun 18 02:08:10 combo ftpd[31278]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:10 2005 
Jun 18 02:08:11 combo ftpd[31279]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:11 2005 
Jun 18 02:08:11 combo ftpd[31280]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:11 2005 
Jun 18 02:08:11 combo ftpd[31281]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:11 2005 
Jun 18 02:08:11 combo ftpd[31282]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:11 2005 
Jun 18 02:08:11 combo ftpd[31283]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:11 2005 
Jun 18 02:08:12 combo ftpd[31284]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:12 2005 
Jun 18 02:08:12 combo ftpd[31285]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:12 2005 
Jun 18 02:08:12 combo ftpd[31286]: connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:12 2005 
Jun 18 02:23:10 combo ftpd[31277]: User unknown timed out after 900 seconds at Sat Jun 18 02:23:10 2005 
Jun 18 04:07:05 combo su(pam_unix)[31791]: session opened for user cyrus by (uid=0)
Jun 18 04:07:06 combo su(pam_unix)[31791]: session closed for user cyrus
Jun 18 04:07:06 combo logrotate: ALERT exited abnormally with [1]
Jun 18 04:12:42 combo su(pam_unix)[32164]: session opened for user news by (uid=0)
Jun 18 04:12:43 combo su(pam_unix)[32164]: session closed for user news
Jun 19 04:08:55 combo su(pam_unix)[2192]: session opened for user cyrus by (uid=0)
Jun 19 04:08:55 combo su(pam_unix)[2192]: session closed for user cyrus
Jun 19 04:08:57 combo cups: cupsd shutdown succeeded
Jun 19 04:09:02 combo cups: cupsd startup succeeded
Jun 19 04:09:11 combo syslogd 1.4.1: restart.
Jun 19 04:09:11 combo logrotate: ALERT exited abnormally with [1]
Jun 19 04:15:18 combo su(pam_unix)[3676]: session opened for user news by (uid=0)
Jun 19 04:15:18 combo su(pam_unix)[3676]: session closed for user news
Jun 20 03:40:59 combo ftpd[8829]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8824]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8828]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8822]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8833]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8827]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8823]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8825]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8826]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8832]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8830]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 03:40:59 combo ftpd[8831]: connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005 
Jun 20 04:02:54 combo su(pam_unix)[9187]: session opened for user cyrus by (uid=0)
Jun 20 04:02:54 combo su(pam_unix)[9187]: session closed for user cyrus
Jun 20 04:02:55 combo logrotate: ALERT exited abnormally with [1]
Jun 20 04:08:37 combo su(pam_unix)[9558]: session opened for user news by (uid=0)
Jun 20 04:08:38 combo su(pam_unix)[9558]: session closed for user news
Jun 20 04:44:39 combo snmpd[2318]: Received SNMP packet(s) from 67.170.148.126 
Jun 20 09:20:05 combo sshd(pam_unix)[10035]: check pass; user unknown
Jun 20 09:20:05 combo sshd(pam_unix)[10035]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=65.166.159.14 
Jun 20 09:20:05 combo sshd(pam_unix)[10037]: check pass; user unknown
Jun 20 09:20:05 combo sshd(pam_unix)[10037]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=65.166.159.14 
Jun 20 09:20:05 combo sshd(pam_unix)[10039]: check pass; user unknown
Jun 20 09:20:05 combo sshd(pam_unix)[10039]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=65.166.159.14 
Jun 20 09:20:06 combo sshd(pam_unix)[10041]: check pass; user unknown
Jun 20 09:20:06 combo sshd(pam_unix)[10041]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=65.166.159.14 
Jun 20 09:20:07 combo sshd(pam_unix)[10043]: check pass; user unknown
Jun 20 09:20:07 combo sshd(pam_unix)[10043]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=65.166.159.14 
Jun 20 09:20:07 combo sshd(pam_unix)[10045]: check pass; user unknown
Jun 20 09:20:07 combo sshd(pam_unix)[10045]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=65.166.159.14 
Jun 20 09:20:07 combo sshd(pam_unix)[10047]: check pass; user unknown
Jun 20 09:20:07 combo sshd(pam_unix)[10047]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=65.166.159.14 
Jun 20 09:20:07 combo sshd(pam_unix)[10049]: check pass; user unknown
Jun 20 09:20:07 combo sshd(pam_unix)[10049]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=65.166.159.14 
Jun 20 09:20:08 combo sshd(pam_unix)[10051]: check pass; user unknown
Jun 20 09:20:08 combo sshd(pam_unix)[10051]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=65.166.159.14 
Jun 20 09:20:08 combo sshd(pam_unix)[10053]: check pass; user unknown
Jun 20 09:20:08 combo sshd(pam_unix)[10053]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=65.166.159.14 
Jun 21 04:06:57 combo su(pam_unix)[12098]: session opened for user cyrus by (uid=0)
Jun 21 04:06:58 combo su(pam_unix)[12098]: session closed for user cyrus
Jun 21 04:06:59 combo logrotate: ALERT exited abnormally with [1]
Jun 21 04:13:03 combo su(pam_unix)[13327]: session opened for user news by (uid=0)
Jun 21 04:13:04 combo su(pam_unix)[13327]: session closed for user news
Jun 21 08:56:36 combo sshd(pam_unix)[14281]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=217.60.212.66  user=guest
Jun 21 08:56:36 combo sshd(pam_unix)[14279]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=217.60.212.66  user=guest
Jun 21 08:56:36 combo sshd(pam_unix)[14282]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=217.60.212.66  user=guest
Jun 21 08:56:36 combo sshd(pam_unix)[14277]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=217.60.212.66  user=guest
Jun 21 08:56:36 combo sshd(pam_unix)[14278]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=217.60.212.66  user=guest
Jun 21 08:56:36 combo sshd(pam_unix)[14280]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=217.60.212.66  user=guest
Jun 22 03:17:26 combo sshd(pam_unix)[16207]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:26 combo sshd(pam_unix)[16206]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:35 combo sshd(pam_unix)[16210]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:36 combo sshd(pam_unix)[16212]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:36 combo sshd(pam_unix)[16213]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:45 combo sshd(pam_unix)[16216]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:46 combo sshd(pam_unix)[16218]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:46 combo sshd(pam_unix)[16219]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:52 combo sshd(pam_unix)[16222]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:55 combo sshd(pam_unix)[16224]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:56 combo sshd(pam_unix)[16226]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:17:56 combo sshd(pam_unix)[16227]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:02 combo sshd(pam_unix)[16230]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:05 combo sshd(pam_unix)[16232]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:06 combo sshd(pam_unix)[16234]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:06 combo sshd(pam_unix)[16235]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:10 combo sshd(pam_unix)[16238]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:12 combo sshd(pam_unix)[16240]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:15 combo sshd(pam_unix)[16242]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:16 combo sshd(pam_unix)[16244]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:16 combo sshd(pam_unix)[16245]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:20 combo sshd(pam_unix)[16248]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 03:18:22 combo sshd(pam_unix)[16250]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root
Jun 22 04:05:58 combo su(pam_unix)[16663]: session opened for user cyrus by (uid=0)
Jun 22 04:05:59 combo su(pam_unix)[16663]: session closed for user cyrus
Jun 22 04:06:00 combo logrotate: ALERT exited abnormally with [1]
Jun 22 04:11:42 combo su(pam_unix)[17037]: session opened for user news by (uid=0)
Jun 22 04:11:42 combo su(pam_unix)[17037]: session closed for user news
Jun 22 04:30:55 combo sshd(pam_unix)[17129]: check pass; user unknown
Jun 22 04:30:55 combo sshd(pam_unix)[17129]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=ip-216-69-169-168.ip.secureserver.net 
Jun 22 04:30:55 combo sshd(pam_unix)[17125]: check pass; user unknown
Jun 22 04:30:55 combo sshd(pam_unix)[17125]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=ip-216-69-169-168.ip.secureserver.net 
Jun 22 04:30:55 combo sshd(pam_unix)[17124]: check pass; user unknown
Jun 22 04:30:55 combo sshd(pam_unix)[17124]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=ip-216-69-169-168.ip.secureserver.net 
Jun 22 04:30:55 combo sshd(pam_unix)[17123]: check pass; user unknown
Jun 22 04:30:55 combo sshd(pam_unix)[17123]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=ip-216-69-169-168.ip.secureserver.net 
Jun 22 04:30:55 combo sshd(pam_unix)[17132]: check pass; user unknown
Jun 22 04:30:55 combo sshd(pam_unix)[17132]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=ip-216-69-169-168.ip.secureserver.net 
Jun 22 04:30:55 combo sshd(pam_unix)[17131]: check pass; user unknown
Jun 22 04:30:55 combo sshd(pam_unix)[17131]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=ip-216-69-169-168.ip.secureserver.net 
Jun 22 04:30:55 combo sshd(pam_unix)[17135]: check pass; user unknown
Jun 22 04:30:55 combo sshd(pam_unix)[17135]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=ip-216-69-169-168.ip.secureserver.net 
Jun 22 04:30:55 combo sshd(pam_unix)[17137]: check pass; user unknown
Jun 22 04:30:55 combo sshd(pam_unix)[17137]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=ip-216-69-169-168.ip.secureserver.net 
Jun 22 04:30:56 combo sshd(pam_unix)[17139]: check pass; user unknown
Jun 22 04:30:56 combo sshd(pam_unix)[17139]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=ip-216-69-169-168.ip.secureserver.net 
Jun 22 04:30:56 combo sshd(pam_unix)[17140]: check pass; user unknown
Jun 22 04:30:56 combo sshd(pam_unix)[17140]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=ip-216-69-169-168.ip.secureserver.net 
Jun 22 13:16:30 combo ftpd[17886]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17889]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17887]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17888]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17890]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17893]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17879]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17892]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17876]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17875]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17877]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17874]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17882]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17872]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17881]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:30 combo ftpd[17878]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:31 combo ftpd[17873]: connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005 
Jun 22 13:16:31 combo ftpd[17885]: connection from 210.245.165.136 () at Wed Jun 22 13:16:31 2005 
Jun 22 13:16:31 combo ftpd[17884]: connection from 210.245.165.136 () at Wed Jun 22 13:16:31 2005 
Jun 22 13:16:31 combo ftpd[17880]: connection from 210.245.165.136 () at Wed Jun 22 13:16:31 2005 
Jun 22 13:16:31 combo ftpd[17883]: connection from 210.245.165.136 () at Wed Jun 22 13:16:31 2005 
Jun 22 13:16:31 combo ftpd[17891]: connection from 210.245.165.136 () at Wed Jun 22 13:16:31 2005 
Jun 22 13:16:32 combo ftpd[17894]: connection from 210.245.165.136 () at Wed Jun 22 13:16:32 2005 
Jun 23 01:41:29 combo sshd(pam_unix)[18969]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=209.152.168.249  user=guest
Jun 23 01:41:29 combo sshd(pam_unix)[18971]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=209.152.168.249  user=guest
Jun 23 01:41:29 combo sshd(pam_unix)[18973]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=209.152.168.249  user=guest
Jun 23 01:41:29 combo sshd(pam_unix)[18975]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=209.152.168.249  user=guest
Jun 23 01:41:29 combo sshd(pam_unix)[18977]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=209.152.168.249  user=guest
Jun 23 01:41:30 combo sshd(pam_unix)[18983]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=209.152.168.249  user=guest
Jun 23 01:41:30 combo sshd(pam_unix)[18982]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=209.152.168.249  user=guest
Jun 23 01:41:30 combo sshd(pam_unix)[18981]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=209.152.168.249  user=guest
Jun 23 01:41:30 combo sshd(pam_unix)[18976]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=209.152.168.249  user=guest
Jun 23 01:41:32 combo sshd(pam_unix)[18967]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=209.152.168.249  user=guest
Jun 23 02:55:14 combo sshd(pam_unix)[19085]: check pass; user unknown
Jun 23 02:55:14 combo sshd(pam_unix)[19085]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=200.60.37.201 
Jun 23 04:05:28 combo su(pam_unix)[19534]: session opened for user cyrus by (uid=0)
Jun 23 04:05:29 combo su(pam_unix)[19534]: session closed for user cyrus
Jun 23 04:05:30 combo logrotate: ALERT exited abnormally with [1]
Jun 23 04:13:08 combo su(pam_unix)[24180]: session opened for user news by (uid=0)
Jun 23 04:13:09 combo su(pam_unix)[24180]: session closed for user news
Jun 23 23:30:03 combo sshd(pam_unix)[26189]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.22.3.51  user=root
Jun 23 23:30:03 combo sshd(pam_unix)[26188]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.22.3.51  user=root
Jun 23 23:30:04 combo sshd(pam_unix)[26193]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.22.3.51  user=root
Jun 23 23:30:04 combo sshd(pam_unix)[26182]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.22.3.51  user=root
Jun 23 23:30:04 combo sshd(pam_unix)[26183]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.22.3.51  user=root
Jun 23 23:30:04 combo sshd(pam_unix)[26184]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.22.3.51  user=root
Jun 23 23:30:05 combo sshd(pam_unix)[26197]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.22.3.51  user=root
Jun 23 23:30:05 combo sshd(pam_unix)[26198]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.22.3.51  user=root
Jun 23 23:30:05 combo sshd(pam_unix)[26190]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.22.3.51  user=root
Jun 24 04:05:34 combo su(pam_unix)[26938]: session opened for user cyrus by (uid=0)
Jun 24 04:05:35 combo su(pam_unix)[26938]: session closed for user cyrus
Jun 24 04:05:35 combo logrotate: ALERT exited abnormally with [1]
Jun 24 04:11:12 combo su(pam_unix)[27311]: session opened for user news by (uid=0)
Jun 24 04:11:12 combo su(pam_unix)[27311]: session closed for user news
Jun 24 18:55:11 combo ftpd[28568]: connection from 218.69.108.57 () at Fri Jun 24 18:55:11 2005 
Jun 24 18:55:11 combo ftpd[28566]: connection from 218.69.108.57 () at Fri Jun 24 18:55:11 2005 
Jun 24 18:55:11 combo ftpd[28562]: connection from 218.69.108.57 () at Fri Jun 24 18:55:11 2005 
Jun 24 18:55:11 combo ftpd[28563]: connection from 218.69.108.57 () at Fri Jun 24 18:55:11 2005 
Jun 24 18:55:11 combo ftpd[28565]: connection from 218.69.108.57 () at Fri Jun 24 18:55:11 2005 
Jun 24 18:55:11 combo ftpd[28564]: connection from 218.69.108.57 () at Fri Jun 24 18:55:11 2005 
Jun 24 18:55:11 combo ftpd[28567]: connection from 218.69.108.57 () at Fri Jun 24 18:55:11 2005 
Jun 24 18:55:18 combo ftpd[28570]: connection from 218.69.108.57 () at Fri Jun 24 18:55:18 2005 
Jun 24 18:55:18 combo ftpd[28571]: connection from 218.69.108.57 () at Fri Jun 24 18:55:18 2005 
Jun 24 18:55:18 combo ftpd[28569]: connection from 218.69.108.57 () at Fri Jun 24 18:55:18 2005 
Jun 24 18:55:18 combo ftpd[28572]: connection from 218.69.108.57 () at Fri Jun 24 18:55:18 2005 
Jun 24 18:55:18 combo ftpd[28574]: connection from 218.69.108.57 () at Fri Jun 24 18:55:18 2005 
Jun 24 18:55:18 combo ftpd[28573]: connection from 218.69.108.57 () at Fri Jun 24 18:55:18 2005 
Jun 25 04:04:25 combo su(pam_unix)[29690]: session opened for user cyrus by (uid=0)
Jun 25 04:04:25 combo su(pam_unix)[29690]: session closed for user cyrus
Jun 25 04:04:26 combo logrotate: ALERT exited abnormally with [1]
Jun 25 04:10:34 combo su(pam_unix)[30934]: session opened for user news by (uid=0)
Jun 25 04:10:35 combo su(pam_unix)[30934]: session closed for user news
Jun 25 04:41:49 combo sshd(pam_unix)[31031]: check pass; user unknown
Jun 25 04:41:49 combo sshd(pam_unix)[31031]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org 
Jun 25 04:41:49 combo sshd(pam_unix)[31035]: check pass; user unknown
Jun 25 04:41:49 combo sshd(pam_unix)[31035]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org 
Jun 25 04:41:49 combo sshd(pam_unix)[31034]: check pass; user unknown
Jun 25 04:41:49 combo sshd(pam_unix)[31034]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org 
Jun 25 04:41:49 combo sshd(pam_unix)[31036]: check pass; user unknown
Jun 25 04:41:49 combo sshd(pam_unix)[31036]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org 
Jun 25 04:41:51 combo sshd(pam_unix)[31025]: check pass; user unknown
Jun 25 04:41:51 combo sshd(pam_unix)[31025]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org 
Jun 25 04:41:51 combo sshd(pam_unix)[31028]: check pass; user unknown
Jun 25 04:41:51 combo sshd(pam_unix)[31028]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org 
Jun 25 04:41:51 combo sshd(pam_unix)[31020]: check pass; user unknown
Jun 25 04:41:51 combo sshd(pam_unix)[31020]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org 
Jun 25 04:41:51 combo sshd(pam_unix)[31027]: check pass; user unknown
Jun 25 04:41:51 combo sshd(pam_unix)[31027]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org 
Jun 25 04:41:51 combo sshd(pam_unix)[31021]: check pass; user unknown
Jun 25 04:41:51 combo sshd(pam_unix)[31021]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org 
Jun 25 04:41:51 combo sshd(pam_unix)[31024]: check pass; user unknown
Jun 25 04:41:51 combo sshd(pam_unix)[31024]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org 
Jun 25 09:20:24 combo ftpd[31475]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31477]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31474]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31476]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31473]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31467]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31470]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31465]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31472]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31468]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31463]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31471]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31469]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31462]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31464]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31466]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31461]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 09:20:24 combo ftpd[31460]: connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005 
Jun 25 19:25:30 combo ftpd[32328]: connection from 211.167.68.59 () at Sat Jun 25 19:25:30 2005 
Jun 25 19:25:30 combo ftpd[32329]: connection from 211.167.68.59 () at Sat Jun 25 19:25:30 2005 
Jun 25 19:25:30 combo ftpd[32324]: connection from 211.167.68.59 () at Sat Jun 25 19:25:30 2005 
Jun 25 19:25:30 combo ftpd[32326]: connection from 211.167.68.59 () at Sat Jun 25 19:25:30 2005 
Jun 25 19:25:30 combo ftpd[32323]: connection from 211.167.68.59 () at Sat Jun 25 19:25:30 2005 
Jun 25 19:25:30 combo ftpd[32327]: connection from 211.167.68.59 () at Sat Jun 25 19:25:30 2005 
Jun 25 19:25:30 combo ftpd[32325]: connection from 211.167.68.59 () at Sat Jun 25 19:25:30 2005 
Jun 25 19:25:30 combo ftpd[32331]: connection from 211.167.68.59 () at Sat Jun 25 19:25:30 2005 
Jun 25 19:25:30 combo ftpd[32330]: connection from 211.167.68.59 () at Sat Jun 25 19:25:30 2005 
Jun 25 19:25:31 combo ftpd[32333]: connection from 211.167.68.59 () at Sat Jun 25 19:25:31 2005 
Jun 25 19:25:31 combo ftpd[32332]: connection from 211.167.68.59 () at Sat Jun 25 19:25:31 2005 
Jun 25 19:25:31 combo ftpd[32334]: connection from 211.167.68.59 () at Sat Jun 25 19:25:31 2005 
Jun 25 19:25:34 combo ftpd[32335]: connection from 211.167.68.59 () at Sat Jun 25 19:25:34 2005 
Jun 26 04:04:17 combo su(pam_unix)[945]: session opened for user cyrus by (uid=0)
Jun 26 04:04:17 combo su(pam_unix)[945]: session closed for user cyrus
Jun 26 04:04:19 combo cups: cupsd shutdown succeeded
Jun 26 04:04:24 combo cups: cupsd startup succeeded
Jun 26 04:04:31 combo syslogd 1.4.1: restart.
Jun 26 04:04:31 combo logrotate: ALERT exited abnormally with [1]
Jun 26 04:10:02 combo su(pam_unix)[1546]: session opened for user news by (uid=0)
Jun 26 04:10:04 combo su(pam_unix)[1546]: session closed for user news
Jun 27 04:02:47 combo su(pam_unix)[7031]: session opened for user cyrus by (uid=0)
Jun 27 04:02:48 combo su(pam_unix)[7031]: session closed for user cyrus
Jun 27 04:02:49 combo logrotate: ALERT exited abnormally with [1]
Jun 27 04:08:56 combo su(pam_unix)[8269]: session opened for user news by (uid=0)
Jun 27 04:08:57 combo su(pam_unix)[8269]: session closed for user news
Jun 27 08:05:37 combo sshd(pam_unix)[8660]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=troi.bluesky-technologies.com  user=root
Jun 27 08:05:39 combo sshd(pam_unix)[8664]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=troi.bluesky-technologies.com  user=root
Jun 27 08:05:39 combo sshd(pam_unix)[8663]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=troi.bluesky-technologies.com  user=root
Jun 27 08:05:39 combo sshd(pam_unix)[8662]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=troi.bluesky-technologies.com  user=root
Jun 27 08:05:39 combo sshd(pam_unix)[8661]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=troi.bluesky-technologies.com  user=root
Jun 28 04:03:15 combo su(pam_unix)[10735]: session opened for user cyrus by (uid=0)
Jun 28 04:03:16 combo su(pam_unix)[10735]: session closed for user cyrus
Jun 28 04:03:17 combo logrotate: ALERT exited abnormally with [1]
Jun 28 04:09:00 combo su(pam_unix)[11106]: session opened for user news by (uid=0)
Jun 28 04:09:01 combo su(pam_unix)[11106]: session closed for user news
Jun 28 08:10:24 combo sshd(pam_unix)[11513]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61.53.154.93  user=root
Jun 28 08:10:24 combo sshd(pam_unix)[11517]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61.53.154.93  user=root
Jun 28 08:10:24 combo sshd(pam_unix)[11521]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61.53.154.93  user=root
Jun 28 08:10:24 combo sshd(pam_unix)[11510]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61.53.154.93  user=root
Jun 28 08:10:25 combo sshd(pam_unix)[11519]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61.53.154.93  user=root
Jun 28 08:10:26 combo sshd(pam_unix)[11514]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61.53.154.93  user=root
Jun 28 08:10:28 combo sshd(pam_unix)[11512]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61.53.154.93  user=root
Jun 28 08:10:29 combo sshd(pam_unix)[11509]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61.53.154.93  user=root
Jun 28 08:10:30 combo sshd(pam_unix)[11515]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61.53.154.93  user=root
Jun 28 20:58:46 combo sshd(pam_unix)[12665]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=62-192-102-94.dsl.easynet.nl  user=root
Jun 28 20:58:46 combo sshd(pam_unix)[12666]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=62-192-102-94.dsl.easynet.nl  user=root
Jun 28 20:58:47 combo sshd(pam_unix)[12669]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=62-192-102-94.dsl.easynet.nl  user=root
Jun 28 20:58:50 combo sshd(pam_unix)[12671]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=62-192-102-94.dsl.easynet.nl  user=root
Jun 28 20:58:52 combo sshd(pam_unix)[12673]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=62-192-102-94.dsl.easynet.nl  user=root
Jun 28 20:58:53 combo sshd(pam_unix)[12675]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=62-192-102-94.dsl.easynet.nl  user=root
Jun 28 20:58:53 combo sshd(pam_unix)[12677]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=62-192-102-94.dsl.easynet.nl  user=root
Jun 28 20:58:55 combo sshd(pam_unix)[12679]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=62-192-102-94.dsl.easynet.nl  user=root
Jun 28 20:58:55 combo sshd(pam_unix)[12681]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=62-192-102-94.dsl.easynet.nl  user=root
Jun 28 20:58:55 combo sshd(pam_unix)[12680]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=62-192-102-94.dsl.easynet.nl  user=root
Jun 28 21:42:46 combo sshd(pam_unix)[12756]: check pass; user unknown
Jun 28 21:42:46 combo sshd(pam_unix)[12756]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.115.206.155 
Jun 28 21:42:46 combo sshd(pam_unix)[12753]: check pass; user unknown
Jun 28 21:42:46 combo sshd(pam_unix)[12753]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.115.206.155 
Jun 28 21:42:46 combo sshd(pam_unix)[12752]: check pass; user unknown
Jun 28 21:42:46 combo sshd(pam_unix)[12752]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.115.206.155 
Jun 28 21:42:46 combo sshd(pam_unix)[12755]: check pass; user unknown
Jun 28 21:42:46 combo sshd(pam_unix)[12755]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.115.206.155 
Jun 28 21:42:46 combo sshd(pam_unix)[12754]: check pass; user unknown
Jun 28 21:42:46 combo sshd(pam_unix)[12754]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.115.206.155 
Jun 29 03:22:22 combo ftpd[13262]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13257]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13261]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13250]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13252]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13260]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13259]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13256]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13258]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13255]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13254]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13264]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13251]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13263]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13245]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13246]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13244]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13243]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13249]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13253]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13247]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:22 combo ftpd[13248]: connection from 61.74.96.178 () at Wed Jun 29 03:22:22 2005 
Jun 29 03:22:23 combo ftpd[13265]: connection from 61.74.96.178 () at Wed Jun 29 03:22:23 2005 
Jun 29 04:03:10 combo su(pam_unix)[13665]: session opened for user cyrus by (uid=0)
Jun 29 04:03:11 combo su(pam_unix)[13665]: session closed for user cyrus
Jun 29 04:03:12 combo logrotate: ALERT exited abnormally with [1]
Jun 29 04:09:29 combo su(pam_unix)[14891]: session opened for user news by (uid=0)
Jun 29 04:09:30 combo su(pam_unix)[14891]: session closed for user news
Jun 29 10:08:19 combo sshd(pam_unix)[15481]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=csnsu.nsuok.edu  user=root
Jun 29 10:08:19 combo sshd(pam_unix)[15477]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=csnsu.nsuok.edu  user=root
Jun 29 10:08:19 combo sshd(pam_unix)[15479]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=csnsu.nsuok.edu  user=root
Jun 29 10:08:19 combo sshd(pam_unix)[15478]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=csnsu.nsuok.edu  user=root
Jun 29 10:08:19 combo sshd(pam_unix)[15480]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=csnsu.nsuok.edu  user=root
Jun 29 10:08:19 combo sshd(pam_unix)[15476]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=csnsu.nsuok.edu  user=root
Jun 29 10:08:19 combo sshd(pam_unix)[15488]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=csnsu.nsuok.edu  user=root
Jun 29 10:08:20 combo sshd(pam_unix)[15490]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=csnsu.nsuok.edu  user=root
Jun 29 10:08:20 combo sshd(pam_unix)[15491]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=csnsu.nsuok.edu  user=root
Jun 29 10:08:20 combo sshd(pam_unix)[15492]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=csnsu.nsuok.edu  user=root
Jun 29 10:48:01 combo ftpd[15547]: connection from 208.62.55.75 () at Wed Jun 29 10:48:01 2005 
Jun 29 10:48:01 combo ftpd[15543]: connection from 208.62.55.75 () at Wed Jun 29 10:48:01 2005 
Jun 29 10:48:01 combo ftpd[15546]: connection from 208.62.55.75 () at Wed Jun 29 10:48:01 2005 
Jun 29 10:48:01 combo ftpd[15542]: connection from 208.62.55.75 () at Wed Jun 29 10:48:01 2005 
Jun 29 10:48:01 combo ftpd[15544]: connection from 208.62.55.75 () at Wed Jun 29 10:48:01 2005 
Jun 29 10:48:01 combo ftpd[15545]: connection from 208.62.55.75 () at Wed Jun 29 10:48:01 2005 
Jun 29 10:48:05 combo ftpd[15548]: connection from 208.62.55.75 () at Wed Jun 29 10:48:05 2005 
Jun 29 10:48:06 combo ftpd[15549]: connection from 208.62.55.75 () at Wed Jun 29 10:48:06 2005 
Jun 29 10:48:06 combo ftpd[15550]: connection from 208.62.55.75 () at Wed Jun 29 10:48:06 2005 
Jun 29 10:48:06 combo ftpd[15551]: connection from 208.62.55.75 () at Wed Jun 29 10:48:06 2005 
Jun 29 10:48:08 combo ftpd[15552]: connection from 208.62.55.75 () at Wed Jun 29 10:48:08 2005 
Jun 29 10:48:08 combo ftpd[15553]: connection from 208.62.55.75 () at Wed Jun 29 10:48:08 2005 
Jun 29 10:48:08 combo ftpd[15554]: connection from 208.62.55.75 () at Wed Jun 29 10:48:08 2005 
Jun 29 10:48:10 combo ftpd[15555]: connection from 208.62.55.75 () at Wed Jun 29 10:48:10 2005 
Jun 29 10:48:12 combo ftpd[15556]: connection from 208.62.55.75 () at Wed Jun 29 10:48:12 2005 
Jun 29 10:48:12 combo ftpd[15557]: connection from 208.62.55.75 () at Wed Jun 29 10:48:12 2005 
Jun 29 10:48:13 combo ftpd[15558]: connection from 208.62.55.75 () at Wed Jun 29 10:48:13 2005 
Jun 29 10:48:15 combo ftpd[15559]: connection from 208.62.55.75 () at Wed Jun 29 10:48:15 2005 
Jun 29 10:48:17 combo ftpd[15560]: connection from 208.62.55.75 () at Wed Jun 29 10:48:17 2005 
Jun 29 10:48:17 combo ftpd[15561]: connection from 208.62.55.75 () at Wed Jun 29 10:48:17 2005 
Jun 29 10:48:18 combo ftpd[15562]: connection from 208.62.55.75 () at Wed Jun 29 10:48:18 2005 
Jun 29 10:48:20 combo ftpd[15563]: connection from 208.62.55.75 () at Wed Jun 29 10:48:20 2005 
Jun 29 10:48:21 combo ftpd[15564]: connection from 208.62.55.75 () at Wed Jun 29 10:48:21 2005 
Jun 29 12:11:53 combo sshd(pam_unix)[15692]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:11:55 combo sshd(pam_unix)[15694]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:11:57 combo sshd(pam_unix)[15696]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:11:59 combo sshd(pam_unix)[15698]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:11:59 combo sshd(pam_unix)[15700]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:12:01 combo sshd(pam_unix)[15702]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:12:02 combo sshd(pam_unix)[15704]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:12:03 combo sshd(pam_unix)[15706]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:12:03 combo sshd(pam_unix)[15708]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:12:05 combo sshd(pam_unix)[15710]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:12:05 combo sshd(pam_unix)[15712]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:12:06 combo sshd(pam_unix)[15714]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 12:12:10 combo sshd(pam_unix)[15716]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=h64-187-1-131.gtconnect.net  user=root
Jun 29 14:44:35 combo ftpd[15917]: connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005 
Jun 29 14:44:35 combo ftpd[15922]: connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005 
Jun 29 14:44:35 combo ftpd[15918]: connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005 
Jun 29 14:44:35 combo ftpd[15919]: connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005 
Jun 29 14:44:35 combo ftpd[15923]: connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005 
Jun 29 14:44:35 combo ftpd[15920]: connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005 
Jun 29 14:44:35 combo ftpd[15921]: connection from 210.223.97.117 () at Wed Jun 29 14:44:35 2005 
Jun 30 04:03:41 combo su(pam_unix)[17407]: session opened for user cyrus by (uid=0)
Jun 30 04:03:42 combo su(pam_unix)[17407]: session closed for user cyrus
Jun 30 04:03:43 combo logrotate: ALERT exited abnormally with [1]
Jun 30 04:09:30 combo su(pam_unix)[17778]: session opened for user news by (uid=0)
Jun 30 04:09:31 combo su(pam_unix)[17778]: session closed for user news
Jun 30 12:48:38 combo sshd(pam_unix)[18559]: check pass; user unknown
Jun 30 12:48:38 combo sshd(pam_unix)[18559]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=biblioteka.wsi.edu.pl 
Jun 30 12:48:41 combo sshd(pam_unix)[18557]: check pass; user unknown
Jun 30 12:48:41 combo sshd(pam_unix)[18557]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=biblioteka.wsi.edu.pl 
Jun 30 12:48:41 combo sshd(pam_unix)[18550]: check pass; user unknown
Jun 30 12:48:41 combo sshd(pam_unix)[18550]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=biblioteka.wsi.edu.pl 
Jun 30 12:48:41 combo sshd(pam_unix)[18547]: check pass; user unknown
Jun 30 12:48:41 combo sshd(pam_unix)[18547]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=biblioteka.wsi.edu.pl 
Jun 30 12:48:41 combo sshd(pam_unix)[18549]: check pass; user unknown
Jun 30 12:48:41 combo sshd(pam_unix)[18549]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=biblioteka.wsi.edu.pl 
Jun 30 12:48:41 combo sshd(pam_unix)[18545]: check pass; user unknown
Jun 30 12:48:41 combo sshd(pam_unix)[18545]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=biblioteka.wsi.edu.pl 
Jun 30 12:48:41 combo sshd(pam_unix)[18548]: check pass; user unknown
Jun 30 12:48:41 combo sshd(pam_unix)[18548]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=biblioteka.wsi.edu.pl 
Jun 30 12:48:41 combo sshd(pam_unix)[18546]: check pass; user unknown
Jun 30 12:48:41 combo sshd(pam_unix)[18546]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=biblioteka.wsi.edu.pl 
Jun 30 19:03:00 combo sshd(pam_unix)[19088]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jun 30 19:03:00 combo sshd(pam_unix)[19094]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jun 30 19:03:00 combo sshd(pam_unix)[19087]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jun 30 19:03:01 combo sshd(pam_unix)[19093]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jun 30 19:03:01 combo sshd(pam_unix)[19085]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jun 30 19:03:03 combo sshd(pam_unix)[19091]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jun 30 19:03:04 combo sshd(pam_unix)[19099]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jun 30 19:03:04 combo sshd(pam_unix)[19101]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jun 30 19:03:05 combo sshd(pam_unix)[19103]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jun 30 19:03:07 combo sshd(pam_unix)[19097]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jun 30 20:16:17 combo sshd(pam_unix)[19202]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jun 30 20:16:17 combo sshd(pam_unix)[19203]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jun 30 20:16:26 combo sshd(pam_unix)[19209]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jun 30 20:16:26 combo sshd(pam_unix)[19208]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jun 30 20:16:30 combo sshd(pam_unix)[19222]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jun 30 20:53:04 combo klogind[19272]: Authentication failed from 163.27.187.39 (163.27.187.39): Permission denied in replay cache code 
Jun 30 20:53:04 combo klogind[19272]: Kerberos authentication failed 
Jun 30 20:53:04 combo klogind[19287]: Authentication failed from 163.27.187.39 (163.27.187.39): Permission denied in replay cache code 
Jun 30 20:53:04 combo klogind[19287]: Kerberos authentication failed 
Jun 30 20:53:04 combo klogind[19286]: Authentication failed from 163.27.187.39 (163.27.187.39): Permission denied in replay cache code 
Jun 30 20:53:04 combo klogind[19286]: Kerberos authentication failed 
Jun 30 20:53:04 combo klogind[19271]: Authentication failed from 163.27.187.39 (163.27.187.39): Permission denied in replay cache code 
Jun 30 20:53:04 combo klogind[19271]: Kerberos authentication failed 
Jun 30 20:53:04 combo klogind[19270]: Authentication failed from 163.27.187.39 (163.27.187.39): Permission denied in replay cache code 
Jun 30 20:53:04 combo klogind[19270]: Kerberos authentication failed 
Jun 30 20:53:04 combo klogind[19269]: Authentication failed from 163.27.187.39 (163.27.187.39): Permission denied in replay cache code 
Jun 30 20:53:04 combo klogind[19269]: Kerberos authentication failed 
Jun 30 20:53:04 combo klogind[19268]: Authentication failed from 163.27.187.39 (163.27.187.39): Permission denied in replay cache code 
Jun 30 20:53:04 combo klogind[19288]: Authentication failed from 163.27.187.39 (163.27.187.39): Permission denied in replay cache code 
Jun 30 20:53:04 combo klogind[19274]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:04 combo klogind[19268]: Kerberos authentication failed 
Jun 30 20:53:04 combo klogind[19288]: Kerberos authentication failed 
Jun 30 20:53:04 combo klogind[19274]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19266]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19266]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19267]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19267]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19278]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19278]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19273]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19273]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19276]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19276]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19275]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19275]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19277]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19277]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19279]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19279]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19280]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19281]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19282]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19280]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19283]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19281]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19285]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19284]: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort 
Jun 30 20:53:06 combo klogind[19282]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19283]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19285]: Kerberos authentication failed 
Jun 30 20:53:06 combo klogind[19284]: Kerberos authentication failed 
Jun 30 22:16:32 combo sshd(pam_unix)[19432]: session opened for user test by (uid=509)
Jun 30 22:16:32 combo sshd(pam_unix)[19431]: session opened for user test by (uid=509)
Jun 30 22:16:32 combo sshd(pam_unix)[19433]: session opened for user test by (uid=509)
Jun 30 22:16:32 combo sshd(pam_unix)[19434]: session opened for user test by (uid=509)
Jun 30 22:16:32 combo sshd(pam_unix)[19435]: session opened for user test by (uid=509)
Jun 30 22:16:32 combo sshd(pam_unix)[19436]: session opened for user test by (uid=509)
Jun 30 22:16:32 combo sshd(pam_unix)[19438]: session opened for user test by (uid=509)
Jun 30 22:16:32 combo sshd(pam_unix)[19437]: session opened for user test by (uid=509)
Jun 30 22:16:32 combo sshd(pam_unix)[19432]: session closed for user test
Jun 30 22:16:32 combo sshd(pam_unix)[19431]: session closed for user test
Jun 30 22:16:32 combo sshd(pam_unix)[19439]: session opened for user test by (uid=509)
Jun 30 22:16:32 combo sshd(pam_unix)[19440]: session opened for user test by (uid=509)
Jun 30 22:16:32 combo sshd(pam_unix)[19434]: session closed for user test
Jun 30 22:16:32 combo sshd(pam_unix)[19435]: session closed for user test
Jun 30 22:16:33 combo sshd(pam_unix)[19433]: session closed for user test
Jun 30 22:16:33 combo sshd(pam_unix)[19436]: session closed for user test
Jun 30 22:16:33 combo sshd(pam_unix)[19437]: session closed for user test
Jun 30 22:16:33 combo sshd(pam_unix)[19438]: session closed for user test
Jun 30 22:16:33 combo sshd(pam_unix)[19439]: session closed for user test
Jun 30 22:16:33 combo sshd(pam_unix)[19440]: session closed for user test
Jul  1 00:21:28 combo sshd(pam_unix)[19630]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jul  1 00:21:28 combo sshd(pam_unix)[19628]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jul  1 00:21:30 combo sshd(pam_unix)[19640]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jul  1 00:21:30 combo sshd(pam_unix)[19642]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jul  1 00:21:30 combo sshd(pam_unix)[19632]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jul  1 00:21:31 combo sshd(pam_unix)[19643]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jul  1 00:21:31 combo sshd(pam_unix)[19631]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jul  1 00:21:32 combo sshd(pam_unix)[19636]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jul  1 00:21:32 combo sshd(pam_unix)[19645]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jul  1 00:21:35 combo sshd(pam_unix)[19637]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=60.30.224.116  user=root
Jul  1 04:05:17 combo su(pam_unix)[20298]: session opened for user cyrus by (uid=0)
Jul  1 04:05:18 combo su(pam_unix)[20298]: session closed for user cyrus
Jul  1 04:05:19 combo logrotate: ALERT exited abnormally with [1]
Jul  1 04:11:35 combo su(pam_unix)[21530]: session opened for user news by (uid=0)
Jul  1 04:11:36 combo su(pam_unix)[21530]: session closed for user news
Jul  1 05:02:26 combo sshd(pam_unix)[21689]: session opened for user test by (uid=509)
Jul  1 05:02:26 combo sshd(pam_unix)[21689]: session closed for user test
Jul  1 05:02:26 combo sshd(pam_unix)[21691]: session opened for user test by (uid=509)
Jul  1 05:02:26 combo sshd(pam_unix)[21691]: session closed for user test
Jul  1 05:02:26 combo sshd(pam_unix)[21692]: session opened for user test by (uid=509)
Jul  1 05:02:26 combo sshd(pam_unix)[21692]: session closed for user test
Jul  1 05:02:27 combo sshd(pam_unix)[21693]: session opened for user test by (uid=509)
Jul  1 05:02:27 combo sshd(pam_unix)[21693]: session closed for user test
Jul  1 07:57:30 combo ftpd[21952]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21951]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21957]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21953]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21954]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21955]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21950]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21956]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21958]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21945]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21959]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21940]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21941]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21946]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21937]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21943]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21944]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21948]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21947]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21942]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21938]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21949]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 07:57:30 combo ftpd[21939]: connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005 
Jul  1 09:14:43 combo sshd(pam_unix)[22099]: session opened for user test by (uid=509)
Jul  1 09:14:43 combo sshd(pam_unix)[22104]: session opened for user test by (uid=509)
Jul  1 09:14:43 combo sshd(pam_unix)[22104]: session closed for user test
Jul  1 09:14:43 combo sshd(pam_unix)[22106]: session opened for user test by (uid=509)
Jul  1 09:14:44 combo sshd(pam_unix)[22099]: session closed for user test
Jul  1 09:14:44 combo sshd(pam_unix)[22112]: session opened for user test by (uid=509)
Jul  1 09:14:44 combo sshd(pam_unix)[22106]: session closed for user test
Jul  1 09:14:44 combo sshd(pam_unix)[22112]: session closed for user test
Jul  1 10:56:41 combo sshd(pam_unix)[22272]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jul  1 10:56:41 combo sshd(pam_unix)[22269]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jul  1 10:56:41 combo sshd(pam_unix)[22277]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jul  1 10:56:42 combo sshd(pam_unix)[22271]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jul  1 10:56:42 combo sshd(pam_unix)[22273]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jul  1 10:56:42 combo sshd(pam_unix)[22270]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jul  1 10:56:43 combo sshd(pam_unix)[22268]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jul  1 10:56:43 combo sshd(pam_unix)[22274]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jul  1 10:56:44 combo sshd(pam_unix)[22276]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jul  1 10:56:44 combo sshd(pam_unix)[22275]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=195.129.24.210  user=root
Jul  2 01:41:32 combo sshd(pam_unix)[23533]: session opened for user test by (uid=509)
Jul  2 01:41:32 combo sshd(pam_unix)[23534]: session opened for user test by (uid=509)
Jul  2 01:41:32 combo sshd(pam_unix)[23535]: session opened for user test by (uid=509)
Jul  2 01:41:32 combo sshd(pam_unix)[23536]: session opened for user test by (uid=509)
Jul  2 01:41:32 combo sshd(pam_unix)[23533]: session closed for user test
Jul  2 01:41:32 combo sshd(pam_unix)[23534]: session closed for user test
Jul  2 01:41:32 combo sshd(pam_unix)[23535]: session closed for user test
Jul  2 01:41:32 combo sshd(pam_unix)[23536]: session closed for user test
Jul  2 01:41:33 combo sshd(pam_unix)[23544]: session opened for user test by (uid=509)
Jul  2 01:41:33 combo sshd(pam_unix)[23544]: session closed for user test
Jul  2 01:41:33 combo sshd(pam_unix)[23545]: session opened for user test by (uid=509)
Jul  2 01:41:33 combo sshd(pam_unix)[23546]: session opened for user test by (uid=509)
Jul  2 01:41:33 combo sshd(pam_unix)[23547]: session opened for user test by (uid=509)
Jul  2 01:41:33 combo sshd(pam_unix)[23545]: session closed for user test
Jul  2 01:41:33 combo sshd(pam_unix)[23546]: session closed for user test
Jul  2 01:41:33 combo sshd(pam_unix)[23547]: session closed for user test
Jul  2 04:04:02 combo su(pam_unix)[24117]: session opened for user cyrus by (uid=0)
Jul  2 04:04:02 combo su(pam_unix)[24117]: session closed for user cyrus
Jul  2 04:04:03 combo logrotate: ALERT exited abnormally with [1]
Jul  2 04:09:53 combo su(pam_unix)[24511]: session opened for user news by (uid=0)
Jul  2 04:09:54 combo su(pam_unix)[24511]: session closed for user news
Jul  2 04:15:33 combo sshd(pam_unix)[24588]: check pass; user unknown
Jul  2 04:15:33 combo sshd(pam_unix)[24588]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=zummit.com 
Jul  2 04:15:33 combo sshd(pam_unix)[24587]: check pass; user unknown
Jul  2 04:15:33 combo sshd(pam_unix)[24587]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=zummit.com 
Jul  2 04:15:33 combo sshd(pam_unix)[24590]: check pass; user unknown
Jul  2 04:15:33 combo sshd(pam_unix)[24590]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=zummit.com 
Jul  2 04:15:43 combo sshd(pam_unix)[24573]: check pass; user unknown
Jul  2 04:15:43 combo sshd(pam_unix)[24573]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=zummit.com 
Jul  2 04:15:43 combo sshd(pam_unix)[24574]: check pass; user unknown
Jul  2 04:15:43 combo sshd(pam_unix)[24574]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=zummit.com 
Jul  2 04:15:43 combo sshd(pam_unix)[24576]: check pass; user unknown
Jul  2 04:15:43 combo sshd(pam_unix)[24576]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=zummit.com 
Jul  2 04:15:43 combo sshd(pam_unix)[24578]: check pass; user unknown
Jul  2 04:15:43 combo sshd(pam_unix)[24578]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=zummit.com 
Jul  2 04:15:43 combo sshd(pam_unix)[24579]: check pass; user unknown
Jul  2 04:15:43 combo sshd(pam_unix)[24579]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=zummit.com 
Jul  2 04:15:43 combo sshd(pam_unix)[24581]: check pass; user unknown
Jul  2 04:15:43 combo sshd(pam_unix)[24581]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=zummit.com 
Jul  2 04:15:43 combo sshd(pam_unix)[24583]: check pass; user unknown
Jul  2 04:15:43 combo sshd(pam_unix)[24583]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=zummit.com 
Jul  3 04:07:47 combo su(pam_unix)[26964]: session opened for user cyrus by (uid=0)
Jul  3 04:07:48 combo su(pam_unix)[26964]: session closed for user cyrus
Jul  3 04:07:49 combo cups: cupsd shutdown succeeded
Jul  3 04:07:55 combo cups: cupsd startup succeeded
Jul  3 04:08:03 combo syslogd 1.4.1: restart.
Jul  3 04:08:03 combo logrotate: ALERT exited abnormally with [1]
Jul  3 04:14:00 combo su(pam_unix)[28416]: session opened for user news by (uid=0)
Jul  3 04:14:01 combo su(pam_unix)[28416]: session closed for user news
Jul  3 10:05:25 combo ftpd[32069]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32067]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32070]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32053]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32066]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32068]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32065]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32062]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32063]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32059]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32060]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32071]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32051]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32057]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32050]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32056]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32058]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32052]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32061]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32055]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32064]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32054]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 10:05:25 combo ftpd[32049]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005 
Jul  3 23:16:09 combo ftpd[768]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[772]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[769]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[767]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[765]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[766]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[770]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[764]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[757]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[763]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[758]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[761]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[762]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[759]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[756]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[773]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[760]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[774]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[775]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[771]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[776]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[777]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  3 23:16:09 combo ftpd[778]: connection from 62.99.164.82 (62.99.164.82.sh.interxion.inode.at) at Sun Jul  3 23:16:09 2005 
Jul  4 04:03:06 combo su(pam_unix)[1583]: session opened for user cyrus by (uid=0)
Jul  4 04:03:07 combo su(pam_unix)[1583]: session closed for user cyrus
Jul  4 04:03:08 combo logrotate: ALERT exited abnormally with [1]
Jul  4 04:08:48 combo su(pam_unix)[1965]: session opened for user news by (uid=0)
Jul  4 04:08:49 combo su(pam_unix)[1965]: session closed for user news
Jul  4 09:33:09 combo sshd(pam_unix)[2543]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.76.59.29  user=root
Jul  4 09:33:10 combo sshd(pam_unix)[2544]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.76.59.29  user=root
Jul  4 09:33:14 combo sshd(pam_unix)[2547]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.76.59.29  user=root
Jul  4 12:52:44 combo ftpd[2839]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2838]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2841]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2840]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2831]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2829]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2835]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2828]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2832]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2830]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2837]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2833]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2834]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2836]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2824]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2822]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2821]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2827]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2823]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2825]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2842]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2826]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 12:52:44 combo ftpd[2843]: connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005 
Jul  4 19:15:48 combo sshd(pam_unix)[3378]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:15:48 combo sshd(pam_unix)[3380]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:15:49 combo sshd(pam_unix)[3382]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:15:51 combo sshd(pam_unix)[3384]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:15:51 combo sshd(pam_unix)[3386]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:15:52 combo sshd(pam_unix)[3388]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:15:54 combo sshd(pam_unix)[3390]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:15:54 combo sshd(pam_unix)[3391]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:15:57 combo sshd(pam_unix)[3394]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:15:59 combo sshd(pam_unix)[3396]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:16:00 combo sshd(pam_unix)[3399]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:16:00 combo sshd(pam_unix)[3398]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  4 19:16:01 combo sshd(pam_unix)[3402]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220.117.241.87  user=root
Jul  5 04:03:16 combo su(pam_unix)[4474]: session opened for user cyrus by (uid=0)
Jul  5 04:03:17 combo su(pam_unix)[4474]: session closed for user cyrus
Jul  5 04:03:18 combo logrotate: ALERT exited abnormally with [1]
Jul  5 04:09:29 combo su(pam_unix)[5699]: session opened for user news by (uid=0)
Jul  5 04:09:30 combo su(pam_unix)[5699]: session closed for user news
Jul  5 13:36:28 combo sshd(pam_unix)[6552]: check pass; user unknown
Jul  5 13:36:28 combo sshd(pam_unix)[6552]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.229.150.228 
Jul  5 13:36:28 combo sshd(pam_unix)[6554]: check pass; user unknown
Jul  5 13:36:28 combo sshd(pam_unix)[6554]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.229.150.228 
Jul  5 13:36:33 combo sshd(pam_unix)[6556]: check pass; user unknown
Jul  5 13:36:33 combo sshd(pam_unix)[6556]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.229.150.228 
Jul  5 13:36:36 combo sshd(pam_unix)[6558]: check pass; user unknown
Jul  5 13:36:36 combo sshd(pam_unix)[6558]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.229.150.228 
Jul  5 13:36:37 combo sshd(pam_unix)[6560]: check pass; user unknown
Jul  5 13:36:37 combo sshd(pam_unix)[6560]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.229.150.228 
Jul  5 13:52:21 combo ftpd[6590]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6582]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6580]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6589]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6586]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6591]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6587]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6583]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6588]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6581]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6585]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6592]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:21 combo ftpd[6584]: connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005 
Jul  5 13:52:23 combo ftpd[6593]: connection from 211.72.2.106 () at Tue Jul  5 13:52:23 2005 
Jul  5 13:52:23 combo ftpd[6595]: connection from 211.72.2.106 () at Tue Jul  5 13:52:23 2005 
Jul  5 13:52:23 combo ftpd[6594]: connection from 211.72.2.106 () at Tue Jul  5 13:52:23 2005 
Jul  5 13:52:23 combo ftpd[6596]: connection from 211.72.2.106 () at Tue Jul  5 13:52:23 2005 
Jul  5 13:52:23 combo ftpd[6597]: connection from 211.72.2.106 () at Tue Jul  5 13:52:23 2005 
Jul  5 13:52:23 combo ftpd[6598]: connection from 211.72.2.106 () at Tue Jul  5 13:52:23 2005 
Jul  5 13:52:23 combo ftpd[6600]: connection from 211.72.2.106 () at Tue Jul  5 13:52:23 2005 
Jul  5 13:52:23 combo ftpd[6601]: connection from 211.72.2.106 () at Tue Jul  5 13:52:23 2005 
Jul  5 13:52:23 combo ftpd[6602]: connection from 211.72.2.106 () at Tue Jul  5 13:52:23 2005 
Jul  5 13:52:23 combo ftpd[6599]: connection from 211.72.2.106 () at Tue Jul  5 13:52:23 2005 
Jul  6 02:22:31 combo sshd(pam_unix)[7694]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.16.122.48  user=root
Jul  6 02:22:32 combo sshd(pam_unix)[7702]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.16.122.48  user=root
Jul  6 02:22:32 combo sshd(pam_unix)[7697]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.16.122.48  user=root
Jul  6 02:22:32 combo sshd(pam_unix)[7696]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.16.122.48  user=root
Jul  6 02:22:33 combo sshd(pam_unix)[7704]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.16.122.48  user=root
Jul  6 04:03:08 combo su(pam_unix)[8195]: session opened for user cyrus by (uid=0)
Jul  6 04:03:09 combo su(pam_unix)[8195]: session closed for user cyrus
Jul  6 04:03:10 combo logrotate: ALERT exited abnormally with [1]
Jul  6 04:08:43 combo su(pam_unix)[8565]: session opened for user news by (uid=0)
Jul  6 04:08:44 combo su(pam_unix)[8565]: session closed for user news
Jul  6 18:00:56 combo ftpd[9772]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9773]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9777]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9774]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9775]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9769]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9776]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9771]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9770]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9765]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9764]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9757]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9766]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9758]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9759]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9760]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9761]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9762]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9767]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9763]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:56 combo ftpd[9768]: connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005 
Jul  6 18:00:57 combo ftpd[9778]: connection from 211.72.151.162 () at Wed Jul  6 18:00:57 2005 
Jul  6 18:00:57 combo ftpd[9779]: connection from 211.72.151.162 () at Wed Jul  6 18:00:57 2005 
Jul  7 04:04:31 combo su(pam_unix)[10961]: session opened for user cyrus by (uid=0)
Jul  7 04:04:32 combo su(pam_unix)[10961]: session closed for user cyrus
Jul  7 04:04:33 combo logrotate: ALERT exited abnormally with [1]
Jul  7 04:10:44 combo su(pam_unix)[12193]: session opened for user news by (uid=0)
Jul  7 04:10:45 combo su(pam_unix)[12193]: session closed for user news
Jul  7 07:18:12 combo sshd(pam_unix)[12518]: session opened for user test by (uid=509)
Jul  7 07:18:12 combo sshd(pam_unix)[12519]: session opened for user test by (uid=509)
Jul  7 07:18:12 combo sshd(pam_unix)[12518]: session closed for user test
Jul  7 07:18:12 combo sshd(pam_unix)[12520]: session opened for user test by (uid=509)
Jul  7 07:18:12 combo sshd(pam_unix)[12520]: session closed for user test
Jul  7 07:18:12 combo sshd(pam_unix)[12519]: session closed for user test
Jul  7 07:18:13 combo sshd(pam_unix)[12524]: session opened for user test by (uid=509)
Jul  7 07:18:13 combo sshd(pam_unix)[12525]: session opened for user test by (uid=509)
Jul  7 07:18:13 combo sshd(pam_unix)[12524]: session closed for user test
Jul  7 07:18:13 combo sshd(pam_unix)[12527]: session opened for user test by (uid=509)
Jul  7 07:18:14 combo sshd(pam_unix)[12525]: session closed for user test
Jul  7 07:18:14 combo sshd(pam_unix)[12527]: session closed for user test
Jul  7 08:06:12 combo gpm[2094]: *** info [mice.c(1766)]: 
Jul  7 08:06:12 combo gpm[2094]: imps2: Auto-detected intellimouse PS/2
Jul  7 08:06:15 combo login(pam_unix)[2421]: session opened for user root by LOGIN(uid=0)
Jul  7 08:06:15 combo  -- root[2421]: ROOT LOGIN ON tty2
Jul  7 08:09:10 combo login(pam_unix)[2421]: session closed for user root
Jul  7 08:09:11 combo udev[12754]: removing device node '/udev/vcsa2'
Jul  7 08:09:11 combo udev[12753]: removing device node '/udev/vcs2'
Jul  7 08:09:11 combo udev[12777]: creating device node '/udev/vcs2'
Jul  7 08:09:11 combo udev[12778]: creating device node '/udev/vcsa2'
Jul  7 08:09:11 combo udev[12786]: removing device node '/udev/vcs2'
Jul  7 08:09:11 combo udev[12790]: removing device node '/udev/vcsa2'
Jul  7 08:09:11 combo udev[12795]: creating device node '/udev/vcs2'
Jul  7 08:09:11 combo udev[12798]: creating device node '/udev/vcsa2'
Jul  7 14:18:55 combo sshd(pam_unix)[13317]: check pass; user unknown
Jul  7 14:18:55 combo sshd(pam_unix)[13317]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c9063558.virtua.com.br 
Jul  7 14:18:57 combo sshd(pam_unix)[13321]: check pass; user unknown
Jul  7 14:18:57 combo sshd(pam_unix)[13321]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c9063558.virtua.com.br 
Jul  7 14:18:58 combo sshd(pam_unix)[13318]: check pass; user unknown
Jul  7 14:18:58 combo sshd(pam_unix)[13318]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c9063558.virtua.com.br 
Jul  7 14:18:59 combo sshd(pam_unix)[13323]: check pass; user unknown
Jul  7 14:18:59 combo sshd(pam_unix)[13323]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c9063558.virtua.com.br 
Jul  7 16:33:52 combo ftpd[13521]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13513]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13511]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13512]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13516]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13515]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13518]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13522]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13517]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13520]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13519]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 16:33:52 combo ftpd[13514]: connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005 
Jul  7 23:09:45 combo ftpd[14105]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14106]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14103]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14107]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14104]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14108]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14109]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14110]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14111]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14112]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14113]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14114]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14115]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14116]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14118]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14119]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:45 combo ftpd[14117]: connection from 221.4.102.93 () at Thu Jul  7 23:09:45 2005 
Jul  7 23:09:47 combo ftpd[14120]: connection from 221.4.102.93 () at Thu Jul  7 23:09:47 2005 
Jul  7 23:09:50 combo ftpd[14121]: connection from 221.4.102.93 () at Thu Jul  7 23:09:50 2005 
Jul  8 04:04:19 combo su(pam_unix)[14943]: session opened for user cyrus by (uid=0)
Jul  8 04:04:19 combo su(pam_unix)[14943]: session closed for user cyrus
Jul  8 04:04:20 combo logrotate: ALERT exited abnormally with [1]
Jul  8 04:12:07 combo su(pam_unix)[19593]: session opened for user news by (uid=0)
Jul  8 04:12:08 combo su(pam_unix)[19593]: session closed for user news
Jul  8 20:14:55 combo sshd(pam_unix)[20963]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=212.0.132.20  user=test
Jul  8 20:14:56 combo sshd(pam_unix)[20969]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=212.0.132.20  user=test
Jul  8 20:14:56 combo sshd(pam_unix)[20968]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=212.0.132.20  user=test
Jul  8 20:14:56 combo sshd(pam_unix)[20964]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=212.0.132.20  user=test
Jul  9 04:04:23 combo su(pam_unix)[21991]: session opened for user cyrus by (uid=0)
Jul  9 04:04:24 combo su(pam_unix)[21991]: session closed for user cyrus
Jul  9 04:04:25 combo logrotate: ALERT exited abnormally with [1]
Jul  9 04:10:11 combo su(pam_unix)[22368]: session opened for user news by (uid=0)
Jul  9 04:10:12 combo su(pam_unix)[22368]: session closed for user news
Jul  9 11:35:59 combo ftpd[23028]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23027]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23026]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23032]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23030]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23031]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23035]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23038]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23037]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23029]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23036]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23046]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23048]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23045]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23043]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23040]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23044]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23039]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23041]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23047]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23033]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23042]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 11:35:59 combo ftpd[23034]: connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005 
Jul  9 12:16:49 combo ftpd[23140]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:49 combo ftpd[23143]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:49 combo ftpd[23142]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:49 combo ftpd[23141]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:49 combo ftpd[23144]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:49 combo ftpd[23145]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:49 combo ftpd[23146]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:49 combo ftpd[23148]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:49 combo ftpd[23149]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:49 combo ftpd[23150]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:49 combo ftpd[23147]: connection from 211.167.68.59 () at Sat Jul  9 12:16:49 2005 
Jul  9 12:16:51 combo ftpd[23151]: connection from 211.167.68.59 () at Sat Jul  9 12:16:51 2005 
Jul  9 12:16:51 combo ftpd[23152]: connection from 211.167.68.59 () at Sat Jul  9 12:16:51 2005 
Jul  9 12:16:51 combo ftpd[23153]: connection from 211.167.68.59 () at Sat Jul  9 12:16:51 2005 
Jul  9 12:16:51 combo ftpd[23155]: connection from 211.167.68.59 () at Sat Jul  9 12:16:51 2005 
Jul  9 12:16:51 combo ftpd[23154]: connection from 211.167.68.59 () at Sat Jul  9 12:16:51 2005 
Jul  9 12:16:52 combo ftpd[23156]: connection from 211.167.68.59 () at Sat Jul  9 12:16:52 2005 
Jul  9 12:16:52 combo ftpd[23157]: connection from 211.167.68.59 () at Sat Jul  9 12:16:52 2005 
Jul  9 12:59:44 combo ftpd[23204]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23216]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23215]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23205]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23217]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23206]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23207]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23208]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23209]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23219]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23210]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23218]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23213]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23212]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23211]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23220]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23214]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:44 combo ftpd[23221]: connection from 81.171.220.226 () at Sat Jul  9 12:59:44 2005 
Jul  9 12:59:45 combo ftpd[23222]: connection from 81.171.220.226 () at Sat Jul  9 12:59:45 2005 
Jul  9 12:59:45 combo ftpd[23223]: connection from 81.171.220.226 () at Sat Jul  9 12:59:45 2005 
Jul  9 12:59:45 combo ftpd[23224]: connection from 81.171.220.226 () at Sat Jul  9 12:59:45 2005 
Jul  9 12:59:45 combo ftpd[23225]: connection from 81.171.220.226 () at Sat Jul  9 12:59:45 2005 
Jul  9 12:59:45 combo ftpd[23226]: connection from 81.171.220.226 () at Sat Jul  9 12:59:45 2005 
Jul  9 19:34:06 combo sshd(pam_unix)[23780]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=p15105218.pureserver.info  user=root
Jul  9 19:34:06 combo sshd(pam_unix)[23781]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=p15105218.pureserver.info  user=root
Jul  9 19:34:06 combo sshd(pam_unix)[23784]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=p15105218.pureserver.info  user=root
Jul  9 19:34:07 combo sshd(pam_unix)[23786]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=p15105218.pureserver.info  user=root
Jul  9 19:34:09 combo sshd(pam_unix)[23788]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=p15105218.pureserver.info  user=root
Jul  9 19:34:09 combo sshd(pam_unix)[23790]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=p15105218.pureserver.info  user=root
Jul  9 19:34:10 combo sshd(pam_unix)[23792]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=p15105218.pureserver.info  user=root
Jul  9 19:34:12 combo sshd(pam_unix)[23794]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=p15105218.pureserver.info  user=root
Jul  9 19:34:13 combo sshd(pam_unix)[23796]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=p15105218.pureserver.info  user=root
Jul  9 19:34:14 combo sshd(pam_unix)[23798]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=p15105218.pureserver.info  user=root
Jul  9 22:53:19 combo ftpd[24085]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:19 2005 
Jul  9 22:53:19 combo ftpd[24088]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:19 2005 
Jul  9 22:53:19 combo ftpd[24087]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:19 2005 
Jul  9 22:53:19 combo ftpd[24089]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:19 2005 
Jul  9 22:53:19 combo ftpd[24090]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:19 2005 
Jul  9 22:53:19 combo ftpd[24091]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:19 2005 
Jul  9 22:53:22 combo ftpd[24081]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24071]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24077]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24086]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24069]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24074]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24079]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24072]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24076]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24075]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24078]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24080]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24084]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24070]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24083]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24082]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul  9 22:53:22 combo ftpd[24073]: connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:22 2005 
Jul 10 03:55:15 combo ftpd[24513]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24512]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24519]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24514]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24515]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24516]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24517]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24521]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24520]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24522]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24518]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24523]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24524]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24525]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24526]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24527]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24528]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24529]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24530]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24531]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24532]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24533]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 03:55:15 combo ftpd[24534]: connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005 
Jul 10 04:04:31 combo su(pam_unix)[24898]: session opened for user cyrus by (uid=0)
Jul 10 04:04:32 combo su(pam_unix)[24898]: session closed for user cyrus
Jul 10 04:04:33 combo cups: cupsd shutdown succeeded
Jul 10 04:04:39 combo cups: cupsd startup succeeded
Jul 10 04:04:46 combo syslogd 1.4.1: restart.
Jul 10 04:04:46 combo logrotate: ALERT exited abnormally with [1]
Jul 10 04:10:47 combo su(pam_unix)[26353]: session opened for user news by (uid=0)
Jul 10 04:10:47 combo su(pam_unix)[26353]: session closed for user news
Jul 10 07:24:24 combo ftpd[29726]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:24 combo ftpd[29725]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:24 combo ftpd[29719]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:24 combo ftpd[29723]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:24 combo ftpd[29720]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:24 combo ftpd[29717]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:24 combo ftpd[29718]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:24 combo ftpd[29724]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:24 combo ftpd[29722]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:24 combo ftpd[29727]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:24 combo ftpd[29721]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:24 2005 
Jul 10 07:24:34 combo ftpd[29728]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29730]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29731]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29732]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29729]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29733]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29734]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29737]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29738]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29739]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29735]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 07:24:34 combo ftpd[29736]: connection from 82.83.227.67 (dsl-082-083-227-067.arcor-ip.net) at Sun Jul 10 07:24:34 2005 
Jul 10 13:17:22 combo ftpd[30278]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30276]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30277]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30292]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30293]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30279]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30280]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30295]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30281]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30282]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30288]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30284]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30290]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30294]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30296]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30283]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30285]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30289]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30286]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30297]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30287]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30298]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 13:17:22 combo ftpd[30291]: connection from 220.94.205.45 () at Sun Jul 10 13:17:22 2005 
Jul 10 16:01:43 combo sshd(pam_unix)[30530]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:44 combo sshd(pam_unix)[30532]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:45 combo sshd(pam_unix)[30534]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:45 combo sshd(pam_unix)[30535]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:46 combo sshd(pam_unix)[30536]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:47 combo sshd(pam_unix)[30540]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:48 combo sshd(pam_unix)[30542]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:48 combo sshd(pam_unix)[30544]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:48 combo sshd(pam_unix)[30546]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:49 combo sshd(pam_unix)[30548]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:56 combo sshd(pam_unix)[30550]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:57 combo sshd(pam_unix)[30552]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:58 combo sshd(pam_unix)[30554]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:58 combo sshd(pam_unix)[30555]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:59 combo sshd(pam_unix)[30558]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:01:59 combo sshd(pam_unix)[30560]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:01 combo sshd(pam_unix)[30562]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:01 combo sshd(pam_unix)[30564]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:01 combo sshd(pam_unix)[30566]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:02 combo sshd(pam_unix)[30568]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:08 combo sshd(pam_unix)[30570]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:09 combo sshd(pam_unix)[30572]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:10 combo sshd(pam_unix)[30574]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:10 combo sshd(pam_unix)[30575]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:12 combo sshd(pam_unix)[30580]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:12 combo sshd(pam_unix)[30578]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:13 combo sshd(pam_unix)[30582]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:14 combo sshd(pam_unix)[30584]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:14 combo sshd(pam_unix)[30585]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:14 combo sshd(pam_unix)[30588]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:21 combo sshd(pam_unix)[30590]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:21 combo sshd(pam_unix)[30592]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:23 combo sshd(pam_unix)[30595]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:23 combo sshd(pam_unix)[30594]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:24 combo sshd(pam_unix)[30598]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:25 combo sshd(pam_unix)[30599]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:26 combo sshd(pam_unix)[30602]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:27 combo sshd(pam_unix)[30604]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:27 combo sshd(pam_unix)[30605]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:27 combo sshd(pam_unix)[30607]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:33 combo sshd(pam_unix)[30610]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:34 combo sshd(pam_unix)[30612]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:35 combo sshd(pam_unix)[30614]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:35 combo sshd(pam_unix)[30615]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:36 combo sshd(pam_unix)[30618]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:37 combo sshd(pam_unix)[30620]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:39 combo sshd(pam_unix)[30622]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:39 combo sshd(pam_unix)[30624]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:40 combo sshd(pam_unix)[30625]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:40 combo sshd(pam_unix)[30628]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:45 combo sshd(pam_unix)[30630]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:46 combo sshd(pam_unix)[30632]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:47 combo sshd(pam_unix)[30634]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:47 combo sshd(pam_unix)[30635]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:49 combo sshd(pam_unix)[30638]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:50 combo sshd(pam_unix)[30640]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:52 combo sshd(pam_unix)[30642]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:52 combo sshd(pam_unix)[30644]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:52 combo sshd(pam_unix)[30645]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:53 combo sshd(pam_unix)[30647]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:57 combo sshd(pam_unix)[30650]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:02:58 combo sshd(pam_unix)[30652]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:00 combo sshd(pam_unix)[30654]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:00 combo sshd(pam_unix)[30655]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:01 combo sshd(pam_unix)[30658]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:03 combo sshd(pam_unix)[30660]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:05 combo sshd(pam_unix)[30662]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:05 combo sshd(pam_unix)[30665]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:05 combo sshd(pam_unix)[30664]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:05 combo sshd(pam_unix)[30666]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:10 combo sshd(pam_unix)[30670]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:11 combo sshd(pam_unix)[30672]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:12 combo sshd(pam_unix)[30675]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:12 combo sshd(pam_unix)[30674]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:13 combo sshd(pam_unix)[30678]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:16 combo sshd(pam_unix)[30680]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:18 combo sshd(pam_unix)[30682]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:18 combo sshd(pam_unix)[30684]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:18 combo sshd(pam_unix)[30685]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:03:18 combo sshd(pam_unix)[30686]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=150.183.249.110  user=root
Jul 10 16:33:01 combo sshd(pam_unix)[30725]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root
Jul 10 16:33:01 combo sshd(pam_unix)[30729]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root
Jul 10 16:33:02 combo sshd(pam_unix)[30731]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root
Jul 10 16:33:02 combo sshd(pam_unix)[30732]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root
Jul 10 16:33:02 combo sshd(pam_unix)[30734]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root
Jul 10 16:33:03 combo sshd(pam_unix)[30739]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root
Jul 10 16:33:03 combo sshd(pam_unix)[30737]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root
Jul 10 16:33:04 combo sshd(pam_unix)[30740]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root
Jul 10 16:33:05 combo sshd(pam_unix)[30743]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root
Jul 10 16:33:05 combo sshd(pam_unix)[30726]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.214.161.141  user=root
Jul 11 03:46:14 combo sshd(pam_unix)[31851]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=82.77.200.128  user=root
Jul 11 03:46:15 combo sshd(pam_unix)[31848]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=82.77.200.128  user=root
Jul 11 03:46:15 combo sshd(pam_unix)[31850]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=82.77.200.128  user=root
Jul 11 03:46:15 combo sshd(pam_unix)[31861]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=82.77.200.128  user=root
Jul 11 03:46:15 combo sshd(pam_unix)[31854]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=82.77.200.128  user=root
Jul 11 03:46:15 combo sshd(pam_unix)[31853]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=82.77.200.128  user=root
Jul 11 03:46:16 combo sshd(pam_unix)[31862]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=82.77.200.128  user=root
Jul 11 03:46:17 combo sshd(pam_unix)[31855]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=82.77.200.128  user=root
Jul 11 03:46:17 combo sshd(pam_unix)[31852]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=82.77.200.128  user=root
Jul 11 03:46:19 combo sshd(pam_unix)[31860]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=82.77.200.128  user=root
Jul 11 04:03:03 combo su(pam_unix)[32237]: session opened for user cyrus by (uid=0)
Jul 11 04:03:04 combo su(pam_unix)[32237]: session closed for user cyrus
Jul 11 04:03:05 combo logrotate: ALERT exited abnormally with [1]
Jul 11 04:08:37 combo su(pam_unix)[32608]: session opened for user news by (uid=0)
Jul 11 04:08:38 combo su(pam_unix)[32608]: session closed for user news
Jul 11 11:33:13 combo gdm(pam_unix)[2803]: check pass; user unknown
Jul 11 11:33:13 combo gdm(pam_unix)[2803]: authentication failure; logname= uid=0 euid=0 tty=:0 ruser= rhost= 
Jul 11 11:33:17 combo gdm-binary[2803]: Couldn't authenticate user
Jul 11 17:58:16 combo sshd(pam_unix)[1325]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.137.205.253  user=root
Jul 11 17:58:16 combo sshd(pam_unix)[1329]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.137.205.253  user=root
Jul 11 17:58:16 combo sshd(pam_unix)[1327]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.137.205.253  user=root
Jul 11 17:58:19 combo sshd(pam_unix)[1331]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.137.205.253  user=root
Jul 11 17:58:20 combo sshd(pam_unix)[1333]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.137.205.253  user=root
Jul 11 17:58:21 combo sshd(pam_unix)[1335]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.137.205.253  user=root
Jul 11 17:58:21 combo sshd(pam_unix)[1337]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.137.205.253  user=root
Jul 11 17:58:21 combo sshd(pam_unix)[1338]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.137.205.253  user=root
Jul 11 17:58:22 combo sshd(pam_unix)[1341]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.137.205.253  user=root
Jul 11 17:58:23 combo sshd(pam_unix)[1343]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.137.205.253  user=root
Jul 12 04:03:40 combo su(pam_unix)[2605]: session opened for user cyrus by (uid=0)
Jul 12 04:03:41 combo su(pam_unix)[2605]: session closed for user cyrus
Jul 12 04:03:42 combo logrotate: ALERT exited abnormally with [1]
Jul 12 04:09:49 combo su(pam_unix)[3833]: session opened for user news by (uid=0)
Jul 12 04:09:50 combo su(pam_unix)[3833]: session closed for user news
Jul 12 06:09:43 combo sshd(pam_unix)[4048]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=68.143.156.89.nw.nuvox.net  user=root
Jul 12 06:09:44 combo sshd(pam_unix)[4053]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=68.143.156.89.nw.nuvox.net  user=root
Jul 12 06:09:44 combo sshd(pam_unix)[4052]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=68.143.156.89.nw.nuvox.net  user=root
Jul 12 06:09:44 combo sshd(pam_unix)[4054]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=68.143.156.89.nw.nuvox.net  user=root
Jul 12 06:09:44 combo sshd(pam_unix)[4050]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=68.143.156.89.nw.nuvox.net  user=root
Jul 12 06:09:44 combo sshd(pam_unix)[4061]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=68.143.156.89.nw.nuvox.net  user=root
Jul 12 06:09:44 combo sshd(pam_unix)[4064]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=68.143.156.89.nw.nuvox.net  user=root
Jul 12 06:09:45 combo sshd(pam_unix)[4058]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=68.143.156.89.nw.nuvox.net  user=root
Jul 12 06:09:45 combo sshd(pam_unix)[4059]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=68.143.156.89.nw.nuvox.net  user=root
Jul 12 06:09:45 combo sshd(pam_unix)[4065]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=68.143.156.89.nw.nuvox.net  user=root
Jul 13 04:04:44 combo su(pam_unix)[6486]: session opened for user cyrus by (uid=0)
Jul 13 04:04:45 combo su(pam_unix)[6486]: session closed for user cyrus
Jul 13 04:04:45 combo logrotate: ALERT exited abnormally with [1]
Jul 13 04:10:41 combo su(pam_unix)[6863]: session opened for user news by (uid=0)
Jul 13 04:10:41 combo su(pam_unix)[6863]: session closed for user news
Jul 13 17:22:28 combo sshd(pam_unix)[8113]: session opened for user test by (uid=509)
Jul 13 17:22:28 combo sshd(pam_unix)[8114]: session opened for user test by (uid=509)
Jul 13 17:22:29 combo sshd(pam_unix)[8113]: session closed for user test
Jul 13 17:22:29 combo sshd(pam_unix)[8114]: session closed for user test
Jul 13 17:22:29 combo sshd(pam_unix)[8117]: session opened for user test by (uid=509)
Jul 13 17:22:29 combo sshd(pam_unix)[8117]: session closed for user test
Jul 14 04:08:24 combo su(pam_unix)[9355]: session opened for user cyrus by (uid=0)
Jul 14 04:08:25 combo su(pam_unix)[9355]: session closed for user cyrus
Jul 14 04:08:25 combo logrotate: ALERT exited abnormally with [1]
Jul 14 04:14:38 combo su(pam_unix)[10583]: session opened for user news by (uid=0)
Jul 14 04:14:39 combo su(pam_unix)[10583]: session closed for user news
Jul 14 14:59:42 combo sshd(pam_unix)[11706]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202-132-40-29.adsl.ttn.net  user=root
Jul 14 14:59:56 combo sshd(pam_unix)[11708]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202-132-40-29.adsl.ttn.net  user=root
Jul 14 15:00:09 combo sshd(pam_unix)[11710]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202-132-40-29.adsl.ttn.net  user=root
Jul 14 15:00:22 combo sshd(pam_unix)[11719]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202-132-40-29.adsl.ttn.net  user=root
Jul 14 15:00:36 combo sshd(pam_unix)[11721]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202-132-40-29.adsl.ttn.net  user=root
Jul 14 15:00:49 combo sshd(pam_unix)[11723]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202-132-40-29.adsl.ttn.net  user=root
Jul 14 15:01:03 combo sshd(pam_unix)[11725]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202-132-40-29.adsl.ttn.net  user=root
Jul 14 15:01:16 combo sshd(pam_unix)[11741]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202-132-40-29.adsl.ttn.net  user=root
Jul 15 01:03:48 combo sshd(pam_unix)[12632]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c51471f2c.cable.wanadoo.nl  user=root
Jul 15 01:03:48 combo sshd(pam_unix)[12628]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c51471f2c.cable.wanadoo.nl  user=root
Jul 15 01:03:48 combo sshd(pam_unix)[12631]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c51471f2c.cable.wanadoo.nl  user=root
Jul 15 01:03:48 combo sshd(pam_unix)[12627]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c51471f2c.cable.wanadoo.nl  user=root
Jul 15 01:03:48 combo sshd(pam_unix)[12629]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c51471f2c.cable.wanadoo.nl  user=root
Jul 15 01:03:48 combo sshd(pam_unix)[12630]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c51471f2c.cable.wanadoo.nl  user=root
Jul 15 01:03:49 combo sshd(pam_unix)[12643]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c51471f2c.cable.wanadoo.nl  user=root
Jul 15 01:03:49 combo sshd(pam_unix)[12645]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c51471f2c.cable.wanadoo.nl  user=root
Jul 15 01:03:50 combo sshd(pam_unix)[12639]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c51471f2c.cable.wanadoo.nl  user=root
Jul 15 01:03:50 combo sshd(pam_unix)[12640]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=c51471f2c.cable.wanadoo.nl  user=root
Jul 15 04:05:02 combo su(pam_unix)[13238]: session opened for user cyrus by (uid=0)
Jul 15 04:05:02 combo su(pam_unix)[13238]: session closed for user cyrus
Jul 15 04:05:03 combo logrotate: ALERT exited abnormally with [1]
Jul 15 04:10:45 combo su(pam_unix)[13611]: session opened for user news by (uid=0)
Jul 15 04:10:46 combo su(pam_unix)[13611]: session closed for user news
Jul 15 23:42:43 combo ftpd[15339]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15337]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15334]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15335]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15338]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15336]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15341]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15340]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15327]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15333]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15323]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15330]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15322]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15324]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15329]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15332]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15328]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15321]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15326]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15325]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:43 combo ftpd[15331]: connection from 211.107.232.1 () at Fri Jul 15 23:42:43 2005 
Jul 15 23:42:44 combo ftpd[15342]: connection from 211.107.232.1 () at Fri Jul 15 23:42:44 2005 
Jul 16 04:07:32 combo su(pam_unix)[16058]: session opened for user cyrus by (uid=0)
Jul 16 04:07:33 combo su(pam_unix)[16058]: session closed for user cyrus
Jul 16 04:07:34 combo logrotate: ALERT exited abnormally with [1]
Jul 16 04:13:59 combo su(pam_unix)[17286]: session opened for user news by (uid=0)
Jul 16 04:14:00 combo su(pam_unix)[17286]: session closed for user news
Jul 16 08:14:04 combo ftpd[17669]: connection from 212.65.68.82 () at Sat Jul 16 08:14:04 2005 
Jul 16 08:14:04 combo ftpd[17670]: connection from 212.65.68.82 () at Sat Jul 16 08:14:04 2005 
Jul 16 08:14:04 combo ftpd[17667]: connection from 212.65.68.82 () at Sat Jul 16 08:14:04 2005 
Jul 16 08:14:07 combo ftpd[17673]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17668]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17671]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17681]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17680]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17672]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17687]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17678]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17675]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17688]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17677]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17682]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17683]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17684]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17685]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17686]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17676]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17674]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17689]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 16 08:14:07 combo ftpd[17679]: connection from 212.65.68.82 () at Sat Jul 16 08:14:07 2005 
Jul 17 04:06:32 combo ftpd[19675]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 04:06:32 2005 
Jul 17 04:06:32 combo ftpd[19674]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 04:06:32 2005 
Jul 17 04:08:08 combo su(pam_unix)[19686]: session opened for user cyrus by (uid=0)
Jul 17 04:08:09 combo su(pam_unix)[19686]: session closed for user cyrus
Jul 17 04:08:10 combo cups: cupsd shutdown succeeded
Jul 17 04:08:16 combo cups: cupsd startup succeeded
Jul 17 04:08:23 combo syslogd 1.4.1: restart.
Jul 17 04:08:23 combo logrotate: ALERT exited abnormally with [1]
Jul 17 04:13:57 combo su(pam_unix)[20282]: session opened for user news by (uid=0)
Jul 17 04:13:58 combo su(pam_unix)[20282]: session closed for user news
Jul 17 06:13:37 combo ftpd[23574]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23575]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23573]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23572]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23578]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23577]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23571]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23576]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23564]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23560]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23570]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23568]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23565]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:37 combo ftpd[23566]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:37 2005 
Jul 17 06:13:38 combo ftpd[23562]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:38 2005 
Jul 17 06:13:38 combo ftpd[23567]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:38 2005 
Jul 17 06:13:38 combo ftpd[23563]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:38 2005 
Jul 17 06:13:38 combo ftpd[23561]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:38 2005 
Jul 17 06:13:38 combo ftpd[23569]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:13:38 2005 
Jul 17 06:14:36 combo ftpd[23579]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 06:14:36 combo ftpd[23580]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 06:14:36 combo ftpd[23581]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 06:14:36 combo ftpd[23582]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 06:14:36 combo ftpd[23583]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 06:14:36 combo ftpd[23584]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 06:14:36 combo ftpd[23586]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 06:14:36 combo ftpd[23585]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 06:14:36 combo ftpd[23587]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 06:14:36 combo ftpd[23588]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 06:14:36 combo ftpd[23589]: connection from 83.116.207.11 (aml-sfh-3310b.adsl.wanadoo.nl) at Sun Jul 17 06:14:36 2005 
Jul 17 08:06:12 combo ftpd[23777]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23779]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23778]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23780]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23781]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23782]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23783]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23785]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23786]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23787]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23784]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23792]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23788]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23793]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23789]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23790]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23791]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23794]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23795]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23796]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23797]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:12 combo ftpd[23798]: connection from 218.146.61.230 () at Sun Jul 17 08:06:12 2005 
Jul 17 08:06:14 combo ftpd[23799]: connection from 218.146.61.230 () at Sun Jul 17 08:06:14 2005 
Jul 17 09:44:07 combo ftpd[23931]: connection from 210.245.165.136 () at Sun Jul 17 09:44:07 2005 
Jul 17 09:44:07 combo ftpd[23933]: connection from 210.245.165.136 () at Sun Jul 17 09:44:07 2005 
Jul 17 09:44:07 combo ftpd[23932]: connection from 210.245.165.136 () at Sun Jul 17 09:44:07 2005 
Jul 17 09:44:07 combo ftpd[23934]: connection from 210.245.165.136 () at Sun Jul 17 09:44:07 2005 
Jul 17 09:44:07 combo ftpd[23935]: connection from 210.245.165.136 () at Sun Jul 17 09:44:07 2005 
Jul 17 09:44:07 combo ftpd[23937]: connection from 210.245.165.136 () at Sun Jul 17 09:44:07 2005 
Jul 17 09:44:07 combo ftpd[23938]: connection from 210.245.165.136 () at Sun Jul 17 09:44:07 2005 
Jul 17 09:44:07 combo ftpd[23936]: connection from 210.245.165.136 () at Sun Jul 17 09:44:07 2005 
Jul 17 09:44:07 combo ftpd[23939]: connection from 210.245.165.136 () at Sun Jul 17 09:44:07 2005 
Jul 17 10:45:07 combo sshd(pam_unix)[24031]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61-220-159-99.hinet-ip.hinet.net  user=root
Jul 17 10:45:07 combo sshd(pam_unix)[24033]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61-220-159-99.hinet-ip.hinet.net  user=root
Jul 17 10:45:07 combo sshd(pam_unix)[24030]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=61-220-159-99.hinet-ip.hinet.net  user=root
Jul 17 12:30:35 combo ftpd[24192]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:35 2005 
Jul 17 12:30:37 combo ftpd[24193]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:37 2005 
Jul 17 12:30:42 combo ftpd[24194]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:42 2005 
Jul 17 12:30:43 combo ftpd[24195]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:43 2005 
Jul 17 12:30:43 combo ftpd[24196]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:43 2005 
Jul 17 12:30:46 combo ftpd[24197]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:46 2005 
Jul 17 12:30:47 combo ftpd[24198]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:47 2005 
Jul 17 12:30:48 combo ftpd[24199]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:48 2005 
Jul 17 12:30:48 combo ftpd[24200]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:48 2005 
Jul 17 12:30:51 combo ftpd[24201]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:51 2005 
Jul 17 12:30:53 combo ftpd[24202]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:53 2005 
Jul 17 12:30:53 combo ftpd[24203]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:53 2005 
Jul 17 12:30:54 combo ftpd[24204]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:54 2005 
Jul 17 12:30:57 combo ftpd[24205]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:57 2005 
Jul 17 12:30:57 combo ftpd[24206]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:57 2005 
Jul 17 12:30:58 combo ftpd[24207]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:58 2005 
Jul 17 12:30:58 combo ftpd[24208]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:58 2005 
Jul 17 12:30:58 combo ftpd[24209]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:58 2005 
Jul 17 12:30:59 combo ftpd[24210]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:59 2005 
Jul 17 12:30:59 combo ftpd[24211]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:59 2005 
Jul 17 12:30:59 combo ftpd[24212]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:30:59 2005 
Jul 17 12:31:00 combo ftpd[24213]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:31:00 2005 
Jul 17 12:31:04 combo ftpd[24214]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 12:31:04 2005 
Jul 17 14:02:39 combo ftpd[24356]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:39 2005 
Jul 17 14:02:43 combo ftpd[24357]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:43 2005 
Jul 17 14:02:43 combo ftpd[24358]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:43 2005 
Jul 17 14:02:44 combo ftpd[24359]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:44 2005 
Jul 17 14:02:47 combo ftpd[24360]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:47 2005 
Jul 17 14:02:48 combo ftpd[24361]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:48 2005 
Jul 17 14:02:49 combo ftpd[24362]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:49 2005 
Jul 17 14:02:49 combo ftpd[24363]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:49 2005 
Jul 17 14:02:49 combo ftpd[24364]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:49 2005 
Jul 17 14:02:49 combo ftpd[24365]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:49 2005 
Jul 17 14:02:54 combo ftpd[24366]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:54 2005 
Jul 17 14:02:54 combo ftpd[24367]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:54 2005 
Jul 17 14:02:54 combo ftpd[24368]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:54 2005 
Jul 17 14:02:55 combo ftpd[24369]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:55 2005 
Jul 17 14:02:57 combo ftpd[24370]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:57 2005 
Jul 17 14:02:59 combo ftpd[24371]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:59 2005 
Jul 17 14:02:59 combo ftpd[24372]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:02:59 2005 
Jul 17 14:03:00 combo ftpd[24373]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:03:00 2005 
Jul 17 14:03:00 combo ftpd[24374]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:03:00 2005 
Jul 17 14:03:00 combo ftpd[24375]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:03:00 2005 
Jul 17 14:03:04 combo ftpd[24376]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:03:04 2005 
Jul 17 14:03:04 combo ftpd[24377]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:03:04 2005 
Jul 17 14:03:05 combo ftpd[24378]: connection from 207.30.238.8 (host8.topspot.net) at Sun Jul 17 14:03:05 2005 
Jul 17 15:09:15 combo ftpd[24465]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24466]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24467]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24468]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24469]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24470]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24471]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24473]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24474]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24475]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24472]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24477]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24478]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24476]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24480]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24479]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24481]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:15 combo ftpd[24482]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:15 2005 
Jul 17 15:09:16 combo ftpd[24483]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:16 2005 
Jul 17 15:09:16 combo ftpd[24484]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:16 2005 
Jul 17 15:09:16 combo ftpd[24485]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:16 2005 
Jul 17 15:09:16 combo ftpd[24486]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:16 2005 
Jul 17 15:09:17 combo ftpd[24487]: connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul 17 15:09:17 2005 
Jul 17 21:23:20 combo ftpd[25038]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25028]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25029]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25035]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25030]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25039]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25040]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25031]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25041]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25032]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25042]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25033]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25043]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25034]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25036]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:20 combo ftpd[25037]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:20 2005 
Jul 17 21:23:23 combo ftpd[25044]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:23 2005 
Jul 17 21:23:24 combo ftpd[25045]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:24 2005 
Jul 17 21:23:24 combo ftpd[25046]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:24 2005 
Jul 17 21:23:24 combo ftpd[25047]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:24 2005 
Jul 17 21:23:24 combo ftpd[25048]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:24 2005 
Jul 17 21:23:24 combo ftpd[25049]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:24 2005 
Jul 17 21:23:24 combo ftpd[25050]: connection from 82.68.222.194 (82-68-222-194.dsl.in-addr.zen.co.uk) at Sun Jul 17 21:23:24 2005 
Jul 17 23:21:50 combo ftpd[25217]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:50 2005 
Jul 17 23:21:54 combo ftpd[25218]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25219]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25220]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25221]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25222]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25223]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25225]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25224]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25226]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25227]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25228]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25229]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25230]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25231]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25232]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25233]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25234]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25235]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25236]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25237]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25238]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 17 23:21:54 combo ftpd[25239]: connection from 82.68.222.195 (82-68-222-195.dsl.in-addr.zen.co.uk) at Sun Jul 17 23:21:54 2005 
Jul 18 03:26:48 combo ftpd[25628]: connection from 211.72.151.162 () at Mon Jul 18 03:26:48 2005 
Jul 18 03:26:48 combo ftpd[25629]: connection from 211.72.151.162 () at Mon Jul 18 03:26:48 2005 
Jul 18 03:26:48 combo ftpd[25630]: connection from 211.72.151.162 () at Mon Jul 18 03:26:48 2005 
Jul 18 03:26:48 combo ftpd[25631]: connection from 211.72.151.162 () at Mon Jul 18 03:26:48 2005 
Jul 18 03:26:48 combo ftpd[25632]: connection from 211.72.151.162 () at Mon Jul 18 03:26:48 2005 
Jul 18 03:26:48 combo ftpd[25633]: connection from 211.72.151.162 () at Mon Jul 18 03:26:48 2005 
Jul 18 03:26:48 combo ftpd[25634]: connection from 211.72.151.162 () at Mon Jul 18 03:26:48 2005 
Jul 18 03:26:48 combo ftpd[25635]: connection from 211.72.151.162 () at Mon Jul 18 03:26:48 2005 
Jul 18 03:26:49 combo ftpd[25638]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25636]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25637]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25639]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25640]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25641]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25643]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25644]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25645]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25642]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25646]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25648]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 03:26:49 combo ftpd[25647]: connection from 211.72.151.162 () at Mon Jul 18 03:26:49 2005 
Jul 18 04:03:24 combo su(pam_unix)[26046]: session opened for user cyrus by (uid=0)
Jul 18 04:03:24 combo su(pam_unix)[26046]: session closed for user cyrus
Jul 18 04:03:25 combo logrotate: ALERT exited abnormally with [1]
Jul 18 04:09:29 combo su(pam_unix)[27272]: session opened for user news by (uid=0)
Jul 18 04:09:30 combo su(pam_unix)[27272]: session closed for user news
Jul 18 23:01:25 combo sshd(pam_unix)[28975]: check pass; user unknown
Jul 18 23:01:25 combo sshd(pam_unix)[28975]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211-76-104-65.ebix.net.tw 
Jul 18 23:01:25 combo sshd(pam_unix)[28977]: check pass; user unknown
Jul 18 23:01:25 combo sshd(pam_unix)[28977]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211-76-104-65.ebix.net.tw 
Jul 18 23:01:25 combo sshd(pam_unix)[28980]: check pass; user unknown
Jul 18 23:01:25 combo sshd(pam_unix)[28980]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211-76-104-65.ebix.net.tw 
Jul 18 23:01:25 combo sshd(pam_unix)[28978]: check pass; user unknown
Jul 18 23:01:25 combo sshd(pam_unix)[28978]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211-76-104-65.ebix.net.tw 
Jul 18 23:01:26 combo sshd(pam_unix)[28983]: check pass; user unknown
Jul 18 23:01:26 combo sshd(pam_unix)[28983]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211-76-104-65.ebix.net.tw 
Jul 18 23:01:27 combo sshd(pam_unix)[28986]: check pass; user unknown
Jul 18 23:01:27 combo sshd(pam_unix)[28986]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211-76-104-65.ebix.net.tw 
Jul 18 23:01:27 combo sshd(pam_unix)[28985]: check pass; user unknown
Jul 18 23:01:27 combo sshd(pam_unix)[28985]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211-76-104-65.ebix.net.tw 
Jul 18 23:01:27 combo sshd(pam_unix)[28987]: check pass; user unknown
Jul 18 23:01:27 combo sshd(pam_unix)[28987]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211-76-104-65.ebix.net.tw 
Jul 18 23:01:27 combo sshd(pam_unix)[28988]: check pass; user unknown
Jul 18 23:01:27 combo sshd(pam_unix)[28988]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211-76-104-65.ebix.net.tw 
Jul 18 23:01:27 combo sshd(pam_unix)[28991]: check pass; user unknown
Jul 18 23:01:27 combo sshd(pam_unix)[28991]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211-76-104-65.ebix.net.tw 
Jul 19 04:03:43 combo su(pam_unix)[29758]: session opened for user cyrus by (uid=0)
Jul 19 04:03:44 combo su(pam_unix)[29758]: session closed for user cyrus
Jul 19 04:03:45 combo logrotate: ALERT exited abnormally with [1]
Jul 19 04:09:28 combo su(pam_unix)[30128]: session opened for user news by (uid=0)
Jul 19 04:09:29 combo su(pam_unix)[30128]: session closed for user news
Jul 19 07:35:41 combo sshd(pam_unix)[30500]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202.181.236.180  user=root
Jul 19 07:35:41 combo sshd(pam_unix)[30510]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202.181.236.180  user=root
Jul 19 07:35:41 combo sshd(pam_unix)[30499]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202.181.236.180  user=root
Jul 19 07:35:41 combo sshd(pam_unix)[30503]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202.181.236.180  user=root
Jul 19 07:35:41 combo sshd(pam_unix)[30505]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202.181.236.180  user=root
Jul 19 07:35:41 combo sshd(pam_unix)[30501]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202.181.236.180  user=root
Jul 19 07:35:41 combo sshd(pam_unix)[30504]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202.181.236.180  user=root
Jul 19 07:35:41 combo sshd(pam_unix)[30498]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202.181.236.180  user=root
Jul 19 07:35:41 combo sshd(pam_unix)[30511]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202.181.236.180  user=root
Jul 19 07:35:41 combo sshd(pam_unix)[30508]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=202.181.236.180  user=root
Jul 20 04:05:02 combo su(pam_unix)[363]: session opened for user cyrus by (uid=0)
Jul 20 04:05:03 combo su(pam_unix)[363]: session closed for user cyrus
Jul 20 04:05:04 combo logrotate: ALERT exited abnormally with [1]
Jul 20 04:11:27 combo su(pam_unix)[1595]: session opened for user news by (uid=0)
Jul 20 04:11:28 combo su(pam_unix)[1595]: session closed for user news
Jul 20 23:37:40 combo sshd(pam_unix)[3307]: check pass; user unknown
Jul 20 23:37:40 combo sshd(pam_unix)[3307]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.55.234.102 
Jul 20 23:37:40 combo sshd(pam_unix)[3306]: check pass; user unknown
Jul 20 23:37:40 combo sshd(pam_unix)[3306]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.55.234.102 
Jul 20 23:37:40 combo sshd(pam_unix)[3308]: check pass; user unknown
Jul 20 23:37:40 combo sshd(pam_unix)[3308]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.55.234.102 
Jul 20 23:37:46 combo sshd(pam_unix)[3313]: check pass; user unknown
Jul 20 23:37:46 combo sshd(pam_unix)[3313]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.55.234.102 
Jul 20 23:37:46 combo sshd(pam_unix)[3312]: check pass; user unknown
Jul 20 23:37:46 combo sshd(pam_unix)[3312]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.55.234.102 
Jul 21 01:30:45 combo sshd(pam_unix)[3489]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.76.59.29  user=root
Jul 21 01:30:49 combo sshd(pam_unix)[3493]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.76.59.29  user=root
Jul 21 01:30:50 combo sshd(pam_unix)[3488]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.76.59.29  user=root
Jul 21 01:30:50 combo sshd(pam_unix)[3487]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=210.76.59.29  user=root
Jul 21 04:11:26 combo su(pam_unix)[4170]: session opened for user cyrus by (uid=0)
Jul 21 04:11:27 combo su(pam_unix)[4170]: session closed for user cyrus
Jul 21 04:11:28 combo logrotate: ALERT exited abnormally with [1]
Jul 21 04:16:55 combo su(pam_unix)[4540]: session opened for user news by (uid=0)
Jul 21 04:16:55 combo su(pam_unix)[4540]: session closed for user news
Jul 21 09:04:41 combo ftpd[5033]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5032]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5030]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5029]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5018]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5017]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5016]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5019]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5022]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5023]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5024]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5025]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5028]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5031]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5020]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5021]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5027]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5026]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5035]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5037]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5034]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:41 combo ftpd[5036]: connection from 216.12.111.241 () at Thu Jul 21 09:04:41 2005 
Jul 21 09:04:43 combo ftpd[5038]: connection from 216.12.111.241 () at Thu Jul 21 09:04:43 2005 
Jul 21 15:18:30 combo sshd(pam_unix)[5587]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=193.110.106.11  user=root
Jul 21 15:18:30 combo sshd(pam_unix)[5586]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=193.110.106.11  user=root
Jul 22 04:07:46 combo su(pam_unix)[7106]: session opened for user cyrus by (uid=0)
Jul 22 04:07:47 combo su(pam_unix)[7106]: session closed for user cyrus
Jul 22 04:07:47 combo logrotate: ALERT exited abnormally with [1]
Jul 22 04:15:14 combo su(pam_unix)[11756]: session opened for user news by (uid=0)
Jul 22 04:15:14 combo su(pam_unix)[11756]: session closed for user news
Jul 22 09:27:24 combo ftpd[12294]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12290]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12296]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12297]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12291]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12293]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12295]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12292]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12282]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12288]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12284]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12279]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12285]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12286]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12278]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12277]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12280]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12289]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12281]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12283]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12287]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12298]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 09:27:24 combo ftpd[12299]: connection from 211.42.188.206 () at Fri Jul 22 09:27:24 2005 
Jul 22 19:29:09 combo ftpd[13152]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13150]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13147]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13149]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13148]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13151]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13153]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13141]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13146]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13140]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13142]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13143]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13145]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13144]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13154]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13155]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13156]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13157]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13158]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13159]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13160]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:09 combo ftpd[13161]: connection from 67.95.49.172 () at Fri Jul 22 19:29:09 2005 
Jul 22 19:29:10 combo ftpd[13162]: connection from 67.95.49.172 () at Fri Jul 22 19:29:10 2005 
Jul 23 04:09:35 combo su(pam_unix)[14314]: session opened for user cyrus by (uid=0)
Jul 23 04:09:36 combo su(pam_unix)[14314]: session closed for user cyrus
Jul 23 04:09:37 combo logrotate: ALERT exited abnormally with [1]
Jul 23 04:15:13 combo su(pam_unix)[14692]: session opened for user news by (uid=0)
Jul 23 04:15:13 combo su(pam_unix)[14692]: session closed for user news
Jul 23 11:46:41 combo sshd(pam_unix)[15385]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=85.44.47.166  user=root
Jul 23 20:04:41 combo sshd(pam_unix)[16159]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.9.58.217  user=root
Jul 23 20:04:41 combo sshd(pam_unix)[16158]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.9.58.217  user=root
Jul 23 20:04:41 combo sshd(pam_unix)[16156]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.9.58.217  user=root
Jul 23 20:04:41 combo sshd(pam_unix)[16157]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.9.58.217  user=root
Jul 23 20:04:41 combo sshd(pam_unix)[16162]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.9.58.217  user=root
Jul 23 20:04:41 combo sshd(pam_unix)[16155]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.9.58.217  user=root
Jul 23 20:04:41 combo sshd(pam_unix)[16160]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.9.58.217  user=root
Jul 23 20:04:41 combo sshd(pam_unix)[16154]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.9.58.217  user=root
Jul 23 20:04:42 combo sshd(pam_unix)[16161]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.9.58.217  user=root
Jul 23 20:04:42 combo sshd(pam_unix)[16153]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.9.58.217  user=root
Jul 24 02:38:22 combo ftpd[16773]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16789]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16783]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16792]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16784]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16781]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16786]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16788]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16785]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16790]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16780]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16779]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16782]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16787]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16774]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16772]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16775]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16778]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16776]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16777]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16791]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16793]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:22 combo ftpd[16794]: connection from 84.102.20.2 () at Sun Jul 24 02:38:22 2005 
Jul 24 02:38:23 combo ftpd[16781]: ANONYMOUS FTP LOGIN FROM 84.102.20.2,  (anonymous)
Jul 24 02:38:23 combo ftpd[16782]: ANONYMOUS FTP LOGIN FROM 84.102.20.2,  (anonymous)
Jul 24 04:20:19 combo su(pam_unix)[17283]: session opened for user cyrus by (uid=0)
Jul 24 04:20:19 combo su(pam_unix)[17283]: session closed for user cyrus
Jul 24 04:20:21 combo cups: cupsd shutdown succeeded
Jul 24 04:20:26 combo cups: cupsd startup succeeded
Jul 24 04:20:42 combo syslogd 1.4.1: restart.
Jul 24 04:20:42 combo logrotate: ALERT exited abnormally with [1]
Jul 24 04:33:57 combo su(pam_unix)[21805]: session opened for user news by (uid=0)
Jul 24 04:33:58 combo su(pam_unix)[21805]: session closed for user news
Jul 24 08:31:57 combo sshd(pam_unix)[22185]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=203.251.225.101  user=root
Jul 24 08:31:57 combo sshd(pam_unix)[22184]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=203.251.225.101  user=root
Jul 24 08:31:59 combo sshd(pam_unix)[22188]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=203.251.225.101  user=root
Jul 24 08:31:59 combo sshd(pam_unix)[22189]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=203.251.225.101  user=root
Jul 24 08:31:59 combo sshd(pam_unix)[22191]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=203.251.225.101  user=root
Jul 24 13:46:32 combo ftpd[22655]: connection from 211.107.232.1 () at Sun Jul 24 13:46:32 2005 
Jul 24 13:46:32 combo ftpd[22652]: connection from 211.107.232.1 () at Sun Jul 24 13:46:32 2005 
Jul 24 13:46:32 combo ftpd[22658]: connection from 211.107.232.1 () at Sun Jul 24 13:46:32 2005 
Jul 24 13:46:32 combo ftpd[22653]: connection from 211.107.232.1 () at Sun Jul 24 13:46:32 2005 
Jul 24 13:46:32 combo ftpd[22657]: connection from 211.107.232.1 () at Sun Jul 24 13:46:32 2005 
Jul 24 13:46:32 combo ftpd[22659]: connection from 211.107.232.1 () at Sun Jul 24 13:46:32 2005 
Jul 24 13:46:32 combo ftpd[22656]: connection from 211.107.232.1 () at Sun Jul 24 13:46:32 2005 
Jul 24 13:46:32 combo ftpd[22651]: connection from 211.107.232.1 () at Sun Jul 24 13:46:32 2005 
Jul 24 13:46:32 combo ftpd[22654]: connection from 211.107.232.1 () at Sun Jul 24 13:46:32 2005 
Jul 24 13:46:34 combo ftpd[22660]: connection from 211.107.232.1 () at Sun Jul 24 13:46:34 2005 
Jul 24 13:46:34 combo ftpd[22661]: connection from 211.107.232.1 () at Sun Jul 24 13:46:34 2005 
Jul 24 13:46:34 combo ftpd[22662]: connection from 211.107.232.1 () at Sun Jul 24 13:46:34 2005 
Jul 24 13:46:34 combo ftpd[22663]: connection from 211.107.232.1 () at Sun Jul 24 13:46:34 2005 
Jul 24 13:46:34 combo ftpd[22665]: connection from 211.107.232.1 () at Sun Jul 24 13:46:34 2005 
Jul 24 13:46:34 combo ftpd[22666]: connection from 211.107.232.1 () at Sun Jul 24 13:46:34 2005 
Jul 24 13:46:34 combo ftpd[22667]: connection from 211.107.232.1 () at Sun Jul 24 13:46:34 2005 
Jul 24 13:46:34 combo ftpd[22664]: connection from 211.107.232.1 () at Sun Jul 24 13:46:34 2005 
Jul 24 13:46:34 combo ftpd[22668]: connection from 211.107.232.1 () at Sun Jul 24 13:46:34 2005 
Jul 24 13:46:35 combo ftpd[22669]: connection from 211.107.232.1 () at Sun Jul 24 13:46:35 2005 
Jul 25 04:03:58 combo su(pam_unix)[24312]: session opened for user cyrus by (uid=0)
Jul 25 04:03:59 combo su(pam_unix)[24312]: session closed for user cyrus
Jul 25 04:04:00 combo logrotate: ALERT exited abnormally with [1]
Jul 25 04:09:32 combo su(pam_unix)[24683]: session opened for user news by (uid=0)
Jul 25 04:09:33 combo su(pam_unix)[24683]: session closed for user news
Jul 25 06:39:18 combo ftpd[24970]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24971]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24972]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24977]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24976]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24974]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24973]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24975]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24978]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24962]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24967]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24966]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24968]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24965]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24964]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24958]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24956]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24957]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24959]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24969]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24960]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24963]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 06:39:18 combo ftpd[24961]: connection from 206.47.209.10 () at Mon Jul 25 06:39:18 2005 
Jul 25 12:09:06 combo named[2306]: notify question section contains no SOA
Jul 25 12:18:50 combo named[2306]: notify question section contains no SOA
Jul 25 12:27:05 combo named[2306]: notify question section contains no SOA
Jul 25 12:35:24 combo named[2306]: notify question section contains no SOA
Jul 25 12:54:46 combo named[2306]: notify question section contains no SOA
Jul 25 13:33:41 combo named[2306]: notify question section contains no SOA
Jul 25 13:41:34 combo named[2306]: notify question section contains no SOA
Jul 25 13:50:24 combo named[2306]: notify question section contains no SOA
Jul 25 14:00:03 combo named[2306]: notify question section contains no SOA
Jul 25 14:08:28 combo named[2306]: notify question section contains no SOA
Jul 25 14:17:19 combo named[2306]: notify question section contains no SOA
Jul 25 14:33:46 combo named[2306]: notify question section contains no SOA
Jul 25 14:48:30 combo named[2306]: notify question section contains no SOA
Jul 25 14:56:13 combo named[2306]: notify question section contains no SOA
Jul 25 15:50:04 combo named[2306]: notify question section contains no SOA
Jul 25 16:29:20 combo named[2306]: notify question section contains no SOA
Jul 25 23:23:13 combo ftpd[26463]: getpeername (ftpd): Transport endpoint is not connected
Jul 25 23:23:13 combo ftpd[26466]: getpeername (ftpd): Transport endpoint is not connected
Jul 25 23:23:13 combo xinetd[26482]: warning: can't get client address: Connection reset by peer
Jul 25 23:23:13 combo ftpd[26482]: getpeername (ftpd): Transport endpoint is not connected
Jul 25 23:23:13 combo xinetd[26484]: warning: can't get client address: Connection reset by peer
Jul 25 23:23:13 combo ftpd[26484]: getpeername (ftpd): Transport endpoint is not connected
Jul 25 23:24:09 combo ftpd[26479]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26478]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26477]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26476]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26475]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26474]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26473]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26467]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26471]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26472]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26468]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26470]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26469]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26464]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26465]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26480]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26481]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26483]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 25 23:24:09 combo ftpd[26485]: connection from 217.187.83.50 () at Mon Jul 25 23:24:09 2005 
Jul 26 04:05:22 combo su(pam_unix)[27285]: session opened for user cyrus by (uid=0)
Jul 26 04:05:23 combo su(pam_unix)[27285]: session closed for user cyrus
Jul 26 04:05:24 combo logrotate: ALERT exited abnormally with [1]
Jul 26 04:11:23 combo su(pam_unix)[28514]: session opened for user news by (uid=0)
Jul 26 04:11:23 combo su(pam_unix)[28514]: session closed for user news
Jul 26 05:47:42 combo ftpd[28699]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28703]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28700]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28701]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28702]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28696]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28694]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28695]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28697]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28698]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28693]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28689]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28690]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28691]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28686]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28687]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28688]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:42 combo ftpd[28692]: connection from 172.181.208.156 () at Tue Jul 26 05:47:42 2005 
Jul 26 05:47:51 combo ftpd[28705]: connection from 172.181.208.156 () at Tue Jul 26 05:47:51 2005 
Jul 26 05:47:51 combo ftpd[28704]: connection from 172.181.208.156 () at Tue Jul 26 05:47:51 2005 
Jul 26 05:47:51 combo ftpd[28706]: connection from 172.181.208.156 () at Tue Jul 26 05:47:51 2005 
Jul 26 05:47:51 combo ftpd[28707]: connection from 172.181.208.156 () at Tue Jul 26 05:47:51 2005 
Jul 26 05:47:51 combo ftpd[28708]: connection from 172.181.208.156 () at Tue Jul 26 05:47:51 2005 
Jul 26 07:02:27 combo sshd(pam_unix)[28842]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:02:35 combo sshd(pam_unix)[28844]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:02:37 combo sshd(pam_unix)[28846]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:02:45 combo sshd(pam_unix)[28848]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:02:47 combo sshd(pam_unix)[28850]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:02:55 combo sshd(pam_unix)[28852]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:02:57 combo sshd(pam_unix)[28854]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:05 combo sshd(pam_unix)[28856]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:07 combo sshd(pam_unix)[28858]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:15 combo sshd(pam_unix)[28860]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:17 combo sshd(pam_unix)[28862]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:26 combo sshd(pam_unix)[28864]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:27 combo sshd(pam_unix)[28866]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:35 combo sshd(pam_unix)[28868]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:37 combo sshd(pam_unix)[28870]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:45 combo sshd(pam_unix)[28872]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:47 combo sshd(pam_unix)[28874]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:55 combo sshd(pam_unix)[28876]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:03:57 combo sshd(pam_unix)[28878]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:04:02 combo sshd(pam_unix)[28880]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:04:05 combo sshd(pam_unix)[28882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:04:07 combo sshd(pam_unix)[28884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 26 07:04:12 combo sshd(pam_unix)[28886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=207.243.167.114  user=root
Jul 27 04:16:07 combo su(pam_unix)[30999]: session opened for user cyrus by (uid=0)
Jul 27 04:16:08 combo su(pam_unix)[30999]: session closed for user cyrus
Jul 27 04:16:09 combo logrotate: ALERT exited abnormally with [1]
Jul 27 04:21:39 combo su(pam_unix)[31373]: session opened for user news by (uid=0)
Jul 27 04:21:40 combo su(pam_unix)[31373]: session closed for user news
Jul 27 10:59:53 combo ftpd[31985]: connection from 218.38.58.3 () at Wed Jul 27 10:59:53 2005 
Jul 27 14:41:57 combo syslogd 1.4.1: restart.
Jul 27 14:41:57 combo syslog: syslogd startup succeeded
Jul 27 14:41:57 combo kernel: klogd 1.4.1, log source = /proc/kmsg started.
Jul 27 14:41:57 combo kernel: Linux version 2.6.5-1.358 (bhcompile@bugs.build.redhat.com) (gcc version 3.3.3 20040412 (Red Hat Linux 3.3.3-7)) #1 Sat May 8 09:04:50 EDT 2004
Jul 27 14:41:57 combo kernel: BIOS-provided physical RAM map:
Jul 27 14:41:57 combo kernel:  BIOS-e820: 0000000000000000 - 00000000000a0000 (usable)
Jul 27 14:41:57 combo kernel:  BIOS-e820: 00000000000f0000 - 0000000000100000 (reserved)
Jul 27 14:41:57 combo kernel:  BIOS-e820: 0000000000100000 - 0000000007eae000 (usable)
Jul 27 14:41:57 combo kernel:  BIOS-e820: 0000000007eae000 - 0000000008000000 (reserved)
Jul 27 14:41:57 combo kernel:  BIOS-e820: 00000000ffb00000 - 0000000100000000 (reserved)
Jul 27 14:41:57 combo kernel: 0MB HIGHMEM available.
Jul 27 14:41:57 combo kernel: 126MB LOWMEM available.
Jul 27 14:41:57 combo kernel: zapping low mappings.
Jul 27 14:41:57 combo syslog: klogd startup succeeded
Jul 27 14:41:57 combo kernel: On node 0 totalpages: 32430
Jul 27 14:41:57 combo kernel:   DMA zone: 4096 pages, LIFO batch:1
Jul 27 14:41:57 combo kernel:   Normal zone: 28334 pages, LIFO batch:6
Jul 27 14:41:57 combo irqbalance: irqbalance startup succeeded
Jul 27 14:41:57 combo kernel:   HighMem zone: 0 pages, LIFO batch:1
Jul 27 14:41:57 combo kernel: DMI 2.3 present.
Jul 27 14:41:57 combo kernel: ACPI disabled because your bios is from 2000 and too old
Jul 27 14:41:57 combo kernel: You can enable it with acpi=force
Jul 27 14:41:57 combo kernel: Built 1 zonelists
Jul 27 14:41:57 combo kernel: Kernel command line: ro root=LABEL=/ rhgb quiet
Jul 27 14:41:57 combo kernel: mapped 4G/4G trampoline to ffff3000.
Jul 27 14:41:57 combo kernel: Initializing CPU#0
Jul 27 14:41:57 combo kernel: CPU 0 irqstacks, hard=02345000 soft=02344000
Jul 27 14:41:57 combo portmap: portmap startup succeeded
Jul 27 14:41:57 combo kernel: PID hash table entries: 512 (order 9: 4096 bytes)
Jul 27 14:41:57 combo kernel: Detected 731.219 MHz processor.
Jul 27 14:41:57 combo rpc.statd[1618]: Version 1.0.6 Starting
Jul 27 14:41:57 combo kernel: Using tsc for high-res timesource
Jul 27 14:41:58 combo nfslock: rpc.statd startup succeeded
Jul 27 14:41:58 combo kernel: Console: colour VGA+ 80x25
Jul 27 14:41:58 combo kernel: Memory: 125312k/129720k available (1540k kernel code, 3860k reserved, 599k data, 144k init, 0k highmem)
Jul 27 14:41:58 combo kernel: Calibrating delay loop... 1445.88 BogoMIPS
Jul 27 14:41:58 combo kernel: Security Scaffold v1.0.0 initialized
Jul 27 14:41:58 combo kernel: SELinux:  Initializing.
Jul 27 14:41:58 combo kernel: SELinux:  Starting in permissive mode
Jul 27 14:41:58 combo kernel: There is already a security framework initialized, register_security failed.
Jul 27 14:41:58 combo kernel: Failure registering capabilities with the kernel
Jul 27 14:41:58 combo kernel: selinux_register_security:  Registering secondary module capability
Jul 27 14:41:58 combo kernel: Capability LSM initialized
Jul 27 14:41:58 combo rpcidmapd: rpc.idmapd startup succeeded
Jul 27 14:41:58 combo kernel: Dentry cache hash table entries: 16384 (order: 4, 65536 bytes)
Jul 27 14:41:58 combo kernel: Inode-cache hash table entries: 8192 (order: 3, 32768 bytes)
Jul 27 14:41:58 combo kernel: Mount-cache hash table entries: 512 (order: 0, 4096 bytes)
Jul 27 14:41:58 combo kernel: CPU: L1 I cache: 16K, L1 D cache: 16K
Jul 27 14:41:58 combo kernel: CPU: L2 cache: 256K
Jul 27 14:41:58 combo kernel: Intel machine check architecture supported.
Jul 27 14:41:58 combo kernel: Intel machine check reporting enabled on CPU#0.
Jul 27 14:41:58 combo kernel: CPU: Intel Pentium III (Coppermine) stepping 06
Jul 27 14:41:58 combo kernel: Enabling fast FPU save and restore... done.
Jul 27 14:41:58 combo kernel: Enabling unmasked SIMD FPU exception support... done.
Jul 27 14:41:58 combo kernel: Checking 'hlt' instruction... OK.
Jul 27 14:41:58 combo random: Initializing random number generator:  succeeded
Jul 27 14:41:58 combo kernel: POSIX conformance testing by UNIFIX
Jul 27 14:41:58 combo kernel: NET: Registered protocol family 16
Jul 27 14:41:58 combo kernel: PCI: PCI BIOS revision 2.10 entry at 0xfc0ce, last bus=1
Jul 27 14:41:58 combo kernel: PCI: Using configuration type 1
Jul 27 14:41:58 combo kernel: mtrr: v2.0 (20020519)
Jul 27 14:41:58 combo kernel: ACPI: Subsystem revision 20040326
Jul 27 14:41:58 combo kernel: ACPI: Interpreter disabled.
Jul 27 14:41:58 combo kernel: Linux Plug and Play Support v0.97 (c) Adam Belay
Jul 27 14:41:58 combo rc: Starting pcmcia:  succeeded
Jul 27 14:41:58 combo kernel: usbcore: registered new driver usbfs
Jul 27 14:41:58 combo kernel: usbcore: registered new driver hub
Jul 27 14:41:58 combo kernel: ACPI: ACPI tables contain no PCI IRQ routing entries
Jul 27 14:41:59 combo kernel: PCI: Invalid ACPI-PCI IRQ routing table
Jul 27 14:41:59 combo kernel: PCI: Probing PCI hardware
Jul 27 14:41:59 combo kernel: PCI: Probing PCI hardware (bus 00)
Jul 27 14:41:59 combo kernel: Transparent bridge - 0000:00:1e.0
Jul 27 14:41:59 combo kernel: PCI: Using IRQ router PIIX/ICH [8086/2410] at 0000:00:1f.0
Jul 27 14:41:59 combo kernel: apm: BIOS version 1.2 Flags 0x03 (Driver version 1.16ac)
Jul 27 14:41:59 combo kernel: audit: initializing netlink socket (disabled)
Jul 27 14:41:54 combo sysctl: kernel.core_uses_pid = 1 
Jul 27 14:41:59 combo hcid[1690]: HCI daemon ver 2.4 started
Jul 27 14:41:59 combo bluetooth: hcid startup succeeded
Jul 27 14:41:59 combo kernel: audit(1122475266.4294965305:0): initialized
Jul 27 14:41:54 combo network: Setting network parameters:  succeeded 
Jul 27 14:41:59 combo bluetooth: sdpd startup succeeded
Jul 27 14:41:59 combo sdpd[1696]: sdpd v1.5 started 
Jul 27 14:41:59 combo kernel: Total HugeTLB memory allocated, 0
Jul 27 14:41:54 combo network: Bringing up loopback interface:  succeeded 
Jul 27 14:41:59 combo kernel: VFS: Disk quotas dquot_6.5.1
Jul 27 14:41:59 combo kernel: Dquot-cache hash table entries: 1024 (order 0, 4096 bytes)
Jul 27 14:41:59 combo kernel: SELinux:  Registering netfilter hooks
Jul 27 14:41:59 combo kernel: Initializing Cryptographic API
Jul 27 14:41:59 combo kernel: pci_hotplug: PCI Hot Plug PCI Core version: 0.5
Jul 27 14:42:00 combo kernel: isapnp: Scanning for PnP cards...
Jul 27 14:42:00 combo kernel: isapnp: No Plug & Play device found
Jul 27 14:42:00 combo kernel: Real Time Clock Driver v1.12
Jul 27 14:42:00 combo kernel: Linux agpgart interface v0.100 (c) Dave Jones
[Sun Dec 04 04:47:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:47:44 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:51:08 2005] [notice] jk2_init() Found child 6725 in scoreboard slot 10
[Sun Dec 04 04:51:09 2005] [notice] jk2_init() Found child 6726 in scoreboard slot 8
[Sun Dec 04 04:51:09 2005] [notice] jk2_init() Found child 6728 in scoreboard slot 6
[Sun Dec 04 04:51:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:51:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:51:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:51:18 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:51:18 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:51:18 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:51:37 2005] [notice] jk2_init() Found child 6736 in scoreboard slot 10
[Sun Dec 04 04:51:38 2005] [notice] jk2_init() Found child 6733 in scoreboard slot 7
[Sun Dec 04 04:51:38 2005] [notice] jk2_init() Found child 6734 in scoreboard slot 9
[Sun Dec 04 04:51:52 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:51:52 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:51:55 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:52:04 2005] [notice] jk2_init() Found child 6738 in scoreboard slot 6
[Sun Dec 04 04:52:04 2005] [notice] jk2_init() Found child 6741 in scoreboard slot 9
[Sun Dec 04 04:52:05 2005] [notice] jk2_init() Found child 6740 in scoreboard slot 7
[Sun Dec 04 04:52:05 2005] [notice] jk2_init() Found child 6737 in scoreboard slot 8
[Sun Dec 04 04:52:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:52:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:52:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:52:15 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:52:15 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 04:52:15 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 04:52:36 2005] [notice] jk2_init() Found child 6748 in scoreboard slot 6
[Sun Dec 04 04:52:36 2005] [notice] jk2_init() Found child 6744 in scoreboard slot 10
[Sun Dec 04 04:52:36 2005] [notice] jk2_init() Found child 6745 in scoreboard slot 8
[Sun Dec 04 04:52:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:52:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:52:52 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 04:52:52 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:53:05 2005] [notice] jk2_init() Found child 6750 in scoreboard slot 7
[Sun Dec 04 04:53:05 2005] [notice] jk2_init() Found child 6751 in scoreboard slot 9
[Sun Dec 04 04:53:05 2005] [notice] jk2_init() Found child 6752 in scoreboard slot 10
[Sun Dec 04 04:53:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:53:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:53:16 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 04:53:16 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:53:29 2005] [notice] jk2_init() Found child 6754 in scoreboard slot 8
[Sun Dec 04 04:53:29 2005] [notice] jk2_init() Found child 6755 in scoreboard slot 6
[Sun Dec 04 04:53:40 2005] [notice] jk2_init() Found child 6756 in scoreboard slot 7
[Sun Dec 04 04:53:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:53:54 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 04:54:15 2005] [notice] jk2_init() Found child 6763 in scoreboard slot 10
[Sun Dec 04 04:54:15 2005] [notice] jk2_init() Found child 6766 in scoreboard slot 6
[Sun Dec 04 04:54:15 2005] [notice] jk2_init() Found child 6767 in scoreboard slot 7
[Sun Dec 04 04:54:15 2005] [notice] jk2_init() Found child 6765 in scoreboard slot 8
[Sun Dec 04 04:54:18 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:54:18 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:54:18 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:54:18 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:54:18 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:54:18 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:54:18 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 04:54:18 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 04:54:20 2005] [notice] jk2_init() Found child 6768 in scoreboard slot 9
[Sun Dec 04 04:54:20 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:54:20 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:56:52 2005] [notice] jk2_init() Found child 8527 in scoreboard slot 10
[Sun Dec 04 04:56:52 2005] [notice] jk2_init() Found child 8533 in scoreboard slot 8
[Sun Dec 04 04:56:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:56:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:56:59 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:57:00 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:57:20 2005] [notice] jk2_init() Found child 8536 in scoreboard slot 6
[Sun Dec 04 04:57:20 2005] [notice] jk2_init() Found child 8539 in scoreboard slot 7
[Sun Dec 04 04:57:24 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:57:24 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:57:24 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:57:24 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:57:49 2005] [notice] jk2_init() Found child 8541 in scoreboard slot 9
[Sun Dec 04 04:58:11 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:58:18 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:58:45 2005] [notice] jk2_init() Found child 8547 in scoreboard slot 10
[Sun Dec 04 04:58:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:58:58 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:59:28 2005] [notice] jk2_init() Found child 8554 in scoreboard slot 6
[Sun Dec 04 04:59:27 2005] [notice] jk2_init() Found child 8553 in scoreboard slot 8
[Sun Dec 04 04:59:35 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:59:35 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 04:59:38 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 04:59:38 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:00:03 2005] [notice] jk2_init() Found child 8560 in scoreboard slot 7
[Sun Dec 04 05:00:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:00:09 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:00:13 2005] [notice] jk2_init() Found child 8565 in scoreboard slot 9
[Sun Dec 04 05:00:13 2005] [notice] jk2_init() Found child 8573 in scoreboard slot 10
[Sun Dec 04 05:00:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:00:15 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:00:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:00:15 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:01:20 2005] [notice] jk2_init() Found child 8584 in scoreboard slot 7
[Sun Dec 04 05:01:20 2005] [notice] jk2_init() Found child 8587 in scoreboard slot 9
[Sun Dec 04 05:02:14 2005] [notice] jk2_init() Found child 8603 in scoreboard slot 10
[Sun Dec 04 05:02:14 2005] [notice] jk2_init() Found child 8605 in scoreboard slot 8
[Sun Dec 04 05:04:03 2005] [notice] jk2_init() Found child 8764 in scoreboard slot 10
[Sun Dec 04 05:04:03 2005] [notice] jk2_init() Found child 8765 in scoreboard slot 11
[Sun Dec 04 05:04:03 2005] [notice] jk2_init() Found child 8763 in scoreboard slot 9
[Sun Dec 04 05:04:03 2005] [notice] jk2_init() Found child 8744 in scoreboard slot 8
[Sun Dec 04 05:04:03 2005] [notice] jk2_init() Found child 8743 in scoreboard slot 7
[Sun Dec 04 05:04:03 2005] [notice] jk2_init() Found child 8738 in scoreboard slot 6
[Sun Dec 04 05:04:03 2005] [notice] jk2_init() Found child 8766 in scoreboard slot 12
[Sun Dec 04 05:04:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:04:04 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 05:04:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:04:04 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:04:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:04:04 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 05:04:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:04:04 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 05:04:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:04:04 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 05:04:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:04:04 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:04:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:04:04 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:11:51 2005] [notice] jk2_init() Found child 25792 in scoreboard slot 6
[Sun Dec 04 05:12:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:12:10 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:12:26 2005] [notice] jk2_init() Found child 25798 in scoreboard slot 7
[Sun Dec 04 05:12:26 2005] [notice] jk2_init() Found child 25803 in scoreboard slot 8
[Sun Dec 04 05:12:28 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:12:28 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:12:28 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:12:28 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:12:30 2005] [notice] jk2_init() Found child 25805 in scoreboard slot 9
[Sun Dec 04 05:12:30 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:12:30 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 05:15:09 2005] [error] [client 222.166.160.184] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 05:15:13 2005] [notice] jk2_init() Found child 1000 in scoreboard slot 10
[Sun Dec 04 05:15:16 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 05:15:16 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:01:00 2005] [notice] jk2_init() Found child 32347 in scoreboard slot 6
[Sun Dec 04 06:01:00 2005] [notice] jk2_init() Found child 32348 in scoreboard slot 7
[Sun Dec 04 06:01:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:01:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:01:30 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:01:42 2005] [notice] jk2_init() Found child 32352 in scoreboard slot 9
[Sun Dec 04 06:01:42 2005] [notice] jk2_init() Found child 32353 in scoreboard slot 10
[Sun Dec 04 06:01:42 2005] [notice] jk2_init() Found child 32354 in scoreboard slot 6
[Sun Dec 04 06:02:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:02:02 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:02:05 2005] [notice] jk2_init() Found child 32359 in scoreboard slot 9
[Sun Dec 04 06:02:05 2005] [notice] jk2_init() Found child 32360 in scoreboard slot 11
[Sun Dec 04 06:02:05 2005] [notice] jk2_init() Found child 32358 in scoreboard slot 8
[Sun Dec 04 06:02:05 2005] [notice] jk2_init() Found child 32355 in scoreboard slot 7
[Sun Dec 04 06:02:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:02:07 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:02:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:02:07 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:02:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:02:07 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:02:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:02:07 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:06:00 2005] [notice] jk2_init() Found child 32388 in scoreboard slot 8
[Sun Dec 04 06:06:00 2005] [notice] jk2_init() Found child 32387 in scoreboard slot 7
[Sun Dec 04 06:06:00 2005] [notice] jk2_init() Found child 32386 in scoreboard slot 6
[Sun Dec 04 06:06:10 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:06:11 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:06:12 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:06:12 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:06:20 2005] [notice] jk2_init() Found child 32389 in scoreboard slot 9
[Sun Dec 04 06:06:24 2005] [notice] jk2_init() Found child 32391 in scoreboard slot 10
[Sun Dec 04 06:06:24 2005] [notice] jk2_init() Found child 32390 in scoreboard slot 8
[Sun Dec 04 06:06:24 2005] [notice] jk2_init() Found child 32392 in scoreboard slot 6
[Sun Dec 04 06:06:26 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:06:26 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:06:26 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:06:26 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:06:26 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:06:26 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:06:26 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:06:26 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:11:11 2005] [notice] jk2_init() Found child 32410 in scoreboard slot 7
[Sun Dec 04 06:11:11 2005] [notice] jk2_init() Found child 32411 in scoreboard slot 9
[Sun Dec 04 06:12:31 2005] [notice] jk2_init() Found child 32423 in scoreboard slot 9
[Sun Dec 04 06:12:31 2005] [notice] jk2_init() Found child 32422 in scoreboard slot 8
[Sun Dec 04 06:12:31 2005] [notice] jk2_init() Found child 32419 in scoreboard slot 6
[Sun Dec 04 06:12:31 2005] [notice] jk2_init() Found child 32421 in scoreboard slot 11
[Sun Dec 04 06:12:31 2005] [notice] jk2_init() Found child 32420 in scoreboard slot 7
[Sun Dec 04 06:12:31 2005] [notice] jk2_init() Found child 32424 in scoreboard slot 10
[Sun Dec 04 06:12:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:12:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:12:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:12:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:12:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:12:40 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:12:40 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:12:40 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:12:40 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:12:40 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:12:59 2005] [notice] jk2_init() Found child 32425 in scoreboard slot 6
[Sun Dec 04 06:13:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:13:01 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:16:10 2005] [notice] jk2_init() Found child 32432 in scoreboard slot 7
[Sun Dec 04 06:16:10 2005] [notice] jk2_init() Found child 32434 in scoreboard slot 9
[Sun Dec 04 06:16:10 2005] [notice] jk2_init() Found child 32433 in scoreboard slot 8
[Sun Dec 04 06:16:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:16:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:16:23 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:16:23 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:16:21 2005] [notice] jk2_init() Found child 32435 in scoreboard slot 10
[Sun Dec 04 06:16:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:16:23 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:16:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:16:39 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:16:51 2005] [notice] jk2_init() Found child 32436 in scoreboard slot 6
[Sun Dec 04 06:16:51 2005] [notice] jk2_init() Found child 32437 in scoreboard slot 7
[Sun Dec 04 06:17:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:17:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:17:05 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:17:06 2005] [notice] jk2_init() Found child 32438 in scoreboard slot 8
[Sun Dec 04 06:17:18 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:17:24 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:17:23 2005] [notice] jk2_init() Found child 32440 in scoreboard slot 10
[Sun Dec 04 06:17:23 2005] [notice] jk2_init() Found child 32439 in scoreboard slot 9
[Sun Dec 04 06:17:23 2005] [notice] jk2_init() Found child 32441 in scoreboard slot 6
[Sun Dec 04 06:17:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:17:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:17:35 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:17:35 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:17:55 2005] [notice] jk2_init() Found child 32442 in scoreboard slot 7
[Sun Dec 04 06:17:55 2005] [notice] jk2_init() Found child 32443 in scoreboard slot 8
[Sun Dec 04 06:17:55 2005] [notice] jk2_init() Found child 32444 in scoreboard slot 9
[Sun Dec 04 06:18:08 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:18:08 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:18:11 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:18:11 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:18:12 2005] [notice] jk2_init() Found child 32445 in scoreboard slot 10
[Sun Dec 04 06:18:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:18:31 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:18:41 2005] [notice] jk2_init() Found child 32447 in scoreboard slot 7
[Sun Dec 04 06:18:39 2005] [notice] jk2_init() Found child 32446 in scoreboard slot 6
[Sun Dec 04 06:18:40 2005] [notice] jk2_init() Found child 32448 in scoreboard slot 8
[Sun Dec 04 06:18:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:18:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:18:55 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:18:55 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:19:05 2005] [notice] jk2_init() Found child 32449 in scoreboard slot 9
[Sun Dec 04 06:19:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:19:19 2005] [notice] jk2_init() Found child 32450 in scoreboard slot 10
[Sun Dec 04 06:19:18 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:19:19 2005] [notice] jk2_init() Found child 32452 in scoreboard slot 7
[Sun Dec 04 06:19:19 2005] [notice] jk2_init() Found child 32451 in scoreboard slot 6
[Sun Dec 04 06:19:31 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:19:31 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:19:34 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:19:34 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:19:56 2005] [notice] jk2_init() Found child 32454 in scoreboard slot 7
[Sun Dec 04 06:19:56 2005] [notice] jk2_init() Found child 32453 in scoreboard slot 8
[Sun Dec 04 06:19:56 2005] [notice] jk2_init() Found child 32455 in scoreboard slot 9
[Sun Dec 04 06:20:30 2005] [notice] jk2_init() Found child 32467 in scoreboard slot 9
[Sun Dec 04 06:20:30 2005] [notice] jk2_init() Found child 32464 in scoreboard slot 8
[Sun Dec 04 06:20:30 2005] [notice] jk2_init() Found child 32465 in scoreboard slot 7
[Sun Dec 04 06:20:30 2005] [notice] jk2_init() Found child 32466 in scoreboard slot 11
[Sun Dec 04 06:20:30 2005] [notice] jk2_init() Found child 32457 in scoreboard slot 6
[Sun Dec 04 06:20:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:20:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:20:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:20:46 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:20:46 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:20:46 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 06:22:18 2005] [notice] jk2_init() Found child 32475 in scoreboard slot 8
[Sun Dec 04 06:22:48 2005] [notice] jk2_init() Found child 32478 in scoreboard slot 11
[Sun Dec 04 06:22:48 2005] [notice] jk2_init() Found child 32477 in scoreboard slot 10
[Sun Dec 04 06:22:48 2005] [notice] jk2_init() Found child 32479 in scoreboard slot 6
[Sun Dec 04 06:22:48 2005] [notice] jk2_init() Found child 32480 in scoreboard slot 8
[Sun Dec 04 06:22:48 2005] [notice] jk2_init() Found child 32476 in scoreboard slot 7
[Sun Dec 04 06:22:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:22:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:22:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:22:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:22:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:22:55 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:22:55 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:22:55 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:22:55 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:22:55 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 06:23:12 2005] [notice] jk2_init() Found child 32483 in scoreboard slot 7
[Sun Dec 04 06:23:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:23:15 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:30:41 2005] [notice] jk2_init() Found child 32507 in scoreboard slot 9
[Sun Dec 04 06:30:43 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:30:43 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:36:07 2005] [notice] jk2_init() Found child 32529 in scoreboard slot 6
[Sun Dec 04 06:36:07 2005] [notice] jk2_init() Found child 32528 in scoreboard slot 10
[Sun Dec 04 06:36:10 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:36:10 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:36:10 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:36:10 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:40:54 2005] [notice] jk2_init() Found child 32548 in scoreboard slot 9
[Sun Dec 04 06:40:54 2005] [notice] jk2_init() Found child 32546 in scoreboard slot 8
[Sun Dec 04 06:40:55 2005] [notice] jk2_init() Found child 32547 in scoreboard slot 7
[Sun Dec 04 06:41:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:41:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:41:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:41:07 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:41:08 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:41:08 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:41:29 2005] [notice] jk2_init() Found child 32549 in scoreboard slot 10
[Sun Dec 04 06:41:29 2005] [notice] jk2_init() Found child 32550 in scoreboard slot 6
[Sun Dec 04 06:41:45 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:41:45 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:41:46 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:41:46 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:42:11 2005] [notice] jk2_init() Found child 32551 in scoreboard slot 8
[Sun Dec 04 06:42:11 2005] [notice] jk2_init() Found child 32552 in scoreboard slot 7
[Sun Dec 04 06:42:25 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:42:23 2005] [notice] jk2_init() Found child 32554 in scoreboard slot 10
[Sun Dec 04 06:42:25 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:42:23 2005] [notice] jk2_init() Found child 32553 in scoreboard slot 9
[Sun Dec 04 06:42:30 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:42:30 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:42:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:42:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:42:58 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:42:58 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:43:20 2005] [notice] jk2_init() Found child 32556 in scoreboard slot 8
[Sun Dec 04 06:43:20 2005] [notice] jk2_init() Found child 32555 in scoreboard slot 6
[Sun Dec 04 06:43:20 2005] [notice] jk2_init() Found child 32557 in scoreboard slot 7
[Sun Dec 04 06:43:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:43:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:43:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:43:40 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:43:40 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:43:56 2005] [notice] jk2_init() Found child 32558 in scoreboard slot 9
[Sun Dec 04 06:44:18 2005] [notice] jk2_init() Found child 32560 in scoreboard slot 6
[Sun Dec 04 06:44:18 2005] [notice] jk2_init() Found child 32561 in scoreboard slot 8
[Sun Dec 04 06:44:39 2005] [notice] jk2_init() Found child 32563 in scoreboard slot 9
[Sun Dec 04 06:44:39 2005] [notice] jk2_init() Found child 32564 in scoreboard slot 10
[Sun Dec 04 06:44:39 2005] [notice] jk2_init() Found child 32565 in scoreboard slot 11
[Sun Dec 04 06:45:32 2005] [notice] jk2_init() Found child 32575 in scoreboard slot 6
[Sun Dec 04 06:45:32 2005] [notice] jk2_init() Found child 32576 in scoreboard slot 7
[Sun Dec 04 06:45:32 2005] [notice] jk2_init() Found child 32569 in scoreboard slot 9
[Sun Dec 04 06:45:32 2005] [notice] jk2_init() Found child 32572 in scoreboard slot 10
[Sun Dec 04 06:45:32 2005] [notice] jk2_init() Found child 32577 in scoreboard slot 11
[Sun Dec 04 06:45:50 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:45:50 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:45:57 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:46:13 2005] [notice] jk2_init() Found child 32578 in scoreboard slot 8
[Sun Dec 04 06:46:13 2005] [notice] jk2_init() Found child 32580 in scoreboard slot 6
[Sun Dec 04 06:46:12 2005] [notice] jk2_init() Found child 32581 in scoreboard slot 7
[Sun Dec 04 06:46:13 2005] [notice] jk2_init() Found child 32579 in scoreboard slot 9
[Sun Dec 04 06:46:30 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:46:30 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:46:31 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 06:46:31 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:46:32 2005] [notice] jk2_init() Found child 32582 in scoreboard slot 10
[Sun Dec 04 06:46:32 2005] [notice] jk2_init() Found child 32584 in scoreboard slot 9
[Sun Dec 04 06:46:32 2005] [notice] jk2_init() Found child 32583 in scoreboard slot 8
[Sun Dec 04 06:46:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:46:33 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 06:46:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:46:34 2005] [error] mod_jk child workerEnv in error state 10
[Sun Dec 04 06:46:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:46:34 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 06:47:19 2005] [notice] jk2_init() Found child 32585 in scoreboard slot 6
[Sun Dec 04 06:47:30 2005] [notice] jk2_init() Found child 32587 in scoreboard slot 10
[Sun Dec 04 06:47:30 2005] [notice] jk2_init() Found child 32586 in scoreboard slot 7
[Sun Dec 04 06:47:34 2005] [notice] jk2_init() Found child 32588 in scoreboard slot 8
[Sun Dec 04 06:47:38 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:47:39 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:47:39 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:47:43 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:48:09 2005] [notice] jk2_init() Found child 32592 in scoreboard slot 10
[Sun Dec 04 06:48:09 2005] [notice] jk2_init() Found child 32591 in scoreboard slot 7
[Sun Dec 04 06:48:22 2005] [notice] jk2_init() Found child 32594 in scoreboard slot 6
[Sun Dec 04 06:48:22 2005] [notice] jk2_init() Found child 32593 in scoreboard slot 8
[Sun Dec 04 06:48:48 2005] [notice] jk2_init() Found child 32597 in scoreboard slot 10
[Sun Dec 04 06:49:06 2005] [notice] jk2_init() Found child 32600 in scoreboard slot 9
[Sun Dec 04 06:49:06 2005] [notice] jk2_init() Found child 32601 in scoreboard slot 7
[Sun Dec 04 06:49:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:49:24 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 06:49:40 2005] [notice] jk2_init() Found child 32605 in scoreboard slot 9
[Sun Dec 04 06:49:40 2005] [notice] jk2_init() Found child 32604 in scoreboard slot 6
[Sun Dec 04 06:51:13 2005] [notice] jk2_init() Found child 32622 in scoreboard slot 7
[Sun Dec 04 06:51:14 2005] [notice] jk2_init() Found child 32623 in scoreboard slot 11
[Sun Dec 04 06:51:13 2005] [notice] jk2_init() Found child 32624 in scoreboard slot 8
[Sun Dec 04 06:51:13 2005] [notice] jk2_init() Found child 32621 in scoreboard slot 9
[Sun Dec 04 06:51:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:51:23 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:51:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:51:23 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:51:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:51:23 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 06:51:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:51:23 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:51:25 2005] [notice] jk2_init() Found child 32626 in scoreboard slot 6
[Sun Dec 04 06:51:26 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:51:26 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 06:52:07 2005] [notice] jk2_init() Found child 32627 in scoreboard slot 9
[Sun Dec 04 06:52:08 2005] [notice] jk2_init() Found child 32628 in scoreboard slot 7
[Sun Dec 04 06:52:13 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:52:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:52:15 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:52:15 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:52:27 2005] [notice] jk2_init() Found child 32630 in scoreboard slot 8
[Sun Dec 04 06:52:27 2005] [notice] jk2_init() Found child 32629 in scoreboard slot 10
[Sun Dec 04 06:52:39 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:52:39 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:52:41 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 06:52:41 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:53:04 2005] [notice] jk2_init() Found child 32633 in scoreboard slot 9
[Sun Dec 04 06:53:04 2005] [notice] jk2_init() Found child 32634 in scoreboard slot 11
[Sun Dec 04 06:53:04 2005] [notice] jk2_init() Found child 32632 in scoreboard slot 7
[Sun Dec 04 06:53:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:53:26 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:53:38 2005] [notice] jk2_init() Found child 32636 in scoreboard slot 6
[Sun Dec 04 06:53:37 2005] [notice] jk2_init() Found child 32637 in scoreboard slot 7
[Sun Dec 04 06:53:37 2005] [notice] jk2_init() Found child 32638 in scoreboard slot 9
[Sun Dec 04 06:54:04 2005] [notice] jk2_init() Found child 32640 in scoreboard slot 8
[Sun Dec 04 06:54:04 2005] [notice] jk2_init() Found child 32641 in scoreboard slot 6
[Sun Dec 04 06:54:04 2005] [notice] jk2_init() Found child 32642 in scoreboard slot 7
[Sun Dec 04 06:54:35 2005] [notice] jk2_init() Found child 32646 in scoreboard slot 6
[Sun Dec 04 06:55:00 2005] [notice] jk2_init() Found child 32648 in scoreboard slot 9
[Sun Dec 04 06:55:00 2005] [notice] jk2_init() Found child 32652 in scoreboard slot 7
[Sun Dec 04 06:55:00 2005] [notice] jk2_init() Found child 32649 in scoreboard slot 10
[Sun Dec 04 06:55:00 2005] [notice] jk2_init() Found child 32651 in scoreboard slot 6
[Sun Dec 04 06:55:00 2005] [notice] jk2_init() Found child 32650 in scoreboard slot 8
[Sun Dec 04 06:55:19 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:55:19 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:55:19 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:55:23 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:55:23 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 06:55:23 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 06:55:55 2005] [notice] jk2_init() Found child 32660 in scoreboard slot 6
[Sun Dec 04 06:55:54 2005] [notice] jk2_init() Found child 32658 in scoreboard slot 10
[Sun Dec 04 06:55:54 2005] [notice] jk2_init() Found child 32659 in scoreboard slot 8
[Sun Dec 04 06:55:54 2005] [notice] jk2_init() Found child 32657 in scoreboard slot 9
[Sun Dec 04 06:56:10 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:56:17 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:56:37 2005] [notice] jk2_init() Found child 32663 in scoreboard slot 10
[Sun Dec 04 06:56:37 2005] [notice] jk2_init() Found child 32664 in scoreboard slot 8
[Sun Dec 04 06:57:19 2005] [notice] jk2_init() Found child 32670 in scoreboard slot 6
[Sun Dec 04 06:57:19 2005] [notice] jk2_init() Found child 32667 in scoreboard slot 9
[Sun Dec 04 06:57:19 2005] [notice] jk2_init() Found child 32668 in scoreboard slot 10
[Sun Dec 04 06:57:19 2005] [notice] jk2_init() Found child 32669 in scoreboard slot 8
[Sun Dec 04 06:57:19 2005] [notice] jk2_init() Found child 32671 in scoreboard slot 7
[Sun Dec 04 06:57:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:57:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:57:23 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:57:23 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:57:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:57:23 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 06:57:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:57:23 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 06:57:24 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:57:24 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 06:58:12 2005] [notice] jk2_init() Found child 32674 in scoreboard slot 8
[Sun Dec 04 06:58:13 2005] [notice] jk2_init() Found child 32672 in scoreboard slot 9
[Sun Dec 04 06:58:13 2005] [notice] jk2_init() Found child 32673 in scoreboard slot 10
[Sun Dec 04 06:58:27 2005] [notice] jk2_init() Found child 32675 in scoreboard slot 6
[Sun Dec 04 06:58:28 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:58:28 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:58:29 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:58:29 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:58:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:58:53 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:58:54 2005] [notice] jk2_init() Found child 32677 in scoreboard slot 7
[Sun Dec 04 06:58:54 2005] [notice] jk2_init() Found child 32676 in scoreboard slot 9
[Sun Dec 04 06:58:54 2005] [notice] jk2_init() Found child 32678 in scoreboard slot 10
[Sun Dec 04 06:59:28 2005] [notice] jk2_init() Found child 32679 in scoreboard slot 8
[Sun Dec 04 06:59:28 2005] [notice] jk2_init() Found child 32680 in scoreboard slot 6
[Sun Dec 04 06:59:34 2005] [notice] jk2_init() Found child 32681 in scoreboard slot 9
[Sun Dec 04 06:59:34 2005] [notice] jk2_init() Found child 32682 in scoreboard slot 7
[Sun Dec 04 06:59:38 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:59:40 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 06:59:45 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:59:45 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 06:59:47 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 06:59:47 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 06:59:59 2005] [notice] jk2_init() Found child 32683 in scoreboard slot 10
[Sun Dec 04 07:00:06 2005] [notice] jk2_init() Found child 32685 in scoreboard slot 6
[Sun Dec 04 07:00:32 2005] [notice] jk2_init() Found child 32688 in scoreboard slot 11
[Sun Dec 04 07:00:32 2005] [notice] jk2_init() Found child 32695 in scoreboard slot 8
[Sun Dec 04 07:00:32 2005] [notice] jk2_init() Found child 32696 in scoreboard slot 6
[Sun Dec 04 07:01:25 2005] [notice] jk2_init() Found child 32701 in scoreboard slot 10
[Sun Dec 04 07:01:26 2005] [notice] jk2_init() Found child 32702 in scoreboard slot 11
[Sun Dec 04 07:01:55 2005] [notice] jk2_init() Found child 32711 in scoreboard slot 10
[Sun Dec 04 07:01:55 2005] [notice] jk2_init() Found child 32708 in scoreboard slot 7
[Sun Dec 04 07:01:55 2005] [notice] jk2_init() Found child 32710 in scoreboard slot 9
[Sun Dec 04 07:01:55 2005] [notice] jk2_init() Found child 32709 in scoreboard slot 8
[Sun Dec 04 07:01:57 2005] [notice] jk2_init() Found child 32712 in scoreboard slot 6
[Sun Dec 04 07:02:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:02:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:02:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:02:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:02:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:02:03 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 07:02:03 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 07:02:03 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 07:02:03 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:02:03 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:02:52 2005] [notice] jk2_init() Found child 32713 in scoreboard slot 7
[Sun Dec 04 07:03:23 2005] [notice] jk2_init() Found child 32717 in scoreboard slot 10
[Sun Dec 04 07:03:48 2005] [notice] jk2_init() Found child 32720 in scoreboard slot 8
[Sun Dec 04 07:04:27 2005] [notice] jk2_init() Found child 32726 in scoreboard slot 8
[Sun Dec 04 07:04:55 2005] [notice] jk2_init() Found child 32730 in scoreboard slot 7
[Sun Dec 04 07:04:55 2005] [notice] jk2_init() Found child 32729 in scoreboard slot 6
[Sun Dec 04 07:04:55 2005] [notice] jk2_init() Found child 32731 in scoreboard slot 8
[Sun Dec 04 07:05:44 2005] [notice] jk2_init() Found child 32739 in scoreboard slot 7
[Sun Dec 04 07:05:44 2005] [notice] jk2_init() Found child 32740 in scoreboard slot 8
[Sun Dec 04 07:06:11 2005] [notice] jk2_init() Found child 32742 in scoreboard slot 10
[Sun Dec 04 07:07:23 2005] [notice] jk2_init() Found child 32758 in scoreboard slot 7
[Sun Dec 04 07:07:23 2005] [notice] jk2_init() Found child 32755 in scoreboard slot 8
[Sun Dec 04 07:07:23 2005] [notice] jk2_init() Found child 32754 in scoreboard slot 11
[Sun Dec 04 07:07:30 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:07:30 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:07:30 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:07:30 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 07:07:30 2005] [error] mod_jk child workerEnv in error state 10
[Sun Dec 04 07:07:30 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 07:08:02 2005] [notice] jk2_init() Found child 32761 in scoreboard slot 6
[Sun Dec 04 07:08:02 2005] [notice] jk2_init() Found child 32762 in scoreboard slot 9
[Sun Dec 04 07:08:02 2005] [notice] jk2_init() Found child 32763 in scoreboard slot 10
[Sun Dec 04 07:08:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:08:04 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 07:08:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:08:04 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:08:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:08:04 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 07:10:54 2005] [notice] jk2_init() Found child 308 in scoreboard slot 8
[Sun Dec 04 07:11:04 2005] [notice] jk2_init() Found child 310 in scoreboard slot 6
[Sun Dec 04 07:11:04 2005] [notice] jk2_init() Found child 309 in scoreboard slot 7
[Sun Dec 04 07:11:05 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:11:05 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:11:13 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:11:13 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:11:22 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:11:22 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:11:49 2005] [notice] jk2_init() Found child 311 in scoreboard slot 9
[Sun Dec 04 07:12:05 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:12:08 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:12:22 2005] [notice] jk2_init() Found child 312 in scoreboard slot 10
[Sun Dec 04 07:12:22 2005] [notice] jk2_init() Found child 313 in scoreboard slot 8
[Sun Dec 04 07:12:40 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:12:40 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:12:44 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:12:44 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:13:09 2005] [notice] jk2_init() Found child 314 in scoreboard slot 7
[Sun Dec 04 07:13:09 2005] [notice] jk2_init() Found child 315 in scoreboard slot 6
[Sun Dec 04 07:13:10 2005] [notice] jk2_init() Found child 316 in scoreboard slot 9
[Sun Dec 04 07:13:36 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:13:36 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:13:36 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:13:41 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:13:41 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:14:07 2005] [notice] jk2_init() Found child 319 in scoreboard slot 7
[Sun Dec 04 07:14:07 2005] [notice] jk2_init() Found child 317 in scoreboard slot 10
[Sun Dec 04 07:14:08 2005] [notice] jk2_init() Found child 318 in scoreboard slot 8
[Sun Dec 04 07:14:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:14:29 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 07:14:47 2005] [notice] jk2_init() Found child 321 in scoreboard slot 9
[Sun Dec 04 07:15:09 2005] [notice] jk2_init() Found child 324 in scoreboard slot 11
[Sun Dec 04 07:15:09 2005] [notice] jk2_init() Found child 323 in scoreboard slot 8
[Sun Dec 04 07:17:56 2005] [notice] jk2_init() Found child 350 in scoreboard slot 9
[Sun Dec 04 07:17:56 2005] [notice] jk2_init() Found child 353 in scoreboard slot 12
[Sun Dec 04 07:17:56 2005] [notice] jk2_init() Found child 352 in scoreboard slot 11
[Sun Dec 04 07:17:56 2005] [notice] jk2_init() Found child 349 in scoreboard slot 8
[Sun Dec 04 07:17:56 2005] [notice] jk2_init() Found child 348 in scoreboard slot 7
[Sun Dec 04 07:17:56 2005] [notice] jk2_init() Found child 347 in scoreboard slot 6
[Sun Dec 04 07:17:56 2005] [notice] jk2_init() Found child 351 in scoreboard slot 10
[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 07:18:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 07:18:00 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 07:45:45 2005] [error] [client 63.13.186.196] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 08:54:17 2005] [error] [client 147.31.138.75] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 09:35:12 2005] [error] [client 207.203.80.15] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 10:53:30 2005] [error] [client 218.76.139.20] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 11:11:07 2005] [error] [client 24.147.151.74] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 11:33:18 2005] [error] [client 211.141.93.88] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 11:42:43 2005] [error] [client 216.127.124.16] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 12:33:13 2005] [error] [client 208.51.151.210] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 13:32:32 2005] [error] [client 65.68.235.27] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 14:29:00 2005] [error] [client 4.245.93.87] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 15:18:36 2005] [error] [client 67.154.58.130] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 15:59:01 2005] [error] [client 24.83.37.136] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 16:24:03 2005] [notice] jk2_init() Found child 1219 in scoreboard slot 6
[Sun Dec 04 16:24:05 2005] [error] [client 58.225.62.140] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 16:24:06 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:24:06 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:31:07 2005] [notice] jk2_init() Found child 1248 in scoreboard slot 7
[Sun Dec 04 16:32:37 2005] [notice] jk2_init() Found child 1253 in scoreboard slot 9
[Sun Dec 04 16:32:56 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:32:56 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:32:58 2005] [notice] jk2_init() Found child 1254 in scoreboard slot 7
[Sun Dec 04 16:32:58 2005] [notice] jk2_init() Found child 1256 in scoreboard slot 6
[Sun Dec 04 16:32:58 2005] [notice] jk2_init() Found child 1255 in scoreboard slot 8
[Sun Dec 04 16:32:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:32:58 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 16:32:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:32:58 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:32:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:32:58 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:35:49 2005] [notice] jk2_init() Found child 1262 in scoreboard slot 9
[Sun Dec 04 16:35:52 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:35:52 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:41:15 2005] [notice] jk2_init() Found child 1275 in scoreboard slot 6
[Sun Dec 04 16:41:16 2005] [notice] jk2_init() Found child 1276 in scoreboard slot 9
[Sun Dec 04 16:41:22 2005] [notice] jk2_init() Found child 1277 in scoreboard slot 7
[Sun Dec 04 16:41:22 2005] [notice] jk2_init() Found child 1278 in scoreboard slot 8
[Sun Dec 04 16:41:22 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:41:22 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:41:22 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:41:22 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:41:22 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:41:22 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:41:22 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:41:22 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:45:52 2005] [notice] jk2_init() Found child 1283 in scoreboard slot 6
[Sun Dec 04 16:45:52 2005] [notice] jk2_init() Found child 1284 in scoreboard slot 9
[Sun Dec 04 16:46:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:46:13 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:46:45 2005] [notice] jk2_init() Found child 1288 in scoreboard slot 9
[Sun Dec 04 16:47:11 2005] [notice] jk2_init() Found child 1291 in scoreboard slot 6
[Sun Dec 04 16:47:59 2005] [notice] jk2_init() Found child 1296 in scoreboard slot 6
[Sun Dec 04 16:47:59 2005] [notice] jk2_init() Found child 1300 in scoreboard slot 10
[Sun Dec 04 16:47:59 2005] [notice] jk2_init() Found child 1298 in scoreboard slot 8
[Sun Dec 04 16:47:59 2005] [notice] jk2_init() Found child 1297 in scoreboard slot 7
[Sun Dec 04 16:47:59 2005] [notice] jk2_init() Found child 1299 in scoreboard slot 9
[Sun Dec 04 16:48:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:48:01 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 16:48:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:48:01 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 16:48:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:48:01 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:48:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:48:01 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:48:01 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:48:01 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:50:53 2005] [notice] jk2_init() Found child 1308 in scoreboard slot 6
[Sun Dec 04 16:50:53 2005] [notice] jk2_init() Found child 1309 in scoreboard slot 7
[Sun Dec 04 16:51:26 2005] [notice] jk2_init() Found child 1313 in scoreboard slot 6
[Sun Dec 04 16:51:26 2005] [notice] jk2_init() Found child 1312 in scoreboard slot 10
[Sun Dec 04 16:52:34 2005] [notice] jk2_init() Found child 1320 in scoreboard slot 8
[Sun Dec 04 16:52:45 2005] [notice] jk2_init() Found child 1321 in scoreboard slot 9
[Sun Dec 04 16:52:45 2005] [notice] jk2_init() Found child 1322 in scoreboard slot 10
[Sun Dec 04 16:52:45 2005] [notice] jk2_init() Found child 1323 in scoreboard slot 6
[Sun Dec 04 16:52:46 2005] [notice] jk2_init() Found child 1324 in scoreboard slot 7
[Sun Dec 04 16:52:46 2005] [notice] jk2_init() Found child 1325 in scoreboard slot 8
[Sun Dec 04 16:52:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:52:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:52:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:52:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:52:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:52:49 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:52:49 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 16:52:49 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 16:52:49 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 16:52:49 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:55:54 2005] [notice] jk2_init() Found child 1331 in scoreboard slot 10
[Sun Dec 04 16:56:25 2005] [notice] jk2_init() Found child 1338 in scoreboard slot 7
[Sun Dec 04 16:56:25 2005] [notice] jk2_init() Found child 1334 in scoreboard slot 8
[Sun Dec 04 16:56:25 2005] [notice] jk2_init() Found child 1336 in scoreboard slot 10
[Sun Dec 04 16:56:25 2005] [notice] jk2_init() Found child 1337 in scoreboard slot 6
[Sun Dec 04 16:56:25 2005] [notice] jk2_init() Found child 1335 in scoreboard slot 9
[Sun Dec 04 16:56:27 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:56:27 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:56:27 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:56:27 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 16:56:27 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:56:27 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:56:27 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:56:27 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 16:56:27 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 16:56:27 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:01:43 2005] [notice] jk2_init() Found child 1358 in scoreboard slot 8
[Sun Dec 04 17:01:43 2005] [notice] jk2_init() Found child 1356 in scoreboard slot 6
[Sun Dec 04 17:01:43 2005] [notice] jk2_init() Found child 1354 in scoreboard slot 9
[Sun Dec 04 17:01:43 2005] [notice] jk2_init() Found child 1357 in scoreboard slot 7
[Sun Dec 04 17:01:43 2005] [notice] jk2_init() Found child 1355 in scoreboard slot 10
[Sun Dec 04 17:01:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:01:47 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:01:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:01:47 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:01:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:01:47 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:01:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:01:47 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:01:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:01:47 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:05:45 2005] [notice] jk2_init() Found child 1375 in scoreboard slot 9
[Sun Dec 04 17:05:45 2005] [notice] jk2_init() Found child 1376 in scoreboard slot 10
[Sun Dec 04 17:05:45 2005] [notice] jk2_init() Found child 1377 in scoreboard slot 6
[Sun Dec 04 17:05:48 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:05:48 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:05:48 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:05:48 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:05:48 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:05:48 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:11:23 2005] [notice] jk2_init() Found child 1387 in scoreboard slot 7
[Sun Dec 04 17:11:37 2005] [notice] jk2_init() Found child 1390 in scoreboard slot 10
[Sun Dec 04 17:11:37 2005] [notice] jk2_init() Found child 1388 in scoreboard slot 8
[Sun Dec 04 17:11:37 2005] [notice] jk2_init() Found child 1389 in scoreboard slot 9
[Sun Dec 04 17:12:42 2005] [notice] jk2_init() Found child 1393 in scoreboard slot 8
[Sun Dec 04 17:12:50 2005] [notice] jk2_init() Found child 1395 in scoreboard slot 10
[Sun Dec 04 17:12:50 2005] [notice] jk2_init() Found child 1396 in scoreboard slot 6
[Sun Dec 04 17:12:50 2005] [notice] jk2_init() Found child 1394 in scoreboard slot 9
[Sun Dec 04 17:12:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:12:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:12:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:12:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:12:55 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:12:55 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 17:12:55 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 17:12:55 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 17:12:56 2005] [notice] jk2_init() Found child 1397 in scoreboard slot 7
[Sun Dec 04 17:12:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:12:57 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 17:17:07 2005] [notice] jk2_init() Found child 1414 in scoreboard slot 7
[Sun Dec 04 17:17:07 2005] [notice] jk2_init() Found child 1412 in scoreboard slot 10
[Sun Dec 04 17:17:07 2005] [notice] jk2_init() Found child 1413 in scoreboard slot 6
[Sun Dec 04 17:20:38 2005] [notice] jk2_init() Found child 1448 in scoreboard slot 6
[Sun Dec 04 17:20:38 2005] [notice] jk2_init() Found child 1439 in scoreboard slot 7
[Sun Dec 04 17:20:38 2005] [notice] jk2_init() Found child 1441 in scoreboard slot 9
[Sun Dec 04 17:20:38 2005] [notice] jk2_init() Found child 1450 in scoreboard slot 11
[Sun Dec 04 17:20:39 2005] [notice] jk2_init() Found child 1449 in scoreboard slot 10
[Sun Dec 04 17:20:39 2005] [notice] jk2_init() Found child 1440 in scoreboard slot 8
[Sun Dec 04 17:20:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:20:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:20:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:20:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:20:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:20:46 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 17:20:46 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:20:46 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:20:46 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 17:20:46 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:21:01 2005] [notice] jk2_init() Found child 1452 in scoreboard slot 7
[Sun Dec 04 17:21:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:21:04 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 17:26:04 2005] [notice] jk2_init() Found child 1461 in scoreboard slot 8
[Sun Dec 04 17:26:39 2005] [notice] jk2_init() Found child 1462 in scoreboard slot 6
[Sun Dec 04 17:27:13 2005] [notice] jk2_init() Found child 1466 in scoreboard slot 8
[Sun Dec 04 17:28:00 2005] [notice] jk2_init() Found child 1470 in scoreboard slot 7
[Sun Dec 04 17:28:42 2005] [notice] jk2_init() Found child 1477 in scoreboard slot 6
[Sun Dec 04 17:28:41 2005] [notice] jk2_init() Found child 1476 in scoreboard slot 8
[Sun Dec 04 17:31:00 2005] [notice] jk2_init() Found child 1501 in scoreboard slot 7
[Sun Dec 04 17:31:00 2005] [notice] jk2_init() Found child 1502 in scoreboard slot 6
[Sun Dec 04 17:31:00 2005] [notice] jk2_init() Found child 1498 in scoreboard slot 8
[Sun Dec 04 17:31:00 2005] [notice] jk2_init() Found child 1499 in scoreboard slot 11
[Sun Dec 04 17:31:10 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:31:10 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:31:10 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:31:11 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:31:12 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:31:12 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:31:12 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 17:31:12 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 17:31:43 2005] [notice] jk2_init() Found child 1503 in scoreboard slot 9
[Sun Dec 04 17:31:43 2005] [notice] jk2_init() Found child 1504 in scoreboard slot 8
[Sun Dec 04 17:31:45 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:31:45 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:31:45 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:31:45 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:34:52 2005] [notice] jk2_init() Found child 1507 in scoreboard slot 10
[Sun Dec 04 17:34:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:34:57 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:34:57 2005] [error] [client 61.138.216.82] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 17:36:14 2005] [notice] jk2_init() Found child 1512 in scoreboard slot 7
[Sun Dec 04 17:36:14 2005] [notice] jk2_init() Found child 1513 in scoreboard slot 6
[Sun Dec 04 17:37:08 2005] [notice] jk2_init() Found child 1517 in scoreboard slot 7
[Sun Dec 04 17:37:08 2005] [notice] jk2_init() Found child 1518 in scoreboard slot 6
[Sun Dec 04 17:37:47 2005] [notice] jk2_init() Found child 1520 in scoreboard slot 8
[Sun Dec 04 17:37:47 2005] [notice] jk2_init() Found child 1521 in scoreboard slot 10
[Sun Dec 04 17:39:00 2005] [notice] jk2_init() Found child 1529 in scoreboard slot 9
[Sun Dec 04 17:39:01 2005] [notice] jk2_init() Found child 1530 in scoreboard slot 8
[Sun Dec 04 17:39:00 2005] [notice] jk2_init() Found child 1528 in scoreboard slot 7
[Sun Dec 04 17:39:00 2005] [notice] jk2_init() Found child 1527 in scoreboard slot 6
[Sun Dec 04 17:43:08 2005] [notice] jk2_init() Found child 1565 in scoreboard slot 9
[Sun Dec 04 17:43:08 2005] [error] jk2_init() Can't find child 1566 in scoreboard
[Sun Dec 04 17:43:08 2005] [notice] jk2_init() Found child 1561 in scoreboard slot 6
[Sun Dec 04 17:43:08 2005] [notice] jk2_init() Found child 1563 in scoreboard slot 8
[Sun Dec 04 17:43:08 2005] [notice] jk2_init() Found child 1562 in scoreboard slot 7
[Sun Dec 04 17:43:08 2005] [error] jk2_init() Can't find child 1567 in scoreboard
[Sun Dec 04 17:43:08 2005] [notice] jk2_init() Found child 1568 in scoreboard slot 13
[Sun Dec 04 17:43:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:43:12 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 17:43:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:43:12 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 17:43:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:43:12 2005] [error] mod_jk child init 1 -2
[Sun Dec 04 17:43:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:43:12 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 17:43:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:43:12 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 17:43:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:43:12 2005] [error] mod_jk child init 1 -2
[Sun Dec 04 17:43:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 17:43:12 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 17:53:43 2005] [error] [client 218.39.132.175] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 18:24:22 2005] [error] [client 125.30.38.52] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 19:25:51 2005] [notice] jk2_init() Found child 1763 in scoreboard slot 6
[Sun Dec 04 19:25:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:25:53 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:32:20 2005] [notice] jk2_init() Found child 1786 in scoreboard slot 8
[Sun Dec 04 19:32:20 2005] [notice] jk2_init() Found child 1787 in scoreboard slot 9
[Sun Dec 04 19:32:32 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:32:33 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:32:34 2005] [notice] jk2_init() Found child 1788 in scoreboard slot 6
[Sun Dec 04 19:32:34 2005] [notice] jk2_init() Found child 1790 in scoreboard slot 8
[Sun Dec 04 19:32:34 2005] [notice] jk2_init() Found child 1789 in scoreboard slot 7
[Sun Dec 04 19:32:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:32:34 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:32:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:32:34 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 19:32:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:32:34 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:35:58 2005] [notice] jk2_init() Found child 1797 in scoreboard slot 9
[Sun Dec 04 19:35:58 2005] [notice] jk2_init() Found child 1798 in scoreboard slot 6
[Sun Dec 04 19:35:58 2005] [notice] jk2_init() Found child 1799 in scoreboard slot 7
[Sun Dec 04 19:35:58 2005] [notice] jk2_init() Found child 1800 in scoreboard slot 10
[Sun Dec 04 19:35:58 2005] [notice] jk2_init() Found child 1801 in scoreboard slot 12
[Sun Dec 04 19:36:05 2005] [error] [client 61.37.222.240] Directory index forbidden by rule: /var/www/html/
[Sun Dec 04 19:36:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:36:07 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:36:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:36:07 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:36:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:36:07 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:36:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:36:07 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:36:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:36:07 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:41:20 2005] [notice] jk2_init() Found child 1816 in scoreboard slot 9
[Sun Dec 04 19:41:20 2005] [notice] jk2_init() Found child 1814 in scoreboard slot 7
[Sun Dec 04 19:41:20 2005] [notice] jk2_init() Found child 1813 in scoreboard slot 6
[Sun Dec 04 19:41:20 2005] [notice] jk2_init() Found child 1815 in scoreboard slot 8
[Sun Dec 04 19:41:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:41:21 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:41:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:41:21 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:41:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:41:21 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:41:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:41:21 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:46:04 2005] [notice] jk2_init() Found child 1821 in scoreboard slot 6
[Sun Dec 04 19:46:04 2005] [notice] jk2_init() Found child 1822 in scoreboard slot 7
[Sun Dec 04 19:46:13 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:46:13 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:46:13 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:46:13 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:46:16 2005] [notice] jk2_init() Found child 1823 in scoreboard slot 8
[Sun Dec 04 19:46:19 2005] [notice] jk2_init() Found child 1824 in scoreboard slot 9
[Sun Dec 04 19:46:20 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:46:20 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:46:20 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:46:20 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:50:39 2005] [notice] jk2_init() Found child 1833 in scoreboard slot 7
[Sun Dec 04 19:50:39 2005] [notice] jk2_init() Found child 1832 in scoreboard slot 6
[Sun Dec 04 19:50:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:50:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:50:55 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:50:57 2005] [notice] jk2_init() Found child 1834 in scoreboard slot 8
[Sun Dec 04 19:50:55 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:51:16 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:51:18 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:51:43 2005] [notice] jk2_init() Found child 1835 in scoreboard slot 9
[Sun Dec 04 19:51:52 2005] [notice] jk2_init() Found child 1836 in scoreboard slot 6
[Sun Dec 04 19:51:52 2005] [notice] jk2_init() Found child 1837 in scoreboard slot 7
[Sun Dec 04 19:51:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:51:54 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:51:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:51:54 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:51:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:51:54 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:56:51 2005] [notice] jk2_init() Found child 1851 in scoreboard slot 6
[Sun Dec 04 19:56:51 2005] [notice] jk2_init() Found child 1852 in scoreboard slot 9
[Sun Dec 04 19:56:51 2005] [notice] jk2_init() Found child 1853 in scoreboard slot 7
[Sun Dec 04 19:56:51 2005] [notice] jk2_init() Found child 1850 in scoreboard slot 8
[Sun Dec 04 19:56:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:56:53 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:56:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:56:53 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:56:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:56:53 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 19:56:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 19:56:53 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:01:00 2005] [notice] jk2_init() Found child 1861 in scoreboard slot 8
[Sun Dec 04 20:01:00 2005] [notice] jk2_init() Found child 1862 in scoreboard slot 6
[Sun Dec 04 20:01:30 2005] [notice] jk2_init() Found child 1867 in scoreboard slot 8
[Sun Dec 04 20:01:30 2005] [notice] jk2_init() Found child 1864 in scoreboard slot 7
[Sun Dec 04 20:01:30 2005] [notice] jk2_init() Found child 1868 in scoreboard slot 6
[Sun Dec 04 20:01:30 2005] [notice] jk2_init() Found child 1863 in scoreboard slot 9
[Sun Dec 04 20:01:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:01:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:01:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:01:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:01:37 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:01:37 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:01:37 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:01:37 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 20:05:55 2005] [notice] jk2_init() Found child 1887 in scoreboard slot 8
[Sun Dec 04 20:05:55 2005] [notice] jk2_init() Found child 1885 in scoreboard slot 9
[Sun Dec 04 20:05:55 2005] [notice] jk2_init() Found child 1888 in scoreboard slot 6
[Sun Dec 04 20:05:55 2005] [notice] jk2_init() Found child 1886 in scoreboard slot 7
[Sun Dec 04 20:05:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:05:59 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:05:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:05:59 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:05:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:05:59 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:05:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:05:59 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:11:09 2005] [notice] jk2_init() Found child 1899 in scoreboard slot 7
[Sun Dec 04 20:11:09 2005] [notice] jk2_init() Found child 1900 in scoreboard slot 8
[Sun Dec 04 20:11:09 2005] [notice] jk2_init() Found child 1901 in scoreboard slot 6
[Sun Dec 04 20:11:09 2005] [notice] jk2_init() Found child 1898 in scoreboard slot 9
[Sun Dec 04 20:11:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:11:14 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:11:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:11:14 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:11:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:11:14 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:11:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:11:14 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:16:10 2005] [notice] jk2_init() Found child 1912 in scoreboard slot 9
[Sun Dec 04 20:16:10 2005] [notice] jk2_init() Found child 1915 in scoreboard slot 6
[Sun Dec 04 20:16:10 2005] [notice] jk2_init() Found child 1913 in scoreboard slot 7
[Sun Dec 04 20:16:10 2005] [notice] jk2_init() Found child 1914 in scoreboard slot 8
[Sun Dec 04 20:16:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:16:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:16:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:16:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:16:15 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:16:15 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:16:15 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:16:15 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:20:57 2005] [notice] jk2_init() Found child 1931 in scoreboard slot 7
[Sun Dec 04 20:21:09 2005] [notice] jk2_init() Found child 1932 in scoreboard slot 8
[Sun Dec 04 20:21:08 2005] [notice] jk2_init() Found child 1933 in scoreboard slot 6
[Sun Dec 04 20:21:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:21:31 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:21:31 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:21:37 2005] [notice] jk2_init() Found child 1934 in scoreboard slot 9
[Sun Dec 04 20:21:36 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:22:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:22:12 2005] [notice] jk2_init() Found child 1936 in scoreboard slot 8
[Sun Dec 04 20:22:12 2005] [notice] jk2_init() Found child 1935 in scoreboard slot 7
[Sun Dec 04 20:22:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:22:52 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 20:22:57 2005] [notice] jk2_init() Found child 1937 in scoreboard slot 6
[Sun Dec 04 20:23:12 2005] [notice] jk2_init() Found child 1938 in scoreboard slot 9
[Sun Dec 04 20:24:45 2005] [notice] jk2_init() Found child 1950 in scoreboard slot 9
[Sun Dec 04 20:24:45 2005] [notice] jk2_init() Found child 1951 in scoreboard slot 7
[Sun Dec 04 20:24:45 2005] [notice] jk2_init() Found child 1949 in scoreboard slot 6
[Sun Dec 04 20:24:45 2005] [notice] jk2_init() Found child 1948 in scoreboard slot 8
[Sun Dec 04 20:24:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:24:49 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 20:24:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:24:49 2005] [error] mod_jk child workerEnv in error state 8
[Sun Dec 04 20:24:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:24:49 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:24:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:24:49 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 20:26:10 2005] [notice] jk2_init() Found child 1957 in scoreboard slot 8
[Sun Dec 04 20:26:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:26:54 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:26:58 2005] [notice] jk2_init() Found child 1959 in scoreboard slot 9
[Sun Dec 04 20:26:58 2005] [notice] jk2_init() Found child 1958 in scoreboard slot 6
[Sun Dec 04 20:27:43 2005] [notice] jk2_init() Found child 1961 in scoreboard slot 8
[Sun Dec 04 20:28:00 2005] [notice] jk2_init() Found child 1962 in scoreboard slot 6
[Sun Dec 04 20:28:00 2005] [notice] jk2_init() Found child 1963 in scoreboard slot 9
[Sun Dec 04 20:28:26 2005] [notice] jk2_init() Found child 1964 in scoreboard slot 7
134681 node-246 unix.hw state_change.unavailable 1077804742 1 Component State Change: Component \042SCSI-WWID:01000010:6005-08b4-0001-00c6-0006-3000-003d-0000\042 is in the unavailable state (HWID=1973)
350766 node-109 unix.hw state_change.unavailable 1084680778 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=3180)
344518 node-246 unix.hw state_change.unavailable 1084270955 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=5089)
344448 node-153 unix.hw state_change.unavailable 1084270952 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=4088)
366633 node-200 unix.hw state_change.unavailable 1085100843 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=2538)
366463 node-122 unix.hw state_change.unavailable 1085084674 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=2480)
438190 node-228 unix.hw state_change.unavailable 1097194780 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=3713)
225111 node-10 unix.hw state_change.unavailable 1117296789 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=3891)
360778 node-130 unix.hw state_change.unavailable 1141108031 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=2478)
401569 node-169 unix.hw state_change.unavailable 1142550406 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=2969)
401855 node-187 unix.hw state_change.unavailable 1142553646 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=4159)
460773 node-199 unix.hw state_change.unavailable 1145552100 1 Component State Change: Component \042alt0\042 is in the unavailable state (HWID=2608)
2568643 node-70 action start 1074119817 1 clusterAddMember  (command 1902)
2570772 node-124 action start 1074123150 1 clusterAddMember  (command 1900)
2571927 node-28 action start 1074125371 1 risBoot  (command 1903)
2572286 node-17 action start 1074126278 1 bootGenvmunix  (command 1903)
2575909 node-162 action start 1074178193 1 boot  (command 1911)
2576195 node-181 action start 1074178628 1 boot  (command 1910)
2599298 node-198 action start 1074297419 1 boot  (command 1978)
2600743 node-57 action start 1074298084 1 boot  (command 1967)
2601401 node-184 action start 1074298390 1 wait  (command 1975)
2612635 node-88 action start 1074535847 1 boot  (command 1999)
2608062 node-238 action start 1074461014 1 halt  (command 1982)
2607813 node-243 action start 1074459063 1 boot  (command 1981)
2600616 node-152 action start 1074298056 1 boot  (command 1973)
2601430 node-159 action start 1074298398 1 wait  (command 1973)
3515 node-216 action start 1075629790 1 wait  (command 2057)
41108 node-93 action start 1076538873 1 boot  (command 2152)
39962 node-134 action start 1076538533 1 wait  (command 2154)
38426 node-17 action start 1076537141 1 boot  (command 2141)
33697 node-251 action start 1076435713 1 boot  (command 2110)
46302 node-114 action start 1076546290 1 boot  (command 2160)
76306 node-57 action start 1077204892 1 halt  (command 2221)
75035 node-116 action start 1077172847 1 wait  (command 2217)
75026 node-119 action start 1077172842 1 wait  (command 2217)
66410 node-226 action start 1076874863 1 wait  (command 2201)
64231 node-224 action start 1076871482 1 wait  (command 2199)
61193 node-25 action start 1076865929 1 boot  (command 2183)
59888 node-105 action start 1076865608 1 wait  (command 2189)
106268 node-178 action start 1077739942 1 wait  (command 2302)
95046 node-118 action start 1077647123 1 wait  (command 2270)
93084 node-239 action start 1077645240 1 wait  (command 2257)
93083 node-242 action start 1077645240 1 boot  (command 2257)
137423 node-28 action start 1077809713 1 boot  (command 2316)
138728 node-25 action start 1077810165 1 boot  (command 2316)
139640 node-120 action start 1077810494 1 wait  (command 2322)
143923 node-195 action start 1077816813 1 wait  (command 2354)
146443 node-123 action start 1077817656 1 wait  (command 2347)
157583 node-196 action start 1077873102 1 boot  (command 2368)
160960 node-3 action start 1077875823 1 boot  (command 2392)
162927 node-2 action start 1077877886 1 boot  (command 2406)
163146 node-206 action start 1077878060 1 boot  (command 2417)
164418 node-45 action start 1077881141 1 boot  (command 2423)
165793 node-89 action start 1077881713 1 wait  (command 2424)
177730 node-200 action start 1077894469 1 wait  (command 2449)
185712 node-60 action start 1077902963 1 boot  (command 2464)
186693 node-125 action start 1077903223 1 wait  (command 2468)
222526 node-206 action start 1078490898 1 boot  (command 2520)
217214 node-241 action start 1078468230 1 wait  (command 2517)
217168 node-248 action start 1078468218 1 boot  (command 2517)
217100 node-148 action start 1078468203 1 wait  (command 2511)
217007 node-179 action start 1078468187 1 wait  (command 2513)
214978 node-161 action start 1078467481 1 boot  (command 2514)
201811 node-76 action start 1078162366 1 wait  (command 2484)
264003 node-176 action start 1079249218 1 boot  (command 2625)
256981 node-58 action start 1079070194 1 wait  (command 2607)
256874 node-90 action start 1079070177 1 boot  (command 2616)
256848 node-185 action start 1079070175 1 boot  (command 2619)
256223 node-82 action start 1079070000 1 boot  (command 2616)
256162 node-80 action start 1079069993 1 boot  (command 2616)
254534 node-129 action start 1079069377 1 boot  (command 2602)
247848 node-157 action start 1079049069 1 wait  (command 2582)
245027 node-175 action start 1079048210 1 boot  (command 2584)
236520 node-43 action start 1079015170 1 wait  (command 2549)
235608 node-34 action start 1079014799 1 boot  (command 2550)
235599 node-2 action start 1079014796 1 boot  (command 2548)
233507 node-227 action start 1079012648 1 boot  (command 2538)
274455 node-100 action start 1079616550 1 wait  (command 2683)
276059 node-190 action start 1079617054 1 boot  (command 2686)
276581 node-120 action start 1079617260 1 wait  (command 2682)
294548 node-216 action start 1081392152 1 wait  (command 2763)
301979 node-1 action start 1081998493 1 wait  (command 2800)
300380 node-109 action start 1081880821 1 boot  (command 2790)
307531 node-9 action start 1082038490 1 wait  (command 2817)
307999 node-112 action start 1082038630 1 wait  (command 2823)
309387 node-27 action start 1082039069 1 wait  (command 2817)
326403 node-150 action start 1083202417 1 wait  (command 2900)
324109 node-232 action start 1083200791 1 wait  (command 2885)
338620 node-237 action start 1083304818 1 wait  (command 2909)
362348 node-220 action start 1085071596 1 boot  (command 2960)
360562 node-229 action start 1085071024 1 wait  (command 2963)
374171 node-26 action start 1086022370 1 boot  (command 2999)
373784 node-147 action start 1085979758 1 halt  (command 2992)
402485 node-205 action start 1089897107 1 wait  (command 3087)
402097 node-73 action start 1089897051 1 wait  (command 3079)
401359 node-34 action start 1089896823 1 wait  (command 3078)
429535 node-124 action start 1095347018 1 wait  (command 3141)
426656 node-33 action start 1095346191 1 boot  (command 3138)
437355 node-19 action start 1096995511 1 boot  (command 3169)
437261 node-10 action start 1096995263 1 boot  (command 3169)
443445 node-80 action start 1098389763 1 boot  (command 3194)
19244 node-194 action start 1100785255 1 boot  (command 3320)
19258 node-167 action start 1100785255 1 boot  (command 3318)
20797 node-188 action start 1100785975 1 boot  (command 3317)
21457 node-216 action start 1100786301 1 wait  (command 3319)
32761 node-17 action start 1102475613 1 wait  (command 3334)
32609 node-10 action start 1102475187 1 boot  (command 3334)
70287 node-70 action start 1108647096 1 wait  (command 3412)
70708 node-115 action start 1108647304 1 boot  (command 3413)
112147 node-82 action start 1111072078 1 boot  (command 3473)
136495 node-150 action start 1111865182 1 wait  (command 3523)
136459 node-208 action start 1111865169 1 wait  (command 3525)
134514 node-254 action start 1111862672 1 wait  (command 3519)
134299 node-251 action start 1111862280 1 boot  (command 3519)
128650 node-231 action start 1111855559 1 boot  (command 3512)
163965 node-195 action start 1114095465 1 boot  (command 3579)
164990 node-114 action start 1114095917 1 boot  (command 3572)
165357 node-246 action start 1114095982 1 boot  (command 3580)
208690 node-65 action start 1116601343 1 wait  (command 3612)
208761 node-147 action start 1116601359 1 boot  (command 3615)
210411 node-40 action start 1116601982 1 wait  (command 3609)
211063 node-18 action start 1116604606 1 boot  (command 3624)
212813 node-239 action start 1116611765 1 wait  (command 3649)
213303 node-83 action start 1116611976 1 boot  (command 3639)
252407 node-46 action start 1118929077 1 wait  (command 3690)
252602 node-145 action start 1118929116 1 boot  (command 3696)
252613 node-174 action start 1118929118 1 wait  (command 3698)
276418 node-84 action start 1119845504 1 wait  (command 3709)
55715 node-89 action start 1126813838 1 wait  (command 3837)
55940 node-255 action start 1126814027 1 wait  (command 3847)
173915 node-59 action start 1131220838 1 boot  (command 3951)
172312 node-27 action start 1131216880 1 boot  (command 3928)
170334 node-161 action start 1131215738 1 boot  (command 3939)
191604 node-13 action start 1131239917 1 boot  (command 3964)
193032 node-46 action start 1131241563 1 wait  (command 3973)
194265 node-186 action start 1131242308 1 wait  (command 3981)
194301 node-185 action start 1131242355 1 wait  (command 3981)
220288 node-194 action start 1132159257 1 boot  (command 4024)
288620 node-150 action start 1134671414 1 wait  (command 4061)
329026 node-111 action start 1140100216 1 wait  (command 4110)
328665 node-234 action start 1140100074 1 boot  (command 4118)
328498 node-218 action start 1140099980 1 wait  (command 4116)
328095 node-194 action start 1140099718 1 wait  (command 4117)
328040 node-67 action start 1140099563 1 boot  (command 4109)
374387 node-116 action start 1141676760 1 wait  (command 4145)
397088 node-148 action start 1142527921 1 boot  (command 4176)
397742 node-56 action start 1142528184 1 boot  (command 4170)
397899 node-150 action start 1142528212 1 wait  (command 4176)
398541 node-154 action start 1142528540 1 wait  (command 4176)
401417 node-129 action start 1142537875 1 bootGenvmunix  (command 4185)
460486 node-181 action start 1145552039 1 boot  (command 4223)
460596 node-143 action start 1145552056 1 wait  (command 4221)
461572 node-116 action start 1145552379 1 wait  (command 4219)
2566692 1897 boot_cmd success 1073991950 1 Command has completed successfully
2614626 2001 boot_cmd success 1074752045 1 Command has completed successfully
178176 2428 boot_cmd success 1077894737 1 Command has completed successfully
247935 2568 boot_cmd success 1079049124 1 Command has completed successfully
274749 2685 boot_cmd success 1079616724 1 Command has completed successfully
301526 2797 boot_cmd success 1081993794 1 Command has completed successfully
427258 3142 boot_cmd success 1095346421 1 Command has completed successfully
91679 3440 boot_cmd success 1109728975 1 Command has completed successfully
130127 3501 boot_cmd success 1111856688 1 Command has completed successfully
240717 3664 boot_cmd success 1118282468 1 Command has completed successfully
2599204 1971 boot_cmd new 1074297378 1 Targeting domains:node-D3 and nodes:node-[104-127] child of command 1963
2599217 1973 boot_cmd new 1074297389 1 Targeting domains:node-D4 and nodes:node-[136-159] child of command 1964
70088 2206 shutdown_cmd new 1077059790 1 Targeting domains:node-D7 and nodes:node-[224\ 228\ 232]
162883 2414 boot_cmd new 1077877853 1 Targeting domains:node-D4 and nodes:node-[128-135] child of command 2401
210240 2490 boot_cmd new 1078453922 1 Targeting domains:node-D0 and nodes:node-[8-31] child of command 2489
451683 3223 boot_cmd new 1098825780 1 Targeting domains:node-D0 and nodes:node-0
38390 3344 boot_cmd new 1104070308 1 Targeting domains:node-D[0\ 5] and nodes:node-[27\ 191]
128520 3491 boot_cmd new 1111855423 1 Targeting domains:node-D2 and nodes:node-[64-95] child of command 3488
276415 3709 boot_cmd new 1119845316 1 Targeting domains:node-D2 and nodes:node-84
51338 node-3 node psu 1106496000 1 psu failure\ ambient=28
191898 node-238 node psu 1131240275 1 psu failure\ ambient=28
236618 node-104 node psu 1132434391 1 psu failure\ ambient=28
341834 node-118 node psu 1140312091 1 psu failure\ ambient=28
347972 node-118 node psu 1140430530 1 psu failure\ ambient=31
147394 Interconnect-0N00 switch_module temphigh 1129812510 1 Temperature (41C) exceeds warning threshold
147494 Interconnect-0N00 switch_module temphigh 1129813980 1 Temperature (41C) exceeds warning threshold
2594656 node-D0 clusterfilesystem clusterfilesystem.no_server 1074279038 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage234
2609055 node-D7 clusterfilesystem clusterfilesystem.no_server 1074464654 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1412
2608601 node-D7 clusterfilesystem clusterfilesystem.no_server 1074462833 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1024
2608475 node-D7 clusterfilesystem clusterfilesystem.no_server 1074462062 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1534
2596321 node-D7 clusterfilesystem clusterfilesystem.no_server 1074289164 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1231
32255 node-D7 clusterfilesystem clusterfilesystem.no_server 1076398960 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage934
58466 node-D0 clusterfilesystem clusterfilesystem.no_server 1076864677 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage131
57981 node-D7 clusterfilesystem clusterfilesystem.no_server 1076864401 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1144
57709 node-D0 clusterfilesystem clusterfilesystem.no_server 1076864348 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage444
171644 node-D0 clusterfilesystem clusterfilesystem.no_server 1077889836 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage141
172740 node-D7 clusterfilesystem clusterfilesystem.no_server 1077891659 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1622
172788 node-D7 clusterfilesystem clusterfilesystem.no_server 1077891661 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1612
183109 node-D7 clusterfilesystem clusterfilesystem.no_server 1077901705 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1622
213053 node-D7 clusterfilesystem clusterfilesystem.no_server 1078465745 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1542
213050 node-D7 clusterfilesystem clusterfilesystem.no_server 1078465745 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1644
229271 node-D0 clusterfilesystem clusterfilesystem.no_server 1078963582 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage242
311726 node-D7 clusterfilesystem clusterfilesystem.no_server 1082041350 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1031
400170 node-D7 clusterfilesystem clusterfilesystem.no_server 1089893907 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1331
399288 node-D0 clusterfilesystem clusterfilesystem.no_server 1089893729 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage742
399278 node-D0 clusterfilesystem clusterfilesystem.no_server 1089893727 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage221
417925 node-D0 clusterfilesystem clusterfilesystem.no_server 1093627911 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage622
420807 node-D0 clusterfilesystem clusterfilesystem.no_server 1094568654 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage112
442349 node-D7 clusterfilesystem clusterfilesystem.no_server 1098365526 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1432
51652 node-D0 clusterfilesystem clusterfilesystem.no_server 1106588485 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage142
68757 node-D0 clusterfilesystem clusterfilesystem.no_server 1108645899 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage742
69243 node-D7 clusterfilesystem clusterfilesystem.no_server 1108646021 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1231
110887 node-D7 clusterfilesystem clusterfilesystem.no_server 1111067897 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage1311
256889 node-D0 clusterfilesystem clusterfilesystem.no_server 1118940596 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage411
220014 node-D0 clusterfilesystem clusterfilesystem.no_server 1132158358 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage132
286137 node-D0 clusterfilesystem clusterfilesystem.no_server 1134669644 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage832
286151 node-D0 clusterfilesystem clusterfilesystem.no_server 1134669644 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage531
327002 node-D0 clusterfilesystem clusterfilesystem.no_server 1140098601 1 ClusterFileSystem: There is no server for ServerFileSystem domain storage321
365140 node-69 unix.hw net.niff.down 1085075228 1 NIFF: node node-69 detected a failed network connection on network 5.5.224.0 via interface alt0
401608 node-162 unix.hw net.niff.down 1142550442 1 NIFF: node node-162 detected a failed network connection on network 5.5.224.0 via interface alt0
180537 node-D0 clusterfilesystem fdmn.panic 1131228351 1 ServerFileSystem: An ServerFileSystem domain panic has occurred on storage442
268543 Interconnect-0N00 switch_module fan 1119372267 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
268265 Interconnect-0N00 switch_module fan 1119290224 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
292663 Interconnect-0N00 switch_module fan 1121581239 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
291810 Interconnect-0N00 switch_module fan 1121525534 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
289739 Interconnect-0N00 switch_module fan 1121303273 1 Fan speeds ( 3552 3534 3375 4245 3515 3479 )
327293 Interconnect-0N00 switch_module fan 1123408044 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
26132 Interconnect-0N00 switch_module fan 1124831967 1 Fan speeds ( 3552 3534 3375 11637 3515 3479 )
37743 Interconnect-0N00 switch_module fan 1125827345 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
37524 Interconnect-0N00 switch_module fan 1125819002 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
36180 Interconnect-0N00 switch_module fan 1125718237 1 Fan speeds ( 3552 3534 3375 4245 3515 3479 )
35581 Interconnect-0N00 switch_module fan 1125706262 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
35374 Interconnect-0N00 switch_module fan 1125677285 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
34697 Interconnect-0N00 switch_module fan 1125668027 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
34204 Interconnect-0N00 switch_module fan 1125664672 1 Fan speeds ( 3552 3534 3375 4470 3515 3479 )
34116 Interconnect-0N00 switch_module fan 1125660960 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
33940 Interconnect-0N00 switch_module fan 1125651957 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
33385 Interconnect-0N00 switch_module fan 1125557358 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
33086 Interconnect-0N00 switch_module fan 1125545019 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
45567 Interconnect-0N00 switch_module fan 1126010536 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
45526 Interconnect-0N00 switch_module fan 1126010284 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
45496 Interconnect-0N00 switch_module fan 1126001523 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
45232 Interconnect-0N00 switch_module fan 1125974682 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
44885 Interconnect-0N00 switch_module fan 1125968635 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
44792 Interconnect-0N00 switch_module fan 1125967289 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
44437 Interconnect-0N00 switch_module fan 1125958618 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
44077 Interconnect-0N00 switch_module fan 1125942184 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
43807 Interconnect-0N00 switch_module fan 1125941020 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
43763 Interconnect-0N00 switch_module fan 1125940778 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
43379 Interconnect-0N00 switch_module fan 1125938946 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
43219 Interconnect-0N00 switch_module fan 1125938127 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
42827 Interconnect-0N00 switch_module fan 1125936247 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
42774 Interconnect-0N00 switch_module fan 1125935993 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
42301 Interconnect-0N00 switch_module fan 1125933864 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
42057 Interconnect-0N00 switch_module fan 1125932744 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
41384 Interconnect-0N00 switch_module fan 1125929387 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
41142 Interconnect-0N00 switch_module fan 1125922192 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
41066 Interconnect-0N00 switch_module fan 1125920701 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
40628 Interconnect-0N00 switch_module fan 1125915215 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
40627 Interconnect-0N00 switch_module fan 1125915213 1 Fan speeds ( 3552 3534 3375 4166 3515 3479 )
40365 Interconnect-0N00 switch_module fan 1125913604 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
40096 Interconnect-0N00 switch_module fan 1125911516 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
39347 Interconnect-0N00 switch_module fan 1125873598 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
104906 Interconnect-0N00 switch_module fan 1128271361 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
104883 Interconnect-0N00 switch_module fan 1128271267 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
121157 Interconnect-0N00 switch_module fan 1129314010 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
120901 Interconnect-0N00 switch_module fan 1129307838 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
120373 Interconnect-0N00 switch_module fan 1129295995 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
120348 Interconnect-0N00 switch_module fan 1129295109 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
119419 Interconnect-0N00 switch_module fan 1129159678 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
119213 Interconnect-0N00 switch_module fan 1129157128 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
119202 Interconnect-0N00 switch_module fan 1129156854 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
119142 Interconnect-0N00 switch_module fan 1129155779 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
119082 Interconnect-0N00 switch_module fan 1129155045 1 Fan speeds ( 3552 3534 3375 4530 3515 3479 )
118806 Interconnect-0N00 switch_module fan 1129150679 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
117431 Interconnect-0N00 switch_module fan 1128907389 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
117146 Interconnect-0N00 switch_module fan 1128901378 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
116927 Interconnect-0N00 switch_module fan 1128899272 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
116705 Interconnect-0N00 switch_module fan 1128888604 1 Fan speeds ( 3552 3534 3375 4192 3515 3479 )
116664 Interconnect-0N00 switch_module fan 1128887986 1 Fan speeds ( 3552 3534 3375 **** 3515 3479 )
236285 Interconnect-0N00 switch_module fan 1132421073 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
243687 Interconnect-0N00 switch_module fan 1132558950 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
260011 Interconnect-0N00 switch_module fan 1133183830 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
259847 Interconnect-0N00 switch_module fan 1133182195 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
259763 Interconnect-0N00 switch_module fan 1133181426 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
259536 Interconnect-0N00 switch_module fan 1133178458 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
259457 Interconnect-0N00 switch_module fan 1133177775 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
259180 Interconnect-0N00 switch_module fan 1133174442 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
259133 Interconnect-0N00 switch_module fan 1133174185 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
259101 Interconnect-0N00 switch_module fan 1133174025 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
258955 Interconnect-0N00 switch_module fan 1133173381 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
258413 Interconnect-0N00 switch_module fan 1133169111 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
258185 Interconnect-0N00 switch_module fan 1133168036 1 Fan speeds ( 3552 3534 3391 4299 3515 3479 )
257965 Interconnect-0N00 switch_module fan 1133166706 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
257953 Interconnect-0N00 switch_module fan 1133166616 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
257691 Interconnect-0N00 switch_module fan 1133165477 1 Fan speeds ( 3552 3534 3391 **** 3515 3479 )
349086 Interconnect-0N00 switch_module fan 1140616724 1 Fan speeds ( 3552 3552 3391 **** 3515 3497 )
349016 Interconnect-0N00 switch_module fan 1140616395 1 Fan speeds ( 3552 3552 3391 **** 3515 3497 )
380336 Interconnect-0N00 switch_module fan 1141884709 1 Fan speeds ( 3534 3534 3375 **** 3497 3479 )
379923 Interconnect-0N00 switch_module fan 1141832424 1 Fan speeds ( 3534 3534 3375 **** 3497 3479 )
379647 Interconnect-0N00 switch_module fan 1141828356 1 Fan speeds ( 3534 3534 3375 4272 3497 3479 )
379641 Interconnect-0N00 switch_module fan 1141828195 1 Fan speeds ( 3534 3534 3375 4470 3497 3479 )
445104 Interconnect-0N00 switch_module fan 1144366153 1 Fan speeds ( 3534 3534 3375 **** 3497 3479 )
442494 Interconnect-0N00 switch_module fan 1143993115 1 Fan speeds ( 3534 3534 3375 **** 3497 3479 )
442338 Interconnect-0N00 switch_module fan 1143991258 1 Fan speeds ( 3534 3534 3375 **** 3497 3479 )
442189 Interconnect-0N00 switch_module fan 1143988225 1 Fan speeds ( 3534 3534 3375 **** 3497 3479 )
442187 Interconnect-0N00 switch_module fan 1143988205 1 Fan speeds ( 3534 3534 3375 **** 3497 3479 )
441407 Interconnect-0N00 switch_module fan 1143980889 1 Fan speeds ( 3534 3534 3375 4115 3497 3479 )
441258 Interconnect-0N00 switch_module fan 1143979891 1 Fan speeds ( 3534 3534 3375 **** 3497 3479 )
441254 Interconnect-0N00 switch_module fan 1143979871 1 Fan speeds ( 3534 3534 3375 **** 3497 3479 )
441208 Interconnect-0N00 switch_module fan 1143979502 1 Fan speeds ( 3534 3534 3375 **** 3497 3479 )
130987 3504 boot_cmd abort 1111858191 1 Command has been aborted
27385 Interconnect-0N02 switch_module control 1101771720 1 power/control problem
30700 Interconnect-1T00 switch_module bcast-error 1076189965 1 Link error
115576 Interconnect-1T00 switch_module bcast-error 1077793578 1 Link error
115153 Interconnect-0T00 switch_module bcast-error 1077757190 1 Link error
259323 Interconnect-1T00 switch_module bcast-error 1079076297 1 Link error
285855 Interconnect-1T00 switch_module bcast-error 1080298218 1 Link in reset
285823 Interconnect-0T00 switch_module bcast-error 1080292236 1 Link error
289491 Interconnect-0T00 switch_module bcast-error 1080825593 1 Link error
288385 Interconnect-0T00 switch_module bcast-error 1080668603 1 Link error
295965 Interconnect-1T00 switch_module bcast-error 1081534663 1 Link error
293098 Interconnect-1T00 switch_module bcast-error 1081296321 1 Link error
303852 Interconnect-1T00 switch_module bcast-error 1082033409 1 Link in reset
323647 Interconnect-1T00 switch_module bcast-error 1083198788 1 Link ok
26281 Interconnect-0T00 switch_module bcast-error 1124878893 1 Link error on broadcast tree Interconnect-0T00:00:2:1
407973 Interconnect-1T00 switch_module bcast-error 1142703109 1 Link error on broadcast tree Interconnect-1T00:00:3:6
408176 Interconnect-1T00 switch_module bcast-error 1142743438 1 Link error on broadcast tree Interconnect-1T00:00:2:0
408221 Interconnect-1T00 switch_module bcast-error 1142752287 1 Link error on broadcast tree Interconnect-1T00:00:2:0
191911 node-D4 clusterfilesystem fdmn.full 1077949575 1 ServerFileSystem: ServerFileSystem domain root14_local is full
280513 node-D0 clusterfilesystem fdmn.full 1079715693 1 ServerFileSystem: ServerFileSystem domain root10_local is full
284487 node-D3 clusterfilesystem fdmn.full 1080155118 1 ServerFileSystem: ServerFileSystem domain root22_local is full
289398 node-D5 clusterfilesystem fdmn.full 1080811198 1 ServerFileSystem: ServerFileSystem domain root13_local is full
383570 node-D7 clusterfilesystem fdmn.full 1086596948 1 ServerFileSystem: ServerFileSystem domain root26_tmp is full
389984 node-D3 clusterfilesystem fdmn.full 1087531181 1 ServerFileSystem: ServerFileSystem domain root7_local is full
178949 node-D2 clusterfilesystem fdmn.full 1114672353 1 ServerFileSystem: ServerFileSystem domain root1_local is full
178450 node-D4 clusterfilesystem fdmn.full 1114553373 1 ServerFileSystem: ServerFileSystem domain root22_local is full
177934 node-D3 clusterfilesystem fdmn.full 1114546822 1 ServerFileSystem: ServerFileSystem domain root3_local is full
218553 node-D4 clusterfilesystem fdmn.full 1117133933 1 ServerFileSystem: ServerFileSystem domain root25_local is full
218536 node-D1 clusterfilesystem fdmn.full 1117131308 1 ServerFileSystem: ServerFileSystem domain root26_local is full
218505 node-D3 clusterfilesystem fdmn.full 1117130918 1 ServerFileSystem: ServerFileSystem domain root22_local is full
2558879 node-D3 clusterfilesystem clusterfilesystem.not_served 1073126274 1 ClusterFileSystem: ServerFileSystem domain cluster_root_backup is no longer served by node node-96
2567825 node-D5 clusterfilesystem clusterfilesystem.not_served 1074100162 1 ClusterFileSystem: ServerFileSystem domain root_domain is no longer served by node node-160
2568742 node-D4 clusterfilesystem clusterfilesystem.not_served 1074120046 1 ClusterFileSystem: ServerFileSystem domain root3_domain is no longer served by node node-128
2569870 node-D7 clusterfilesystem clusterfilesystem.not_served 1074121737 1 ClusterFileSystem: ServerFileSystem domain root9_domain is no longer served by node node-224
2570007 node-D7 clusterfilesystem clusterfilesystem.not_served 1074121911 1 ClusterFileSystem: ServerFileSystem domain root10_domain is no longer served by node node-224
2571489 node-D3 clusterfilesystem clusterfilesystem.not_served 1074124407 1 ClusterFileSystem: ServerFileSystem domain root28_domain is no longer served by node node-96
2572034 node-D0 clusterfilesystem clusterfilesystem.not_served 1074125608 1 ClusterFileSystem: ServerFileSystem domain root30_domain is no longer served by node node-0
2572991 node-D1 clusterfilesystem clusterfilesystem.not_served 1074129956 1 ClusterFileSystem: ServerFileSystem domain root26_domain is no longer served by node node-57
2589848 node-D0 clusterfilesystem clusterfilesystem.not_served 1074257445 1 ClusterFileSystem: ServerFileSystem domain root29_local is no longer served by node node-28
2589995 node-D0 clusterfilesystem clusterfilesystem.not_served 1074257629 1 ClusterFileSystem: ServerFileSystem domain root12_domain is no longer served by node node-11
37555 node-D6 clusterfilesystem clusterfilesystem.not_served 1076533718 1 ClusterFileSystem: ServerFileSystem domain root5_domain is no longer served by node node-196
36350 node-D5 clusterfilesystem clusterfilesystem.not_served 1076533356 1 ClusterFileSystem: ServerFileSystem domain root28_domain is no longer served by node node-187
69304 node-D7 clusterfilesystem clusterfilesystem.not_served 1076959081 1 ClusterFileSystem: ServerFileSystem domain root5_tmp is no longer served by node node-228
156291 node-D7 clusterfilesystem clusterfilesystem.not_served 1077864767 1 ClusterFileSystem: ServerFileSystem domain storage1422 is no longer served by node node-245
156587 node-D0 clusterfilesystem clusterfilesystem.not_served 1077865593 1 ClusterFileSystem: ServerFileSystem domain storage343 is no longer served by node node-11
159914 node-D3 clusterfilesystem clusterfilesystem.not_served 1077875435 1 ClusterFileSystem: ServerFileSystem domain root19_domain is no longer served by node node-114
160172 node-D2 clusterfilesystem clusterfilesystem.not_served 1077875490 1 ClusterFileSystem: ServerFileSystem domain root14_local is no longer served by node node-77
160413 node-D1 clusterfilesystem clusterfilesystem.not_served 1077875542 1 ClusterFileSystem: ServerFileSystem domain root6_domain is no longer served by node node-37
209092 node-D0 clusterfilesystem clusterfilesystem.not_served 1078438019 1 ClusterFileSystem: ServerFileSystem domain storage534 is no longer served by node node-18
253842 node-D0 clusterfilesystem clusterfilesystem.not_served 1079069186 1 ClusterFileSystem: ServerFileSystem domain root12_tmp is no longer served by node node-11
253687 node-D5 clusterfilesystem clusterfilesystem.not_served 1079069131 1 ClusterFileSystem: ServerFileSystem domain root23_local is no longer served by node node-182
254062 node-D1 clusterfilesystem clusterfilesystem.not_served 1079069211 1 ClusterFileSystem: ServerFileSystem domain root14_tmp is no longer served by node node-45
252335 node-D0 clusterfilesystem clusterfilesystem.not_served 1079068559 1 ClusterFileSystem: ServerFileSystem domain storage211 is no longer served by node node-4
305331 node-D5 clusterfilesystem clusterfilesystem.not_served 1082036035 1 ClusterFileSystem: ServerFileSystem domain root8_domain is no longer served by node node-167
304677 node-D6 clusterfilesystem clusterfilesystem.not_served 1082035793 1 ClusterFileSystem: ServerFileSystem domain root25_local is no longer served by node node-216
182781 node-D5 clusterfilesystem clusterfilesystem.not_served 1114854715 1 ClusterFileSystem: ServerFileSystem domain cluster_usr_backup is no longer served by node node-160
190287 node-D4 clusterfilesystem clusterfilesystem.not_served 1115830055 1 ClusterFileSystem: ServerFileSystem domain root_domain is no longer served by node node-128
203954 node-D7 clusterfilesystem clusterfilesystem.not_served 1116530480 1 ClusterFileSystem: ServerFileSystem domain root24_local is no longer served by node node-247
203956 node-D7 clusterfilesystem clusterfilesystem.not_served 1116530480 1 ClusterFileSystem: ServerFileSystem domain root18_tmp is no longer served by node node-241
204008 node-D7 clusterfilesystem clusterfilesystem.not_served 1116530487 1 ClusterFileSystem: ServerFileSystem domain root17_domain is no longer served by node node-240
218198 node-D0 clusterfilesystem clusterfilesystem.not_served 1117056658 1 ClusterFileSystem: ServerFileSystem domain sc_cluster_backup is no longer served by node node-0
280288 node-D6 clusterfilesystem clusterfilesystem.not_served 1120297769 1 ClusterFileSystem: ServerFileSystem domain cluster_usr_backup is no longer served by node node-192
19318 node-D7 clusterfilesystem clusterfilesystem.not_served 1124532600 1 ClusterFileSystem: ServerFileSystem domain cluster_var_backup is no longer served by node node-224
72165 node-D3 clusterfilesystem clusterfilesystem.not_served 1127045031 1 ClusterFileSystem: ServerFileSystem domain root1_tmp_backup is no longer served by node node-96
197958 node-D3 clusterfilesystem clusterfilesystem.not_served 1131282056 1 ClusterFileSystem: ServerFileSystem domain root1_domain_backup is no longer served by node node-96
226890 node-D6 clusterfilesystem clusterfilesystem.not_served 1132171167 1 ClusterFileSystem: ServerFileSystem domain root28_domain is no longer served by node node-219
2286759 full partition status 1061219795 -1 running
2274013 full partition status 1060340885 -1 running
2556866 full partition status 1072881064 1 running
2279427 full partition status 1060773635 -1 running
2279283 full partition status 1060744865 -1 running
2273811 full partition status 1060324505 -1 running
2272849 full partition status 1060239078 -1 running
2561542 full partition status 1073427660 1 blocked
2561161 full partition status 1073359530 1 blocked
2313012 full partition status 1061552195 -1 running
2277349 full partition status 1060507389 -1 running
2567354 node-147 node status 1074098611 1 not responding
2567461 node-C0 domain status 1074098612 1 not responding
2567534 node-97 node status 1074098640 1 configured out
2579921 node-140 node status 1074216940 1 running
2579951 node-170 node status 1074216940 1 running
2580020 node-239 node status 1074216940 1 running
2580712 node-100 node status 1074217199 1 not responding
2581412 node-211 node status 1074217360 1 not responding
2590658 node-123 node status 1074262967 1 running
2595860 node-46 node status 1074288846 1 not responding
2596116 node-185 node status 1074289057 1 not responding
2596202 node-216 node status 1074289110 1 configured out
2596232 node-239 node status 1074289147 1 not responding
2287804 full partition status 1061267556 -1 running
2271403 full partition status 1060163570 -1 running
2608517 full partition status 1074462770 1 running
2590751 node-216 node status 1074262967 1 running
2595682 node-12 node status 1074288817 1 not responding
2596033 node-131 node status 1074288991 1 not responding
2596168 node-211 node status 1074289105 1 not responding
2599698 node-D5 domain status 1074297720 1 inconsistent nodesets node-160 0x0001edfe <ok> node-161 0x1001fdfe <ok> node-162 0x1001fdfe <ok> node-163 0x1001fdfe <ok>
2601823 node-49 node status 1074298784 1 running
2601830 node-56 node status 1074298784 1 running
2601896 node-122 node status 1074298784 0 running
2601953 node-144 node status 1074299123 1 running
2602004 node-195 node status 1074299123 1 running
41982 node-202 node status 1076539387 1 running
41798 node-17 node status 1076539387 1 running
35712 node-71 node status 1076533037 1 configured out
44677 node-112 node status 1076542145 1 running
45685 node-109 node status 1076544000 1 active
47692 node-161 node status 1076615678 1 configured out
47913 node-163 node status 1076619203 1 running
47952 node-202 node status 1076619203 1 running
54740 node-238 node status 1076837827 1 configured out
80830 node-65 node status 1077321390 1 configured out
77067 node-35 node status 1077215250 1 configured out
76769 node-61 node status 1077207138 1 running
76110 node-37 node status 1077204510 1 configured out
57489 node-27 node status 1076864329 1 not responding
114954 node-206 node status 1077744705 1 running
114862 node-114 node status 1077744705 1 running
100995 node-22 node status 1077723931 1 configured out
100947 node-87 node status 1077723931 1 not responding
100048 node-215 node status 1077649605 1 running
90661 node-61 node status 1077630630 1 configured out
135341 node-225 node status 1077805320 1 not responding
160937 node-D1 domain status 1077875820 1 inconsistent nodesets node-32 0x00000002 <ok> node-34 0x000001fe <ok> node-33 0x000001fe <ok> node-35 0x000001fe <ok>
172472 node-100 node status 1077891510 1 configured out
172572 node-168 node status 1077891560 1 not responding
178780 node-147 node status 1077897241 0 running
182635 node-44 node status 1077901440 1 configured out
182754 node-109 node status 1077901531 1 configured out
182817 node-143 node status 1077901590 1 configured out
182920 node-218 node status 1077901649 1 not responding
182943 node-212 node status 1077901650 0 configured out
222217 node-204 node status 1078490191 1 configured out
178781 node-148 node status 1077897241 1 running
219135 node-99 node status 1078470966 1 running
219117 node-81 node status 1078470966 1 running
212764 node-120 node status 1078465560 1 configured out
212647 node-60 node status 1078465470 1 configured out
212610 node-49 node status 1078465457 1 not responding
212601 node-40 node status 1078465457 1 not responding
205653 node-175 node status 1078406503 1 configured out
200920 node-165 node status 1078122733 1 running
199728 node-186 node status 1078118432 1 configured out
199720 node-178 node status 1078118432 1 configured out
264741 node-209 node status 1079261513 0 running
264527 node-159 node status 1079259783 1 not responding
264395 node-196 node status 1079255686 1 active
264038 full partition status 1079249252 1 running
263641 full partition status 1079248586 1 running
263520 node-170 node status 1079248515 1 configured out
263068 node-173 node status 1079245564 1 running
262672 node-36 node status 1079243476 1 configured out
262650 node-139 node status 1079243475 1 running
262432 full partition status 1079241153 1 running
261973 full partition status 1079220450 1 blocked
261817 node-89 node status 1079218683 1 not responding
261699 node-230 node status 1079218682 1 not responding
261582 node-161 node status 1079216417 1 running
261427 node-233 node status 1079216415 1 not responding
261004 node-25 node status 1079190995 1 not responding
260472 node-13 node status 1079143598 1 not responding
260233 node-166 node status 1079127020 1 running
260225 node-254 node status 1079127019 1 running
260158 node-155 node status 1079127017 1 running
260078 node-11 node status 1079127017 1 not responding
259819 node-212 node status 1079111625 1 running
259458 node-30 node status 1079081093 1 running
252790 node-229 node status 1079068924 1 configured out
252605 node-44 node status 1079068924 1 configured out
250426 node-250 node status 1079054309 1 running
250337 node-161 node status 1079054308 1 running
244332 node-34 node status 1079047060 1 not responding
239558 node-164 node status 1079019273 1 running
239458 node-64 node status 1079019273 1 running
235493 node-D6 domain status 1079014470 1 not responding
234468 node-203 node status 1079013901 1 configured out
234328 node-61 node status 1079013840 1 configured out
234321 node-54 node status 1079013840 1 configured out
232876 node-144 node status 1079011232 1 configured out
229430 node-6 node status 1078970342 1 configured out
272707 node-169 node status 1079615369 1 not responding
272591 node-153 node status 1079615344 0 not responding
272170 node-62 node status 1079615293 1 not responding
277642 node-165 node status 1079618688 1 running
277686 node-209 node status 1079618688 1 running
273131 node-253 node status 1079615432 0 configured out
273112 node-234 node status 1079615432 0 configured out
272780 node-174 node status 1079615371 0 configured out
272777 node-171 node status 1079615371 0 configured out
284353 node-23 node status 1080145602 1 running
284195 full partition status 1080133799 1 running
273098 node-217 node status 1079615432 0 configured out
277540 node-119 node status 1079618410 0 running
272731 node-185 node status 1079615370 0 not responding
272589 node-151 node status 1079615344 0 not responding
296297 node-225 node status 1081571034 1 not responding
277540 node-119 node status 1079618410 0 running
277545 node-124 node status 1079618410 0 running
272769 node-163 node status 1079615371 1 configured out
296384 node-77 node status 1081571037 1 not responding
300968 full partition status 1081932726 1 running
300890 node-220 node status 1081931335 1 not responding
310009 node-13 node status 1082040658 1 running
315748 node-19 node status 1082665230 1 configured out
329774 node-235 node status 1083207608 0 running
329596 node-57 node status 1083207607 1 running
323276 node-189 node status 1083197700 1 not responding
323246 node-170 node status 1083197700 1 not responding
321846 node-91 node status 1083195752 1 configured out
321826 node-65 node status 1083195751 1 configured out
316706 node-90 node status 1082894753 1 running
340207 node-245 node status 1083317700 0 running
329646 node-107 node status 1083207607 0 running
339296 node-125 node status 1083307405 0 configured out
340120 node-158 node status 1083317700 0 running
340209 node-247 node status 1083317700 0 running
344357 node-88 node status 1084269621 0 configured out
344301 node-52 node status 1084269345 0 configured out
343802 node-52 node status 1084264230 1 configured out
366141 node-95 node status 1085078720 1 running
366113 node-67 node status 1085078720 1 running
371329 full partition status 1085797473 1 running
370250 full partition status 1085689890 1 blocked
369970 full partition status 1085682780 1 blocked
369548 node-112 node status 1085659157 1 not responding
369300 node-79 node status 1085569402 1 running
369186 node-168 node status 1085568282 0 running
368987 node-95 node status 1085567825 1 configured out
379999 node-231 node status 1086028827 0 running
379995 node-227 node status 1086028827 0 running
379993 node-225 node status 1086028827 0 running
379872 node-103 node status 1086028827 0 running
379859 node-90 node status 1086028826 1 running
372036 node-219 node status 1085921340 1 configured out
371847 node-49 node status 1085921312 1 configured out
371602 node--tc6 tserver status 1085921002 1 not-responding
379912 node-144 node status 1086028827 0 running
386689 full partition status 1087478923 1 running
386783 node-224 node status 1087499940 1 active
392804 node-41 node status 1088023736 0 running
392696 node-126 node status 1088023734 0 running
392653 node-51 node status 1088023733 1 running
392736 node-198 node status 1088023734 0 running
392729 node-159 node status 1088023734 0 running
392693 node-123 node status 1088023734 0 running
399657 node-155 node status 1089893790 1 not responding
399519 node-85 node status 1089893761 1 configured out
399366 node-39 node status 1089893731 1 configured out
399176 node-54 node status 1089893717 1 not responding
398501 node-235 node status 1089811825 1 running
410509 node-221 node status 1091255504 1 running
409269 node-195 node status 1091248620 1 configured out
371878 node-64 node status 1085921340 -1 configured out
413605 node-154 node status 1092321126 1 running
413350 node-41 node status 1092320602 1 running
416005 full partition status 1092949734 1 running
368988 node-96 node status 1085567825 -1 configured out
418211 node-D0 domain status 1093628640 1 inconsistent nodesets node-1 0x000001fc <ok> node-2 0x000001fc <ok> node-7 0x000001fc <ok> node-4 0x000001fc <ok> node-6 0x000001fc <ok> node-5 0x000001fc <ok> node-3 0x000001fc <ok> node-14 0x0003fdfc <ok> node-13 0x0003fdfc <ok> node-12 0x0003fdfc <ok> node-10 0x0003fdfc <ok> node-16 0x0003fdfc <ok> node-15 0x0003fdfc <ok>
413243 node-191 node status 1092320499 0 configured out
417113 full partition status 1093473150 1 blocked
413275 node-222 node status 1092320499 0 configured out
421363 full partition status 1094571984 1 running
421349 node-18 node status 1094571950 1 running
432939 node-254 node status 1095349520 0 running
413246 node-193 node status 1092320499 1 configured out
425784 node-186 node status 1095342830 1 not responding
425734 node-133 node status 1095342780 1 configured out
425522 node-37 node status 1095342574 1 not responding
425174 node-109 node status 1095341304 1 running
425071 node-174 node status 1095341277 1 active
424807 node-53 node status 1095339932 1 not responding
424635 node-211 node status 1095338588 1 configured out
424634 node-210 node status 1095338588 1 configured out
432838 node-153 node status 1095349520 0 running
432843 node-158 node status 1095349520 1 running
371910 node-96 node status 1085921340 -1 configured out
442459 node-42 node status 1098366960 1 not responding
448690 node-48 node status 1098394861 1 running
448947 node-66 node status 1098395762 1 configured out
449708 full partition status 1098490050 1 blocked
453178 full partition status 1098987334 1 running
451647 node-9 node status 1098825692 1 configured out
450090 node-14 node status 1098716306 1 not responding
360067 node-96 node status 1085056650 -1 configured out
369020 node-160 node status 1085567825 -1 configured out
382264 node-0 node status 1086300660 -1 configured out
457562 node-238 node status 1100413674 1 configured out
448744 node-102 node status 1098394861 0 running
18624 node-205 node status 1100784210 1 configured out
18387 node-184 node status 1100784181 1 configured out
24011 node-77 node status 1100788549 1 running
24024 node-90 node status 1100788549 1 running
24139 node-205 node status 1100788549 0 running
30351 full partition status 1102218393 1 running
24181 node-247 node status 1100788549 1 running
27536 node-0 node status 1101792840 1 configured out
33966 node-16 node status 1102693232 1 configured out
32980 node-26 node status 1102476182 1 running
41359 full partition status 1104950640 1 blocked
53241 node-175 node status 1107152281 1 configured out
56392 node-224 node status 1107904598 1 running
73494 node-29 node status 1108652943 1 running
73690 node-165 node status 1108653007 1 running
77384 node-233 node status 1108744158 1 running
84322 full partition status 1109007900 1 blocked
91139 node-172 node status 1109725656 1 configured out
97235 full partition status 1110138244 1 running
110520 node-73 node status 1111066002 1 not responding
117663 node-151 node status 1111074926 0 running
117632 node-120 node status 1111074926 0 running
117588 node-76 node status 1111074926 1 running
117564 node-52 node status 1111074926 1 running
111216 node-192 node status 1111071420 1 configured out
110959 node-252 node status 1111068030 1 configured out
110554 node-174 node status 1111067067 1 not responding
138134 node-73 node status 1111869296 1 running
117751 node-239 node status 1111074926 1 running
117724 node-212 node status 1111074926 1 running
126627 node-160 node status 1111828200 1 not responding
141504 node-104 node status 1112420952 1 running
140900 node-239 node status 1112389915 0 running
140782 node-220 node status 1112389862 1 not responding
140676 full partition status 1112346856 1 starting
139541 node-231 node status 1112055605 1 configured out
139404 node-242 node status 1112054374 1 configured out
139086 node-228 node status 1112051942 1 configured out
154699 node-86 node status 1113371286 1 configured out
169348 node-35 node status 1114098974 1 running
176994 node-218 node status 1114435800 1 configured out
202074 node-210 node status 1116448620 1 not responding
202181 node-34 node status 1116448621 1 not responding
202342 node-165 node status 1116448650 1 configured out
216915 node-113 node status 1116619994 1 running
216956 node-154 node status 1116619994 1 running
251283 node-200 node status 1118926110 1 configured out
251334 node-254 node status 1118926152 1 not responding
277298 full partition status 1120076193 1 running
251065 node-92 node status 1118925810 0 configured out
284996 node-191 node status 1120682568 1 running
284693 node-184 node status 1120680817 1 configured out
284668 full partition status 1120680810 1 blocked
846 node-95 node status 1123709833 1 running
923 node-172 node status 1123709833 1 running
1004 node-253 node status 1123709833 1 running
14730 node-172 node status 1124378018 1 running
26628 full partition status 1124999460 1 blocked
46680 node-126 node status 1126168979 1 running
46615 node-181 node status 1126168951 1 running
56225 node-74 node status 1126814969 1 running
56504 node-55 node status 1126815181 1 configured out
58843 full partition status 1126818053 1 closing
56256 node-105 node status 1126814969 0 running
56261 node-110 node status 1126814969 0 running
56327 node-175 node status 1126814970 0 running
89931 full partition status 1128014704 1 running
98836 node-83 node status 1128237754 1 running
56298 node-146 node status 1126814970 0 running
56335 node-183 node status 1126814970 0 running
56329 node-177 node status 1126814970 0 running
56404 node-254 node status 1126814970 0 running
116947 node-202 node status 1128899526 1 running
56349 node-198 node status 1126814970 1 running
46114 node-182 node status 1126165404 -1 configured out
169688 node-44 node status 1131210631 1 not responding
169664 node-40 node status 1131210631 1 not responding
168084 full partition status 1131032850 1 blocked
197110 node-253 node status 1131247704 1 running
205176 node-15 node status 1131532770 1 not responding
205095 full partition status 1131530296 1 running
204732 node-44 node status 1131527920 1 active
203294 node-215 node status 1131298338 1 not responding
219581 node-76 node status 1132157850 1 configured out
220122 node-D3 domain status 1132158690 1 not responding
260401 full partition status 1133290901 1 running
46105 full partition status 1126164412 -1 running
274783 full partition status 1133702160 1 blocked
285000 node-11 node status 1134656644 1 running
286085 node-4 node status 1134669635 1 not responding
286957 node-162 node status 1134670027 1 not responding
290042 node-183 node status 1134672259 1 running
289977 node-127 node status 1134672205 0 running
309872 full partition status 1135802130 1 blocked
323320 node-255 node status 1139735822 1 not responding
327050 node-7 node status 1140098610 1 configured out
366404 node-237 node status 1141512720 1 configured out
378400 node-232 node status 1141679919 0 running
378260 node-92 node status 1141679919 1 running
378195 node-27 node status 1141679919 1 running
372004 node-90 node status 1141671840 0 configured out
371954 node-40 node status 1141671840 1 configured out
371941 node-27 node status 1141671840 1 configured out
401242 node-145 node status 1142533309 0 running
401248 node-151 node status 1142533309 0 running
401202 node-104 node status 1142533309 0 running
415345 node-D0 domain status 1142942280 1 inconsistent nodesets node-31 0x1fffffffe <ok> node-0 0xfffffffe <ok> node-1 0xfffffffe <ok> node-2 0xfffffffe <ok> node-30 0xfffffffe <ok>
414885 node-225 node status 1142872170 1 configured out
401232 node-135 node status 1142533309 1 running
454307 full partition status 1145174910 1 blocked
451133 node-130 node status 1144858896 1 running
459286 node-208 node status 1145542009 1 not responding
459133 node-108 node status 1145541960 1 configured out
463655 node-135 node status 1145554526 0 running
463680 node-160 node status 1145554526 0 running
463769 node-249 node status 1145554526 0 running
463684 node-164 node status 1145554526 0 running
2559971 gige7 gige temperature 1073151998 1 normal
2552510 gige7 gige temperature 1072553474 1 normal
2556843 gige5 gige temperature 1072878243 1 warning
2555058 gige7 gige temperature 1072734978 1 normal
2554674 gige7 gige temperature 1072671675 1 normal
2554666 gige5 gige temperature 1072670036 1 warning
2552992 node-212 node temperature 1072633140 1 ambient=30
2563073 gige7 gige temperature 1073610711 1 normal
2562762 node-232 node temperature 1073579610 1 ambient=30
2562656 gige7 gige temperature 1073561505 1 normal
2561340 gige7 gige temperature 1073394404 1 normal
2561239 gige4 gige temperature 1073375350 1 critical
2566893 gige7 gige temperature 1074020818 1 normal
2565183 gige7 gige temperature 1073933218 1 normal
2585341 gige7 gige temperature 1074234092 1 critical
2615431 gige7 gige temperature 1074848953 1 normal
2614737 node-166 node temperature 1074776943 1 ambient=31
2614640 gige7 gige temperature 1074755051 1 normal
2613815 gige5 gige temperature 1074625028 1 normal
2594414 node-239 node temperature 1074274436 1 ambient=30
2594419 node-235 node temperature 1074274436 1 ambient=29
19801 gige7 gige temperature 1076047551 1 normal
40754 node-172 node temperature 1076538724 1 ambient=26
35006 node-238 node temperature 1076506869 1 ambient=30
34905 gige7 gige temperature 1076493252 1 warning
31923 gige4 gige temperature 1076386562 1 critical
31128 gige7 gige temperature 1076272557 1 normal
46581 node-117 node temperature 1076546891 1 ambient=32
48065 node-41 node temperature 1076619404 1 ambient=29
48312 gige7 gige temperature 1076650230 1 normal
80683 node-198 node temperature 1077307801 1 ambient=33
80531 node-174 node temperature 1077300930 1 ambient=28
77270 node-166 node temperature 1077215565 1 ambient=31
76723 node-55 node temperature 1077205904 1 ambient=33
74418 gige7 gige temperature 1077159058 1 normal
69858 gige7 gige temperature 1077042957 1 warning
69672 gige7 gige temperature 1077015055 1 warning
69378 gige7 gige temperature 1076961352 1 warning
61851 node-60 node temperature 1076866153 1 ambient=31
86744 gige7 gige temperature 1077371471 1 warning
87515 gige7 gige temperature 1077440772 1 normal
94825 node-45 node temperature 1077647048 1 ambient=30
90542 node-157 node temperature 1077630300 1 ambient=30
88946 gige7 gige temperature 1077576377 1 normal
88671 node-148 node temperature 1077556261 1 ambient=31
87831 gige2 gige temperature 1077480553 1 warning
87693 gige4 gige temperature 1077462433 1 warning
198942 gige5 gige temperature 1078030594 1 normal
225213 gige7 gige temperature 1078547960 1 warning
225104 gige7 gige temperature 1078536860 1 normal
224786 node-158 node temperature 1078514733 1 ambient=31
224581 gige7 gige temperature 1078508057 1 warning
217618 node-20 node temperature 1078468448 1 ambient=26
217370 node-54 node temperature 1078468284 1 ambient=31
206594 gige7 gige temperature 1078407820 1 normal
204767 gige7 gige temperature 1078361621 1 warning
203383 gige6 gige temperature 1078197816 1 normal
201365 node-74 node temperature 1078160650 1 ambient=29
199616 gige5 gige temperature 1078110095 1 warning
199198 gige7 gige temperature 1078057114 1 warning
261378 gige7 gige temperature 1079210977 1 warning
239708 node-14 node temperature 1079019604 1 ambient=27
234084 node-240 node temperature 1079013507 1 ambient=30
229917 node-20 node temperature 1078979550 1 ambient=26
227358 gige7 gige temperature 1078816469 1 warning
226272 gige7 gige temperature 1078672165 1 warning
285818 gige7 gige temperature 1080291435 1 normal
284101 gige7 gige temperature 1080110219 1 warning
283501 gige6 gige temperature 1080058332 1 normal
289816 gige7 gige temperature 1080845907 1 normal
287985 gige7 gige temperature 1080628083 1 normal
287951 gige7 gige temperature 1080622682 1 normal
287022 gige7 gige temperature 1080493066 1 warning
295921 gige7 gige temperature 1081524579 1 warning
295528 gige7 gige temperature 1081463375 1 normal
301157 gige7 gige temperature 1081959922 1 warning
300050 gige7 gige temperature 1081830909 1 normal
298512 gige7 gige temperature 1081791006 1 normal
297326 gige7 gige temperature 1081684787 1 normal
309472 node-30 node temperature 1082039149 1 ambient=34
314590 gige7 gige temperature 1082202403 1 warning
316541 gige7 gige temperature 1082824335 1 warning
316533 gige7 gige temperature 1082819535 1 normal
316105 gige7 gige temperature 1082742429 1 warning
315684 gige7 gige temperature 1082629326 1 normal
330049 gige7 gige temperature 1083231744 1 warning
329889 node-217 node temperature 1083208215 1 ambient=31
316991 gige7 gige temperature 1083025647 1 normal
340621 gige7 gige temperature 1083424951 1 warning
342541 gige7 gige temperature 1083973367 1 warning
342400 gige7 gige temperature 1083930762 1 warning
342193 node-244 node temperature 1083879601 1 ambient=32
341967 gige7 gige temperature 1083844658 1 normal
341924 node-237 node temperature 1083831510 1 ambient=33
341629 gige7 gige temperature 1083744155 1 warning
341358 gige7 gige temperature 1083688955 1 warning
341168 gige7 gige temperature 1083639753 1 warning
340799 gige7 gige temperature 1083522751 1 warning
350670 node-240 node temperature 1084653390 1 ambient=35
350386 node-234 node temperature 1084631343 1 ambient=31
350295 node-239 node temperature 1084629363 1 ambient=31
350242 node-108 node temperature 1084626810 1 ambient=31
350134 node-109 node temperature 1084621410 1 ambient=30
349094 node-111 node temperature 1084529940 1 ambient=31
348979 node-112 node temperature 1084518061 1 ambient=29
348683 gige7 gige temperature 1084476183 1 warning
346095 gige7 gige temperature 1084427882 1 normal
346052 node-137 node temperature 1084418581 1 ambient=26
346034 gige7 gige temperature 1084413482 1 normal
345974 gige6 gige temperature 1084380782 1 warning
345944 gige6 gige temperature 1084370883 1 normal
345634 node-237 node temperature 1084314360 1 ambient=33
367104 gige7 gige temperature 1085206086 1 normal
371489 node-237 node temperature 1085883660 1 ambient=31
371437 gige7 gige temperature 1085855902 1 warning
369384 gige4 gige temperature 1085598845 1 critical
383080 node-239 node temperature 1086491940 1 ambient=34
382894 node-238 node temperature 1086438690 1 ambient=35
381787 node-111 node temperature 1086206761 1 ambient=30
381569 gige4 gige temperature 1086197563 1 warning
381210 node-166 node temperature 1086194970 1 ambient=35
381139 node-16 node temperature 1086194820 1 ambient=33
380887 node-247 node temperature 1086178590 1 ambient=33
380233 gige7 gige temperature 1086096701 1 normal
376418 node-47 node temperature 1086025702 1 ambient=30
376386 node-105 node temperature 1086025689 1 ambient=28
375624 node-194 node temperature 1086025505 1 ambient=27
374004 gige4 gige temperature 1085997759 1 warning
372299 gige6 gige temperature 1085939607 1 warning
385237 gige3 gige temperature 1086927360 1 normal
385089 gige3 gige temperature 1086882060 1 normal
384693 gige7 gige temperature 1086805621 1 warning
384641 node-225 node temperature 1086794700 1 ambient=32
384130 node-239 node temperature 1086699270 1 ambient=31
383593 node-238 node temperature 1086602640 1 ambient=35
383318 node-108 node temperature 1086543570 1 ambient=32
386596 node-20 node temperature 1087478914 1 ambient=28
386775 gige7 gige temperature 1087499233 1 warning
387324 node-225 node temperature 1087501911 1 ambient=30
391092 gige7 gige temperature 1087643842 1 normal
391300 gige6 gige temperature 1087728445 1 warning
393242 gige7 gige temperature 1088265456 1 normal
392859 gige7 gige temperature 1088038353 1 normal
392847 gige7 gige temperature 1088034153 1 normal
391811 gige7 gige temperature 1087867648 1 normal
391646 node-234 node temperature 1087853310 1 ambient=31
395148 gige7 gige temperature 1088937476 1 normal
394089 node-188 node temperature 1088608620 1 ambient=31
393440 gige7 gige temperature 1088371359 1 warning
396477 gige7 gige temperature 1089388091 1 normal
395945 gige5 gige temperature 1089210393 1 warning
395216 gige5 gige temperature 1088965285 1 warning
395178 gige6 gige temperature 1088948576 1 warning
407464 gige7 gige temperature 1090176813 1 warning
409366 node-212 node temperature 1091251446 1 ambient=29
409190 gige7 gige temperature 1091231048 1 warning
409063 gige6 gige temperature 1091154558 1 normal
408905 gige7 gige temperature 1091074742 1 warning
408669 gige6 gige temperature 1090924449 1 warning
411736 gige7 gige temperature 1091851474 1 normal
410909 gige7 gige temperature 1091423054 1 normal
415132 gige7 gige temperature 1092507298 1 warning
416347 gige6 gige temperature 1093125020 1 normal
416032 gige7 gige temperature 1092958814 1 warning
415725 gige7 gige temperature 1092870612 1 normal
415585 gige7 gige temperature 1092794408 1 normal
415440 gige7 gige temperature 1092718806 1 normal
415407 gige7 gige temperature 1092699905 1 warning
415297 gige7 gige temperature 1092632401 1 warning
415274 gige7 gige temperature 1092610501 1 normal
417446 gige7 gige temperature 1093560329 1 normal
417398 gige7 gige temperature 1093553730 1 warning
417284 gige7 gige temperature 1093529428 1 warning
419465 gige7 gige temperature 1093902333 1 warning
419403 gige7 gige temperature 1093856733 1 warning
422863 node-11 node temperature 1094755801 1 ambient=29
421827 gige7 gige temperature 1094640341 1 normal
420523 gige7 gige temperature 1094479239 1 normal
431406 node-108 node temperature 1095349170 1 ambient=30
429727 node-153 node temperature 1095347103 1 ambient=30
425381 node-154 node temperature 1095341774 1 ambient=33
433158 node-132 node temperature 1095352261 1 ambient=32
433405 gige7 gige temperature 1095412800 1 normal
433626 node-167 node temperature 1095486301 1 ambient=27
433841 gige7 gige temperature 1095539107 1 warning
435317 gige7 gige temperature 1096178141 1 normal
435259 gige7 gige temperature 1096137336 1 normal
435206 node-41 node temperature 1096130881 1 ambient=32
434839 gige5 gige temperature 1095964227 1 normal
434251 gige7 gige temperature 1095742219 1 warning
434198 node-93 node temperature 1095712050 1 ambient=33
434077 gige7 gige temperature 1095656714 1 warning
434057 gige7 gige temperature 1095644114 1 normal
436643 gige6 gige temperature 1096581363 1 normal
436603 gige7 gige temperature 1096548961 1 normal
436574 gige7 gige temperature 1096527660 1 warning
436113 node-22 node temperature 1096388010 1 ambient=29
440293 gige7 gige temperature 1098009695 1 warning
439731 gige7 gige temperature 1097861497 1 warning
439698 gige4 gige temperature 1097832089 1 normal
439070 gige7 gige temperature 1097642792 1 warning
453662 node-127 node temperature 1099084141 1 ambient=32
453378 node-111 node temperature 1099068271 1 ambient=29
453317 gige7 gige temperature 1099041896 1 normal
453176 node-53 node temperature 1098987302 1 ambient=36
452826 node-206 node temperature 1098903930 1 ambient=29
452434 node-203 node temperature 1098866010 1 ambient=30
451528 gige7 gige temperature 1098821990 1 warning
451472 node-246 node temperature 1098818850 1 ambient=29
456184 gige7 gige temperature 1099776905 1 normal
455679 node-122 node temperature 1099504413 1 ambient=32
455240 node-10 node temperature 1099408230 1 ambient=30
455088 node-111 node temperature 1099356810 1 ambient=32
455065 node-113 node temperature 1099355101 1 ambient=30
455027 node-93 node temperature 1099354051 1 ambient=33
455001 node-107 node temperature 1099351950 1 ambient=31
454698 gige5 gige temperature 1099335170 1 normal
454690 node-153 node temperature 1099334221 1 ambient=33
457137 gige7 gige temperature 1100184616 1 normal
456744 node-133 node temperature 1100077083 1 ambient=33
24290 node-213 node temperature 1100796480 1 ambient=31
24877 node-255 node temperature 1100880930 1 ambient=33
25358 gige4 gige temperature 1100910194 1 critical
25493 gige6 gige temperature 1100956034 1 normal
26895 node-36 node temperature 1101549755 1 ambient=27
26727 gige7 gige temperature 1101454038 1 normal
26462 node-255 node temperature 1101295260 1 ambient=33
26313 gige3 gige temperature 1101216972 1 normal
26135 node-43 node temperature 1101154260 1 ambient=33
28128 gige7 gige temperature 1102067235 1 normal
27901 gige7 gige temperature 1101934034 1 warning
27802 gige7 gige temperature 1101859333 1 normal
27654 gige6 gige temperature 1101803531 1 warning
27294 gige5 gige temperature 1101737723 1 normal
34281 node-115 node temperature 1102771261 1 ambient=29
33709 node-255 node temperature 1102628970 1 ambient=33
33505 node-255 node temperature 1102575540 1 ambient=36
33189 node-244 node temperature 1102493340 1 ambient=30
33160 gige7 gige temperature 1102486347 1 normal
33137 gige6 gige temperature 1102480648 1 warning
31458 gige7 gige temperature 1102388245 1 warning
31380 node-230 node temperature 1102354830 1 ambient=28
31352 node-235 node temperature 1102353540 1 ambient=26
30505 gige3 gige temperature 1102334781 1 warning
30504 gige5 gige temperature 1102331115 1 normal
35981 node-10 node temperature 1103220120 1 ambient=29
36262 gige4 gige temperature 1103307514 1 normal
37165 node-86 node temperature 1103662741 1 ambient=25
37077 node-35 node temperature 1103654431 1 ambient=29
36752 gige6 gige temperature 1103575367 1 warning
36566 gige6 gige temperature 1103486264 1 warning
37260 gige3 gige temperature 1103681506 1 warning
37291 gige7 gige temperature 1103691161 1 warning
37307 gige3 gige temperature 1103694707 1 warning
37322 gige4 gige temperature 1103700221 1 normal
37628 gige4 gige temperature 1103811222 1 warning
37715 node-187 node temperature 1103815680 1 ambient=34
37818 node-163 node temperature 1103824141 1 ambient=28
37868 node-197 node temperature 1103826540 1 ambient=27
38135 gige6 gige temperature 1103925772 1 normal
39618 gige3 gige temperature 1104640924 1 warning
39589 gige6 gige temperature 1104627481 1 normal
39407 gige7 gige temperature 1104551270 1 warning
39391 gige7 gige temperature 1104545871 1 warning
39355 gige3 gige temperature 1104532022 1 normal
39302 gige6 gige temperature 1104505981 1 warning
39219 gige6 gige temperature 1104471779 1 normal
39036 gige7 gige temperature 1104388669 1 warning
38659 gige6 gige temperature 1104182576 1 warning
38486 gige6 gige temperature 1104108775 1 normal
41132 gige7 gige temperature 1104916076 1 warning
41131 gige7 gige temperature 1104915777 1 normal
40791 node-71 node temperature 1104864150 1 ambient=27
40790 gige7 gige temperature 1104863876 1 normal
40785 node-208 node temperature 1104861840 1 ambient=33
40769 node-39 node temperature 1104858241 1 ambient=27
40766 gige7 gige temperature 1104857875 1 warning
40705 node-169 node temperature 1104854462 1 ambient=30
40636 node-80 node temperature 1104852811 1 ambient=33
40559 node-174 node temperature 1104852241 1 ambient=27
40385 gige3 gige temperature 1104800827 1 normal
39995 node-15 node temperature 1104762876 1 ambient=27
39888 gige7 gige temperature 1104751375 1 normal
42364 gige7 gige temperature 1105072680 1 normal
42511 gige7 gige temperature 1105117680 1 warning
43144 gige3 gige temperature 1105251135 1 warning
45189 gige3 gige temperature 1105867047 1 normal
45076 gige4 gige temperature 1105824253 1 normal
45043 gige6 gige temperature 1105811893 1 normal
44937 gige7 gige temperature 1105776193 1 warning
44619 gige3 gige temperature 1105687639 1 warning
44539 gige6 gige temperature 1105655293 1 warning
44491 gige7 gige temperature 1105642092 1 warning
44457 gige3 gige temperature 1105639339 1 normal
44219 gige7 gige temperature 1105558990 1 warning
44095 gige7 gige temperature 1105522389 1 normal
44021 gige6 gige temperature 1105496889 1 warning
49162 node-8 node temperature 1106245773 1 ambient=27
50839 gige6 gige temperature 1106288603 1 normal
50947 gige6 gige temperature 1106322803 1 normal
50987 gige7 gige temperature 1106341403 1 warning
52888 gige6 gige temperature 1107017268 1 normal
52599 gige7 gige temperature 1106920116 1 normal
52488 gige7 gige temperature 1106877516 1 warning
52025 gige7 gige temperature 1106737711 1 warning
51902 gige7 gige temperature 1106700511 1 warning
51800 gige7 gige temperature 1106651611 1 warning
51735 gige7 gige temperature 1106624009 1 normal
51616 gige3 gige temperature 1106579555 1 normal
55567 gige7 gige temperature 1107627478 1 normal
55519 gige6 gige temperature 1107611276 1 normal
53863 node-239 node temperature 1107191073 1 ambient=26
53651 gige6 gige temperature 1107180771 1 normal
57305 gige7 gige temperature 1108233484 1 normal
57066 gige3 gige temperature 1108137117 1 normal
56448 gige7 gige temperature 1107916085 1 normal
83640 gige6 gige temperature 1108849993 1 normal
90019 gige6 gige temperature 1109484216 1 warning
89824 gige4 gige temperature 1109420069 1 critical
85087 gige7 gige temperature 1109195000 1 warning
84830 gige5 gige temperature 1109137876 1 warning
84299 gige4 gige temperature 1109002453 1 warning
84013 gige4 gige temperature 1108943353 1 warning
96925 node-238 node temperature 1110105780 1 ambient=29
96865 gige7 gige temperature 1110104935 1 warning
96465 gige3 gige temperature 1110065877 1 normal
92111 node-241 node temperature 1109806620 1 ambient=28
92085 gige2 gige temperature 1109800311 1 normal
92043 gige3 gige temperature 1109791669 1 normal
91595 gige5 gige temperature 1109727101 1 normal
90705 gige3 gige temperature 1109653662 1 warning
90242 gige7 gige temperature 1109546914 1 normal
90216 node-241 node temperature 1109539320 1 ambient=28
102726 gige4 gige temperature 1110708631 1 warning
98883 gige3 gige temperature 1110506587 1 normal
98436 gige2 gige temperature 1110397337 1 warning
98251 gige5 gige temperature 1110349933 1 normal
97834 gige2 gige temperature 1110237730 1 normal
97800 node-236 node temperature 1110230580 1 ambient=26
97470 gige3 gige temperature 1110205381 1 normal
97286 gige1 gige temperature 1110144671 1 normal
124326 gige5 gige temperature 1111320610 1 warning
124216 gige1 gige temperature 1111286054 1 warning
124170 gige1 gige temperature 1111273753 1 warning
124124 gige3 gige temperature 1111258570 1 warning
118270 gige2 gige temperature 1111182908 1 normal
118228 gige2 gige temperature 1111173008 1 warning
111952 node-97 node temperature 1111072025 1 ambient=35
126122 node-232 node temperature 1111798051 1 ambient=33
126093 node-135 node temperature 1111797870 1 ambient=32
126054 node-141 node temperature 1111797510 1 ambient=31
126049 node-242 node temperature 1111797480 1 ambient=31
125920 gige7 gige temperature 1111792040 1 critical
125729 gige5 gige temperature 1111693818 1 warning
125532 gige5 gige temperature 1111613118 1 warning
124948 gige2 gige temperature 1111488013 1 normal
124947 gige2 gige temperature 1111487713 1 warning
124701 gige1 gige temperature 1111438759 1 warning
124625 gige5 gige temperature 1111413911 1 warning
141025 node-113 node temperature 1112395021 1 ambient=27
140533 gige5 gige temperature 1112280080 1 normal
139872 node-93 node temperature 1112115481 1 ambient=33
138597 gige6 gige temperature 1111943595 1 normal
138555 gige6 gige temperature 1111928595 1 normal
147904 gige6 gige temperature 1112888964 1 normal
147779 gige7 gige temperature 1112816360 1 warning
147517 node-70 node temperature 1112734890 1 ambient=32
147327 gige5 gige temperature 1112704897 1 warning
147280 node-238 node temperature 1112688811 1 ambient=30
147279 gige3 gige temperature 1112688738 1 normal
146876 gige5 gige temperature 1112544988 1 warning
159193 gige6 gige temperature 1113645796 1 warning
155760 gige5 gige temperature 1113610268 1 warning
155652 gige7 gige temperature 1113559395 1 normal
155643 gige7 gige temperature 1113554594 1 critical
155613 gige4 gige temperature 1113543556 1 warning
155522 gige5 gige temperature 1113509172 1 warning
155332 gige5 gige temperature 1113456065 1 warning
155294 gige7 gige temperature 1113442992 1 critical
155289 node-139 node temperature 1113441604 1 ambient=34
154200 gige2 gige temperature 1113298318 1 warning
153760 node-236 node temperature 1113250590 1 ambient=27
153488 gige6 gige temperature 1113144874 1 normal
153474 gige4 gige temperature 1113137369 1 warning
166381 node-215 node temperature 1114096282 1 ambient=23
169704 gige4 gige temperature 1114120157 1 normal
176320 gige7 gige temperature 1114274309 1 warning
183966 gige6 gige temperature 1114918724 1 normal
179416 node-244 node temperature 1114763670 1 ambient=29
179410 gige4 gige temperature 1114762773 1 normal
178946 node-114 node temperature 1114669020 1 ambient=28
178846 node-241 node temperature 1114642410 1 ambient=26
177386 gige6 gige temperature 1114545212 1 normal
177118 node-243 node temperature 1114465230 1 ambient=28
189804 node-240 node temperature 1115511240 1 ambient=25
185349 node-86 node temperature 1115269410 1 ambient=24
185020 gige4 gige temperature 1115247878 1 warning
190782 node-255 node temperature 1116036905 1 ambient=34
190669 node-166 node temperature 1116014551 1 ambient=31
190500 gige7 gige temperature 1115931540 1 critical
190419 gige5 gige temperature 1115894507 1 normal
205135 node-67 node temperature 1116531298 1 ambient=28
207216 node-80 node temperature 1116538204 1 ambient=30
207240 node-189 node temperature 1116538205 1 ambient=33
207254 node-54 node temperature 1116538205 1 ambient=31
207333 node-198 node temperature 1116539762 1 ambient=31
207349 node-55 node temperature 1116539787 1 ambient=33
209931 node-176 node temperature 1116601733 1 ambient=32
210653 node-223 node temperature 1116602075 1 ambient=27
213496 node-75 node temperature 1116612015 1 ambient=29
217083 gige7 gige temperature 1116640501 1 warning
217146 gige7 gige temperature 1116685205 1 critical
218807 gige6 gige temperature 1117170619 1 normal
218381 gige7 gige temperature 1117120516 1 warning
218172 gige7 gige temperature 1117044015 1 normal
217921 gige7 gige temperature 1116963312 1 warning
217384 gige6 gige temperature 1116789307 1 normal
217344 gige6 gige temperature 1116771007 1 normal
231230 gige4 gige temperature 1117581672 1 normal
230781 gige7 gige temperature 1117520719 1 normal
230496 gige6 gige temperature 1117489521 1 warning
241069 gige4 gige temperature 1118461882 1 normal
241019 gige6 gige temperature 1118436040 1 normal
240873 gige7 gige temperature 1118360437 1 normal
240782 gige6 gige temperature 1118295037 1 normal
240702 gige6 gige temperature 1118280037 1 warning
240641 gige7 gige temperature 1118270735 1 normal
240511 gige6 gige temperature 1118186737 1 normal
240399 gige7 gige temperature 1118112635 1 normal
257180 gige6 gige temperature 1119070315 1 warning
262646 gige4 gige temperature 1119093476 1 normal
272910 gige3 gige temperature 1119744967 1 warning
272853 gige7 gige temperature 1119710825 1 critical
269486 gige7 gige temperature 1119658625 1 normal
269014 gige7 gige temperature 1119541023 1 warning
281216 gige3 gige temperature 1120354574 1 warning
281126 gige3 gige temperature 1120304472 1 warning
277688 gige7 gige temperature 1120225336 1 critical
277575 gige7 gige temperature 1120164132 1 warning
277041 gige3 gige temperature 1119985870 1 normal
277038 gige4 gige temperature 1119984490 1 warning
276977 node-212 node temperature 1119972360 1 ambient=25
276947 gige6 gige temperature 1119966728 1 warning
276921 gige6 gige temperature 1119953528 1 warning
276353 gige3 gige temperature 1119836470 1 warning
285669 node-120 node temperature 1120882920 1 ambient=28
285349 node-95 node temperature 1120807232 1 ambient=33
285294 node-95 node temperature 1120784281 1 ambient=33
285232 node-90 node temperature 1120765771 1 ambient=36
284710 node-161 node temperature 1120680847 1 ambient=36
284566 node-116 node temperature 1120659780 1 ambient=28
284160 gige7 gige temperature 1120556837 1 warning
290254 gige7 gige temperature 1121400147 1 critical
290139 node-159 node temperature 1121375430 1 ambient=35
289970 gige6 gige temperature 1121318857 1 normal
289193 gige3 gige temperature 1121126193 1 warning
289181 node-218 node temperature 1121124240 1 ambient=29
288975 gige6 gige temperature 1121048549 1 warning
288882 gige6 gige temperature 1121010147 1 warning
307487 gige4 gige temperature 1122195175 1 warning
318114 gige4 gige temperature 1122807481 1 warning
317892 gige4 gige temperature 1122755580 1 normal
314014 node-228 node temperature 1122565110 1 ambient=25
313879 gige6 gige temperature 1122529625 1 normal
313753 gige7 gige temperature 1122502623 1 normal
313234 node-133 node temperature 1122335162 1 ambient=34
313169 node-176 node temperature 1122334380 1 ambient=35
312853 node-12 node temperature 1122319890 1 ambient=36
312821 node-23 node temperature 1122317820 1 ambient=35
312505 gige7 gige temperature 1122289618 1 warning
323620 gige3 gige temperature 1123303567 1 warning
323469 gige7 gige temperature 1123268232 1 warning
322167 gige7 gige temperature 1123098731 1 warning
322105 gige4 gige temperature 1123054982 1 normal
322072 gige6 gige temperature 1123032131 1 warning
321663 node-109 node temperature 1122928071 1 ambient=27
321485 gige4 gige temperature 1122862981 1 warning
41 node-23 node temperature 1123705518 1 ambient=29
98 node-206 node temperature 1123705544 1 ambient=26
160 node-222 node temperature 1123706820 1 ambient=26
557 node-158 node temperature 1123708507 1 ambient=30
1091 gige3 gige temperature 1123712436 1 warning
1574 gige6 gige temperature 1123832800 1 normal
1674 gige7 gige temperature 1123867002 1 warning
1755 gige7 gige temperature 1123896401 1 normal
15077 gige7 gige temperature 1124455610 1 normal
15135 gige4 gige temperature 1124477566 1 warning
26607 gige7 gige temperature 1124991417 1 warning
26553 gige7 gige temperature 1124967716 1 warning
26385 gige3 gige temperature 1124924165 1 warning
26334 gige3 gige temperature 1124895065 1 normal
26225 gige6 gige temperature 1124857316 1 warning
24723 gige6 gige temperature 1124654213 1 warning
37279 gige3 gige temperature 1125743188 1 normal
35410 gige7 gige temperature 1125678723 1 normal
32680 node-196 node temperature 1125447841 1 ambient=24
32520 gige7 gige temperature 1125430919 1 normal
32503 node-226 node temperature 1125430560 1 ambient=30
32496 node-242 node temperature 1125430411 1 ambient=31
32407 node-80 node temperature 1125423420 1 ambient=33
32328 node-166 node temperature 1125421620 1 ambient=31
47196 gige7 gige temperature 1126281735 1 normal
46807 gige4 gige temperature 1126181283 1 warning
45833 gige6 gige temperature 1126060332 1 normal
39641 gige6 gige temperature 1125893831 1 warning
39375 gige3 gige temperature 1125876685 1 warning
47306 gige6 gige temperature 1126315336 1 warning
47324 node-46 node temperature 1126319850 1 ambient=27
47351 gige3 gige temperature 1126332995 1 normal
48285 node-246 node temperature 1126363680 1 ambient=29
48313 node-121 node temperature 1126366020 1 ambient=31
59663 gige4 gige temperature 1126835843 1 normal
66528 gige6 gige temperature 1126992985 1 warning
74681 gige4 gige temperature 1127529154 1 warning
74599 gige4 gige temperature 1127513254 1 warning
74503 node-165 node temperature 1127509530 1 ambient=30
74471 node-28 node temperature 1127507370 1 ambient=32
74327 gige7 gige temperature 1127466391 1 warning
73580 node-149 node temperature 1127289450 1 ambient=29
73035 gige6 gige temperature 1127171787 1 normal
80742 node-212 node temperature 1127580150 1 ambient=24
91359 node-117 node temperature 1128146611 1 ambient=30
91307 gige3 gige temperature 1128140438 1 warning
91099 node-239 node temperature 1128121110 1 ambient=28
90766 node-154 node temperature 1128090030 1 ambient=34
90706 node-227 node temperature 1128087510 1 ambient=25
90571 node-240 node temperature 1128071251 1 ambient=26
90446 node-227 node temperature 1128058591 1 ambient=25
90309 node-227 node temperature 1128043770 1 ambient=25
90256 node-227 node temperature 1128036930 1 ambient=25
89726 node-237 node temperature 1127970240 1 ambient=27
88896 node-112 node temperature 1127858430 1 ambient=30
88524 gige7 gige temperature 1127805997 1 warning
88301 node-31 node temperature 1127779980 1 ambient=31
87994 node-14 node temperature 1127746890 1 ambient=29
87957 gige4 gige temperature 1127740956 1 warning
87888 gige3 gige temperature 1127731836 1 warning
98691 node-238 node temperature 1128232080 1 ambient=28
98807 node-67 node temperature 1128237243 1 ambient=28
112690 gige6 gige temperature 1128819714 1 normal
112636 gige3 gige temperature 1128807948 1 normal
108135 node-234 node temperature 1128750244 1 ambient=25
108039 node-245 node temperature 1128731340 1 ambient=30
107885 node-139 node temperature 1128717151 1 ambient=30
107355 node-243 node temperature 1128677790 1 ambient=26
107123 gige4 gige temperature 1128638866 1 warning
106908 node-234 node temperature 1128605040 1 ambient=25
106121 gige4 gige temperature 1128486765 1 warning
106107 node-110 node temperature 1128485251 1 ambient=29
105925 gige4 gige temperature 1128444765 1 normal
105589 node-231 node temperature 1128355020 1 ambient=29
105329 node-229 node temperature 1128323820 1 ambient=27
105277 gige3 gige temperature 1128317445 1 warning
105218 node-246 node temperature 1128307170 1 ambient=26
105201 gige3 gige temperature 1128304844 1 normal
105166 gige7 gige temperature 1128298902 1 normal
105013 gige7 gige temperature 1128273402 1 warning
124392 node-227 node temperature 1129414680 1 ambient=25
124249 gige7 gige temperature 1129373817 1 warning
122478 gige7 gige temperature 1129368717 1 normal
121756 node-233 node temperature 1129356780 1 ambient=28
121634 node-88 node temperature 1129326780 1 ambient=29
120755 node-233 node temperature 1129305602 1 ambient=28
120039 gige6 gige temperature 1129263419 1 normal
119973 node-105 node temperature 1129243651 1 ambient=30
118999 node-48 node temperature 1129152541 1 ambient=30
118614 node-247 node temperature 1129131870 1 ambient=27
118574 gige3 gige temperature 1129121155 1 normal
118491 node-227 node temperature 1129105830 1 ambient=28
118358 gige3 gige temperature 1129078855 1 warning
117682 gige6 gige temperature 1128949016 1 normal
117530 node-104 node temperature 1128922170 1 ambient=30
148089 gige3 gige temperature 1129897264 1 normal
147747 gige7 gige temperature 1129829823 1 normal
163574 node-113 node temperature 1130664241 1 ambient=30
163424 node-241 node temperature 1130652991 1 ambient=27
163206 gige3 gige temperature 1130632279 1 warning
162000 node-115 node temperature 1130582460 1 ambient=26
160223 node-113 node temperature 1130569170 1 ambient=27
159900 node-237 node temperature 1130535210 1 ambient=24
159023 gige7 gige temperature 1130469428 1 normal
158572 node-231 node temperature 1130390160 1 ambient=26
156104 node-109 node temperature 1130358332 1 ambient=31
155335 gige7 gige temperature 1130287327 1 warning
155242 gige6 gige temperature 1130271428 1 warning
154670 node-243 node temperature 1130177820 1 ambient=29
154630 node-115 node temperature 1130172270 1 ambient=29
154424 gige3 gige temperature 1130137268 1 warning
154188 node-113 node temperature 1130090640 1 ambient=27
154068 gige7 gige temperature 1130069824 1 normal
176845 node-16 node temperature 1131226925 1 ambient=33
163677 node-237 node temperature 1130673420 1 ambient=30
173985 node-130 node temperature 1131220932 1 ambient=29
170998 node-78 node temperature 1131216252 1 ambient=33
167818 gige3 gige temperature 1130949082 1 normal
167448 node-119 node temperature 1130880870 1 ambient=29
167414 gige7 gige temperature 1130878335 1 warning
167256 node-237 node temperature 1130853781 1 ambient=30
167189 node-115 node temperature 1130845650 1 ambient=29
167091 node-227 node temperature 1130833170 1 ambient=28
167030 node-240 node temperature 1130825340 1 ambient=32
167025 gige4 gige temperature 1130825000 1 warning
166502 node-114 node temperature 1130781811 1 ambient=28
166309 node-242 node temperature 1130770290 1 ambient=28
166161 gige4 gige temperature 1130758999 1 warning
207192 gige5 gige temperature 1131716359 1 normal
204253 gige5 gige temperature 1131439295 1 normal
203805 gige4 gige temperature 1131366874 1 warning
227361 node-214 node temperature 1132274790 1 ambient=29
227833 node-67 node temperature 1132284720 1 ambient=30
227957 node-157 node temperature 1132285800 1 ambient=31
228160 node-187 node temperature 1132287030 1 ambient=31
228175 node-33 node temperature 1132287150 1 ambient=34
243555 gige3 gige temperature 1132547419 1 warning
263026 node-250 node temperature 1133576640 1 ambient=36
261998 node-65 node temperature 1133387520 1 ambient=33
275632 node-220 node temperature 1134164880 1 ambient=25
275052 node-250 node temperature 1133856960 1 ambient=36
285651 node-33 node temperature 1134668085 1 ambient=33
288181 node-173 node temperature 1134671246 1 ambient=25
303686 gige3 gige temperature 1135185449 1 normal
303647 gige3 gige temperature 1135163847 1 normal
303523 gige3 gige temperature 1135051042 1 normal
311373 gige3 gige temperature 1136079262 1 warning
312587 node-116 node temperature 1136537520 1 ambient=30
312445 gige3 gige temperature 1136462966 1 normal
312232 gige3 gige temperature 1136305464 1 normal
312131 node-97 node temperature 1136206230 1 ambient=33
313273 node-249 node temperature 1136914380 1 ambient=37
313257 gige6 gige temperature 1136905246 1 warning
313146 node-66 node temperature 1136846130 1 ambient=27
315731 node-41 node temperature 1138153620 1 ambient=30
315644 node-249 node temperature 1138132001 1 ambient=35
315096 node-152 node temperature 1138047991 1 ambient=27
315071 node-175 node temperature 1138046940 1 ambient=29
321202 gige6 gige temperature 1139481957 1 warning
321182 gige3 gige temperature 1139469899 1 normal
320143 node-222 node temperature 1139331900 1 ambient=28
335435 gige3 gige temperature 1140254414 1 normal
334882 node-231 node temperature 1140199680 1 ambient=28
334573 node-241 node temperature 1140184290 1 ambient=27
334522 node-239 node temperature 1140174390 1 ambient=26
334127 node-231 node temperature 1140143250 1 ambient=25
334100 node-51 node temperature 1140142440 1 ambient=30
333962 node-43 node temperature 1140139500 1 ambient=35
333864 node-145 node temperature 1140138780 1 ambient=32
333848 node-168 node temperature 1140138750 1 ambient=29
333509 node-168 node temperature 1140135990 1 ambient=35
333067 gige7 gige temperature 1140124268 1 critical
341488 node-240 node temperature 1140266130 1 ambient=27
341499 node-242 node temperature 1140267870 1 ambient=29
341605 node-227 node temperature 1140281700 1 ambient=28
341711 node-233 node temperature 1140296011 1 ambient=28
355102 node-238 node temperature 1140888121 1 ambient=25
355008 node-238 node temperature 1140877380 1 ambient=25
349339 node-235 node temperature 1140676800 1 ambient=27
348402 node-241 node temperature 1140555150 1 ambient=30
348398 node-236 node temperature 1140554910 1 ambient=28
348173 gige7 gige temperature 1140469273 1 critical
348112 node-238 node temperature 1140446850 1 ambient=25
366358 gige6 gige temperature 1141480585 1 normal
361170 node-245 node temperature 1141238460 1 ambient=27
360990 gige6 gige temperature 1141136483 1 normal
360474 node-236 node temperature 1141005000 1 ambient=25
360467 node-114 node temperature 1141003082 1 ambient=28
381136 node-88 node temperature 1142037660 1 ambient=33
380953 node-21 node temperature 1141983900 1 ambient=36
380813 node-98 node temperature 1141951865 1 ambient=30
379973 node-48 node temperature 1141835730 1 ambient=36
379970 node-63 node temperature 1141834980 1 ambient=33
379954 gige6 gige temperature 1141834439 1 normal
379511 node-184 node temperature 1141817851 1 ambient=31
379230 gige3 gige temperature 1141762078 1 normal
374787 node-91 node temperature 1141676911 1 ambient=27
372864 node-34 node temperature 1141676137 1 ambient=26
371466 gige3 gige temperature 1141598132 1 warning
387363 gige7 gige temperature 1142103247 1 critical
401995 gige4 gige temperature 1142584554 1 normal
402021 gige3 gige temperature 1142592234 1 normal
402094 node-37 node temperature 1142615010 1 ambient=33
402312 gige7 gige temperature 1142637295 1 warning
408247 gige3 gige temperature 1142754835 1 normal
416519 node-159 node temperature 1143214291 1 ambient=36
415931 node-84 node temperature 1143096421 1 ambient=33
415780 gige3 gige temperature 1143058438 1 normal
415395 node-194 node temperature 1142947861 1 ambient=26
431593 node-233 node temperature 1143831330 1 ambient=25
431548 node-114 node temperature 1143825180 1 ambient=27
431421 node-110 node temperature 1143811410 1 ambient=31
428615 node-229 node temperature 1143728610 1 ambient=25
428555 node-220 node temperature 1143724650 1 ambient=28
428281 gige5 gige temperature 1143696187 1 warning
426064 gige6 gige temperature 1143571810 1 normal
425832 node-121 node temperature 1143482567 1 ambient=32
425772 node-80 node temperature 1143482491 1 ambient=33
425740 node-138 node temperature 1143482449 1 ambient=28
425701 node-36 node temperature 1143482413 1 ambient=31
425504 gige7 gige temperature 1143447307 1 critical
425034 gige3 gige temperature 1143379144 1 normal
436350 node-241 node temperature 1143896370 1 ambient=30
436394 node-239 node temperature 1143899370 1 ambient=29
436897 node-114 node temperature 1143962820 1 ambient=30
448165 gige3 gige temperature 1144567339 1 normal
445454 gige6 gige temperature 1144440503 1 normal
445396 gige6 gige temperature 1144416803 1 normal
443918 gige4 gige temperature 1144223486 1 warning
443814 gige7 gige temperature 1144183213 1 warning
442984 node-114 node temperature 1144018980 1 ambient=27
454305 gige6 gige temperature 1145174613 1 normal
451250 gige7 gige temperature 1144890808 1 critical
451247 gige7 gige temperature 1144889909 1 warning
451175 gige7 gige temperature 1144867107 1 warning
450483 gige6 gige temperature 1144802008 1 normal
450206 gige3 gige temperature 1144711043 1 warning
450087 gige3 gige temperature 1144667241 1 normal
449962 gige6 gige temperature 1144627407 1 warning
458832 node-115 node temperature 1145540522 1 ambient=28
460291 node-103 node temperature 1145551988 1 ambient=26
462282 node-94 node temperature 1145552738 1 ambient=34
465923 gige7 gige temperature 1145638393 1 warning
466009 gige3 gige temperature 1145664432 1 warning
480082 gige7 gige temperature 1146100398 1 critical
479981 gige4 gige temperature 1146058156 1 warning
2560209 Interconnect-1N01 switch_module error 1073192214 1 Link ok
2558342 Interconnect-1N01 switch_module error 1073114649 1 Link ok
2558132 Interconnect-1N01 switch_module error 1073080543 1 Link ok
2558082 Interconnect-1N01 switch_module error 1073073636 1 Link ok
2557405 Interconnect-1T01 switch_module error 1072965457 1 Link error
2557080 Interconnect-1N01 switch_module error 1072913499 1 Link error
2556365 Interconnect-1N01 switch_module error 1072790564 1 Link in reset
2550228 Interconnect-1N01 switch_module error 1072442018 1 Link ok
2553872 Interconnect-0N03 switch_module error 1072642871 1 Link ok
2553727 Interconnect-1N03 switch_module error 1072642564 1 Link in reset
2553390 Interconnect-1N03 switch_module error 1072641409 1 Link ok
2553308 Interconnect-1N03 switch_module error 1072638589 1 Link error
2560289 Interconnect-1N01 switch_module error 1073206850 1 Link ok
2560175 Interconnect-1N01 switch_module error 1073185824 1 Link ok
2559445 Interconnect-1N01 switch_module error 1073128433 1 Link ok
2558415 Interconnect-1T02 switch_module error 1073119551 1 Link error
2561694 Interconnect-1N01 switch_module error 1073459190 1 Link in reset
2561093 Interconnect-1N01 switch_module error 1073345520 1 Link ok
2560515 Interconnect-1N01 switch_module error 1073246148 1 Link ok
2560493 Interconnect-1N01 switch_module error 1073242467 1 Link in reset
2566680 Interconnect-1N03 switch_module error 1073991556 1 Link in reset
2565131 Interconnect-1N01 switch_module error 1073923347 1 Link in reset
2565070 Interconnect-1N01 switch_module error 1073911806 1 Link in reset
2569061 Interconnect-0N03 switch_module error 1074120426 1 Linkerror event interval expired
2569549 Interconnect-0N03 switch_module error 1074121227 1 Linkerror event interval expired
2570021 node-5 action error 1074121927 1 risBoot (command 1903) Error: Timed out while waiting for SRM prompt: <ABORT code completed>
2570091 Interconnect-0N01 switch_module error 1074122028 1 Linkerror event interval expired
2570813 Interconnect-1N01 switch_module error 1074123216 1 Linkerror event interval expired
2572181 Interconnect-0N01 switch_module error 1074125925 1 Linkerror event interval expired
2572865 Interconnect-1N00 switch_module error 1074129389 1 Linkerror event interval expired
2572925 Interconnect-1N02 switch_module error 1074129669 1 Linkerror event interval expired
2573009 Interconnect-0N00 switch_module error 1074130036 1 Linkerror event interval expired
2573041 Interconnect-0N00 switch_module error 1074130207 1 Linkerror event interval expired
2573682 Interconnect-0N02 switch_module error 1074131794 1 Linkerror event interval expired
2573870 Interconnect-1N02 switch_module error 1074131950 1 Linkerror event interval expired
2574049 Interconnect-1N02 switch_module error 1074132111 1 Linkerror event interval expired
2574588 Interconnect-1N03 switch_module error 1074137477 1 Linkerror event interval expired
2575083 Interconnect-1N01 switch_module error 1074161541 1 Linkerror event interval expired
2575089 Interconnect-1N01 switch_module error 1074162647 1 Linkerror event interval expired
2575104 Interconnect-1N01 switch_module error 1074165830 1 Linkerror event interval expired
2575213 Interconnect-0N02 switch_module error 1074175913 1 Linkerror event interval expired
2575239 Interconnect-1N02 switch_module error 1074175929 1 Linkerror event interval expired
2575308 Interconnect-1N02 switch_module error 1074175999 1 Linkerror event interval expired
2576243 Interconnect-0N02 switch_module error 1074178679 1 Linkerror event interval expired
2577675 Interconnect-1N02 switch_module error 1074187413 1 Linkerror event interval expired
2577929 Interconnect-0N01 switch_module error 1074189714 1 Linkerror event interval expired
2579066 Interconnect-1N02 switch_module error 1074207069 1 Linkerror event interval expired
2579144 Interconnect-1N01 switch_module error 1074208208 1 Linkerror event interval expired
2580497 Interconnect-1N01 switch_module error 1074217133 1 Linkerror event interval expired
2580613 Interconnect-0N01 switch_module error 1074217167 1 Linkerror event interval expired
2580989 Interconnect-1N01 switch_module error 1074217255 1 Linkerror event interval expired
2581234 Interconnect-1N03 switch_module error 1074217317 1 Linkerror event interval expired
2581250 Interconnect-1N03 switch_module error 1074217320 1 Linkerror event interval expired
2583068 Interconnect-1N03 switch_module error 1074225629 1 Linkerror event interval expired
2583136 Interconnect-1N03 switch_module error 1074225798 1 Linkerror event interval expired
2583982 Interconnect-1N00 switch_module error 1074230178 1 Linkerror event interval expired
2584628 Interconnect-0N01 switch_module error 1074232068 1 Linkerror event interval expired
2584639 Interconnect-0N01 switch_module error 1074232126 1 Linkerror event interval expired
2585028 Interconnect-1N02 switch_module error 1074233208 1 Linkerror event interval expired
2585588 Interconnect-0N02 switch_module error 1074234738 1 Linkerror event interval expired
2585665 Interconnect-1N03 switch_module error 1074234966 1 Linkerror event interval expired
2585816 Interconnect-1N03 switch_module error 1074235394 1 Linkerror event interval expired
2586025 Interconnect-1N03 switch_module error 1074236020 1 Linkerror event interval expired
2586061 Interconnect-1N03 switch_module error 1074236099 1 Linkerror event interval expired
2586170 Interconnect-1N03 switch_module error 1074236447 1 Linkerror event interval expired
2586186 Interconnect-1N03 switch_module error 1074236525 1 Linkerror event interval expired
2586431 Interconnect-0N03 switch_module error 1074237429 1 Linkerror event interval expired
2587495 Interconnect-0N00 switch_module error 1074241888 1 Linkerror event interval expired
2588164 Interconnect-1N02 switch_module error 1074242677 1 Linkerror event interval expired
2588737 Interconnect-0N03 switch_module error 1074243063 1 Linkerror event interval expired
2588743 Interconnect-0N03 switch_module error 1074243064 1 Linkerror event interval expired
2590123 Interconnect-1N00 switch_module error 1074261819 1 Linkerror event interval expired
2594227 Interconnect-1N01 switch_module error 1074272896 1 Linkerror event interval expired
2595676 Interconnect-1N01 switch_module error 1074288628 1 Linkerror event interval expired
2597017 Interconnect-1N00 switch_module error 1074290313 1 Linkerror event interval expired
2597376 Interconnect-1N00 switch_module error 1074290438 1 Linkerror event interval expired
2597537 Interconnect-0N01 switch_module error 1074290483 1 Linkerror event interval expired
2597642 Interconnect-0N02 switch_module error 1074290511 1 Linkerror event interval expired
2597772 Interconnect-1N02 switch_module error 1074290544 1 Linkerror event interval expired
2599314 Interconnect-1N01 switch_module error 1074297487 1 Linkerror event interval expired
2600221 Interconnect-0N02 switch_module error 1074297871 1 Linkerror event interval expired
2615716 Interconnect-1N01 switch_module error 1074900640 1 Linkerror event interval expired
2615428 Interconnect-1N01 switch_module error 1074848454 1 Linkerror event interval expired
2615364 Interconnect-1N01 switch_module error 1074835654 1 Linkerror event interval expired
2615204 Interconnect-1N01 switch_module error 1074801720 1 Linkerror event interval expired
2614572 Interconnect-1N01 switch_module error 1074747779 1 Linkerror event interval expired
2614352 Interconnect-1N01 switch_module error 1074709464 1 Linkerror event interval expired
2612089 Interconnect-1N01 switch_module error 1074510127 1 Linkerror event interval expired
2590181 Interconnect-0N00 switch_module error 1074261994 1 Linkerror event interval expired
2591699 Interconnect-1T03 switch_module error 1074268486 1 Linkerror event interval expired
2595669 Interconnect-1N01 switch_module error 1074287600 1 Linkerror event interval expired
2597578 Interconnect-1N02 switch_module error 1074290494 1 Linkerror event interval expired
2597608 Interconnect-0N01 switch_module error 1074290502 1 Linkerror event interval expired
2597811 Interconnect-1N02 switch_module error 1074290555 1 Linkerror event interval expired
2597893 Interconnect-0N02 switch_module error 1074290575 1 Linkerror event interval expired
2598048 Interconnect-0N03 switch_module error 1074290616 1 Linkerror event interval expired
2598092 Interconnect-0N03 switch_module error 1074290638 1 Linkerror event interval expired
2598112 Interconnect-1N03 switch_module error 1074290649 1 Linkerror event interval expired
2598151 Interconnect-0N03 switch_module error 1074290671 1 Linkerror event interval expired
2598235 Interconnect-1N01 switch_module error 1074294936 1 Linkerror event interval expired
2598907 Interconnect-0N00 switch_module error 1074296653 1 Linkerror event interval expired
2600960 Interconnect-0N02 switch_module error 1074298131 1 Linkerror event interval expired
2606824 Interconnect-1N01 switch_module error 1074356930 1 Linkerror event interval expired
256 Interconnect-1N01 switch_module error 1075463047 1 Linkerror event interval expired
292 Interconnect-1T01 switch_module error 1075470509 1 Linkerror event interval expired
3098 Interconnect-1N01 switch_module error 1075572430 1 Linkerror event interval expired
3496 Interconnect-1N03 switch_module error 1075629657 1 Linkerror event interval expired
28360 Interconnect-1N00 switch_module error 1076182055 1 Linkerror event interval expired
27373 node-55 action error 1076176599 1 boot (command 2092) Error: HALT asserted\  cannot continue
27185 Interconnect-1N00 switch_module error 1076173770 1 Linkerror event interval expired
26955 Interconnect-1N01 switch_module error 1076161132 1 Linkerror event interval expired
19621 Interconnect-1N01 switch_module error 1076023220 1 Linkerror event interval expired
29376 Interconnect-1N00 switch_module error 1076183342 1 Linkerror event interval expired
30791 Interconnect-1N01 switch_module error 1076205835 1 Linkerror event interval expired
40208 Interconnect-1N03 switch_module error 1076538599 1 Linkerror event interval expired
35564 Interconnect-1N01 switch_module error 1076523236 1 Linkerror event interval expired
35539 Interconnect-0N03 switch_module error 1076519774 1 Linkerror event interval expired
35463 Interconnect-0N00 switch_module error 1076517688 1 Linkerror event interval expired
34478 Interconnect-1N01 switch_module error 1076446111 1 Linkerror event interval expired
33331 Interconnect-1N03 switch_module error 1076434499 1 Linkerror event interval expired
32793 Interconnect-1N01 switch_module error 1076424334 1 Linkerror event interval expired
32304 Interconnect-0N03 switch_module error 1076399082 1 Linkerror event interval expired
31891 Interconnect-1T02 switch_module error 1076379650 1 Linkerror event interval expired
31579 Interconnect-1N01 switch_module error 1076336786 1 Linkerror event interval expired
48830 Interconnect-0N00 switch_module error 1076703125 1 Linkerror event interval expired
48900 Interconnect-1N01 switch_module error 1076713837 1 Linkerror event interval expired
54223 Interconnect-1N01 switch_module error 1076776381 1 Linkerror event interval expired
54268 Interconnect-1N01 switch_module error 1076785690 1 Linkerror event interval expired
54489 Interconnect-1N01 switch_module error 1076819370 1 Linkerror event interval expired
55989 Interconnect-1N01 switch_module error 1076843300 1 Linkerror event interval expired
80119 Interconnect-1N01 switch_module error 1077260187 1 Linkerror event interval expired
78503 Interconnect-1N01 switch_module error 1077243903 1 Linkerror event interval expired
78479 Interconnect-1N01 switch_module error 1077238683 1 Linkerror event interval expired
77224 Interconnect-0N00 switch_module error 1077215336 1 Linkerror event interval expired
77005 Interconnect-0N00 switch_module error 1077215215 1 Linkerror event interval expired
74930 Interconnect-0N01 switch_module error 1077172601 1 Linkerror event interval expired
74611 Interconnect-0N01 switch_module error 1077171912 1 Linkerror event interval expired
73728 Interconnect-1N01 switch_module error 1077094121 1 Linkerror event interval expired
71637 Interconnect-1N00 switch_module error 1077066136 1 Linkerror event interval expired
71623 Interconnect-1N00 switch_module error 1077066129 1 Linkerror event interval expired
71554 Interconnect-0N00 switch_module error 1077066063 1 Linkerror event interval expired
70353 Interconnect-1N03 switch_module error 1077061443 1 Linkerror event interval expired
70211 Interconnect-0N03 switch_module error 1077061058 1 Linkerror event interval expired
70014 Interconnect-1N01 switch_module error 1077057705 1 Linkerror event interval expired
69261 Interconnect-1N01 switch_module error 1076933680 1 Linkerror event interval expired
61506 Interconnect-1N00 switch_module error 1076866050 1 Linkerror event interval expired
61078 Interconnect-1N01 switch_module error 1076865899 1 Linkerror event interval expired
59022 Interconnect-0N03 switch_module error 1076865397 1 Linkerror event interval expired
56743 Interconnect-0N03 switch_module error 1076858266 1 Linkerror event interval expired
86690 Interconnect-1T02 switch_module error 1077364380 1 Linkerror event interval expired
87093 Interconnect-1N01 switch_module error 1077384074 1 Linkerror event interval expired
87621 Interconnect-1N01 switch_module error 1077451653 1 Linkerror event interval expired
115237 Interconnect-1N01 switch_module error 1077763844 1 Linkerror event interval expired
106635 Interconnect-1N01 switch_module error 1077740032 1 Linkerror event interval expired
105870 Interconnect-1N02 switch_module error 1077739756 1 Linkerror event interval expired
105792 Interconnect-0N01 switch_module error 1077739731 1 Linkerror event interval expired
102642 Interconnect-0N02 switch_module error 1077735930 1 Linkerror event interval expired
102627 Interconnect-1N03 switch_module error 1077735926 1 Linkerror event interval expired
102593 Interconnect-1N02 switch_module error 1077735915 1 Linkerror event interval expired
102079 Interconnect-0N01 switch_module error 1077735752 1 Linkerror event interval expired
101832 Interconnect-1N00 switch_module error 1077735628 1 Linkerror event interval expired
100619 Interconnect-0N03 switch_module error 1077716299 1 Linkerror event interval expired
100546 Interconnect-0N03 switch_module error 1077711917 1 Linkerror event interval expired
100398 Interconnect-1N02 switch_module error 1077691525 1 Linkerror event interval expired
100284 Interconnect-1N01 switch_module error 1077676650 1 Linkerror event interval expired
100157 Interconnect-1T01 switch_module error 1077654632 1 Linkerror event interval expired
94573 Interconnect-0N01 switch_module error 1077646914 1 Linkerror event interval expired
93616 Interconnect-0N00 switch_module error 1077646552 1 Linkerror event interval expired
93592 Interconnect-0N00 switch_module error 1077646545 1 Linkerror event interval expired
92943 Interconnect-1N03 switch_module error 1077644946 1 Linkerror event interval expired
92934 Interconnect-1N03 switch_module error 1077644940 1 Linkerror event interval expired
92067 Interconnect-0N01 switch_module error 1077631886 1 Linkerror event interval expired
92001 Interconnect-1N03 switch_module error 1077631249 1 Linkerror event interval expired
91936 Interconnect-0N03 switch_module error 1077631217 1 Linkerror event interval expired
91927 Interconnect-1N03 switch_module error 1077631212 1 Linkerror event interval expired
91802 Interconnect-1N01 switch_module error 1077631166 1 Linkerror event interval expired
91770 Interconnect-1N03 switch_module error 1077631159 1 Linkerror event interval expired
89037 Interconnect-1N01 switch_module error 1077578416 1 Linkerror event interval expired
88370 Interconnect-1N01 switch_module error 1077549077 1 Linkerror event interval expired
87770 Interconnect-1T02 switch_module error 1077473123 1 Linkerror event interval expired
87674 Interconnect-1N01 switch_module error 1077460673 1 Linkerror event interval expired
135982 Interconnect-1N01 switch_module error 1077808440 1 Linkerror event interval expired
136333 Interconnect-0N02 switch_module error 1077808562 1 Linkerror event interval expired
138084 Interconnect-1N01 switch_module error 1077809945 1 Linkerror event interval expired
138177 Interconnect-0N00 switch_module error 1077809967 1 Linkerror event interval expired
138184 Interconnect-0N02 switch_module error 1077809969 1 Linkerror event interval expired
139225 Interconnect-1N01 switch_module error 1077810259 1 Linkerror event interval expired
139238 Interconnect-1N02 switch_module error 1077810262 1 Linkerror event interval expired
141757 Interconnect-1N00 switch_module error 1077815530 1 Linkerror event interval expired
141792 Interconnect-0N00 switch_module error 1077815549 1 Linkerror event interval expired
141816 Interconnect-1N00 switch_module error 1077815561 1 Linkerror event interval expired
141819 Interconnect-0N00 switch_module error 1077815564 1 Linkerror event interval expired
142130 Interconnect-0N01 switch_module error 1077815632 1 Linkerror event interval expired
142380 Interconnect-1N01 switch_module error 1077815724 1 Linkerror event interval expired
142698 Interconnect-1N02 switch_module error 1077815841 1 Linkerror event interval expired
142994 Interconnect-0N03 switch_module error 1077816061 1 Linkerror event interval expired
145825 Interconnect-1N02 switch_module error 1077817398 1 Linkerror event interval expired
156575 Interconnect-1N03 switch_module error 1077865581 1 Linkerror event interval expired
157160 Interconnect-1N03 switch_module error 1077867221 1 Linkerror event interval expired
157378 Interconnect-1N03 switch_module error 1077868285 1 Linkerror event interval expired
157601 Interconnect-1N03 switch_module error 1077873194 1 Linkerror event interval expired
160886 Interconnect-1N01 switch_module error 1077875763 1 Linkerror event interval expired
161403 Interconnect-0N03 switch_module error 1077876337 1 Linkerror event interval expired
161699 Interconnect-1N00 switch_module error 1077877379 1 Linkerror event interval expired
162068 Interconnect-1N00 switch_module error 1077877486 1 Linkerror event interval expired
162291 Interconnect-1N02 switch_module error 1077877544 1 Linkerror event interval expired
162735 Interconnect-0N03 switch_module error 1077877671 1 Linkerror event interval expired
163650 Interconnect-1N03 switch_module error 1077878624 1 Linkerror event interval expired
163677 Interconnect-1T01 switch_module error 1077878640 1 Linkerror event interval expired
165107 Interconnect-1N00 switch_module error 1077881440 1 Linkerror event interval expired
173860 Interconnect-0N00 switch_module error 1077892961 1 Linkerror event interval expired
174001 Interconnect-0N00 switch_module error 1077893002 1 Linkerror event interval expired
174925 Interconnect-0N03 switch_module error 1077893261 1 Linkerror event interval expired
176394 Interconnect-1N01 switch_module error 1077893959 1 Linkerror event interval expired
176521 Interconnect-0N00 switch_module error 1077893995 1 Linkerror event interval expired
177328 Interconnect-0N01 switch_module error 1077894231 1 Linkerror event interval expired
178485 Interconnect-1N01 switch_module error 1077896308 1 Linkerror event interval expired
183763 Interconnect-0N01 switch_module error 1077902463 1 Linkerror event interval expired
183905 Interconnect-1N01 switch_module error 1077902508 1 Linkerror event interval expired
185325 Interconnect-1N03 switch_module error 1077902873 1 Linkerror event interval expired
198629 Interconnect-1N01 switch_module error 1077979769 1 Linkerror event interval expired
226042 Interconnect-1N00 switch_module error 1078639210 1 Linkerror event interval expired
225744 Interconnect-1N02 switch_module error 1078594479 1 Linkerror event interval expired
225055 Interconnect-1N01 switch_module error 1078529930 1 Linkerror event interval expired
224686 Interconnect-1T03 switch_module error 1078513350 1 Linkerror event interval expired
223805 Interconnect-0N00 switch_module error 1078499737 1 Linkerror event interval expired
222409 Interconnect-1N03 switch_module error 1078490573 1 Linkerror event interval expired
222069 Interconnect-1N01 switch_module error 1078476765 1 Linkerror event interval expired
217077 Interconnect-1N03 switch_module error 1078468199 1 Linkerror event interval expired
216344 Interconnect-1N02 switch_module error 1078467983 1 Linkerror event interval expired
215082 Interconnect-1N00 switch_module error 1078467571 1 Linkerror event interval expired
214078 Interconnect-1N02 switch_module error 1078466489 1 Linkerror event interval expired
213970 Interconnect-0N00 switch_module error 1078466456 1 Linkerror event interval expired
213969 Interconnect-1N00 switch_module error 1078466455 1 Linkerror event interval expired
213842 Interconnect-1N01 switch_module error 1078466414 1 Linkerror event interval expired
210873 Interconnect-1N00 switch_module error 1078455697 1 Linkerror event interval expired
210170 Interconnect-0N00 switch_module error 1078453534 1 Linkerror event interval expired
205020 Interconnect-1N01 switch_module error 1078382220 1 Linkerror event interval expired
203971 Interconnect-1N01 switch_module error 1078262512 1 Linkerror event interval expired
203905 Interconnect-1N01 switch_module error 1078253002 1 Linkerror event interval expired
203519 Interconnect-1N01 switch_module error 1078221896 1 Linkerror event interval expired
201862 Interconnect-1N01 switch_module error 1078162442 1 Linkerror event interval expired
201411 Interconnect-1N01 switch_module error 1078161468 1 Linkerror event interval expired
201274 Interconnect-1N01 switch_module error 1078157542 1 Linkerror event interval expired
200983 Interconnect-1N01 switch_module error 1078126141 1 Linkerror event interval expired
200080 Interconnect-1N02 switch_module error 1078120174 1 Linkerror event interval expired
199652 Interconnect-1N01 switch_module error 1078113076 1 Linkerror event interval expired
199360 Interconnect-1N01 switch_module error 1078080253 1 Linkerror event interval expired
199189 Interconnect-1T01 switch_module error 1078055222 1 Linkerror event interval expired
264490 Interconnect-1N01 switch_module error 1079259526 1 Linkerror event interval expired
262454 Interconnect-1N01 switch_module error 1079242615 1 Linkerror event interval expired
262189 Interconnect-1N01 switch_module error 1079231184 1 Linkerror event interval expired
260815 Interconnect-1N01 switch_module error 1079172043 1 Linkerror event interval expired
260718 Interconnect-1N01 switch_module error 1079152857 1 Linkerror event interval expired
256643 Interconnect-0N03 switch_module error 1079070099 1 Linkerror event interval expired
255821 Interconnect-1N02 switch_module error 1079069906 1 Linkerror event interval expired
255352 Interconnect-0N03 switch_module error 1079069816 1 Linkerror event interval expired
255061 Interconnect-1N00 switch_module error 1079069705 1 Linkerror event interval expired
254564 Interconnect-1N01 switch_module error 1079069444 1 Linkerror event interval expired
247218 Interconnect-0N03 switch_module error 1079048791 1 Linkerror event interval expired
246277 Interconnect-0N02 switch_module error 1079048528 1 Linkerror event interval expired
243953 Interconnect-1N03 switch_module error 1079046909 1 Linkerror event interval expired
243924 Interconnect-1N03 switch_module error 1079046891 1 Linkerror event interval expired
243822 Interconnect-0N01 switch_module error 1079046846 1 Linkerror event interval expired
243616 Interconnect-1N01 switch_module error 1079046788 1 Linkerror event interval expired
243531 Interconnect-1N00 switch_module error 1079046762 1 Linkerror event interval expired
243508 Interconnect-0N00 switch_module error 1079046747 1 Linkerror event interval expired
240969 Interconnect-0N03 switch_module error 1079031627 1 Linkerror event interval expired
236991 Interconnect-0N02 switch_module error 1079015266 1 Linkerror event interval expired
235379 Interconnect-0N02 switch_module error 1079014242 1 Linkerror event interval expired
235089 Interconnect-1N01 switch_module error 1079014154 1 Linkerror event interval expired
234818 Interconnect-0N00 switch_module error 1079014094 1 Linkerror event interval expired
232896 Interconnect-0N02 switch_module error 1079011268 1 Linkerror event interval expired
231122 Interconnect-1N01 switch_module error 1079007841 1 Linkerror event interval expired
228981 Interconnect-1T01 switch_module error 1078947615 1 Linkerror event interval expired
228733 Interconnect-1T02 switch_module error 1078905214 1 Linkerror event interval expired
228624 Interconnect-1N01 switch_module error 1078885801 1 Linkerror event interval expired
227776 Interconnect-1T01 switch_module error 1078852836 1 Linkerror event interval expired
227742 Interconnect-1T01 switch_module error 1078850816 1 Linkerror event interval expired
227725 Interconnect-1T01 switch_module error 1078849778 1 Linkerror event interval expired
227465 Interconnect-1N01 switch_module error 1078828243 1 Linkerror event interval expired
227192 Interconnect-1N00 switch_module error 1078790431 1 Linkerror event interval expired
227111 Interconnect-1T01 switch_module error 1078780795 1 Linkerror event interval expired
226915 Interconnect-1N01 switch_module error 1078758255 1 Linkerror event interval expired
226872 Interconnect-0N02 switch_module error 1078754632 1 Linkerror event interval expired
273020 Interconnect-1N03 switch_module error 1079615422 0 Linkerror event interval expired
272687 Interconnect-0N01 switch_module error 1079615365 1 Linkerror event interval expired
274509 Interconnect-0N03 switch_module error 1079616584 1 Linkerror event interval expired
275533 Interconnect-0N00 switch_module error 1079616864 1 Linkerror event interval expired
275552 Interconnect-1N00 switch_module error 1079616871 1 Linkerror event interval expired
286779 Interconnect-1N01 switch_module error 1080448427 1 Linkerror event interval expired
286758 Interconnect-1N01 switch_module error 1080446120 1 Linkerror event interval expired
285257 Interconnect-1N01 switch_module error 1080238353 1 Linkerror event interval expired
285240 Interconnect-1N01 switch_module error 1080235155 1 Linkerror event interval expired
283145 Interconnect-1N01 switch_module error 1080051195 1 Linkerror event interval expired
283041 Interconnect-1N01 switch_module error 1080030329 1 Linkerror event interval expired
282174 Interconnect-1N01 switch_module error 1079945592 1 Linkerror event interval expired
291446 Interconnect-1T01 switch_module error 1081074961 1 Linkerror event interval expired
291303 Interconnect-1N01 switch_module error 1081058364 1 Linkerror event interval expired
290814 Interconnect-1N01 switch_module error 1080987611 1 Linkerror event interval expired
290402 Interconnect-1N01 switch_module error 1080923106 1 Linkerror event interval expired
290318 Interconnect-1N01 switch_module error 1080908787 1 Linkerror event interval expired
290101 Interconnect-1N01 switch_module error 1080880454 1 Linkerror event interval expired
289347 Interconnect-1N01 switch_module error 1080807214 1 Linkerror event interval expired
289250 Interconnect-1T01 switch_module error 1080791516 1 Linkerror event interval expired
288816 Interconnect-1N01 switch_module error 1080727457 1 Linkerror event interval expired
288650 Interconnect-1N01 switch_module error 1080695124 1 Linkerror event interval expired
288593 Interconnect-1N01 switch_module error 1080683420 1 Linkerror event interval expired
288353 Interconnect-1T01 switch_module error 1080665531 1 Linkerror event interval expired
288291 Interconnect-1T01 switch_module error 1080661262 1 Linkerror event interval expired
288224 Interconnect-1N01 switch_module error 1080649743 1 Linkerror event interval expired
287874 Interconnect-1T02 switch_module error 1080609737 1 Linkerror event interval expired
287694 Interconnect-1T03 switch_module error 1080583057 1 Linkerror event interval expired
287173 Interconnect-1N03 switch_module error 1080515046 1 Linkerror event interval expired
287172 Interconnect-1N03 switch_module error 1080515045 1 Linkerror event interval expired
286945 Interconnect-1N01 switch_module error 1080480275 1 Linkerror event interval expired
297281 Interconnect-1N01 switch_module error 1081677869 1 Linkerror event interval expired
297179 Interconnect-1N01 switch_module error 1081658335 1 Linkerror event interval expired
296875 Interconnect-1T03 switch_module error 1081595628 1 Linkerror event interval expired
296699 Interconnect-1N01 switch_module error 1081573002 1 Linkerror event interval expired
295833 Interconnect-1N01 switch_module error 1081511496 1 Linkerror event interval expired
295149 Interconnect-1T03 switch_module error 1081396012 1 Linkerror event interval expired
295144 Interconnect-1N01 switch_module error 1081395907 1 Linkerror event interval expired
293808 Interconnect-1T01 switch_module error 1081373483 1 Linkerror event interval expired
293029 Interconnect-1N01 switch_module error 1081284631 1 Linkerror event interval expired
292872 Interconnect-1N01 switch_module error 1081271905 1 Linkerror event interval expired
292868 Interconnect-1N01 switch_module error 1081270739 1 Linkerror event interval expired
292481 Interconnect-1T02 switch_module error 1081214271 1 Linkerror event interval expired
291882 Interconnect-1N01 switch_module error 1081122675 1 Linkerror event interval expired
291723 Interconnect-1T01 switch_module error 1081100411 1 Linkerror event interval expired
306222 Interconnect-1N03 switch_module error 1082037261 1 Linkerror event interval expired
306105 Interconnect-1N03 switch_module error 1082037229 1 Linkerror event interval expired
305664 Interconnect-0N01 switch_module error 1082037120 1 Linkerror event interval expired
305641 Interconnect-1N00 switch_module error 1082037113 1 Linkerror event interval expired
305545 Interconnect-0N01 switch_module error 1082037074 1 Linkerror event interval expired
305459 Interconnect-0N01 switch_module error 1082037037 1 Linkerror event interval expired
305441 Interconnect-1N00 switch_module error 1082037026 1 Linkerror event interval expired
303847 Interconnect-1T01 switch_module error 1082033407 1 Linkerror event interval expired
303819 Interconnect-1T03 switch_module error 1082033397 1 Linkerror event interval expired
303756 Interconnect-1T02 switch_module error 1082033374 1 Linkerror event interval expired
303530 Interconnect-1T00 switch_module error 1082033291 1 Linkerror event interval expired
303519 Interconnect-1T03 switch_module error 1082033288 1 Linkerror event interval expired
303314 Interconnect-1T03 switch_module error 1082033218 1 Linkerror event interval expired
303074 Interconnect-1T01 switch_module error 1082030618 1 Linkerror event interval expired
302921 Interconnect-1T01 switch_module error 1082024309 1 Linkerror event interval expired
301917 Interconnect-0N00 switch_module error 1081998320 1 Linkerror event interval expired
301866 Interconnect-1N00 switch_module error 1081998024 1 Linkerror event interval expired
300741 Interconnect-1N01 switch_module error 1081918069 1 Linkerror event interval expired
300560 Interconnect-1N01 switch_module error 1081901930 1 Linkerror event interval expired
300105 Interconnect-1N01 switch_module error 1081841899 1 Linkerror event interval expired
299480 Interconnect-1N02 switch_module error 1081818106 1 Linkerror event interval expired
297882 Interconnect-1T01 switch_module error 1081750251 1 Linkerror event interval expired
297409 Interconnect-1N01 switch_module error 1081692713 1 Linkerror event interval expired
306675 Interconnect-0N01 switch_module error 1082038166 1 Linkerror event interval expired
307671 Interconnect-0N00 switch_module error 1082038562 1 Linkerror event interval expired
309088 Interconnect-1N03 switch_module error 1082038957 1 Linkerror event interval expired
314658 Interconnect-0N03 switch_module error 1082228382 1 Linkerror event interval expired
316037 Interconnect-1N01 switch_module error 1082717020 1 Linkerror event interval expired
326762 Interconnect-1N00 switch_module error 1083202557 1 Linkerror event interval expired
326728 Interconnect-1N00 switch_module error 1083202526 1 Linkerror event interval expired
326592 Interconnect-1N03 switch_module error 1083202466 1 Linkerror event interval expired
325902 Interconnect-1N03 switch_module error 1083202210 1 Linkerror event interval expired
323098 Interconnect-0N01 switch_module error 1083197614 1 Linkerror event interval expired
322969 Interconnect-0N03 switch_module error 1083197557 1 Linkerror event interval expired
322834 Interconnect-0N03 switch_module error 1083197500 1 Linkerror event interval expired
322682 Interconnect-0N02 switch_module error 1083197432 1 Linkerror event interval expired
322273 Interconnect-1N00 switch_module error 1083197259 1 Linkerror event interval expired
317191 Interconnect-0N00 switch_module error 1083128576 1 Linkerror event interval expired
341260 Interconnect-0N01 switch_module error 1083655512 1 Linkerror event interval expired
350143 Interconnect-1T02 switch_module error 1084622968 1 Linkerror event interval expired
345658 Interconnect-1N03 switch_module error 1084315496 1 Linkerror event interval expired
362580 Interconnect-1N03 switch_module error 1085071662 1 Linkerror event interval expired
362382 Interconnect-0N02 switch_module error 1085071601 1 Linkerror event interval expired
362262 Interconnect-1N02 switch_module error 1085071576 1 Linkerror event interval expired
362202 Interconnect-0N02 switch_module error 1085071561 1 Linkerror event interval expired
361739 Interconnect-1N03 switch_module error 1085071397 1 Linkerror event interval expired
360648 Interconnect-0N02 switch_module error 1085071054 1 Linkerror event interval expired
380245 Interconnect-0N03 switch_module error 1086097126 1 Linkerror event interval expired
385551 Interconnect-0N03 switch_module error 1087072863 1 Linkerror event interval expired
384539 Interconnect-0N01 switch_module error 1086776851 1 Linkerror event interval expired
384532 Interconnect-1N01 switch_module error 1086776710 1 Linkerror event interval expired
387955 Interconnect-1N03 switch_module error 1087504540 1 Linkerror event interval expired
394811 Interconnect-1N03 switch_module error 1088833837 1 Linkerror event interval expired
394805 Interconnect-0N03 switch_module error 1088833780 1 Linkerror event interval expired
395454 Interconnect-1T02 switch_module error 1089054435 1 Linkerror event interval expired
403369 Interconnect-0N00 switch_module error 1089897372 1 Linkerror event interval expired
399843 Interconnect-1N03 switch_module error 1089893840 1 Linkerror event interval expired
399137 Interconnect-0N00 switch_module error 1089893708 1 Linkerror event interval expired
399095 Interconnect-0N00 switch_module error 1089893696 1 Linkerror event interval expired
402900 Interconnect-0N00 switch_module error 1089897286 1 Linkerror event interval expired
402752 Interconnect-1N02 switch_module error 1089897180 1 Linkerror event interval expired
411450 Interconnect-1T03 switch_module error 1091668219 1 Linkerror event interval expired
412053 Interconnect-1N03 switch_module error 1092010383 1 Linkerror event interval expired
416294 Interconnect-0N01 switch_module error 1093123113 1 Linkerror event interval expired
416218 Interconnect-1T01 switch_module error 1093072443 1 Linkerror event interval expired
418117 Interconnect-1N00 switch_module error 1093628496 1 Linkerror event interval expired
416807 Interconnect-1N00 switch_module error 1093353016 1 Linkerror event interval expired
422997 Interconnect-1N00 switch_module error 1094756198 1 Linkerror event interval expired
422170 Interconnect-1T01 switch_module error 1094723305 1 Linkerror event interval expired
422093 Interconnect-1T01 switch_module error 1094689661 1 Linkerror event interval expired
421655 Interconnect-0N00 switch_module error 1094591249 1 Linkerror event interval expired
421555 Interconnect-1N00 switch_module error 1094587409 1 Linkerror event interval expired
421509 Interconnect-0N00 switch_module error 1094585825 1 Linkerror event interval expired
429612 Interconnect-1N03 switch_module error 1095347033 1 Linkerror event interval expired
428659 Interconnect-1N03 switch_module error 1095346765 1 Linkerror event interval expired
426750 Interconnect-1N00 switch_module error 1095346268 1 Linkerror event interval expired
434433 Interconnect-0N00 switch_module error 1095798699 1 Linkerror event interval expired
72725 3398 boot_cmd error 1108648264 1 Failed subcommands 3406
114402 Interconnect-1N00 switch_module error 1111072635 1 Linkerror event interval expired
112502 Interconnect-1N00 switch_module error 1111072142 1 Linkerror event interval expired
111911 Interconnect-1N02 switch_module error 1111071995 1 Linkerror event interval expired
111529 Interconnect-0N00 switch_module error 1111071796 1 Linkerror event interval expired
111084 Interconnect-1N03 switch_module error 1111071109 1 Linkerror event interval expired
134429 Interconnect-0N03 switch_module error 1111862374 1 Linkerror event interval expired
128445 Interconnect-0N01 switch_module error 1111848291 1 Linkerror event interval expired
126458 Interconnect-0N03 switch_module error 1111805418 1 Linkerror event interval expired
146683 Interconnect-0N03 switch_module error 1112477787 1 Linkerror event interval expired
163616 Interconnect-1N02 switch_module error 1114093896 1 Linkerror event interval expired
165338 Interconnect-0N00 switch_module error 1114095980 1 Linkerror event interval expired
177431 Interconnect-1N00 switch_module error 1114545444 1 Linkerror event interval expired
207120 Interconnect-0N02 switch_module error 1116534650 1 link errors remain current
285082 Interconnect-0N02 switch_module error 1120712655 1 link errors remain current
66621 Interconnect-1T01 switch_module error 1127018764 1 link errors remain current
74687 Interconnect-1T01 switch_module error 1127530580 1 link errors remain current
73130 Interconnect-1T01 switch_module error 1127205865 1 link errors remain current
80945 Interconnect-1T01 switch_module error 1127633812 1 link errors remain current
90917 Interconnect-1T01 switch_module error 1128098598 1 link errors remain current
88545 Interconnect-1T01 switch_module error 1127812228 1 link errors remain current
112591 Interconnect-1T01 switch_module error 1128795669 1 link errors remain current
106636 Interconnect-1N01 switch_module error 1128546820 1 link errors remain current
106563 Interconnect-0N03 switch_module error 1128538056 1 link errors remain current
119191 Interconnect-1T01 switch_module error 1129156719 1 link errors remain current
117927 Interconnect-1T01 switch_module error 1129005435 1 link errors remain current
163591 Interconnect-1T01 switch_module error 1130665941 1 link errors remain current
155348 Interconnect-1T01 switch_module error 1130288994 1 link errors remain current
154298 Interconnect-1T01 switch_module error 1130114502 1 link errors remain current
163678 Interconnect-1T01 switch_module error 1130673545 1 link errors remain current
167309 Interconnect-1T01 switch_module error 1130861712 1 link errors remain current
166452 Interconnect-0N03 switch_module error 1130779845 1 link errors remain current
165502 Interconnect-1T01 switch_module error 1130700556 1 link errors remain current
165392 Interconnect-1T01 switch_module error 1130688133 1 link errors remain current
197294 Interconnect-0N01 switch_module error 1131268024 1 link errors remain current
212267 Interconnect-1T01 switch_module error 1131857400 1 link errors remain current
203897 Interconnect-0N01 switch_module error 1131386680 1 link errors remain current
203650 Interconnect-0N01 switch_module error 1131341748 1 link errors remain current
203182 Interconnect-1T01 switch_module error 1131293685 1 link errors remain current
227118 Interconnect-1N00 switch_module error 1132212763 1 link errors remain current
229109 Interconnect-1N00 switch_module error 1132369090 1 link errors remain current
236414 Interconnect-1T01 switch_module error 1132422989 1 link errors remain current
236835 Interconnect-1T01 switch_module error 1132474878 1 link errors remain current
244496 Interconnect-1T01 switch_module error 1133000231 1 link errors remain current
244394 Interconnect-1T01 switch_module error 1132933346 1 link errors remain current
262885 Interconnect-1T01 switch_module error 1133518990 1 link errors remain current
261997 Interconnect-1T01 switch_module error 1133387468 1 link errors remain current
280293 Interconnect-1T01 switch_module error 1134279282 1 link errors remain current
292522 Interconnect-1T01 switch_module error 1134758547 1 link errors remain current
298653 Interconnect-1T01 switch_module error 1134866941 1 link errors remain current
307604 Interconnect-1T01 switch_module error 1135453027 1 link errors remain current
304336 Interconnect-1T01 switch_module error 1135418418 1 link errors remain current
303787 Interconnect-1T01 switch_module error 1135214320 1 link errors remain current
303668 Interconnect-1T01 switch_module error 1135179178 1 link errors remain current
314828 Interconnect-1T01 switch_module error 1138020376 1 link errors remain current
335192 Interconnect-1N03 switch_module error 1140228727 1 link errors remain current
335191 Interconnect-0N03 switch_module error 1140228727 1 link errors remain current
327915 Interconnect-0N02 switch_module error 1140099136 1 link errors remain current
379417 Interconnect-1N00 switch_module error 1141791316 1 link errors remain current
379300 Interconnect-1N00 switch_module error 1141770248 1 link errors remain current
379111 Interconnect-0T01 switch_module error 1141726583 1 link errors remain current
396457 Interconnect-1N00 switch_module error 1142527572 1 link errors remain current
415687 Interconnect-0T00 switch_module error 1143027001 1 link errors remain current
414825 Interconnect-1N00 switch_module error 1142865108 1 link errors remain current
414569 Interconnect-1N02 switch_module error 1142843014 1 link errors remain current
414493 Interconnect-1N02 switch_module error 1142837578 1 link errors remain current
414390 Interconnect-1N00 switch_module error 1142825359 1 link errors remain current
443580 Interconnect-1T01 switch_module error 1144142493 1 link errors remain current
479886 Interconnect-1T02 switch_module error 1146011009 1 link errors remain current
2573188 node-129 unix.hw net.niff.up 1074131255 1 NIFF: node node-129 has detected an available network connection on network 10.128.0.0 via interface ee0
2573624 node-148 unix.hw net.niff.up 1074131743 1 NIFF: node node-148 has detected an available network connection on network 5.5.226.0 via interface alt0
2576312 node-24 unix.hw net.niff.up 1074178812 1 NIFF: node node-24 has detected an available network connection on network 10.128.0.0 via interface ee0
2576662 node-232 unix.hw net.niff.up 1074179408 1 NIFF: node node-232 has detected an available network connection on network 10.128.0.0 via interface ee0
2587654 node-17 unix.hw net.niff.up 1074241968 1 NIFF: node node-17 has detected an available network connection on network 10.128.0.0 via interface ee0
2589343 node-241 unix.hw net.niff.up 1074243421 1 NIFF: node node-241 has detected an available network connection on network 10.128.0.0 via interface ee0
2590507 node-26 unix.hw net.niff.up 1074262519 1 NIFF: node node-26 has detected an available network connection on network 5.5.226.0 via interface alt0
2601700 node-120 unix.hw net.niff.up 1074298522 1 NIFF: node node-120 has detected an available network connection on network 5.5.226.0 via interface alt0
2602507 node-142 unix.hw net.niff.up 1074300633 1 NIFF: node node-142 has detected an available network connection on network 0.0.0.0 via interface alt0
2612551 node-73 unix.hw net.niff.up 1074535576 1 NIFF: node node-73 has detected an available network connection on network 5.5.226.0 via interface alt0
2599118 node-252 unix.hw net.niff.up 1074296980 1 NIFF: node node-252 has detected an available network connection on network 5.5.226.0 via interface alt0
2600073 node-173 unix.hw net.niff.up 1074297838 1 NIFF: node node-173 has detected an available network connection on network 10.128.0.0 via interface ee0
2601486 node-189 unix.hw net.niff.up 1074298415 1 NIFF: node node-189 has detected an available network connection on network 10.128.0.0 via interface ee0
2602451 node-163 unix.hw net.niff.up 1074300629 1 NIFF: node node-163 has detected an available network connection on network 0.0.0.0 via interface alt0
41331 node-186 unix.hw net.niff.up 1076538895 1 NIFF: node node-186 has detected an available network connection on network 5.5.226.0 via interface alt0
40578 node-205 unix.hw net.niff.up 1076538681 1 NIFF: node node-205 has detected an available network connection on network 10.128.0.0 via interface ee0
61810 node-155 unix.hw net.niff.up 1076866090 1 NIFF: node node-155 has detected an available network connection on network 10.128.0.0 via interface ee0
106492 node-176 unix.hw net.niff.up 1077739982 1 NIFF: node node-176 has detected an available network connection on network 5.5.226.0 via interface alt0
105447 node-75 unix.hw net.niff.up 1077739647 1 NIFF: node node-75 has detected an available network connection on network 10.128.0.0 via interface ee0
138243 node-9 unix.hw net.niff.up 1077809985 1 NIFF: node node-9 has detected an available network connection on network 10.128.0.0 via interface ee0
138312 node-237 unix.hw net.niff.up 1077810015 1 NIFF: node node-237 has detected an available network connection on network 5.5.226.0 via interface alt0
138478 node-208 unix.hw net.niff.up 1077810112 1 NIFF: node node-208 has detected an available network connection on network 10.128.0.0 via interface ee0
139925 node-157 unix.hw net.niff.up 1077810564 1 NIFF: node node-157 has detected an available network connection on network 5.5.226.0 via interface alt0
144893 node-139 unix.hw net.niff.up 1077817112 1 NIFF: node node-139 has detected an available network connection on network 10.128.0.0 via interface ee0
145471 node-84 unix.hw net.niff.up 1077817325 1 NIFF: node node-84 has detected an available network connection on network 10.128.0.0 via interface ee0
156756 node-217 unix.hw net.niff.up 1077865700 1 NIFF: node node-217 has detected an available network connection on network 10.128.0.0 via interface ee0
156907 node-214 unix.hw net.niff.up 1077865914 1 NIFF: node node-214 has detected an available network connection on network 5.5.226.0 via interface alt0
161226 node-202 unix.hw net.niff.up 1077876101 1 NIFF: node node-202 has detected an available network connection on network 10.128.0.0 via interface ee0
163923 node-254 unix.hw net.niff.up 1077879014 1 NIFF: node node-254 has detected an available network connection on network 10.128.0.0 via interface ee0
185300 node-234 unix.hw net.niff.up 1077902844 1 NIFF: node node-234 has detected an available network connection on network 10.128.0.0 via interface ee0
186232 node-244 unix.hw net.niff.up 1077903095 1 NIFF: node node-244 has detected an available network connection on network 10.128.0.0 via interface ee0
257478 node-30 unix.hw net.niff.up 1079070455 1 NIFF: node node-30 has detected an available network connection on network 10.128.0.0 via interface ee0
252431 node-130 unix.hw net.niff.up 1079068798 1 NIFF: node node-130 has detected an available network connection on network 10.96.0.0 via interface scip0
245674 node-45 unix.hw net.niff.up 1079048409 1 NIFF: node node-45 has detected an available network connection on network 5.5.224.0 via interface alt0
274822 node-44 unix.hw net.niff.up 1079616738 1 NIFF: node node-44 has detected an available network connection on network 10.128.0.0 via interface ee0
276361 node-151 unix.hw net.niff.up 1079617105 1 NIFF: node node-151 has detected an available network connection on network 5.5.224.0 via interface alt0
277244 node-255 unix.hw net.niff.up 1079617522 1 NIFF: node node-255 has detected an available network connection on network 10.128.0.0 via interface ee0
299686 node-188 unix.hw net.niff.up 1081818666 1 NIFF: node node-188 has detected an available network connection on network 10.128.0.0 via interface ee0
298912 node-29 unix.hw net.niff.up 1081814578 1 NIFF: node node-29 has detected an available network connection on network 0.0.0.0 via interface alt0
298129 node-87 unix.hw net.niff.up 1081759726 1 NIFF: node node-87 has detected an available network connection on network 5.5.224.0 via interface alt0
308248 node-203 unix.hw net.niff.up 1082038683 1 NIFF: node node-203 has detected an available network connection on network 5.5.224.0 via interface alt0
326992 node-72 unix.hw net.niff.up 1083202727 1 NIFF: node node-72 has detected an available network connection on network 10.128.0.0 via interface ee0
326478 node-113 unix.hw net.niff.up 1083202436 1 NIFF: node node-113 has detected an available network connection on network 5.5.224.0 via interface alt0
363203 node-152 unix.hw net.niff.up 1085071937 1 NIFF: node node-152 has detected an available network connection on network 10.128.0.0 via interface ee0
363108 node-104 unix.hw net.niff.up 1085071904 1 NIFF: node node-104 has detected an available network connection on network 5.5.224.0 via interface alt0
361177 node-142 unix.hw net.niff.up 1085071269 1 NIFF: node node-142 has detected an available network connection on network 10.128.0.0 via interface ee0
389636 node-193 unix.hw net.niff.up 1087510100 1 NIFF: node node-193 has detected an available network connection on network 5.5.224.0 via interface alt0
403516 node-139 unix.hw net.niff.up 1089897394 1 NIFF: node node-139 has detected an available network connection on network 5.5.224.0 via interface alt0
411578 node-143 unix.hw net.niff.up 1091737151 1 NIFF: node node-143 has detected an available network connection on network 5.5.224.0 via interface alt0
427679 node-45 unix.hw net.niff.up 1095346545 1 NIFF: node node-45 has detected an available network connection on network 10.128.0.0 via interface ee0
434678 node-106 unix.hw net.niff.up 1095875337 1 NIFF: node node-106 has detected an available network connection on network 0.0.0.0 via interface alt0
437276 node-7 unix.hw net.niff.up 1096995297 1 NIFF: node node-7 has detected an available network connection on network 5.5.224.0 via interface alt0
444181 node-233 unix.hw net.niff.up 1098390018 1 NIFF: node node-233 has detected an available network connection on network 10.128.0.0 via interface ee0
451976 node-23 unix.hw net.niff.up 1098829410 1 NIFF: node node-23 has detected an available network connection on network 10.128.0.0 via interface ee0
21611 node-190 unix.hw net.niff.up 1100786337 1 NIFF: node node-190 has detected an available network connection on network 10.128.0.0 via interface ee0
71579 node-84 unix.hw net.niff.up 1108647602 1 NIFF: node node-84 has detected an available network connection on network 10.128.0.0 via interface ee0
71997 node-19 unix.hw net.niff.up 1108647796 1 NIFF: node node-19 has detected an available network connection on network 10.128.0.0 via interface ee0
72634 node-253 unix.hw net.niff.up 1108648087 1 NIFF: node node-253 has detected an available network connection on network 10.128.0.0 via interface ee0
72710 node-29 unix.hw net.niff.up 1108648186 1 NIFF: node node-29 has detected an available network connection on network 10.128.0.0 via interface ee0
113455 node-240 unix.hw net.niff.up 1111072373 1 NIFF: node node-240 has detected an available network connection on network 5.5.224.0 via interface alt0
113162 node-86 unix.hw net.niff.up 1111072309 1 NIFF: node node-86 has detected an available network connection on network 10.128.0.0 via interface ee0
112896 node-204 unix.hw net.niff.up 1111072255 1 NIFF: node node-204 has detected an available network connection on network 5.5.224.0 via interface alt0
112456 node-46 unix.hw net.niff.up 1111072123 1 NIFF: node node-46 has detected an available network connection on network 5.5.224.0 via interface alt0
136580 node-150 unix.hw net.niff.up 1111865217 1 NIFF: node node-150 has detected an available network connection on network 5.5.224.0 via interface alt0
130081 node-42 unix.hw net.niff.up 1111856616 1 NIFF: node node-42 has detected an available network connection on network 10.128.0.0 via interface ee0
167108 node-91 unix.hw net.niff.up 1114096578 1 NIFF: node node-91 has detected an available network connection on network 5.5.224.0 via interface alt0
178019 node-26 unix.hw net.niff.up 1114547033 1 NIFF: node node-26 has detected an available network connection on network 10.128.0.0 via interface ee0
205376 node-44 unix.hw net.niff.up 1116531677 1 NIFF: node node-44 has detected an available network connection on network 5.5.224.0 via interface alt0
206244 node-244 unix.hw net.niff.up 1116532845 1 NIFF: node node-244 has detected an available network connection on network 5.5.224.0 via interface alt0
209845 node-182 unix.hw net.niff.up 1116601699 1 NIFF: node node-182 has detected an available network connection on network 10.128.0.0 via interface ee0
211427 node-144 unix.hw net.niff.up 1116610182 1 NIFF: node node-144 has detected an available network connection on network 10.96.0.0 via interface scip0
225253 node-17 unix.hw net.niff.up 1117296958 1 NIFF: node node-17 has detected an available network connection on network 0.0.0.0 via interface alt0
225060 node-69 unix.hw net.niff.up 1117295720 1 NIFF: node node-69 has detected an available network connection on network 10.128.0.0 via interface ee0
244823 node-202 unix.hw net.niff.up 1118546476 1 NIFF: node node-202 has detected an available network connection on network 5.5.224.0 via interface alt0
244815 node-205 unix.hw net.niff.up 1118546462 1 NIFF: node node-205 has detected an available network connection on network 5.5.224.0 via interface alt0
252295 node-44 unix.hw net.niff.up 1118929048 1 NIFF: node node-44 has detected an available network connection on network 5.5.224.0 via interface alt0
253483 node-183 unix.hw net.niff.up 1118929407 1 NIFF: node node-183 has detected an available network connection on network 10.128.0.0 via interface ee0
253628 node-212 unix.hw net.niff.up 1118929438 1 NIFF: node node-212 has detected an available network connection on network 10.128.0.0 via interface ee0
254151 node-187 unix.hw net.niff.up 1118929744 1 NIFF: node node-187 has detected an available network connection on network 5.5.224.0 via interface alt0
254193 node-221 unix.hw net.niff.up 1118929750 1 NIFF: node node-221 has detected an available network connection on network 5.5.224.0 via interface alt0
55567 node-189 unix.hw net.niff.up 1126813684 1 NIFF: node node-189 has detected an available network connection on network 5.5.224.0 via interface alt0
55724 node-218 unix.hw net.niff.up 1126813843 1 NIFF: node node-218 has detected an available network connection on network 5.5.224.0 via interface alt0
171898 node-94 unix.hw net.niff.up 1131216735 1 NIFF: node node-94 has detected an available network connection on network 5.5.224.0 via interface alt0
171611 node-248 unix.hw net.niff.up 1131216568 1 NIFF: node node-248 has detected an available network connection on network 10.128.0.0 via interface ee0
171259 node-176 unix.hw net.niff.up 1131216398 1 NIFF: node node-176 has detected an available network connection on network 10.128.0.0 via interface ee0
193084 node-77 unix.hw net.niff.up 1131241578 1 NIFF: node node-77 has detected an available network connection on network 5.5.224.0 via interface alt0
193772 node-176 unix.hw net.niff.up 1131242001 1 NIFF: node node-176 has detected an available network connection on network 10.128.0.0 via interface ee0
219086 node-178 unix.hw net.niff.up 1132154180 1 NIFF: node node-178 has detected an available network connection on network 0.0.0.0 via interface alt0
219199 node-161 unix.hw net.niff.up 1132154183 1 NIFF: node node-161 has detected an available network connection on network 0.0.0.0 via interface alt0
219402 node-73 unix.hw net.niff.up 1132154292 1 NIFF: node node-73 has detected an available network connection on network 0.0.0.0 via interface alt0
288035 node-171 unix.hw net.niff.up 1134671139 1 NIFF: node node-171 has detected an available network connection on network 10.128.0.0 via interface ee0

[Sun Dec 04 20:28:39 2005] [notice] jk2_init() Found child 1966 in scoreboard slot 6
[Sun Dec 04 20:28:39 2005] [notice] jk2_init() Found child 1967 in scoreboard slot 9
[Sun Dec 04 20:28:39 2005] [notice] jk2_init() Found child 1965 in scoreboard slot 8
[Sun Dec 04 20:29:34 2005] [notice] jk2_init() Found child 1970 in scoreboard slot 6
[Sun Dec 04 20:30:59 2005] [notice] jk2_init() Found child 1984 in scoreboard slot 10
[Sun Dec 04 20:31:35 2005] [notice] jk2_init() Found child 1990 in scoreboard slot 9
[Sun Dec 04 20:32:37 2005] [notice] jk2_init() Found child 1999 in scoreboard slot 6
[Sun Dec 04 20:32:37 2005] [notice] jk2_init() Found child 2000 in scoreboard slot 7
[Sun Dec 04 20:32:37 2005] [notice] jk2_init() Found child 1998 in scoreboard slot 9
[Sun Dec 04 20:32:50 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:32:50 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:32:50 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:32:55 2005] [error] mod_jk child workerEnv in error state 10
[Sun Dec 04 20:32:55 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:32:55 2005] [error] mod_jk child workerEnv in error state 10
[Sun Dec 04 20:33:35 2005] [notice] jk2_init() Found child 2002 in scoreboard slot 8
[Sun Dec 04 20:33:35 2005] [notice] jk2_init() Found child 2001 in scoreboard slot 9
[Sun Dec 04 20:33:47 2005] [notice] jk2_init() Found child 2005 in scoreboard slot 7
[Sun Dec 04 20:33:47 2005] [notice] jk2_init() Found child 2004 in scoreboard slot 6
[Sun Dec 04 20:34:13 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:34:14 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:34:20 2005] [notice] jk2_init() Found child 2007 in scoreboard slot 8
[Sun Dec 04 20:34:20 2005] [notice] jk2_init() Found child 2006 in scoreboard slot 9
[Sun Dec 04 20:34:21 2005] [notice] jk2_init() Found child 2008 in scoreboard slot 6
[Sun Dec 04 20:34:25 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:34:25 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 20:34:25 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:34:25 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 20:34:25 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:34:25 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 20:37:29 2005] [notice] jk2_init() Found child 2028 in scoreboard slot 9
[Sun Dec 04 20:37:29 2005] [notice] jk2_init() Found child 2027 in scoreboard slot 7
[Sun Dec 04 20:37:29 2005] [notice] jk2_init() Found child 2029 in scoreboard slot 8
[Sun Dec 04 20:37:46 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:37:46 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:37:49 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:37:49 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:38:10 2005] [notice] jk2_init() Found child 2030 in scoreboard slot 6
[Sun Dec 04 20:38:10 2005] [notice] jk2_init() Found child 2031 in scoreboard slot 7
[Sun Dec 04 20:38:11 2005] [notice] jk2_init() Found child 2032 in scoreboard slot 9
[Sun Dec 04 20:38:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:38:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:38:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:38:14 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:38:14 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:38:14 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 20:41:12 2005] [notice] jk2_init() Found child 2042 in scoreboard slot 8
[Sun Dec 04 20:41:47 2005] [notice] jk2_init() Found child 2045 in scoreboard slot 9
[Sun Dec 04 20:42:42 2005] [notice] jk2_init() Found child 2051 in scoreboard slot 8
[Sun Dec 04 20:44:29 2005] [notice] jk2_init() Found child 2059 in scoreboard slot 7
[Sun Dec 04 20:44:29 2005] [notice] jk2_init() Found child 2060 in scoreboard slot 9
[Sun Dec 04 20:44:30 2005] [notice] jk2_init() Found child 2061 in scoreboard slot 8
[Sun Dec 04 20:47:16 2005] [notice] jk2_init() Found child 2081 in scoreboard slot 6
[Sun Dec 04 20:47:16 2005] [error] jk2_init() Can't find child 2082 in scoreboard
[Sun Dec 04 20:47:16 2005] [notice] jk2_init() Found child 2083 in scoreboard slot 8
[Sun Dec 04 20:47:16 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:47:16 2005] [error] mod_jk child workerEnv in error state 6
[Sun Dec 04 20:47:16 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:47:16 2005] [error] mod_jk child init 1 -2
[Sun Dec 04 20:47:16 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:47:16 2005] [error] mod_jk child workerEnv in error state 9
[Sun Dec 04 20:47:17 2005] [error] jk2_init() Can't find child 2085 in scoreboard
[Sun Dec 04 20:47:17 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:47:17 2005] [error] mod_jk child init 1 -2
[Sun Dec 04 20:47:17 2005] [error] jk2_init() Can't find child 2086 in scoreboard
[Sun Dec 04 20:47:17 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:47:17 2005] [error] mod_jk child init 1 -2
[Sun Dec 04 20:47:17 2005] [error] jk2_init() Can't find child 2087 in scoreboard
[Sun Dec 04 20:47:17 2005] [notice] jk2_init() Found child 2084 in scoreboard slot 9
[Sun Dec 04 20:47:17 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:47:17 2005] [error] mod_jk child workerEnv in error state 7
[Sun Dec 04 20:47:17 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Sun Dec 04 20:47:17 2005] [error] mod_jk child init 1 -2
[Mon Dec 05 01:04:31 2005] [error] [client 218.62.18.218] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 01:30:32 2005] [error] [client 211.62.201.48] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 03:21:00 2005] [notice] jk2_init() Found child 2760 in scoreboard slot 6
[Mon Dec 05 03:21:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:21:02 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:23:21 2005] [notice] jk2_init() Found child 2763 in scoreboard slot 7
[Mon Dec 05 03:23:24 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:23:24 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:23:24 2005] [error] [client 218.207.61.7] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 03:25:44 2005] [notice] jk2_init() Found child 2773 in scoreboard slot 6
[Mon Dec 05 03:25:46 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:25:46 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:36:51 2005] [notice] jk2_init() Found child 2813 in scoreboard slot 7
[Mon Dec 05 03:36:51 2005] [notice] jk2_init() Found child 2815 in scoreboard slot 8
[Mon Dec 05 03:36:51 2005] [notice] jk2_init() Found child 2812 in scoreboard slot 6
[Mon Dec 05 03:36:51 2005] [notice] jk2_init() Found child 2811 in scoreboard slot 9
[Mon Dec 05 03:36:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:36:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:36:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:36:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:36:57 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:36:57 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:36:57 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:36:57 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:40:46 2005] [notice] jk2_init() Found child 2823 in scoreboard slot 9
[Mon Dec 05 03:40:55 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:40:55 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:44:50 2005] [notice] jk2_init() Found child 2824 in scoreboard slot 10
[Mon Dec 05 03:44:50 2005] [error] [client 168.20.198.21] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 03:44:50 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:44:50 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:46:38 2005] [notice] jk2_init() Found child 2838 in scoreboard slot 10
[Mon Dec 05 03:46:38 2005] [notice] jk2_init() Found child 2836 in scoreboard slot 9
[Mon Dec 05 03:46:38 2005] [notice] jk2_init() Found child 2837 in scoreboard slot 6
[Mon Dec 05 03:46:50 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:47:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:47:19 2005] [notice] jk2_init() Found child 2840 in scoreboard slot 8
[Mon Dec 05 03:47:19 2005] [notice] jk2_init() Found child 2841 in scoreboard slot 6
[Mon Dec 05 03:47:19 2005] [notice] jk2_init() Found child 2842 in scoreboard slot 9
[Mon Dec 05 03:47:53 2005] [notice] jk2_init() Found child 2846 in scoreboard slot 9
[Mon Dec 05 03:47:53 2005] [notice] jk2_init() Found child 2843 in scoreboard slot 7
[Mon Dec 05 03:47:53 2005] [notice] jk2_init() Found child 2844 in scoreboard slot 8
[Mon Dec 05 03:47:53 2005] [notice] jk2_init() Found child 2845 in scoreboard slot 6
[Mon Dec 05 03:47:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:47:54 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 03:47:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:47:54 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 03:47:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:47:54 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 03:47:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:47:54 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:50:49 2005] [notice] jk2_init() Found child 2857 in scoreboard slot 9
[Mon Dec 05 03:50:50 2005] [notice] jk2_init() Found child 2854 in scoreboard slot 7
[Mon Dec 05 03:50:49 2005] [notice] jk2_init() Found child 2855 in scoreboard slot 8
[Mon Dec 05 03:50:49 2005] [notice] jk2_init() Found child 2856 in scoreboard slot 6
[Mon Dec 05 03:50:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:50:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:50:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:50:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:50:59 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:50:59 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:50:59 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:50:59 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:56:12 2005] [notice] jk2_init() Found child 2866 in scoreboard slot 7
[Mon Dec 05 03:56:12 2005] [notice] jk2_init() Found child 2867 in scoreboard slot 8
[Mon Dec 05 03:56:12 2005] [notice] jk2_init() Found child 2865 in scoreboard slot 9
[Mon Dec 05 03:56:12 2005] [notice] jk2_init() Found child 2864 in scoreboard slot 6
[Mon Dec 05 03:56:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:56:15 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:56:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:56:15 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:56:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:56:15 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 03:56:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 03:56:15 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 04:00:55 2005] [notice] jk2_init() Found child 2877 in scoreboard slot 10
[Mon Dec 05 04:01:18 2005] [notice] jk2_init() Found child 2883 in scoreboard slot 9
[Mon Dec 05 04:01:18 2005] [notice] jk2_init() Found child 2878 in scoreboard slot 7
[Mon Dec 05 04:01:18 2005] [notice] jk2_init() Found child 2880 in scoreboard slot 8
[Mon Dec 05 04:01:18 2005] [notice] jk2_init() Found child 2879 in scoreboard slot 6
[Mon Dec 05 04:01:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:01:23 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 04:01:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:01:23 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 04:01:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:01:23 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 04:01:23 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:01:23 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 04:06:19 2005] [notice] jk2_init() Found child 3667 in scoreboard slot 7
[Mon Dec 05 04:06:19 2005] [notice] jk2_init() Found child 3669 in scoreboard slot 6
[Mon Dec 05 04:06:27 2005] [notice] jk2_init() Found child 3670 in scoreboard slot 8
[Mon Dec 05 04:06:43 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:06:43 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:06:45 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 04:06:45 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 04:07:23 2005] [notice] jk2_init() Found child 3672 in scoreboard slot 7
[Mon Dec 05 04:07:37 2005] [notice] jk2_init() Found child 3673 in scoreboard slot 6
[Mon Dec 05 04:07:48 2005] [notice] jk2_init() Found child 3675 in scoreboard slot 9
[Mon Dec 05 04:07:48 2005] [notice] jk2_init() Found child 3674 in scoreboard slot 8
[Mon Dec 05 04:08:37 2005] [notice] jk2_init() Found child 3678 in scoreboard slot 8
[Mon Dec 05 04:08:37 2005] [notice] jk2_init() Found child 3681 in scoreboard slot 6
[Mon Dec 05 04:08:37 2005] [notice] jk2_init() Found child 3679 in scoreboard slot 9
[Mon Dec 05 04:08:37 2005] [notice] jk2_init() Found child 3680 in scoreboard slot 7
[Mon Dec 05 04:08:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:09:32 2005] [notice] jk2_init() Found child 3685 in scoreboard slot 6
[Mon Dec 05 04:10:47 2005] [notice] jk2_init() Found child 3698 in scoreboard slot 9
[Mon Dec 05 04:10:47 2005] [notice] jk2_init() Found child 3690 in scoreboard slot 6
[Mon Dec 05 04:10:47 2005] [notice] jk2_init() Found child 3691 in scoreboard slot 8
[Mon Dec 05 04:13:54 2005] [notice] jk2_init() Found child 3744 in scoreboard slot 6
[Mon Dec 05 04:13:54 2005] [notice] jk2_init() Found child 3747 in scoreboard slot 8
[Mon Dec 05 04:13:54 2005] [notice] jk2_init() Found child 3754 in scoreboard slot 12
[Mon Dec 05 04:13:54 2005] [notice] jk2_init() Found child 3755 in scoreboard slot 13
[Mon Dec 05 04:13:54 2005] [notice] jk2_init() Found child 3753 in scoreboard slot 10
[Mon Dec 05 04:13:54 2005] [notice] jk2_init() Found child 3752 in scoreboard slot 9
[Mon Dec 05 04:13:54 2005] [notice] jk2_init() Found child 3746 in scoreboard slot 7
[Mon Dec 05 04:14:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:14:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:14:00 2005] [error] mod_jk child workerEnv in error state 9
[Mon Dec 05 04:14:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:14:00 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 04:14:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:14:00 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 04:14:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:14:00 2005] [error] mod_jk child workerEnv in error state 9
[Mon Dec 05 04:14:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:14:00 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 04:14:00 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 04:14:00 2005] [error] mod_jk child workerEnv in error state 10
[Mon Dec 05 04:14:00 2005] [error] mod_jk child workerEnv in error state 9
[Mon Dec 05 05:06:42 2005] [notice] jk2_init() Found child 4596 in scoreboard slot 8
[Mon Dec 05 05:06:42 2005] [notice] jk2_init() Found child 4595 in scoreboard slot 7
[Mon Dec 05 05:06:42 2005] [notice] jk2_init() Found child 4594 in scoreboard slot 6
[Mon Dec 05 05:06:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 05:06:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 05:06:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 05:06:47 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 05:06:47 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 05:06:47 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 05:11:04 2005] [notice] jk2_init() Found child 4609 in scoreboard slot 7
[Mon Dec 05 05:11:04 2005] [notice] jk2_init() Found child 4608 in scoreboard slot 6
[Mon Dec 05 05:11:34 2005] [notice] jk2_init() Found child 4611 in scoreboard slot 9
[Mon Dec 05 05:11:54 2005] [notice] jk2_init() Found child 4613 in scoreboard slot 7
[Mon Dec 05 05:11:54 2005] [notice] jk2_init() Found child 4612 in scoreboard slot 6
[Mon Dec 05 05:12:32 2005] [notice] jk2_init() Found child 4615 in scoreboard slot 9
[Mon Dec 05 05:12:56 2005] [notice] jk2_init() Found child 4616 in scoreboard slot 6
[Mon Dec 05 05:12:56 2005] [notice] jk2_init() Found child 4617 in scoreboard slot 7
[Mon Dec 05 05:12:56 2005] [notice] jk2_init() Found child 4618 in scoreboard slot 8
[Mon Dec 05 05:15:29 2005] [notice] jk2_init() Found child 4634 in scoreboard slot 6
[Mon Dec 05 05:15:29 2005] [notice] jk2_init() Found child 4637 in scoreboard slot 7
[Mon Dec 05 05:15:29 2005] [notice] jk2_init() Found child 4631 in scoreboard slot 9
[Mon Dec 05 05:15:29 2005] [notice] jk2_init() Found child 4630 in scoreboard slot 8
[Mon Dec 05 05:15:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 05:15:33 2005] [error] mod_jk child workerEnv in error state 9
[Mon Dec 05 05:15:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 05:15:33 2005] [error] mod_jk child workerEnv in error state 9
[Mon Dec 05 05:15:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 05:15:33 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 05:15:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 05:15:33 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 06:35:27 2005] [notice] jk2_init() Found child 4820 in scoreboard slot 8
[Mon Dec 05 06:35:27 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 06:35:27 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 06:36:58 2005] [notice] jk2_init() Found child 4821 in scoreboard slot 10
[Mon Dec 05 06:36:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 06:36:58 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 06:36:59 2005] [error] [client 221.232.178.24] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 07:16:00 2005] [notice] jk2_init() Found child 4893 in scoreboard slot 7
[Mon Dec 05 07:16:00 2005] [notice] jk2_init() Found child 4892 in scoreboard slot 6
[Mon Dec 05 07:16:03 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:16:03 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:16:03 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:16:03 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:21:03 2005] [notice] jk2_init() Found child 4907 in scoreboard slot 6
[Mon Dec 05 07:21:02 2005] [notice] jk2_init() Found child 4906 in scoreboard slot 9
[Mon Dec 05 07:21:02 2005] [notice] jk2_init() Found child 4905 in scoreboard slot 8
[Mon Dec 05 07:21:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:21:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:21:09 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:21:09 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:21:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:21:09 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:25:55 2005] [notice] jk2_init() Found child 4916 in scoreboard slot 8
[Mon Dec 05 07:25:55 2005] [notice] jk2_init() Found child 4917 in scoreboard slot 9
[Mon Dec 05 07:25:55 2005] [notice] jk2_init() Found child 4915 in scoreboard slot 7
[Mon Dec 05 07:25:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:25:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:25:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:26:00 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:26:00 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:26:00 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:31:22 2005] [notice] jk2_init() Found child 4932 in scoreboard slot 6
[Mon Dec 05 07:32:03 2005] [notice] jk2_init() Found child 4938 in scoreboard slot 8
[Mon Dec 05 07:32:03 2005] [notice] jk2_init() Found child 4935 in scoreboard slot 9
[Mon Dec 05 07:32:03 2005] [notice] jk2_init() Found child 4936 in scoreboard slot 6
[Mon Dec 05 07:32:03 2005] [notice] jk2_init() Found child 4937 in scoreboard slot 7
[Mon Dec 05 07:32:06 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:32:06 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:32:06 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:32:06 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:32:06 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:32:06 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:32:06 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:32:06 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:36:19 2005] [notice] jk2_init() Found child 4950 in scoreboard slot 7
[Mon Dec 05 07:37:47 2005] [notice] jk2_init() Found child 4961 in scoreboard slot 6
[Mon Dec 05 07:37:48 2005] [notice] jk2_init() Found child 4962 in scoreboard slot 7
[Mon Dec 05 07:37:48 2005] [notice] jk2_init() Found child 4960 in scoreboard slot 9
[Mon Dec 05 07:37:48 2005] [notice] jk2_init() Found child 4959 in scoreboard slot 8
[Mon Dec 05 07:37:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:37:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:37:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:37:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:37:58 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:37:58 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:37:58 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:37:58 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:41:07 2005] [notice] jk2_init() Found child 4974 in scoreboard slot 9
[Mon Dec 05 07:41:35 2005] [notice] jk2_init() Found child 4975 in scoreboard slot 6
[Mon Dec 05 07:41:50 2005] [notice] jk2_init() Found child 4977 in scoreboard slot 8
[Mon Dec 05 07:41:50 2005] [notice] jk2_init() Found child 4976 in scoreboard slot 7
[Mon Dec 05 07:43:07 2005] [notice] jk2_init() Found child 4984 in scoreboard slot 7
[Mon Dec 05 07:43:08 2005] [notice] jk2_init() Found child 4985 in scoreboard slot 10
[Mon Dec 05 07:43:07 2005] [notice] jk2_init() Found child 4983 in scoreboard slot 6
[Mon Dec 05 07:43:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:43:16 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 07:43:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:43:16 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 07:43:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:43:16 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:43:19 2005] [notice] jk2_init() Found child 4986 in scoreboard slot 8
[Mon Dec 05 07:43:19 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:43:19 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 07:46:01 2005] [notice] jk2_init() Found child 4991 in scoreboard slot 6
[Mon Dec 05 07:46:01 2005] [notice] jk2_init() Found child 4992 in scoreboard slot 7
[Mon Dec 05 07:46:46 2005] [notice] jk2_init() Found child 4996 in scoreboard slot 7
[Mon Dec 05 07:46:46 2005] [notice] jk2_init() Found child 4995 in scoreboard slot 6
[Mon Dec 05 07:47:13 2005] [notice] jk2_init() Found child 4998 in scoreboard slot 8
[Mon Dec 05 07:47:13 2005] [notice] jk2_init() Found child 4999 in scoreboard slot 6
[Mon Dec 05 07:47:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:47:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:47:21 2005] [notice] jk2_init() Found child 5000 in scoreboard slot 7
[Mon Dec 05 07:47:22 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 07:47:21 2005] [notice] jk2_init() Found child 5001 in scoreboard slot 9
[Mon Dec 05 07:47:23 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:47:36 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:47:36 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:47:40 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 07:47:40 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 07:48:04 2005] [notice] jk2_init() Found child 5002 in scoreboard slot 8
[Mon Dec 05 07:48:04 2005] [notice] jk2_init() Found child 5003 in scoreboard slot 6
[Mon Dec 05 07:48:46 2005] [notice] jk2_init() Found child 5005 in scoreboard slot 9
[Mon Dec 05 07:48:46 2005] [notice] jk2_init() Found child 5006 in scoreboard slot 8
[Mon Dec 05 07:48:55 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:48:55 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:48:55 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:48:55 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 07:48:56 2005] [notice] jk2_init() Found child 5007 in scoreboard slot 6
[Mon Dec 05 07:48:56 2005] [notice] jk2_init() Found child 5008 in scoreboard slot 7
[Mon Dec 05 07:48:56 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:48:56 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:48:56 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:48:56 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 07:50:54 2005] [notice] jk2_init() Found child 5017 in scoreboard slot 8
[Mon Dec 05 07:50:54 2005] [notice] jk2_init() Found child 5016 in scoreboard slot 9
[Mon Dec 05 07:51:22 2005] [notice] jk2_init() Found child 5018 in scoreboard slot 6
[Mon Dec 05 07:51:20 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:51:23 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:51:39 2005] [notice] jk2_init() Found child 5020 in scoreboard slot 9
[Mon Dec 05 07:51:39 2005] [notice] jk2_init() Found child 5019 in scoreboard slot 7
[Mon Dec 05 07:51:56 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:51:56 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:52:02 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:52:02 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 07:52:29 2005] [notice] jk2_init() Found child 5021 in scoreboard slot 8
[Mon Dec 05 07:52:29 2005] [notice] jk2_init() Found child 5022 in scoreboard slot 6
[Mon Dec 05 07:52:56 2005] [notice] jk2_init() Found child 5024 in scoreboard slot 9
[Mon Dec 05 07:52:56 2005] [notice] jk2_init() Found child 5023 in scoreboard slot 7
[Mon Dec 05 07:52:55 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:53:01 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 07:53:24 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:54:01 2005] [notice] jk2_init() Found child 5029 in scoreboard slot 8
[Mon Dec 05 07:54:02 2005] [notice] jk2_init() Found child 5030 in scoreboard slot 6
[Mon Dec 05 07:54:48 2005] [notice] jk2_init() Found child 5033 in scoreboard slot 8
[Mon Dec 05 07:54:48 2005] [notice] jk2_init() Found child 5032 in scoreboard slot 9
[Mon Dec 05 07:55:00 2005] [notice] jk2_init() Found child 5035 in scoreboard slot 7
[Mon Dec 05 07:55:00 2005] [notice] jk2_init() Found child 5034 in scoreboard slot 6
[Mon Dec 05 07:55:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:55:08 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 07:55:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:55:08 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 07:55:07 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:55:08 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 07:55:13 2005] [notice] jk2_init() Found child 5036 in scoreboard slot 9
[Mon Dec 05 07:57:01 2005] [notice] jk2_init() Found child 5050 in scoreboard slot 8
[Mon Dec 05 07:57:01 2005] [notice] jk2_init() Found child 5049 in scoreboard slot 7
[Mon Dec 05 07:57:01 2005] [notice] jk2_init() Found child 5048 in scoreboard slot 6
[Mon Dec 05 07:57:02 2005] [notice] jk2_init() Found child 5051 in scoreboard slot 9
[Mon Dec 05 07:57:02 2005] [error] jk2_init() Can't find child 5053 in scoreboard
[Mon Dec 05 07:57:02 2005] [error] jk2_init() Can't find child 5054 in scoreboard
[Mon Dec 05 07:57:02 2005] [notice] jk2_init() Found child 5052 in scoreboard slot 10
[Mon Dec 05 07:57:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:57:02 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:57:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:57:02 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:57:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:57:02 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:57:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:57:02 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 07:57:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:57:02 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 07:57:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:57:02 2005] [error] mod_jk child init 1 -2
[Mon Dec 05 07:57:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 07:57:02 2005] [error] mod_jk child init 1 -2
[Mon Dec 05 09:09:48 2005] [error] [client 207.12.15.211] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 09:36:13 2005] [notice] jk2_init() Found child 5271 in scoreboard slot 7
[Mon Dec 05 09:36:13 2005] [notice] jk2_init() Found child 5270 in scoreboard slot 6
[Mon Dec 05 09:36:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 09:36:14 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 09:36:14 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 09:36:14 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 09:55:21 2005] [notice] jk2_init() Found child 5295 in scoreboard slot 8
[Mon Dec 05 09:55:21 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 09:55:21 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:10:32 2005] [notice] jk2_init() Found child 5330 in scoreboard slot 9
[Mon Dec 05 10:10:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:10:33 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:16:20 2005] [notice] jk2_init() Found child 5344 in scoreboard slot 7
[Mon Dec 05 10:16:52 2005] [notice] jk2_init() Found child 5347 in scoreboard slot 6
[Mon Dec 05 10:16:53 2005] [notice] jk2_init() Found child 5348 in scoreboard slot 7
[Mon Dec 05 10:17:45 2005] [notice] jk2_init() Found child 5350 in scoreboard slot 9
[Mon Dec 05 10:17:45 2005] [notice] jk2_init() Found child 5349 in scoreboard slot 8
[Mon Dec 05 10:17:49 2005] [notice] jk2_init() Found child 5352 in scoreboard slot 7
[Mon Dec 05 10:17:50 2005] [notice] jk2_init() Found child 5351 in scoreboard slot 6
[Mon Dec 05 10:17:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:17:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:17:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:17:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:17:51 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:17:51 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:17:51 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:17:51 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:21:05 2005] [notice] jk2_init() Found child 5366 in scoreboard slot 9
[Mon Dec 05 10:21:05 2005] [notice] jk2_init() Found child 5365 in scoreboard slot 8
[Mon Dec 05 10:21:05 2005] [notice] jk2_init() Found child 5367 in scoreboard slot 6
[Mon Dec 05 10:21:07 2005] [notice] jk2_init() Found child 5368 in scoreboard slot 7
[Mon Dec 05 10:21:13 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:21:13 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:21:13 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:21:13 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:21:13 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:21:13 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:21:13 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:21:13 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:26:26 2005] [notice] jk2_init() Found child 5384 in scoreboard slot 7
[Mon Dec 05 10:26:26 2005] [notice] jk2_init() Found child 5385 in scoreboard slot 8
[Mon Dec 05 10:26:25 2005] [notice] jk2_init() Found child 5386 in scoreboard slot 9
[Mon Dec 05 10:26:25 2005] [notice] jk2_init() Found child 5387 in scoreboard slot 6
[Mon Dec 05 10:26:31 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:26:31 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:26:31 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:26:31 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:26:31 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:26:31 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:26:31 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:26:31 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:26:36 2005] [notice] jk2_init() Found child 5388 in scoreboard slot 10
[Mon Dec 05 10:26:36 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:26:36 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:26:39 2005] [error] [client 141.153.150.164] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 10:28:44 2005] [error] [client 198.232.168.9] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 10:31:40 2005] [notice] jk2_init() Found child 5404 in scoreboard slot 8
[Mon Dec 05 10:31:40 2005] [notice] jk2_init() Found child 5405 in scoreboard slot 9
[Mon Dec 05 10:33:41 2005] [notice] jk2_init() Found child 5418 in scoreboard slot 6
[Mon Dec 05 10:33:41 2005] [notice] jk2_init() Found child 5419 in scoreboard slot 7
[Mon Dec 05 10:33:41 2005] [notice] jk2_init() Found child 5417 in scoreboard slot 9
[Mon Dec 05 10:33:41 2005] [notice] jk2_init() Found child 5416 in scoreboard slot 8
[Mon Dec 05 10:33:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:33:44 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:33:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:33:44 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:33:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:33:44 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:33:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:33:44 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:36:10 2005] [notice] jk2_init() Found child 5426 in scoreboard slot 6
[Mon Dec 05 10:36:10 2005] [notice] jk2_init() Found child 5425 in scoreboard slot 9
[Mon Dec 05 10:36:58 2005] [notice] jk2_init() Found child 5428 in scoreboard slot 8
[Mon Dec 05 10:37:27 2005] [notice] jk2_init() Found child 5429 in scoreboard slot 9
[Mon Dec 05 10:37:27 2005] [notice] jk2_init() Found child 5430 in scoreboard slot 6
[Mon Dec 05 10:38:00 2005] [notice] jk2_init() Found child 5434 in scoreboard slot 6
[Mon Dec 05 10:38:00 2005] [notice] jk2_init() Found child 5433 in scoreboard slot 9
[Mon Dec 05 10:38:00 2005] [notice] jk2_init() Found child 5435 in scoreboard slot 7
[Mon Dec 05 10:38:00 2005] [notice] jk2_init() Found child 5432 in scoreboard slot 8
[Mon Dec 05 10:38:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:38:04 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 10:38:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:38:04 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:38:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:38:04 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:38:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:38:04 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:41:14 2005] [notice] jk2_init() Found child 5470 in scoreboard slot 9
[Mon Dec 05 10:41:14 2005] [notice] jk2_init() Found child 5469 in scoreboard slot 8
[Mon Dec 05 10:42:23 2005] [notice] jk2_init() Found child 5474 in scoreboard slot 9
[Mon Dec 05 10:42:23 2005] [notice] jk2_init() Found child 5475 in scoreboard slot 6
[Mon Dec 05 10:43:19 2005] [notice] jk2_init() Found child 5482 in scoreboard slot 9
[Mon Dec 05 10:43:19 2005] [notice] jk2_init() Found child 5480 in scoreboard slot 7
[Mon Dec 05 10:43:19 2005] [notice] jk2_init() Found child 5479 in scoreboard slot 6
[Mon Dec 05 10:43:19 2005] [notice] jk2_init() Found child 5481 in scoreboard slot 8
[Mon Dec 05 10:43:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:43:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:43:39 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 10:43:41 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:43:41 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:43:48 2005] [notice] jk2_init() Found child 5484 in scoreboard slot 7
[Mon Dec 05 10:43:48 2005] [notice] jk2_init() Found child 5483 in scoreboard slot 6
[Mon Dec 05 10:43:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:43:51 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:43:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:43:51 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 10:46:55 2005] [notice] jk2_init() Found child 5497 in scoreboard slot 7
[Mon Dec 05 10:46:55 2005] [notice] jk2_init() Found child 5495 in scoreboard slot 9
[Mon Dec 05 10:46:55 2005] [notice] jk2_init() Found child 5494 in scoreboard slot 8
[Mon Dec 05 10:46:55 2005] [notice] jk2_init() Found child 5496 in scoreboard slot 6
[Mon Dec 05 10:47:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:47:12 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:47:17 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:47:17 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:47:32 2005] [notice] jk2_init() Found child 5499 in scoreboard slot 9
[Mon Dec 05 10:47:33 2005] [notice] jk2_init() Found child 5498 in scoreboard slot 8
[Mon Dec 05 10:47:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:47:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:47:45 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:47:45 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:47:47 2005] [notice] jk2_init() Found child 5500 in scoreboard slot 6
[Mon Dec 05 10:47:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:47:47 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:48:43 2005] [notice] jk2_init() Found child 5503 in scoreboard slot 10
[Mon Dec 05 10:48:46 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:48:46 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:48:48 2005] [error] [client 67.166.248.235] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 10:51:12 2005] [notice] jk2_init() Found child 5515 in scoreboard slot 7
[Mon Dec 05 10:51:12 2005] [notice] jk2_init() Found child 5516 in scoreboard slot 8
[Mon Dec 05 10:51:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:51:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:51:35 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:51:35 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:51:59 2005] [notice] jk2_init() Found child 5517 in scoreboard slot 6
[Mon Dec 05 10:52:00 2005] [notice] jk2_init() Found child 5518 in scoreboard slot 9
[Mon Dec 05 10:52:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:52:15 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:52:21 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:53:42 2005] [notice] jk2_init() Found child 5527 in scoreboard slot 7
[Mon Dec 05 10:53:42 2005] [notice] jk2_init() Found child 5526 in scoreboard slot 9
[Mon Dec 05 10:55:47 2005] [notice] jk2_init() Found child 5538 in scoreboard slot 9
[Mon Dec 05 10:59:25 2005] [notice] jk2_init() Found child 5565 in scoreboard slot 9
[Mon Dec 05 10:59:25 2005] [notice] jk2_init() Found child 5563 in scoreboard slot 7
[Mon Dec 05 10:59:25 2005] [notice] jk2_init() Found child 5562 in scoreboard slot 6
[Mon Dec 05 10:59:25 2005] [notice] jk2_init() Found child 5564 in scoreboard slot 8
[Mon Dec 05 10:59:25 2005] [notice] jk2_init() Found child 5567 in scoreboard slot 12
[Mon Dec 05 10:59:25 2005] [notice] jk2_init() Found child 5568 in scoreboard slot 13
[Mon Dec 05 10:59:25 2005] [notice] jk2_init() Found child 5566 in scoreboard slot 10
[Mon Dec 05 10:59:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:59:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:59:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:59:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:59:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:59:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:59:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 10:59:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:59:29 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 10:59:29 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 10:59:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:59:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:59:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 10:59:29 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 11:02:05 2005] [notice] jk2_init() Found child 5579 in scoreboard slot 6
[Mon Dec 05 11:04:16 2005] [notice] jk2_init() Found child 5592 in scoreboard slot 8
[Mon Dec 05 11:04:16 2005] [notice] jk2_init() Found child 5593 in scoreboard slot 9
[Mon Dec 05 11:06:50 2005] [notice] jk2_init() Found child 5616 in scoreboard slot 6
[Mon Dec 05 11:06:51 2005] [notice] jk2_init() Found child 5617 in scoreboard slot 7
[Mon Dec 05 11:06:51 2005] [notice] jk2_init() Found child 5618 in scoreboard slot 8
[Mon Dec 05 11:06:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 11:06:51 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 11:06:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 11:06:51 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 11:06:51 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 11:06:51 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 11:06:52 2005] [error] jk2_init() Can't find child 5619 in scoreboard
[Mon Dec 05 11:06:52 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 11:06:52 2005] [error] mod_jk child init 1 -2
[Mon Dec 05 11:06:52 2005] [error] jk2_init() Can't find child 5620 in scoreboard
[Mon Dec 05 11:06:52 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 11:06:52 2005] [error] mod_jk child init 1 -2
[Mon Dec 05 11:06:52 2005] [error] jk2_init() Can't find child 5621 in scoreboard
[Mon Dec 05 11:06:52 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 11:06:52 2005] [error] mod_jk child init 1 -2
[Mon Dec 05 11:06:52 2005] [error] jk2_init() Can't find child 5622 in scoreboard
[Mon Dec 05 11:06:52 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 11:06:52 2005] [error] mod_jk child init 1 -2
[Mon Dec 05 12:35:57 2005] [notice] jk2_init() Found child 5785 in scoreboard slot 6
[Mon Dec 05 12:35:57 2005] [notice] jk2_init() Found child 5786 in scoreboard slot 7
[Mon Dec 05 12:36:36 2005] [notice] jk2_init() Found child 5790 in scoreboard slot 7
[Mon Dec 05 12:36:36 2005] [notice] jk2_init() Found child 5788 in scoreboard slot 9
[Mon Dec 05 12:36:36 2005] [notice] jk2_init() Found child 5789 in scoreboard slot 6
[Mon Dec 05 12:36:36 2005] [notice] jk2_init() Found child 5787 in scoreboard slot 8
[Mon Dec 05 12:36:39 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 12:36:39 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 12:36:39 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 12:36:39 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 12:36:39 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 12:36:39 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 12:36:39 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 12:36:39 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 12:40:37 2005] [notice] jk2_init() Found child 5798 in scoreboard slot 8
[Mon Dec 05 12:40:38 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 12:40:38 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 12:50:42 2005] [notice] jk2_init() Found child 5811 in scoreboard slot 6
[Mon Dec 05 12:50:42 2005] [notice] jk2_init() Found child 5810 in scoreboard slot 9
[Mon Dec 05 12:50:43 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 12:50:43 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 12:50:43 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 12:50:43 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 12:55:48 2005] [notice] jk2_init() Found child 5817 in scoreboard slot 8
[Mon Dec 05 12:55:48 2005] [notice] jk2_init() Found child 5816 in scoreboard slot 7
[Mon Dec 05 12:55:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 12:55:49 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 12:55:49 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 12:55:49 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:00:33 2005] [notice] jk2_init() Found child 5825 in scoreboard slot 9
[Mon Dec 05 13:00:33 2005] [notice] jk2_init() Found child 5826 in scoreboard slot 6
[Mon Dec 05 13:00:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:00:34 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:00:34 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:00:34 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:05:24 2005] [notice] jk2_init() Found child 5845 in scoreboard slot 7
[Mon Dec 05 13:05:24 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:05:24 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:10:55 2005] [notice] jk2_init() Found child 5856 in scoreboard slot 8
[Mon Dec 05 13:10:59 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:10:59 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:16:27 2005] [notice] jk2_init() Found child 5877 in scoreboard slot 9
[Mon Dec 05 13:16:27 2005] [notice] jk2_init() Found child 5876 in scoreboard slot 8
[Mon Dec 05 13:16:27 2005] [notice] jk2_init() Found child 5878 in scoreboard slot 6
[Mon Dec 05 13:16:27 2005] [notice] jk2_init() Found child 5875 in scoreboard slot 7
[Mon Dec 05 13:16:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:16:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:16:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:16:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:16:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:16:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:16:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:16:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:21:35 2005] [notice] jk2_init() Found child 5893 in scoreboard slot 9
[Mon Dec 05 13:21:34 2005] [notice] jk2_init() Found child 5892 in scoreboard slot 8
[Mon Dec 05 13:22:45 2005] [notice] jk2_init() Found child 5901 in scoreboard slot 9
[Mon Dec 05 13:22:45 2005] [notice] jk2_init() Found child 5899 in scoreboard slot 7
[Mon Dec 05 13:22:45 2005] [notice] jk2_init() Found child 5900 in scoreboard slot 8
[Mon Dec 05 13:22:45 2005] [notice] jk2_init() Found child 5898 in scoreboard slot 6
[Mon Dec 05 13:22:48 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:22:48 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 13:22:48 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:22:48 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 13:22:48 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:22:48 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:22:48 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:22:48 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:26:03 2005] [notice] jk2_init() Found child 5912 in scoreboard slot 7
[Mon Dec 05 13:26:37 2005] [notice] jk2_init() Found child 5914 in scoreboard slot 9
[Mon Dec 05 13:26:37 2005] [notice] jk2_init() Found child 5915 in scoreboard slot 6
[Mon Dec 05 13:27:15 2005] [notice] jk2_init() Found child 5917 in scoreboard slot 8
[Mon Dec 05 13:27:14 2005] [notice] jk2_init() Found child 5916 in scoreboard slot 7
[Mon Dec 05 13:27:15 2005] [notice] jk2_init() Found child 5919 in scoreboard slot 6
[Mon Dec 05 13:27:15 2005] [notice] jk2_init() Found child 5918 in scoreboard slot 9
[Mon Dec 05 13:28:14 2005] [notice] jk2_init() Found child 5925 in scoreboard slot 8
[Mon Dec 05 13:28:14 2005] [notice] jk2_init() Found child 5923 in scoreboard slot 6
[Mon Dec 05 13:28:14 2005] [notice] jk2_init() Found child 5924 in scoreboard slot 7
[Mon Dec 05 13:28:14 2005] [notice] jk2_init() Found child 5922 in scoreboard slot 9
[Mon Dec 05 13:28:17 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:28:17 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 13:28:17 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:28:17 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 13:28:17 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:28:17 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 13:28:17 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:28:17 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 13:31:19 2005] [notice] jk2_init() Found child 5935 in scoreboard slot 9
[Mon Dec 05 13:31:19 2005] [notice] jk2_init() Found child 5936 in scoreboard slot 6
[Mon Dec 05 13:31:53 2005] [notice] jk2_init() Found child 5938 in scoreboard slot 8
[Mon Dec 05 13:31:53 2005] [notice] jk2_init() Found child 5937 in scoreboard slot 7
[Mon Dec 05 13:32:01 2005] [notice] jk2_init() Found child 5940 in scoreboard slot 6
[Mon Dec 05 13:32:01 2005] [notice] jk2_init() Found child 5939 in scoreboard slot 9
[Mon Dec 05 13:32:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:32:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:32:04 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:32:04 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:32:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:32:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:32:10 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 13:32:10 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 13:32:28 2005] [notice] jk2_init() Found child 5942 in scoreboard slot 8
[Mon Dec 05 13:32:28 2005] [notice] jk2_init() Found child 5941 in scoreboard slot 7
[Mon Dec 05 13:32:30 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:32:30 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:32:30 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:32:30 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:36:27 2005] [notice] jk2_init() Found child 5954 in scoreboard slot 7
[Mon Dec 05 13:36:27 2005] [notice] jk2_init() Found child 5953 in scoreboard slot 6
[Mon Dec 05 13:36:58 2005] [notice] jk2_init() Found child 5956 in scoreboard slot 9
[Mon Dec 05 13:36:58 2005] [notice] jk2_init() Found child 5957 in scoreboard slot 6
[Mon Dec 05 13:36:58 2005] [notice] jk2_init() Found child 5955 in scoreboard slot 8
[Mon Dec 05 13:37:47 2005] [notice] jk2_init() Found child 5961 in scoreboard slot 6
[Mon Dec 05 13:37:47 2005] [notice] jk2_init() Found child 5960 in scoreboard slot 9
[Mon Dec 05 13:38:52 2005] [notice] jk2_init() Found child 5968 in scoreboard slot 9
[Mon Dec 05 13:38:53 2005] [notice] jk2_init() Found child 5965 in scoreboard slot 6
[Mon Dec 05 13:38:52 2005] [notice] jk2_init() Found child 5967 in scoreboard slot 8
[Mon Dec 05 13:38:53 2005] [notice] jk2_init() Found child 5969 in scoreboard slot 10
[Mon Dec 05 13:38:52 2005] [notice] jk2_init() Found child 5966 in scoreboard slot 7
[Mon Dec 05 13:39:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:39:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:39:14 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:39:13 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 13:39:36 2005] [notice] jk2_init() Found child 5970 in scoreboard slot 6
[Mon Dec 05 13:39:36 2005] [notice] jk2_init() Found child 5971 in scoreboard slot 7
[Mon Dec 05 13:39:41 2005] [notice] jk2_init() Found child 5972 in scoreboard slot 8
[Mon Dec 05 13:39:41 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:39:41 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:39:41 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:39:41 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 13:39:41 2005] [error] mod_jk child workerEnv in error state 9
[Mon Dec 05 13:39:41 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 13:41:11 2005] [notice] jk2_init() Found child 5981 in scoreboard slot 9
[Mon Dec 05 13:41:12 2005] [notice] jk2_init() Found child 5982 in scoreboard slot 6
[Mon Dec 05 13:41:58 2005] [notice] jk2_init() Found child 5984 in scoreboard slot 8
[Mon Dec 05 13:41:58 2005] [notice] jk2_init() Found child 5985 in scoreboard slot 9
[Mon Dec 05 13:43:27 2005] [notice] jk2_init() Found child 5992 in scoreboard slot 8
[Mon Dec 05 13:43:27 2005] [notice] jk2_init() Found child 5993 in scoreboard slot 9
[Mon Dec 05 13:43:27 2005] [notice] jk2_init() Found child 5990 in scoreboard slot 6
[Mon Dec 05 13:43:27 2005] [notice] jk2_init() Found child 5991 in scoreboard slot 7
[Mon Dec 05 13:43:43 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:43:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:43:43 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:43:46 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:43:46 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 13:43:46 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 13:44:18 2005] [notice] jk2_init() Found child 5995 in scoreboard slot 7
[Mon Dec 05 13:44:18 2005] [notice] jk2_init() Found child 5996 in scoreboard slot 8
[Mon Dec 05 13:44:32 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:44:35 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:44:38 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:44:38 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:44:53 2005] [notice] jk2_init() Found child 5997 in scoreboard slot 9
[Mon Dec 05 13:45:01 2005] [notice] jk2_init() Found child 5998 in scoreboard slot 6
[Mon Dec 05 13:45:01 2005] [notice] jk2_init() Found child 5999 in scoreboard slot 7
[Mon Dec 05 13:45:08 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:45:08 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:45:08 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:45:08 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:45:08 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 13:45:08 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:46:20 2005] [notice] jk2_init() Found child 6007 in scoreboard slot 7
[Mon Dec 05 13:46:20 2005] [notice] jk2_init() Found child 6006 in scoreboard slot 6
[Mon Dec 05 13:46:20 2005] [notice] jk2_init() Found child 6005 in scoreboard slot 9
[Mon Dec 05 13:46:50 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:47:06 2005] [notice] jk2_init() Found child 6008 in scoreboard slot 8
[Mon Dec 05 13:47:06 2005] [notice] jk2_init() Found child 6009 in scoreboard slot 9
[Mon Dec 05 13:47:09 2005] [notice] jk2_init() Found child 6011 in scoreboard slot 7
[Mon Dec 05 13:47:09 2005] [notice] jk2_init() Found child 6010 in scoreboard slot 6
[Mon Dec 05 13:47:11 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:47:11 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:47:11 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:47:11 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:47:11 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 13:47:11 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:47:11 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 13:47:11 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 13:51:17 2005] [notice] jk2_init() Found child 6028 in scoreboard slot 9
[Mon Dec 05 13:52:19 2005] [notice] jk2_init() Found child 6036 in scoreboard slot 9
[Mon Dec 05 13:52:19 2005] [notice] jk2_init() Found child 6033 in scoreboard slot 6
[Mon Dec 05 13:52:19 2005] [notice] jk2_init() Found child 6035 in scoreboard slot 8
[Mon Dec 05 13:52:19 2005] [notice] jk2_init() Found child 6034 in scoreboard slot 7
[Mon Dec 05 13:52:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:52:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:52:36 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:52:36 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:53:00 2005] [notice] jk2_init() Found child 6038 in scoreboard slot 7
[Mon Dec 05 13:53:00 2005] [notice] jk2_init() Found child 6037 in scoreboard slot 6
[Mon Dec 05 13:53:00 2005] [notice] jk2_init() Found child 6039 in scoreboard slot 10
[Mon Dec 05 13:53:31 2005] [notice] jk2_init() Found child 6043 in scoreboard slot 9
[Mon Dec 05 13:53:31 2005] [notice] jk2_init() Found child 6042 in scoreboard slot 7
[Mon Dec 05 13:53:31 2005] [notice] jk2_init() Found child 6041 in scoreboard slot 6
[Mon Dec 05 13:53:34 2005] [notice] jk2_init() Found child 6044 in scoreboard slot 8
[Mon Dec 05 13:53:35 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:53:35 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:53:35 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:53:35 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 13:53:35 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:53:35 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 13:53:35 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 13:53:35 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 13:56:21 2005] [notice] jk2_init() Found child 6052 in scoreboard slot 6
[Mon Dec 05 13:56:38 2005] [notice] jk2_init() Found child 6053 in scoreboard slot 7
[Mon Dec 05 13:57:07 2005] [notice] jk2_init() Found child 6054 in scoreboard slot 9
[Mon Dec 05 13:57:07 2005] [notice] jk2_init() Found child 6055 in scoreboard slot 8
[Mon Dec 05 13:58:31 2005] [notice] jk2_init() Found child 6063 in scoreboard slot 8
[Mon Dec 05 13:58:31 2005] [notice] jk2_init() Found child 6062 in scoreboard slot 9
[Mon Dec 05 13:59:43 2005] [notice] jk2_init() Found child 6069 in scoreboard slot 7
[Mon Dec 05 13:59:43 2005] [notice] jk2_init() Found child 6070 in scoreboard slot 9
[Mon Dec 05 13:59:43 2005] [notice] jk2_init() Found child 6071 in scoreboard slot 8
[Mon Dec 05 14:01:47 2005] [notice] jk2_init() Found child 6100 in scoreboard slot 7
[Mon Dec 05 14:01:47 2005] [notice] jk2_init() Found child 6101 in scoreboard slot 8
[Mon Dec 05 14:01:47 2005] [notice] jk2_init() Found child 6099 in scoreboard slot 6
[Mon Dec 05 14:01:48 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 14:01:48 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 14:01:48 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 14:01:48 2005] [error] mod_jk child workerEnv in error state 9
[Mon Dec 05 14:01:48 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 14:01:48 2005] [error] mod_jk child workerEnv in error state 8
[Mon Dec 05 14:11:40 2005] [notice] jk2_init() Found child 6115 in scoreboard slot 10
[Mon Dec 05 14:11:43 2005] [error] [client 141.154.18.244] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 14:11:45 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 14:11:45 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 15:31:06 2005] [notice] jk2_init() Found child 6259 in scoreboard slot 6
[Mon Dec 05 15:31:06 2005] [notice] jk2_init() Found child 6260 in scoreboard slot 7
[Mon Dec 05 15:31:09 2005] [notice] jk2_init() Found child 6261 in scoreboard slot 8
[Mon Dec 05 15:31:10 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:31:10 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 15:31:10 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:31:10 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 15:31:10 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:31:10 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 15:40:59 2005] [notice] jk2_init() Found child 6277 in scoreboard slot 7
[Mon Dec 05 15:40:59 2005] [notice] jk2_init() Found child 6276 in scoreboard slot 6
[Mon Dec 05 15:41:32 2005] [notice] jk2_init() Found child 6280 in scoreboard slot 7
[Mon Dec 05 15:41:32 2005] [notice] jk2_init() Found child 6278 in scoreboard slot 8
[Mon Dec 05 15:41:32 2005] [notice] jk2_init() Found child 6279 in scoreboard slot 6
[Mon Dec 05 15:41:32 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:41:32 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 15:41:32 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:41:32 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 15:41:32 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:41:32 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 15:45:42 2005] [notice] jk2_init() Found child 6285 in scoreboard slot 8
[Mon Dec 05 15:45:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:45:44 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 15:50:53 2005] [notice] jk2_init() Found child 6293 in scoreboard slot 6
[Mon Dec 05 15:50:53 2005] [notice] jk2_init() Found child 6294 in scoreboard slot 7
[Mon Dec 05 15:51:18 2005] [notice] jk2_init() Found child 6297 in scoreboard slot 7
[Mon Dec 05 15:51:18 2005] [notice] jk2_init() Found child 6295 in scoreboard slot 8
[Mon Dec 05 15:51:18 2005] [notice] jk2_init() Found child 6296 in scoreboard slot 6
[Mon Dec 05 15:51:20 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:51:20 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 15:51:20 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:51:20 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 15:51:20 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:51:20 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 15:55:31 2005] [notice] jk2_init() Found child 6302 in scoreboard slot 8
[Mon Dec 05 15:55:32 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 15:55:32 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:01:17 2005] [notice] jk2_init() Found child 6310 in scoreboard slot 6
[Mon Dec 05 16:02:00 2005] [notice] jk2_init() Found child 6315 in scoreboard slot 6
[Mon Dec 05 16:02:00 2005] [notice] jk2_init() Found child 6316 in scoreboard slot 7
[Mon Dec 05 16:02:00 2005] [notice] jk2_init() Found child 6314 in scoreboard slot 8
[Mon Dec 05 16:02:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:02:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:02:02 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:02:02 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:02:02 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 16:02:02 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:06:07 2005] [notice] jk2_init() Found child 6333 in scoreboard slot 8
[Mon Dec 05 16:06:21 2005] [notice] jk2_init() Found child 6335 in scoreboard slot 7
[Mon Dec 05 16:07:08 2005] [notice] jk2_init() Found child 6339 in scoreboard slot 8
[Mon Dec 05 16:07:08 2005] [notice] jk2_init() Found child 6340 in scoreboard slot 6
[Mon Dec 05 16:07:08 2005] [notice] jk2_init() Found child 6338 in scoreboard slot 7
[Mon Dec 05 16:07:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:07:09 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 16:07:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:07:09 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:07:09 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:07:09 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 16:10:43 2005] [notice] jk2_init() Found child 6351 in scoreboard slot 8
[Mon Dec 05 16:10:43 2005] [notice] jk2_init() Found child 6350 in scoreboard slot 7
[Mon Dec 05 16:10:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:10:44 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:10:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:10:44 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:16:34 2005] [notice] jk2_init() Found child 6368 in scoreboard slot 8
[Mon Dec 05 16:16:34 2005] [notice] jk2_init() Found child 6367 in scoreboard slot 7
[Mon Dec 05 16:16:34 2005] [notice] jk2_init() Found child 6366 in scoreboard slot 6
[Mon Dec 05 16:16:36 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:16:36 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:16:36 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:16:36 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:16:36 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:16:36 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:21:25 2005] [notice] jk2_init() Found child 6387 in scoreboard slot 7
[Mon Dec 05 16:21:25 2005] [notice] jk2_init() Found child 6386 in scoreboard slot 6
[Mon Dec 05 16:21:25 2005] [notice] jk2_init() Found child 6385 in scoreboard slot 8
[Mon Dec 05 16:21:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:21:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:21:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:21:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:21:29 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:21:29 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:26:00 2005] [notice] jk2_init() Found child 6400 in scoreboard slot 7
[Mon Dec 05 16:26:00 2005] [notice] jk2_init() Found child 6399 in scoreboard slot 6
[Mon Dec 05 16:26:00 2005] [notice] jk2_init() Found child 6398 in scoreboard slot 8
[Mon Dec 05 16:26:05 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:26:05 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:26:05 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:26:05 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:26:05 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:26:05 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:31:48 2005] [notice] jk2_init() Found child 6420 in scoreboard slot 6
[Mon Dec 05 16:31:49 2005] [notice] jk2_init() Found child 6421 in scoreboard slot 7
[Mon Dec 05 16:31:49 2005] [notice] jk2_init() Found child 6422 in scoreboard slot 8
[Mon Dec 05 16:31:52 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:31:52 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:31:52 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:31:52 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:31:52 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:31:52 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:36:06 2005] [notice] jk2_init() Found child 6434 in scoreboard slot 7
[Mon Dec 05 16:36:06 2005] [notice] jk2_init() Found child 6433 in scoreboard slot 6
[Mon Dec 05 16:36:42 2005] [notice] jk2_init() Found child 6435 in scoreboard slot 8
[Mon Dec 05 16:37:03 2005] [notice] jk2_init() Found child 6437 in scoreboard slot 7
[Mon Dec 05 16:38:17 2005] [notice] jk2_init() Found child 6443 in scoreboard slot 7
[Mon Dec 05 16:38:17 2005] [notice] jk2_init() Found child 6442 in scoreboard slot 6
[Mon Dec 05 16:39:59 2005] [notice] jk2_init() Found child 6453 in scoreboard slot 10
[Mon Dec 05 16:39:59 2005] [notice] jk2_init() Found child 6451 in scoreboard slot 7
[Mon Dec 05 16:39:59 2005] [notice] jk2_init() Found child 6452 in scoreboard slot 8
[Mon Dec 05 16:40:06 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:40:06 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:40:06 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 16:40:06 2005] [error] mod_jk child workerEnv in error state 9
[Mon Dec 05 16:40:06 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:40:06 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 16:45:04 2005] [error] [client 216.216.185.130] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 17:31:37 2005] [notice] jk2_init() Found child 6561 in scoreboard slot 10
[Mon Dec 05 17:31:39 2005] [error] [client 218.75.106.250] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 17:31:41 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:31:41 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 17:35:57 2005] [notice] jk2_init() Found child 6569 in scoreboard slot 8
[Mon Dec 05 17:35:57 2005] [notice] jk2_init() Found child 6568 in scoreboard slot 7
[Mon Dec 05 17:35:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:35:58 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 17:35:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:35:58 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 17:40:38 2005] [notice] jk2_init() Found child 6577 in scoreboard slot 7
[Mon Dec 05 17:40:38 2005] [notice] jk2_init() Found child 6578 in scoreboard slot 8
[Mon Dec 05 17:40:39 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:40:39 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 17:40:39 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:40:39 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 17:46:02 2005] [notice] jk2_init() Found child 6585 in scoreboard slot 7
[Mon Dec 05 17:46:02 2005] [notice] jk2_init() Found child 6586 in scoreboard slot 8
[Mon Dec 05 17:46:06 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:46:06 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:46:06 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 17:46:06 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 17:50:40 2005] [notice] jk2_init() Found child 6595 in scoreboard slot 8
[Mon Dec 05 17:50:40 2005] [notice] jk2_init() Found child 6594 in scoreboard slot 7
[Mon Dec 05 17:50:41 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:50:41 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 17:50:41 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:50:41 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 17:55:35 2005] [notice] jk2_init() Found child 6601 in scoreboard slot 8
[Mon Dec 05 17:55:35 2005] [notice] jk2_init() Found child 6600 in scoreboard slot 7
[Mon Dec 05 17:55:35 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:55:35 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 17:55:35 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 17:55:35 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:00:24 2005] [notice] jk2_init() Found child 6609 in scoreboard slot 7
[Mon Dec 05 18:00:26 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:00:26 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:10:56 2005] [notice] jk2_init() Found child 6639 in scoreboard slot 7
[Mon Dec 05 18:10:56 2005] [notice] jk2_init() Found child 6638 in scoreboard slot 8
[Mon Dec 05 18:10:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:10:58 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:10:58 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:10:58 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:15:45 2005] [notice] jk2_init() Found child 6652 in scoreboard slot 7
[Mon Dec 05 18:15:45 2005] [notice] jk2_init() Found child 6651 in scoreboard slot 8
[Mon Dec 05 18:15:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:15:47 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:15:47 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:15:47 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:20:51 2005] [notice] jk2_init() Found child 6670 in scoreboard slot 7
[Mon Dec 05 18:20:51 2005] [notice] jk2_init() Found child 6669 in scoreboard slot 8
[Mon Dec 05 18:20:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:20:53 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:20:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:20:53 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:26:06 2005] [notice] jk2_init() Found child 6684 in scoreboard slot 7
[Mon Dec 05 18:27:29 2005] [notice] jk2_init() Found child 6688 in scoreboard slot 8
[Mon Dec 05 18:27:33 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:27:33 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:27:37 2005] [notice] jk2_init() Found child 6689 in scoreboard slot 7
[Mon Dec 05 18:27:37 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:27:37 2005] [error] mod_jk child workerEnv in error state 7
[Mon Dec 05 18:35:51 2005] [notice] jk2_init() Found child 6707 in scoreboard slot 8
[Mon Dec 05 18:35:51 2005] [notice] jk2_init() Found child 6708 in scoreboard slot 7
[Mon Dec 05 18:35:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:35:53 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:35:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:35:53 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:40:54 2005] [notice] jk2_init() Found child 6719 in scoreboard slot 7
[Mon Dec 05 18:40:54 2005] [notice] jk2_init() Found child 6718 in scoreboard slot 8
[Mon Dec 05 18:40:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:40:54 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:40:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:40:54 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:45:51 2005] [notice] jk2_init() Found child 6725 in scoreboard slot 7
[Mon Dec 05 18:45:51 2005] [notice] jk2_init() Found child 6724 in scoreboard slot 8
[Mon Dec 05 18:45:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:45:53 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:45:53 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:45:53 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:50:30 2005] [notice] jk2_init() Found child 6733 in scoreboard slot 8
[Mon Dec 05 18:50:31 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:50:31 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:56:03 2005] [notice] jk2_init() Found child 6740 in scoreboard slot 7
[Mon Dec 05 18:56:03 2005] [notice] jk2_init() Found child 6741 in scoreboard slot 8
[Mon Dec 05 18:56:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:56:04 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 18:56:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 18:56:04 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 19:00:43 2005] [notice] jk2_init() Found child 6750 in scoreboard slot 8
[Mon Dec 05 19:00:43 2005] [notice] jk2_init() Found child 6749 in scoreboard slot 7
[Mon Dec 05 19:00:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 19:00:44 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 19:00:44 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 19:00:44 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 19:00:54 2005] [notice] jk2_init() Found child 6751 in scoreboard slot 10
[Mon Dec 05 19:00:54 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 19:00:54 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 19:00:56 2005] [error] [client 68.228.3.15] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 19:11:00 2005] [notice] jk2_init() Found child 6780 in scoreboard slot 7
[Mon Dec 05 19:11:04 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 19:11:04 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 19:14:08 2005] [notice] jk2_init() Found child 6784 in scoreboard slot 8
[Mon Dec 05 19:14:09 2005] [error] [client 61.220.139.68] Directory index forbidden by rule: /var/www/html/
[Mon Dec 05 19:14:11 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 19:14:11 2005] [error] mod_jk child workerEnv in error state 6
[Mon Dec 05 19:15:55 2005] [notice] jk2_init() Found child 6791 in scoreboard slot 8
[Mon Dec 05 19:15:55 2005] [notice] jk2_init() Found child 6790 in scoreboard slot 7
[Mon Dec 05 19:15:57 2005] [notice] workerEnv.init() ok /etc/httpd/conf/workers2.properties
[Mon Dec 05 19:15:57 2005] [error] mod_jk child workerEnv in error state 6
- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.675872 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1117838573 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.53.276129 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1117838976 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.49.36.156884 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1117838978 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.49.38.026704 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1117842440 2005.06.03 R23-M0-NE-C:J05-U01 2005-06-03-16.47.20.730545 R23-M0-NE-C:J05-U01 RAS KERNEL INFO 63543 double-hummer alignment exceptions
- 1117842974 2005.06.03 R24-M0-N1-C:J13-U11 2005-06-03-16.56.14.254137 R24-M0-N1-C:J13-U11 RAS KERNEL INFO 162 double-hummer alignment exceptions
- 1117843015 2005.06.03 R21-M1-N6-C:J08-U11 2005-06-03-16.56.55.309974 R21-M1-N6-C:J08-U11 RAS KERNEL INFO 141 double-hummer alignment exceptions
- 1117848119 2005.06.03 R16-M1-N2-C:J17-U01 2005-06-03-18.21.59.871925 R16-M1-N2-C:J17-U01 RAS KERNEL INFO CE sym 2, at 0x0b85eee0, mask 0x05
APPREAD 1117869872 2005.06.04 R04-M1-N4-I:J18-U11 2005-06-04-00.24.32.432192 R04-M1-N4-I:J18-U11 RAS APP FATAL ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33569
APPREAD 1117869876 2005.06.04 R27-M1-N4-I:J18-U01 2005-06-04-00.24.36.222560 R27-M1-N4-I:J18-U01 RAS APP FATAL ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33370
- 1117942120 2005.06.04 R30-M0-N7-C:J08-U01 2005-06-04-20.28.40.767551 R30-M0-N7-C:J08-U01 RAS KERNEL INFO CE sym 20, at 0x1438f9e0, mask 0x40
- 1117955341 2005.06.05 R25-M0-N7-C:J02-U01 2005-06-05-00.09.01.903373 R25-M0-N7-C:J02-U01 RAS KERNEL INFO generating core.2275
- 1117955392 2005.06.05 R24-M1-N8-C:J09-U11 2005-06-05-00.09.52.516674 R24-M1-N8-C:J09-U11 RAS KERNEL INFO generating core.862
- 1117956980 2005.06.05 R24-M1-NB-C:J15-U11 2005-06-05-00.36.20.945796 R24-M1-NB-C:J15-U11 RAS KERNEL INFO generating core.728
- 1117957045 2005.06.05 R20-M1-N8-C:J04-U01 2005-06-05-00.37.25.012681 R20-M1-N8-C:J04-U01 RAS KERNEL INFO generating core.775
- 1117959501 2005.06.05 R24-M0-NE-C:J14-U11 2005-06-05-01.18.21.778604 R24-M0-NE-C:J14-U11 RAS KERNEL INFO generating core.3276
- 1117959513 2005.06.05 R21-M1-N2-C:J11-U01 2005-06-05-01.18.33.830595 R21-M1-N2-C:J11-U01 RAS KERNEL INFO generating core.1717
- 1117959563 2005.06.05 R24-M0-N8-C:J04-U11 2005-06-05-01.19.23.822135 R24-M0-N8-C:J04-U11 RAS KERNEL INFO generating core.3919
- 1117973759 2005.06.05 R31-M0-NE-C:J05-U11 2005-06-05-05.15.59.416717 R31-M0-NE-C:J05-U11 RAS KERNEL INFO generating core.2079
- 1117973786 2005.06.05 R36-M0-NA-C:J06-U01 2005-06-05-05.16.26.686603 R36-M0-NA-C:J06-U01 RAS KERNEL INFO generating core.1414
- 1117973919 2005.06.05 R33-M0-N4-C:J02-U11 2005-06-05-05.18.39.396608 R33-M0-N4-C:J02-U11 RAS KERNEL INFO generating core.3055
- 1117974206 2005.06.05 R22-M0-ND-C:J10-U11 2005-06-05-05.23.26.239153 R22-M0-ND-C:J10-U11 RAS KERNEL INFO generating core.201
- 1117974463 2005.06.05 R27-M0-N6-C:J10-U01 2005-06-05-05.27.43.336565 R27-M0-N6-C:J10-U01 RAS KERNEL INFO generating core.1125
- 1117975251 2005.06.05 R26-M1-N8-C:J17-U11 2005-06-05-05.40.51.726735 R26-M1-N8-C:J17-U11 RAS KERNEL INFO generating core.412
- 1117976658 2005.06.05 R36-M1-N8-C:J17-U01 2005-06-05-06.04.18.406158 R36-M1-N8-C:J17-U01 RAS KERNEL INFO generating core.7828
- 1117977497 2005.06.05 R33-M1-NB-C:J06-U01 2005-06-05-06.18.17.802159 R33-M1-NB-C:J06-U01 RAS KERNEL INFO generating core.5570
- 1117979227 2005.06.05 R01-M1-N7-C:J04-U11 2005-06-05-06.47.07.157021 R01-M1-N7-C:J04-U11 RAS KERNEL INFO generating core.8275
- 1117982609 2005.06.05 R35-M1-NE-C:J05-U01 2005-06-05-07.43.29.979844 R35-M1-NE-C:J05-U01 RAS KERNEL INFO generating core.4183
- 1117984124 2005.06.05 R36-M1-NF-C:J11-U01 2005-06-05-08.08.44.281729 R36-M1-NF-C:J11-U01 RAS KERNEL INFO generating core.6545
- 1117984130 2005.06.05 R37-M1-NE-C:J13-U01 2005-06-05-08.08.50.547117 R37-M1-NE-C:J13-U01 RAS KERNEL INFO generating core.4245
- 1117984216 2005.06.05 R32-M1-N4-C:J16-U01 2005-06-05-08.10.16.270131 R32-M1-N4-C:J16-U01 RAS KERNEL INFO generating core.6884
- 1117984246 2005.06.05 R30-M1-N3-C:J02-U01 2005-06-05-08.10.46.344235 R30-M1-N3-C:J02-U01 RAS KERNEL FATAL force load/store alignment...............0
- 1117985401 2005.06.05 R34-M1-NE-C:J02-U01 2005-06-05-08.30.01.873693 R34-M1-NE-C:J02-U01 RAS KERNEL INFO generating core.6471
- 1117985413 2005.06.05 R31-M1-N7-C:J05-U11 2005-06-05-08.30.13.824307 R31-M1-N7-C:J05-U11 RAS KERNEL INFO generating core.4155
- 1117985464 2005.06.05 R32-M0-NF-C:J10-U01 2005-06-05-08.31.04.464776 R32-M0-NF-C:J10-U01 RAS KERNEL INFO generating core.449
- 1117985533 2005.06.05 R34-M1-NC-C:J06-U11 2005-06-05-08.32.13.659715 R34-M1-NC-C:J06-U11 RAS KERNEL INFO generating core.6990
- 1117985547 2005.06.05 R31-M1-NC-C:J14-U11 2005-06-05-08.32.27.814949 R31-M1-NC-C:J14-U11 RAS KERNEL INFO generating core.4876
- 1117987420 2005.06.05 R37-M0-N7-C:J08-U11 2005-06-05-09.03.40.673488 R37-M0-N7-C:J08-U11 RAS KERNEL INFO generating core.2218
- 1117988266 2005.06.05 R34-M1-NA-C:J07-U11 2005-06-05-09.17.46.225683 R34-M1-NA-C:J07-U11 RAS KERNEL INFO generating core.7518
- 1117988286 2005.06.05 R33-M1-N8-C:J09-U11 2005-06-05-09.18.06.694851 R33-M1-N8-C:J09-U11 RAS KERNEL INFO generating core.5854
- 1117988416 2005.06.05 R30-M1-N3-C:J10-U01 2005-06-05-09.20.16.681318 R30-M1-N3-C:J10-U01 RAS KERNEL INFO generating core.7457
- 1117988443 2005.06.05 R32-M1-N5-C:J17-U01 2005-06-05-09.20.43.944594 R32-M1-N5-C:J17-U01 RAS KERNEL INFO generating core.6896
- 1117989483 2005.06.05 R37-M0-N3-C:J14-U01 2005-06-05-09.38.03.456120 R37-M0-N3-C:J14-U01 RAS KERNEL INFO generating core.3488
- 1117989508 2005.06.05 R36-M0-NA-C:J17-U01 2005-06-05-09.38.28.957918 R36-M0-NA-C:J17-U01 RAS KERNEL INFO generating core.1172
- 1117989530 2005.06.05 R33-M0-N6-C:J08-U11 2005-06-05-09.38.50.430385 R33-M0-N6-C:J08-U11 RAS KERNEL INFO generating core.2286
- 1117989577 2005.06.05 R30-M0-ND-C:J07-U01 2005-06-05-09.39.37.590924 R30-M0-ND-C:J07-U01 RAS KERNEL INFO generating core.786
- 1117989594 2005.06.05 R35-M0-N5-C:J17-U11 2005-06-05-09.39.54.210760 R35-M0-N5-C:J17-U11 RAS KERNEL INFO generating core.2680
- 1117989601 2005.06.05 R32-M0-N2-C:J15-U01 2005-06-05-09.40.01.191177 R32-M0-N2-C:J15-U01 RAS KERNEL INFO generating core.1524
- 1117989617 2005.06.05 R35-M1-ND-C:J10-U11 2005-06-05-09.40.17.352982 R35-M1-ND-C:J10-U11 RAS KERNEL INFO generating core.4937
- 1117990886 2005.06.05 R36-M1-ND-C:J13-U01 2005-06-05-10.01.26.040379 R36-M1-ND-C:J13-U01 RAS KERNEL INFO generating core.6801
- 1117990951 2005.06.05 R31-M1-NF-C:J15-U01 2005-06-05-10.02.31.242619 R31-M1-NF-C:J15-U01 RAS KERNEL INFO generating core.4368
- 1117990980 2005.06.05 R34-M0-ND-C:J03-U01 2005-06-05-10.03.00.195528 R34-M0-ND-C:J03-U01 RAS KERNEL INFO generating core.851
- 1117991006 2005.06.05 R32-M0-N9-C:J17-U01 2005-06-05-10.03.26.884113 R32-M0-N9-C:J17-U01 RAS KERNEL INFO generating core.1744
- 1117991046 2005.06.05 R30-M0-N0-C:J17-U01 2005-06-05-10.04.06.326828 R30-M0-N0-C:J17-U01 RAS KERNEL INFO generating core.1588
- 1117992519 2005.06.05 R37-M1-N3-C:J05-U11 2005-06-05-10.28.39.717587 R37-M1-N3-C:J05-U11 RAS KERNEL INFO generating core.5307
- 1117992553 2005.06.05 R30-M1-NB-C:J17-U11 2005-06-05-10.29.13.196439 R30-M1-NB-C:J17-U11 RAS KERNEL INFO generating core.7192
- 1118070512 2005.06.06 R01-M1-NB-C:J03-U11 2005-06-06-08.08.32.984716 R01-M1-NB-C:J03-U11 RAS KERNEL INFO CE sym 20, at 0x01228120, mask 0x10
- 1118070518 2005.06.06 R01-M1-ND-C:J15-U11 2005-06-06-08.08.38.414458 R01-M1-ND-C:J15-U11 RAS KERNEL INFO CE sym 5, at 0x11042a80, mask 0x04
- 1118079221 2005.06.06 R02-M1-N0-C:J12-U11 2005-06-06-10.33.41.093251 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118079403 2005.06.06 R02-M1-N0-C:J12-U11 2005-06-06-10.36.43.705821 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118080420 2005.06.06 R02-M1-N0-C:J12-U11 2005-06-06-10.53.40.701796 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118080480 2005.06.06 R02-M1-N0-C:J12-U11 2005-06-06-10.54.40.382678 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118080909 2005.06.06 R02-M1-N0-C:J12-U11 2005-06-06-11.01.49.223879 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118081048 2005.06.06 R02-M1-N0-C:J12-U11 2005-06-06-11.04.08.452393 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118081391 2005.06.06 R02-M1-N0-C:J12-U11 2005-06-06-11.09.51.751862 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118081467 2005.06.06 R02-M1-N0-C:J12-U11 2005-06-06-11.11.07.787886 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118112603 2005.06.06 R22-M0-N0-C:J08-U11 2005-06-06-19.50.03.669394 R22-M0-N0-C:J08-U11 RAS KERNEL INFO generating core.430
- 1118112604 2005.06.06 R22-M0-N4-C:J08-U11 2005-06-06-19.50.04.552706 R22-M0-N4-C:J08-U11 RAS KERNEL INFO generating core.174
- 1118112700 2005.06.06 R23-M0-N0-I:J18-U01 2005-06-06-19.51.40.376079 R23-M0-N0-I:J18-U01 RAS KERNEL INFO ciod: cpu 0 at treeaddr 438 sent unrecognized message 0xffffffff
- 1118114656 2005.06.06 R24-M1-N9-C:J15-U01 2005-06-06-20.24.16.345770 R24-M1-N9-C:J15-U01 RAS KERNEL INFO generating core.976
- 1118122888 2005.06.06 R25-M1-N8-C:J06-U01 2005-06-06-22.41.28.143013 R25-M1-N8-C:J06-U01 RAS KERNEL INFO generating core.1990
- 1118149549 2005.06.07 R20-M0-N2-C:J09-U01 2005-06-07-06.05.49.961844 R20-M0-N2-C:J09-U01 RAS KERNEL INFO generating core.3638
- 1118149569 2005.06.07 R21-M0-NB-C:J06-U01 2005-06-07-06.06.09.288228 R21-M0-NB-C:J06-U01 RAS KERNEL INFO generating core.2690
- 1118149570 2005.06.07 R24-M0-NA-C:J15-U11 2005-06-07-06.06.10.391269 R24-M0-NA-C:J15-U11 RAS KERNEL INFO generating core.3804
- 1118149783 2005.06.07 R20-M1-N5-C:J06-U01 2005-06-07-06.09.43.355329 R20-M1-N5-C:J06-U01 RAS KERNEL INFO generating core.418
- 1118149821 2005.06.07 R20-M1-N4-C:J09-U01 2005-06-07-06.10.21.214757 R20-M1-N4-C:J09-U01 RAS KERNEL INFO generating core.310
- 1118172013 2005.06.07 R35-M0-N7-C:J10-U11 2005-06-07-12.20.13.966416 R35-M0-N7-C:J10-U11 RAS KERNEL INFO generating core.2409
- 1118172017 2005.06.07 R35-M1-NE-C:J09-U01 2005-06-07-12.20.17.344136 R35-M1-NE-C:J09-U01 RAS KERNEL INFO generating core.4182
- 1118172099 2005.06.07 R32-M1-N9-C:J11-U11 2005-06-07-12.21.39.208896 R32-M1-N9-C:J11-U11 RAS KERNEL INFO generating core.8153
- 1118172125 2005.06.07 R36-M1-N8-C:J17-U11 2005-06-07-12.22.05.196450 R36-M1-N8-C:J17-U11 RAS KERNEL INFO generating core.7836
- 1118172147 2005.06.07 R35-M0-NC-C:J03-U11 2005-06-07-12.22.27.613809 R35-M0-NC-C:J03-U11 RAS KERNEL INFO generating core.2911
- 1118172690 2005.06.07 R35-M0-N5-C:J10-U11 2005-06-07-12.31.30.918396 R35-M0-N5-C:J10-U11 RAS KERNEL INFO generating core.2921
- 1118183497 2005.06.07 R34-M0-N7-C:J09-U11 2005-06-07-15.31.37.435791 R34-M0-N7-C:J09-U11 RAS KERNEL INFO generating core.122
- 1118183566 2005.06.07 R36-M0-N0-C:J11-U01 2005-06-07-15.32.46.334191 R36-M0-N0-C:J11-U01 RAS KERNEL INFO generating core.1973
- 1118193358 2005.06.07 R11-M0-NC-I:J18-U01 2005-06-07-18.15.58.583443 R11-M0-NC-I:J18-U01 RAS APP FATAL ciod: LOGIN chdir(/p/gb2/glosli/8M_5000K/t800) failed: No such file or directory
- 1118207879 2005.06.07 R17-M1-N7-C:J15-U11 2005-06-07-22.17.59.587113 R17-M1-N7-C:J15-U11 RAS KERNEL INFO generating core.4984
- 1118207897 2005.06.07 R16-M1-NC-C:J06-U11 2005-06-07-22.18.17.203831 R16-M1-NC-C:J06-U11 RAS KERNEL INFO generating core.1822
- 1118251556 2005.06.08 R16-M1-N2-C:J12-U01 2005-06-08-10.25.56.322381 R16-M1-N2-C:J12-U01 RAS KERNEL INFO CE sym 28, at 0x110067e0, mask 0x02
- 1118271740 2005.06.08 R03-M1-N9-C:J09-U11 2005-06-08-16.02.20.600478 R03-M1-N9-C:J09-U11 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 25, bit 1
- 1118285722 2005.06.08 R15-M1-N6-C:J04-U11 2005-06-08-19.55.22.798062 R15-M1-N6-C:J04-U11 RAS KERNEL INFO CE sym 14, at 0x06047860, mask 0x20
- 1118290076 2005.06.08 R27-M0-ND-C:J10-U01 2005-06-08-21.07.56.397001 R27-M0-ND-C:J10-U01 RAS KERNEL INFO generating core.3265
- 1118290077 2005.06.08 R27-M1-N6-C:J04-U01 2005-06-08-21.07.57.879112 R27-M1-N6-C:J04-U01 RAS KERNEL INFO generating core.2599
- 1118292507 2005.06.08 R26-M0-NB-C:J09-U11 2005-06-08-21.48.27.952411 R26-M0-NB-C:J09-U11 RAS KERNEL INFO generating core.1818
- 1118301090 2005.06.09 R21-M1-NA-C:J07-U11 2005-06-09-00.11.30.925676 R21-M1-NA-C:J07-U11 RAS KERNEL INFO generating core.1694
- 1118343776 2005.06.09 R16-M1-N1-C:J12-U01 2005-06-09-12.02.56.706796 R16-M1-N1-C:J12-U01 RAS KERNEL INFO generating core.3401
- 1118343816 2005.06.09 R07-M1-NE-C:J06-U01 2005-06-09-12.03.36.113136 R07-M1-NE-C:J06-U01 RAS KERNEL INFO generating core.25350
- 1118343958 2005.06.09 R23-M1-NA-C:J17-U11 2005-06-09-12.05.58.258567 R23-M1-NA-C:J17-U11 RAS KERNEL INFO generating core.1820
- 1118351098 2005.06.09 R16-M1-N2-C:J17-U01 2005-06-09-14.04.58.329635 R16-M1-N2-C:J17-U01 RAS KERNEL INFO CE sym 2, at 0x1b85f080, mask 0x02
- 1118354046 2005.06.09 R24-M1-N6-C:J07-U11 2005-06-09-14.54.06.957120 R24-M1-N6-C:J07-U11 RAS KERNEL INFO generating core.254
- 1118354070 2005.06.09 R25-M1-N8-C:J05-U11 2005-06-09-14.54.30.103580 R25-M1-N8-C:J05-U11 RAS KERNEL INFO generating core.1887
- 1118363168 2005.06.09 R16-M1-N2-C:J17-U01 2005-06-09-17.26.08.386218 R16-M1-N2-C:J17-U01 RAS KERNEL INFO CE sym 2, at 0x1b85fe80, mask 0x02
- 1118371043 2005.06.09 R11-M0-N7-C:J16-U11 2005-06-09-19.37.23.866536 R11-M0-N7-C:J16-U11 RAS KERNEL INFO generating core.8280
- 1118371064 2005.06.09 R00-M1-N6-C:J12-U01 2005-06-09-19.37.44.455502 R00-M1-N6-C:J12-U01 RAS KERNEL INFO generating core.12357
KERNDTLB 1118536327 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-17.32.07.581048 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118536959 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-17.42.39.794840 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118537212 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-17.46.52.020435 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118537261 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-17.47.41.103595 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118537622 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-17.53.42.323869 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118537694 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-17.54.54.024829 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118538129 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-18.02.09.666558 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118538182 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-18.03.02.884252 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118538836 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-18.13.56.287869 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118539141 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-18.19.01.277745 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118539342 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-18.22.22.630540 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118539682 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-18.28.02.964604 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118539891 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-18.31.31.276036 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118540457 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-18.40.57.217361 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118541065 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-18.51.05.246165 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118541373 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-18.56.13.781741 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118541965 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.06.05.431824 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118542012 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.06.52.571964 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118542376 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.12.56.115636 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118543043 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.24.03.186547 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118543085 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.24.45.991034 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118543420 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.30.20.585330 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118543510 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.31.50.792512 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118543547 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.32.27.820915 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118543583 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.33.03.793426 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118543955 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.39.15.873091 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118544072 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.41.12.806499 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118544523 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.48.43.752466 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118544625 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.50.25.307011 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118544942 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.55.42.506216 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118545027 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-19.57.07.898567 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118545623 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-20.07.03.218292 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118545700 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-20.08.20.603096 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118548457 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-20.54.17.510380 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118548933 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.02.13.585407 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118549132 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.05.32.007991 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118549466 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.11.06.535786 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118549852 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.17.32.188993 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118549971 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.19.31.616037 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118550514 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.28.34.864892 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118550590 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.29.50.731558 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118550729 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.32.09.069241 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118550833 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.33.53.535990 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118551483 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.44.43.496547 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118552349 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-21.59.09.406185 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118552553 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.02.33.858708 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118552691 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.04.51.794882 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118552811 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.06.51.501364 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118552875 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.07.55.838589 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118553263 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.14.23.973994 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118553888 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.24.48.384418 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118554196 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.29.56.837444 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118554603 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.36.43.913043 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118554838 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.40.38.175497 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118555213 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.46.53.485059 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118555299 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.48.19.442753 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118555531 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-22.52.11.070355 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118556754 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-23.12.34.701151 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118557291 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-23.21.31.495365 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
KERNDTLB 1118557583 2005.06.11 R30-M0-N9-C:J16-U01 2005-06-11-23.26.23.330548 R30-M0-N9-C:J16-U01 RAS KERNEL FATAL data TLB error interrupt
- 1118686962 2005.06.13 R26-M0-NB-C:J07-U01 2005-06-13-11.22.42.601119 R26-M0-NB-C:J07-U01 RAS KERNEL INFO CE sym 10, at 0x08d10580, mask 0x08
- 1118699850 2005.06.13 R22-M0-NC-I:J18-U01 2005-06-13-14.57.30.126106 R22-M0-NC-I:J18-U01 RAS KERNEL INFO ciod: Message code 0 is not 51 or 4294967295
KERNSTOR 1118709403 2005.06.13 R10-M1-N5-C:J15-U11 2005-06-13-17.36.43.927885 R10-M1-N5-C:J15-U11 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118709482 2005.06.13 R16-M0-NB-C:J07-U11 2005-06-13-17.38.02.756042 R16-M0-NB-C:J07-U11 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118709549 2005.06.13 R15-M0-N9-C:J05-U11 2005-06-13-17.39.09.591793 R15-M0-N9-C:J05-U11 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118709578 2005.06.13 R13-M1-N2-C:J17-U01 2005-06-13-17.39.38.100643 R13-M1-N2-C:J17-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118709681 2005.06.13 R01-M1-NA-C:J13-U01 2005-06-13-17.41.21.634261 R01-M1-NA-C:J13-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118709681 2005.06.13 R01-M1-NA-C:J09-U01 2005-06-13-17.41.21.662328 R01-M1-NA-C:J09-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118709717 2005.06.13 R05-M0-N7-C:J10-U11 2005-06-13-17.41.57.658353 R05-M0-N7-C:J10-U11 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118709851 2005.06.13 R02-M0-N0-C:J10-U01 2005-06-13-17.44.11.050768 R02-M0-N0-C:J10-U01 RAS KERNEL FATAL data storage interrupt
- 1118709955 2005.06.13 R10-M0-N1-C:J17-U11 2005-06-13-17.45.55.674418 R10-M0-N1-C:J17-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118709996 2005.06.13 R12-M1-NB-C:J09-U01 2005-06-13-17.46.36.623273 R12-M1-NB-C:J09-U01 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118710134 2005.06.13 R04-M1-N1-C:J11-U11 2005-06-13-17.48.54.474557 R04-M1-N1-C:J11-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118710233 2005.06.13 R01-M0-NE-C:J17-U11 2005-06-13-17.50.33.240802 R01-M0-NE-C:J17-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118710289 2005.06.13 R16-M0-N8-C:J02-U11 2005-06-13-17.51.29.224900 R16-M0-N8-C:J02-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118710341 2005.06.13 R02-M0-N8-C:J08-U11 2005-06-13-17.52.21.811586 R02-M0-N8-C:J08-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118710403 2005.06.13 R06-M0-N8-C:J13-U11 2005-06-13-17.53.23.965757 R06-M0-N8-C:J13-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118732642 2005.06.14 R33-M0-NF-C:J07-U11 2005-06-14-00.04.02.552133 R33-M0-NF-C:J07-U11 RAS KERNEL INFO generating core.2522
- 1118758243 2005.06.14 R36-M0-N0-C:J04-U11 2005-06-14-07.10.43.523299 R36-M0-N0-C:J04-U11 RAS KERNEL INFO generating core.1711
- 1118758269 2005.06.14 R31-M0-NE-C:J15-U11 2005-06-14-07.11.09.281175 R31-M0-NE-C:J15-U11 RAS KERNEL INFO generating core.2332
- 1118758278 2005.06.14 R37-M1-NF-C:J11-U11 2005-06-14-07.11.18.222056 R37-M1-NF-C:J11-U11 RAS KERNEL INFO generating core.4505
- 1118758308 2005.06.14 R32-M0-N3-C:J11-U01 2005-06-14-07.11.48.689764 R32-M0-N3-C:J11-U01 RAS KERNEL INFO generating core.1521
KERNSTOR 1118765205 2005.06.14 R15-M1-NF-C:J11-U11 2005-06-14-09.06.45.752792 R15-M1-NF-C:J11-U11 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118765277 2005.06.14 R12-M0-N2-C:J11-U01 2005-06-14-09.07.57.162326 R12-M0-N2-C:J11-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118765328 2005.06.14 R14-M0-N2-C:J02-U01 2005-06-14-09.08.48.339582 R14-M0-N2-C:J02-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118765360 2005.06.14 R04-M0-ND-C:J15-U01 2005-06-14-09.09.20.459609 R04-M0-ND-C:J15-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118765370 2005.06.14 R01-M1-N6-C:J15-U11 2005-06-14-09.09.30.138103 R01-M1-N6-C:J15-U11 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118765511 2005.06.14 R06-M0-N7-C:J08-U01 2005-06-14-09.11.51.127157 R06-M0-N7-C:J08-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118765631 2005.06.14 R12-M1-NC-C:J13-U11 2005-06-14-09.13.51.017266 R12-M1-NC-C:J13-U11 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118765631 2005.06.14 R06-M0-N0-C:J11-U11 2005-06-14-09.13.51.996954 R06-M0-N0-C:J11-U11 RAS KERNEL FATAL data storage interrupt
- 1118765676 2005.06.14 R16-M1-N2-C:J07-U01 2005-06-14-09.14.36.498843 R16-M1-N2-C:J07-U01 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118765746 2005.06.14 R17-M1-N1-C:J12-U11 2005-06-14-09.15.46.266882 R17-M1-N1-C:J12-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118765795 2005.06.14 R16-M1-N9-C:J16-U11 2005-06-14-09.16.35.364588 R16-M1-N9-C:J16-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118765825 2005.06.14 R13-M0-NA-C:J15-U01 2005-06-14-09.17.05.516605 R13-M0-NA-C:J15-U01 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118765858 2005.06.14 R11-M0-N7-C:J10-U01 2005-06-14-09.17.38.154403 R11-M0-N7-C:J10-U01 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118766090 2005.06.14 R00-M0-N0-C:J13-U11 2005-06-14-09.21.30.885300 R00-M0-N0-C:J13-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118766119 2005.06.14 R02-M1-NC-C:J08-U11 2005-06-14-09.21.59.021659 R02-M1-NC-C:J08-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118766139 2005.06.14 R07-M0-NC-C:J10-U01 2005-06-14-09.22.19.720208 R07-M0-NC-C:J10-U01 RAS KERNEL FATAL instruction address: 0x00004ed8
KERNSTOR 1118766804 2005.06.14 R24-M1-N0-C:J16-U01 2005-06-14-09.33.24.217741 R24-M1-N0-C:J16-U01 RAS KERNEL FATAL data storage interrupt
- 1118766833 2005.06.14 R24-M1-ND-C:J11-U11 2005-06-14-09.33.53.107913 R24-M1-ND-C:J11-U11 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118766935 2005.06.14 R20-M1-NC-C:J05-U01 2005-06-14-09.35.35.569986 R20-M1-NC-C:J05-U01 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118767015 2005.06.14 R24-M1-N3-C:J16-U01 2005-06-14-09.36.55.370589 R24-M1-N3-C:J16-U01 RAS KERNEL FATAL data address: 0x00000002
- 1118767049 2005.06.14 R25-M1-N3-C:J16-U01 2005-06-14-09.37.29.304192 R25-M1-N3-C:J16-U01 RAS KERNEL FATAL data address: 0x00000002
- 1118767283 2005.06.14 R20-M1-N3-C:J14-U01 2005-06-14-09.41.23.815131 R20-M1-N3-C:J14-U01 RAS KERNEL FATAL machine check: i-fetch......................0
- 1118767320 2005.06.14 R25-M1-N9-C:J06-U11 2005-06-14-09.42.00.555171 R25-M1-N9-C:J06-U11 RAS KERNEL FATAL machine check: i-fetch......................0
- 1118767381 2005.06.14 R20-M0-N2-C:J13-U11 2005-06-14-09.43.01.478244 R20-M0-N2-C:J13-U11 RAS KERNEL FATAL program interrupt: illegal instruction......0
KERNSTOR 1118767897 2005.06.14 R20-M0-NE-C:J02-U01 2005-06-14-09.51.37.041158 R20-M0-NE-C:J02-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118767942 2005.06.14 R25-M0-NB-C:J02-U11 2005-06-14-09.52.22.295234 R25-M0-NB-C:J02-U11 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118767955 2005.06.14 R11-M1-N6-C:J12-U01 2005-06-14-09.52.35.590523 R11-M1-N6-C:J12-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118767965 2005.06.14 R24-M1-N6-C:J13-U01 2005-06-14-09.52.45.933587 R24-M1-N6-C:J13-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118768001 2005.06.14 R07-M1-N2-C:J05-U11 2005-06-14-09.53.21.013029 R07-M1-N2-C:J05-U11 RAS KERNEL FATAL data storage interrupt
- 1118768017 2005.06.14 R21-M1-NB-C:J12-U01 2005-06-14-09.53.37.560823 R21-M1-NB-C:J12-U01 RAS KERNEL FATAL instruction address: 0x00004ed8
KERNSTOR 1118768044 2005.06.14 R05-M1-NB-C:J07-U11 2005-06-14-09.54.04.393973 R05-M1-NB-C:J07-U11 RAS KERNEL FATAL data storage interrupt
- 1118768070 2005.06.14 R24-M1-N7-C:J08-U01 2005-06-14-09.54.30.228119 R24-M1-N7-C:J08-U01 RAS KERNEL FATAL instruction address: 0x00004ed8
KERNSTOR 1118768087 2005.06.14 R16-M0-N4-C:J02-U11 2005-06-14-09.54.47.139055 R16-M0-N4-C:J02-U11 RAS KERNEL FATAL data storage interrupt
- 1118768111 2005.06.14 R20-M1-N7-C:J16-U01 2005-06-14-09.55.11.455484 R20-M1-N7-C:J16-U01 RAS KERNEL FATAL data address: 0x00000002
KERNSTOR 1118768121 2005.06.14 R03-M1-N8-C:J13-U11 2005-06-14-09.55.21.391576 R03-M1-N8-C:J13-U11 RAS KERNEL FATAL data storage interrupt
- 1118768158 2005.06.14 R25-M0-NF-C:J05-U01 2005-06-14-09.55.58.097920 R25-M0-NF-C:J05-U01 RAS KERNEL FATAL data address: 0x00000002
- 1118768162 2005.06.14 R25-M0-ND-C:J04-U01 2005-06-14-09.56.02.974678 R25-M0-ND-C:J04-U01 RAS KERNEL FATAL data address: 0x00000002
KERNSTOR 1118768178 2005.06.14 R06-M1-NC-C:J11-U01 2005-06-14-09.56.18.293090 R06-M1-NC-C:J11-U01 RAS KERNEL FATAL data storage interrupt
- 1118768186 2005.06.14 R24-M1-N6-C:J11-U01 2005-06-14-09.56.26.940016 R24-M1-N6-C:J11-U01 RAS KERNEL FATAL data address: 0x00000002
- 1118768357 2005.06.14 R25-M0-N3-C:J13-U11 2005-06-14-09.59.17.720736 R25-M0-N3-C:J13-U11 RAS KERNEL FATAL machine check: i-fetch......................0
KERNSTOR 1118769323 2005.06.14 R20-M0-ND-C:J15-U11 2005-06-14-10.15.23.940275 R20-M0-ND-C:J15-U11 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118769408 2005.06.14 R25-M1-NF-C:J16-U01 2005-06-14-10.16.48.088109 R25-M1-NF-C:J16-U01 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118769443 2005.06.14 R21-M0-NC-C:J13-U11 2005-06-14-10.17.23.235674 R21-M0-NC-C:J13-U11 RAS KERNEL FATAL data storage interrupt
KERNSTOR 1118769444 2005.06.14 R21-M0-N4-C:J05-U11 2005-06-14-10.17.24.504094 R21-M0-N4-C:J05-U11 RAS KERNEL FATAL data storage interrupt
- 1118769450 2005.06.14 R26-M0-N9-C:J12-U01 2005-06-14-10.17.30.392758 R26-M0-N9-C:J12-U01 RAS KERNEL FATAL data address: 0x00000002
- 1118769489 2005.06.14 R20-M0-N3-C:J06-U01 2005-06-14-10.18.09.061481 R20-M0-N3-C:J06-U01 RAS KERNEL FATAL instruction address: 0x00004ed8
- 1118769530 2005.06.14 R27-M1-NA-C:J16-U11 2005-06-14-10.18.50.161104 R27-M1-NA-C:J16-U11 RAS KERNEL FATAL exception syndrome register: 0x00800000
- 1118769561 2005.06.14 R26-M0-N4-C:J16-U01 2005-06-14-10.19.21.829677 R26-M0-N4-C:J16-U01 RAS KERNEL FATAL exception syndrome register: 0x00800000
- 1118769562 2005.06.14 R26-M0-N0-C:J04-U11 2005-06-14-10.19.22.998644 R26-M0-N0-C:J04-U11 RAS KERNEL FATAL exception syndrome register: 0x00800000
- 1118769574 2005.06.14 R26-M1-NC-C:J13-U11 2005-06-14-10.19.34.342222 R26-M1-NC-C:J13-U11 RAS KERNEL FATAL exception syndrome register: 0x00800000
- 1118769649 2005.06.14 R26-M0-N5-C:J08-U01 2005-06-14-10.20.49.630306 R26-M0-N5-C:J08-U01 RAS KERNEL FATAL program interrupt: illegal instruction......0
- 1118769697 2005.06.14 R26-M0-N0-C:J02-U01 2005-06-14-10.21.37.752462 R26-M0-N0-C:J02-U01 RAS KERNEL FATAL program interrupt: illegal instruction......0
- 1118769705 2005.06.14 R21-M1-N4-C:J10-U01 2005-06-14-10.21.45.802128 R21-M1-N4-C:J10-U01 RAS KERNEL FATAL data address: 0x00000002
- 1118769811 2005.06.14 R25-M1-N3-C:J16-U01 2005-06-14-10.23.31.130281 R25-M1-N3-C:J16-U01 RAS KERNEL FATAL exception syndrome register: 0x00800000
- 1118769984 2005.06.14 R20-M1-N8-C:J08-U11 2005-06-14-10.26.24.461952 R20-M1-N8-C:J08-U11 RAS KERNEL FATAL machine check: i-fetch......................0
- 1118769991 2005.06.14 R26-M0-NA-C:J04-U11 2005-06-14-10.26.31.242578 R26-M0-NA-C:J04-U11 RAS KERNEL FATAL data store interrupt caused by dcbf.........0
- 1118769993 2005.06.14 R20-M0-N4-C:J15-U01 2005-06-14-10.26.33.758477 R20-M0-N4-C:J15-U01 RAS KERNEL FATAL machine check: i-fetch......................0
- 1118769997 2005.06.14 R26-M0-N2-C:J05-U01 2005-06-14-10.26.37.865134 R26-M0-N2-C:J05-U01 RAS KERNEL FATAL data store interrupt caused by dcbf.........0
- 1118770028 2005.06.14 R20-M0-ND-C:J05-U01 2005-06-14-10.27.08.151309 R20-M0-ND-C:J05-U01 RAS KERNEL FATAL program interrupt: illegal instruction......0
- 1118770101 2005.06.14 R21-M1-N6-C:J03-U11 2005-06-14-10.28.21.217840 R21-M1-N6-C:J03-U11 RAS KERNEL FATAL program interrupt: illegal instruction......0
- 1118770227 2005.06.14 R25-M1-N6-C:J15-U11 2005-06-14-10.30.27.419268 R25-M1-N6-C:J15-U11 RAS KERNEL FATAL program interrupt: privileged instruction...0
- 1118770236 2005.06.14 R25-M1-N3-C:J14-U11 2005-06-14-10.30.36.559680 R25-M1-N3-C:J14-U11 RAS KERNEL FATAL program interrupt: privileged instruction...0
- 1118770309 2005.06.14 R20-M1-NB-C:J03-U11 2005-06-14-10.31.49.464584 R20-M1-NB-C:J03-U11 RAS KERNEL FATAL program interrupt: trap instruction.........0
- 1118770375 2005.06.14 R27-M0-N3-C:J15-U01 2005-06-14-10.32.55.359107 R27-M0-N3-C:J15-U01 RAS KERNEL FATAL program interrupt: imprecise exception......0
- 1118770603 2005.06.14 R24-M1-NE-C:J15-U11 2005-06-14-10.36.43.928442 R24-M1-NE-C:J15-U11 RAS KERNEL FATAL store operation.............................1
- 1118770625 2005.06.14 R20-M0-NF-C:J07-U01 2005-06-14-10.37.05.332040 R20-M0-NF-C:J07-U01 RAS KERNEL FATAL store operation.............................1
- 1118770646 2005.06.14 R27-M1-N9-C:J11-U11 2005-06-14-10.37.26.925212 R27-M1-N9-C:J11-U11 RAS KERNEL FATAL machine state register: 0x00002000
- 1118770918 2005.06.14 R24-M0-N5-C:J05-U01 2005-06-14-10.41.58.776391 R24-M0-N5-C:J05-U01 RAS KERNEL FATAL data store interrupt caused by icbi.........0
- 1118770954 2005.06.14 R26-M0-N4-C:J15-U11 2005-06-14-10.42.34.598602 R26-M0-N4-C:J15-U11 RAS KERNEL FATAL problem state (0=sup,1=usr).......0
- 1118770989 2005.06.14 R21-M1-N4-C:J17-U11 2005-06-14-10.43.09.420211 R21-M1-N4-C:J17-U11 RAS KERNEL FATAL data store interrupt caused by icbi.........0
- 1118771001 2005.06.14 R25-M0-N8-C:J16-U11 2005-06-14-10.43.21.406138 R25-M0-N8-C:J16-U11 RAS KERNEL FATAL data store interrupt caused by icbi.........0
- 1118771001 2005.06.14 R26-M1-NF-C:J11-U11 2005-06-14-10.43.21.582308 R26-M1-NF-C:J11-U11 RAS KERNEL FATAL floating point instr. enabled.....1
- 1118771007 2005.06.14 R27-M0-N2-C:J12-U11 2005-06-14-10.43.27.316459 R27-M0-N2-C:J12-U11 RAS KERNEL FATAL floating point instr. enabled.....1
- 1118771014 2005.06.14 R27-M1-NF-C:J03-U11 2005-06-14-10.43.34.293126 R27-M1-NF-C:J03-U11 RAS KERNEL FATAL floating point instr. enabled.....1
- 1118771075 2005.06.14 R24-M0-NF-C:J09-U01 2005-06-14-10.44.35.353282 R24-M0-NF-C:J09-U01 RAS KERNEL FATAL auxiliary processor.........................0
- 1118771085 2005.06.14 R27-M1-ND-C:J09-U01 2005-06-14-10.44.45.851799 R27-M1-ND-C:J09-U01 RAS KERNEL FATAL machine check enable..............0
- 1118771098 2005.06.14 R26-M1-N4-C:J13-U11 2005-06-14-10.44.58.632865 R26-M1-N4-C:J13-U11 RAS KERNEL FATAL machine check enable..............0
- 1118771115 2005.06.14 R26-M0-N5-C:J11-U11 2005-06-14-10.45.15.849740 R26-M0-N5-C:J11-U11 RAS KERNEL FATAL floating pt ex mode 0 enable......0
- 1118771259 2005.06.14 R24-M0-NA-C:J16-U11 2005-06-14-10.47.39.811377 R24-M0-NA-C:J16-U11 RAS KERNEL FATAL program interrupt: unimplemented operation..0
- 1118771302 2005.06.14 R27-M1-N8-C:J09-U11 2005-06-14-10.48.22.614188 R27-M1-N8-C:J09-U11 RAS KERNEL FATAL debug interrupt enable............0
- 1118771330 2005.06.14 R26-M1-N6-C:J05-U01 2005-06-14-10.48.50.649332 R26-M1-N6-C:J05-U01 RAS KERNEL FATAL floating pt ex mode 1 enable......0
- 1118771384 2005.06.14 R21-M1-N6-C:J13-U11 2005-06-14-10.49.44.238733 R21-M1-N6-C:J13-U11 RAS KERNEL FATAL byte ordering exception.....................0
- 1118771409 2005.06.14 R20-M1-N8-C:J15-U11 2005-06-14-10.50.09.563848 R20-M1-N8-C:J15-U11 RAS KERNEL FATAL byte ordering exception.....................0
- 1118771469 2005.06.14 R27-M1-N9-C:J03-U01 2005-06-14-10.51.09.848118 R27-M1-N9-C:J03-U01 RAS KERNEL FATAL data address space................0
- 1118771475 2005.06.14 R26-M1-NA-C:J06-U01 2005-06-14-10.51.15.744393 R26-M1-NA-C:J06-U01 RAS KERNEL FATAL data address space................0
- 1118771516 2005.06.14 R25-M0-N6-C:J14-U11 2005-06-14-10.51.56.234430 R25-M0-N6-C:J14-U11 RAS KERNEL FATAL program interrupt: imprecise exception......0
- 1118771528 2005.06.14 R26-M0-NF-C:J15-U01 2005-06-14-10.52.08.434393 R26-M0-NF-C:J15-U01 RAS KERNEL FATAL core configuration register: 0x00002000
- 1118771568 2005.06.14 R25-M1-N0-C:J07-U11 2005-06-14-10.52.48.343774 R25-M1-N0-C:J07-U11 RAS KERNEL FATAL program interrupt: imprecise exception......0
- 1118771679 2005.06.14 R21-M1-N5-C:J05-U11 2005-06-14-10.54.39.063056 R21-M1-N5-C:J05-U11 RAS KERNEL FATAL program interrupt: fp cr update.............0
- 1118771823 2005.06.14 R26-M1-ND-C:J05-U11 2005-06-14-10.57.03.800229 R26-M1-ND-C:J05-U11 RAS KERNEL FATAL guaranteed instruction cache block touch.0
- 1118771847 2005.06.14 R26-M0-N8-C:J03-U01 2005-06-14-10.57.27.640707 R26-M0-N8-C:J03-U01 RAS KERNEL FATAL guaranteed instruction cache block touch.0
- 1118771875 2005.06.14 R20-M1-NB-C:J11-U11 2005-06-14-10.57.55.166387 R20-M1-NB-C:J11-U11 RAS KERNEL FATAL program interrupt: fp cr field .............0
- 1118771909 2005.06.14 R24-M1-N5-C:J09-U01 2005-06-14-10.58.29.332156 R24-M1-N5-C:J09-U01 RAS KERNEL FATAL program interrupt: fp cr field .............0
- 1118771917 2005.06.14 R24-M0-N2-C:J04-U01 2005-06-14-10.58.37.145331 R24-M0-N2-C:J04-U01 RAS KERNEL FATAL program interrupt: fp cr field .............0
- 1118771928 2005.06.14 R27-M1-N4-C:J05-U11 2005-06-14-10.58.48.336960 R27-M1-N4-C:J05-U11 RAS KERNEL FATAL guaranteed data cache block touch........1
- 1118771934 2005.06.14 R24-M0-ND-C:J02-U11 2005-06-14-10.58.54.996783 R24-M0-ND-C:J02-U11 RAS KERNEL FATAL program interrupt: fp cr field .............0
- 1118771988 2005.06.14 R27-M0-N7-C:J02-U01 2005-06-14-10.59.48.153923 R27-M0-N7-C:J02-U01 RAS KERNEL FATAL force load/store alignment...............0
- 1118772022 2005.06.14 R26-M0-N6-C:J14-U11 2005-06-14-11.00.22.056566 R26-M0-N6-C:J14-U11 RAS KERNEL FATAL icache prefetch depth....................0
- 1118772057 2005.06.14 R24-M0-N2-C:J16-U11 2005-06-14-11.00.57.941787 R24-M0-N2-C:J16-U11 RAS KERNEL FATAL machine state register: 0x00002000
- 1118772070 2005.06.14 R21-M1-N2-C:J10-U11 2005-06-14-11.01.10.339106 R21-M1-N2-C:J10-U11 RAS KERNEL FATAL machine state register: 0x00002000
- 1118772122 2005.06.14 R27-M0-N1-C:J15-U01 2005-06-14-11.02.02.534437 R27-M0-N1-C:J15-U01 RAS KERNEL FATAL icache prefetch threshold................0
- 1118772122 2005.06.14 R27-M0-N1-C:J08-U11 2005-06-14-11.02.02.749469 R27-M0-N1-C:J08-U11 RAS KERNEL FATAL icache prefetch threshold................0
- 1118772150 2005.06.14 R24-M0-NC-C:J12-U11 2005-06-14-11.02.30.070774 R24-M0-NC-C:J12-U11 RAS KERNEL FATAL machine state register: 0x00002000
- 1118772188 2005.06.14 R20-M0-N3-C:J06-U11 2005-06-14-11.03.08.491655 R20-M0-N3-C:J06-U11 RAS KERNEL FATAL wait state enable.................0
- 1118772337 2005.06.14 R24-M0-N7-C:J06-U01 2005-06-14-11.05.37.845720 R24-M0-N7-C:J06-U01 RAS KERNEL FATAL critical input interrupt enable...0
- 1118772585 2005.06.14 R20-M1-N1-C:J10-U01 2005-06-14-11.09.45.838343 R20-M1-N1-C:J10-U01 RAS KERNEL FATAL problem state (0=sup,1=usr).......0
- 1118772602 2005.06.14 R21-M1-NA-C:J07-U01 2005-06-14-11.10.02.651943 R21-M1-NA-C:J07-U01 RAS KERNEL FATAL problem state (0=sup,1=usr).......0
- 1118772772 2005.06.14 R21-M0-NF-C:J09-U01 2005-06-14-11.12.52.380819 R21-M0-NF-C:J09-U01 RAS KERNEL FATAL floating point instr. enabled.....1
- 1118772810 2005.06.14 R27-M0-N9-C:J12-U01 2005-06-14-11.13.30.870052 R27-M0-N9-C:J12-U01 RAS KERNEL FATAL special purpose registers:
- 1118772877 2005.06.14 R27-M1-N3-C:J04-U11 2005-06-14-11.14.37.821210 R27-M1-N3-C:J04-U11 RAS KERNEL FATAL lr:00004ed0 cr:28244842 xer:20000002 ctr:00086000
- 1118772927 2005.06.14 R27-M0-N6-C:J02-U11 2005-06-14-11.15.27.823191 R27-M0-N6-C:J02-U11 RAS KERNEL FATAL rts internal error
- 1118773031 2005.06.14 R21-M0-N2-C:J17-U11 2005-06-14-11.17.11.514583 R21-M0-N2-C:J17-U11 RAS KERNEL FATAL floating pt ex mode 0 enable......0
- 1118773049 2005.06.14 R20-M1-NF-C:J14-U11 2005-06-14-11.17.29.948847 R20-M1-NF-C:J14-U11 RAS KERNEL FATAL floating pt ex mode 0 enable......0
- 1118773252 2005.06.14 R20-M1-N0-C:J10-U01 2005-06-14-11.20.52.106874 R20-M1-N0-C:J10-U01 RAS KERNEL FATAL debug wait enable.................0
- 1118773424 2005.06.14 R25-M0-N2-C:J04-U11 2005-06-14-11.23.44.638394 R25-M0-N2-C:J04-U11 RAS KERNEL FATAL floating pt ex mode 1 enable......0
- 1118773618 2005.06.14 R25-M1-N7-C:J09-U11 2005-06-14-11.26.58.490506 R25-M1-N7-C:J09-U11 RAS KERNEL FATAL instruction address space.........0
- 1118773741 2005.06.14 R21-M0-N9-C:J08-U01 2005-06-14-11.29.01.258503 R21-M0-N9-C:J08-U01 RAS KERNEL FATAL data address space................0
- 1118773750 2005.06.14 R25-M1-NF-C:J16-U11 2005-06-14-11.29.10.800711 R25-M1-NF-C:J16-U11 RAS KERNEL FATAL data address space................0
- 1118773772 2005.06.14 R24-M1-N0-C:J04-U11 2005-06-14-11.29.32.480633 R24-M1-N0-C:J04-U11 RAS KERNEL FATAL data address space................0
- 1118773807 2005.06.14 R20-M1-N5-C:J13-U01 2005-06-14-11.30.07.879884 R20-M1-N5-C:J13-U01 RAS KERNEL FATAL core configuration register: 0x00002000
- 1118773808 2005.06.14 R20-M1-NA-C:J03-U01 2005-06-14-11.30.08.524358 R20-M1-NA-C:J03-U01 RAS KERNEL FATAL core configuration register: 0x00002000
- 1118773837 2005.06.14 R24-M0-NF-C:J08-U01 2005-06-14-11.30.37.732517 R24-M0-NF-C:J08-U01 RAS KERNEL FATAL core configuration register: 0x00002000
- 1118773839 2005.06.14 R21-M0-ND-C:J17-U11 2005-06-14-11.30.39.961375 R21-M0-ND-C:J17-U11 RAS KERNEL FATAL core configuration register: 0x00002000
- 1118773862 2005.06.14 R21-M0-N7-C:J02-U11 2005-06-14-11.31.02.213030 R21-M0-N7-C:J02-U11 RAS KERNEL FATAL core configuration register: 0x00002000
- 1118773883 2005.06.14 R24-M1-N6-C:J17-U01 2005-06-14-11.31.23.322583 R24-M1-N6-C:J17-U01 RAS KERNEL FATAL core configuration register: 0x00002000
- 1118773941 2005.06.14 R20-M1-NA-C:J04-U01 2005-06-14-11.32.21.174785 R20-M1-NA-C:J04-U01 RAS KERNEL FATAL disable store gathering..................0
- 1118774011 2005.06.14 R24-M0-NA-C:J14-U01 2005-06-14-11.33.31.835838 R24-M0-NA-C:J14-U01 RAS KERNEL FATAL disable store gathering..................0
- 1118774042 2005.06.14 R25-M1-N8-C:J03-U11 2005-06-14-11.34.02.588297 R25-M1-N8-C:J03-U11 RAS KERNEL FATAL disable store gathering..................0
- 1118783531 2005.06.14 R22-M0-N7-C:J08-U11 2005-06-14-14.12.11.357904 R22-M0-N7-C:J08-U11 RAS KERNEL INFO generating core.42
- 1118784453 2005.06.14 R07-M0-N6-C:J11-U01 2005-06-14-14.27.33.186273 R07-M0-N6-C:J11-U01 RAS KERNEL INFO total of 1 ddr error(s) detected and corrected
- 1118795802 2005.06.14 R23-M0-ND-C:J13-U01 2005-06-14-17.36.42.381657 R23-M0-ND-C:J13-U01 RAS KERNEL INFO generating core.145
- 1118809915 2005.06.14 R02-M1-N0-C:J12-U11 2005-06-14-21.31.55.295748 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118810502 2005.06.14 R02-M1-N0-C:J12-U11 2005-06-14-21.41.42.351016 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118811100 2005.06.14 R02-M1-N0-C:J12-U11 2005-06-14-21.51.40.083003 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118811393 2005.06.14 R02-M1-N0-C:J12-U11 2005-06-14-21.56.33.279265 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118812716 2005.06.14 R02-M1-N0-C:J12-U11 2005-06-14-22.18.36.751583 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118813039 2005.06.14 R02-M1-N0-C:J12-U11 2005-06-14-22.23.59.470421 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118843835 2005.06.15 R21-M1-N1-C:J07-U11 2005-06-15-06.57.15.905673 R21-M1-N1-C:J07-U11 RAS KERNEL INFO generating core.1978
- 1118843857 2005.06.15 R25-M1-N1-C:J15-U11 2005-06-15-06.57.37.567187 R25-M1-N1-C:J15-U11 RAS KERNEL INFO generating core.2040
- 1118846432 2005.06.15 R22-M0-NE-C:J04-U11 2005-06-15-07.40.32.710710 R22-M0-NE-C:J04-U11 RAS KERNEL INFO generating core.15
- 1118852240 2005.06.15 R02-M1-N0-C:J12-U11 2005-06-15-09.17.20.509519 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118852544 2005.06.15 R02-M1-N0-C:J12-U11 2005-06-15-09.22.24.246149 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118852625 2005.06.15 R02-M1-N0-C:J12-U11 2005-06-15-09.23.45.703981 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118852660 2005.06.15 R02-M1-N0-C:J12-U11 2005-06-15-09.24.20.246651 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118852703 2005.06.15 R02-M1-N0-C:J12-U11 2005-06-15-09.25.03.582371 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118852706 2005.06.15 R02-M1-N0-C:J12-U11 2005-06-15-09.25.06.962646 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118852913 2005.06.15 R02-M1-N0-C:J12-U11 2005-06-15-09.28.33.631142 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118853076 2005.06.15 R34-M1-N9-C:J07-U11 2005-06-15-09.31.16.623218 R34-M1-N9-C:J07-U11 RAS KERNEL INFO generating core.16218
- 1118860111 2005.06.15 R02-M1-N0-C:J12-U11 2005-06-15-11.28.31.099655 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1118908990 2005.06.16 R02-M1-N1-C:J12-U01 2005-06-16-01.03.10.008103 R02-M1-N1-C:J12-U01 RAS KERNEL INFO instruction cache parity error corrected
- 1118911762 2005.06.16 R34-M0-NF-C:J03-U01 2005-06-16-01.49.22.748250 R34-M0-NF-C:J03-U01 RAS KERNEL INFO generating core.8531
- 1118911785 2005.06.16 R27-M1-ND-C:J11-U01 2005-06-16-01.49.45.558500 R27-M1-ND-C:J11-U01 RAS KERNEL INFO generating core.2961
- 1118911796 2005.06.16 R04-M1-N8-C:J09-U11 2005-06-16-01.49.56.966924 R04-M1-N8-C:J09-U11 RAS KERNEL INFO generating core.15966
- 1118915888 2005.06.16 R20-M1-N7-C:J08-U11 2005-06-16-02.58.08.116468 R20-M1-N7-C:J08-U11 RAS KERNEL INFO generating core.42
- 1118915889 2005.06.16 R24-M1-NE-C:J13-U11 2005-06-16-02.58.09.310238 R24-M1-NE-C:J13-U11 RAS KERNEL INFO generating core.93
- 1118916047 2005.06.16 R21-M1-N4-C:J12-U11 2005-06-16-03.00.47.249279 R21-M1-N4-C:J12-U11 RAS KERNEL INFO generating core.2605
- 1118916053 2005.06.16 R27-M0-N8-C:J11-U01 2005-06-16-03.00.53.351857 R27-M0-N8-C:J11-U01 RAS KERNEL INFO generating core.6037
- 1118916060 2005.06.16 R24-M1-N4-C:J07-U01 2005-06-16-03.01.00.566385 R24-M1-N4-C:J07-U01 RAS KERNEL INFO generating core.886
- 1118916076 2005.06.16 R23-M1-N4-C:J10-U11 2005-06-16-03.01.16.485975 R23-M1-N4-C:J10-U11 RAS KERNEL INFO generating core.3053
- 1118916076 2005.06.16 R23-M1-N4-C:J08-U01 2005-06-16-03.01.16.662641 R23-M1-N4-C:J08-U01 RAS KERNEL INFO generating core.2790
APPREAD 1118958902 2005.06.16 R32-M1-N8-I:J18-U01 2005-06-16-14.55.02.074340 R32-M1-N8-I:J18-U01 RAS APP FATAL ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:51706
- 1118964552 2005.06.16 R02-M1-N0-C:J12-U11 2005-06-16-16.29.12.853302 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1119055358 2005.06.17 R12-M1-NB-C:J15-U01 2005-06-17-17.42.38.875141 R12-M1-NB-C:J15-U01 RAS KERNEL INFO generating core.1488
- 1119103692 2005.06.18 R16-M1-N2-C:J17-U01 2005-06-18-07.08.12.026766 R16-M1-N2-C:J17-U01 RAS KERNEL INFO CE sym 2, at 0x0b85f680, mask 0x0e
- 1119151510 2005.06.18 R36-M0-NF-C:J13-U01 2005-06-18-20.25.10.503085 R36-M0-NF-C:J13-U01 RAS KERNEL INFO generating core.2186
- 1119166454 2005.06.19 R04-M0-N9-C:J07-U01 2005-06-19-00.34.14.589889 R04-M0-N9-C:J07-U01 RAS KERNEL INFO generating core.978
- 1119166473 2005.06.19 R05-M1-N3-C:J15-U11 2005-06-19-00.34.33.890486 R05-M1-N3-C:J15-U11 RAS KERNEL INFO generating core.2808
- 1119166494 2005.06.19 R01-M1-N4-C:J07-U01 2005-06-19-00.34.54.479358 R01-M1-N4-C:J07-U01 RAS KERNEL INFO generating core.2486
- 1119290966 2005.06.20 R16-M1-N6-C:J08-U01 2005-06-20-11.09.26.582302 R16-M1-N6-C:J08-U01 RAS KERNEL INFO generating core.334
- 1119291000 2005.06.20 R17-M1-ND-C:J12-U11 2005-06-20-11.10.00.255483 R17-M1-ND-C:J12-U11 RAS KERNEL INFO generating core.5401
- 1119291039 2005.06.20 R00-M1-N3-C:J08-U11 2005-06-20-11.10.39.035194 R00-M1-N3-C:J08-U11 RAS KERNEL INFO generating core.14418
- 1119291044 2005.06.20 R12-M1-N2-C:J09-U11 2005-06-20-11.10.44.787932 R12-M1-N2-C:J09-U11 RAS KERNEL INFO generating core.2558
- 1119291099 2005.06.20 R11-M0-ND-C:J11-U11 2005-06-20-11.11.39.810523 R11-M0-ND-C:J11-U11 RAS KERNEL INFO generating core.9785
- 1119291235 2005.06.20 R01-M0-NE-C:J06-U01 2005-06-20-11.13.55.422033 R01-M0-NE-C:J06-U01 RAS KERNEL INFO generating core.4614
- 1119291291 2005.06.20 R13-M1-N8-C:J05-U11 2005-06-20-11.14.51.439913 R13-M1-N8-C:J05-U11 RAS KERNEL INFO generating core.7615
- 1119291293 2005.06.20 R10-M0-N8-C:J03-U11 2005-06-20-11.14.53.357175 R10-M0-N8-C:J03-U11 RAS KERNEL INFO generating core.15935
- 1119309833 2005.06.20 R14-M1-N0-I:J18-U01 2005-06-20-16.23.53.944371 R14-M1-N0-I:J18-U01 RAS APP FATAL ciod: Error loading /home/draeger/testQboxhang-nozerobytebug-nosleepyescomm: invalid or missing program image, No such file or directory
APPCHILD 1119319455 2005.06.20 R12-M1-NC-I:J18-U11 2005-06-20-19.04.15.002425 R12-M1-NC-I:J18-U11 RAS APP FATAL ciod: Error creating node map from file /p/gb2/cabot/miranda/newmaps/8k_128x64x1_8x4x4.map: No child processes
- 1119324666 2005.06.20 R16-M1-N2-C:J17-U01 2005-06-20-20.31.06.282052 R16-M1-N2-C:J17-U01 RAS KERNEL INFO CE sym 0, at 0x1b858280, mask 0x40
- 1119377678 2005.06.21 R20-M0-N9-C:J11-U11 2005-06-21-11.14.38.018978 R20-M0-N9-C:J11-U11 RAS KERNEL INFO generating core.8089
- 1119377680 2005.06.21 R20-M1-N2-C:J17-U01 2005-06-21-11.14.40.347916 R20-M1-N2-C:J17-U01 RAS KERNEL INFO generating core.4660
- 1119377709 2005.06.21 R24-M1-NB-C:J11-U01 2005-06-21-11.15.09.415157 R24-M1-NB-C:J11-U01 RAS KERNEL INFO generating core.721
- 1119377756 2005.06.21 R21-M1-N6-C:J02-U01 2005-06-21-11.15.56.944563 R21-M1-N6-C:J02-U01 RAS KERNEL INFO generating core.5287
- 1119377784 2005.06.21 R20-M1-N8-C:J04-U11 2005-06-21-11.16.24.473585 R20-M1-N8-C:J04-U11 RAS KERNEL INFO generating core.783
- 1119381883 2005.06.21 R05-M1-N8-C:J13-U01 2005-06-21-12.24.43.265881 R05-M1-N8-C:J13-U01 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 17, bit 1
- 1119383898 2005.06.21 R02-M1-N0-C:J12-U11 2005-06-21-12.58.18.149869 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1119402454 2005.06.21 R22-M0-N4-I:J18-U11 2005-06-21-18.07.34.337232 R22-M0-N4-I:J18-U11 RAS APP FATAL ciod: Error loading ./runtime_malloc: invalid or missing program image, No such file or directory
- 1119450937 2005.06.22 R02-M1-N0-C:J12-U11 2005-06-22-07.35.37.412184 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1119475796 2005.06.22 R22-M0-NA-C:J02-U11 2005-06-22-14.29.56.824423 R22-M0-NA-C:J02-U11 RAS KERNEL INFO 199680 double-hummer alignment exceptions
- 1119478812 2005.06.22 R20-M0-N3-C:J14-U01 2005-06-22-15.20.12.174541 R20-M0-N3-C:J14-U01 RAS KERNEL INFO 1969920 double-hummer alignment exceptions
- 1119479310 2005.06.22 R20-M0-N6-C:J03-U01 2005-06-22-15.28.30.643475 R20-M0-N6-C:J03-U01 RAS KERNEL INFO 2576000 double-hummer alignment exceptions
- 1119479317 2005.06.22 R21-M1-N1-C:J08-U11 2005-06-22-15.28.37.811949 R21-M1-N1-C:J08-U11 RAS KERNEL INFO 2576000 double-hummer alignment exceptions
- 1119481305 2005.06.22 R22-M1-NA-C:J04-U11 2005-06-22-16.01.45.870183 R22-M1-NA-C:J04-U11 RAS KERNEL INFO 6182400 double-hummer alignment exceptions
- 1119545636 2005.06.23 R23-M1-N7-C:J02-U01 2005-06-23-09.53.56.123342 R23-M1-N7-C:J02-U01 RAS KERNEL INFO 6182400 double-hummer alignment exceptions
- 1119569236 2005.06.23 R22-M0-N6-C:J12-U11 2005-06-23-16.27.16.266001 R22-M0-N6-C:J12-U11 RAS KERNEL INFO 6182400 double-hummer alignment exceptions
- 1119571364 2005.06.23 R23-M1-N7-C:J14-U01 2005-06-23-17.02.44.950044 R23-M1-N7-C:J14-U01 RAS KERNEL INFO 3091200 double-hummer alignment exceptions
- 1119571369 2005.06.23 R23-M1-ND-C:J02-U01 2005-06-23-17.02.49.991152 R23-M1-ND-C:J02-U01 RAS KERNEL INFO 3091200 double-hummer alignment exceptions
- 1119572030 2005.06.23 R23-M1-N8-C:J06-U01 2005-06-23-17.13.50.341611 R23-M1-N8-C:J06-U01 RAS KERNEL INFO 3091200 double-hummer alignment exceptions
- 1119625799 2005.06.24 R03-M0-NB-C:J10-U11 2005-06-24-08.09.59.585592 R03-M0-NB-C:J10-U11 RAS KERNEL INFO CE sym 12, at 0x1b786f60, mask 0x04
- 1119638161 2005.06.24 R16-M1-N2-C:J17-U01 2005-06-24-11.36.01.774405 R16-M1-N2-C:J17-U01 RAS KERNEL INFO CE sym 0, at 0x1b8584e0, mask 0x80
- 1119677904 2005.06.24 R36-M0-N5-C:J03-U01 2005-06-24-22.38.24.830252 R36-M0-N5-C:J03-U01 RAS KERNEL INFO CE sym 23, at 0x03f89c00, mask 0x10
- 1119693767 2005.06.25 R22-M0-NF-C:J02-U01 2005-06-25-03.02.47.673562 R22-M0-NF-C:J02-U01 RAS KERNEL INFO 23431872 double-hummer alignment exceptions
- 1119695647 2005.06.25 R31-M1-ND-C:J05-U01 2005-06-25-03.34.07.686513 R31-M1-ND-C:J05-U01 RAS KERNEL INFO 259966605 double-hummer alignment exceptions
- 1119695690 2005.06.25 R35-M0-NF-C:J11-U11 2005-06-25-03.34.50.645459 R35-M0-NF-C:J11-U11 RAS KERNEL INFO 255244071 double-hummer alignment exceptions
- 1119695755 2005.06.25 R32-M0-N4-C:J02-U01 2005-06-25-03.35.55.677192 R32-M0-N4-C:J02-U01 RAS KERNEL INFO 259822751 double-hummer alignment exceptions
- 1119714915 2005.06.25 R30-M1-N7-C:J12-U01 2005-06-25-08.55.15.625203 R30-M1-N7-C:J12-U01 RAS KERNEL INFO generating core.24721
- 1119714992 2005.06.25 R20-M1-NB-C:J17-U01 2005-06-25-08.56.32.658269 R20-M1-NB-C:J17-U01 RAS KERNEL INFO generating core.4184
- 1119715100 2005.06.25 R26-M0-N6-C:J13-U11 2005-06-25-08.58.20.635863 R26-M0-N6-C:J13-U11 RAS KERNEL INFO generating core.58109
- 1119715124 2005.06.25 R27-M1-N7-C:J10-U01 2005-06-25-08.58.44.769752 R27-M1-N7-C:J10-U01 RAS KERNEL INFO generating core.9881
- 1119715242 2005.06.25 R21-M0-ND-C:J03-U11 2005-06-25-09.00.42.287070 R21-M0-ND-C:J03-U11 RAS KERNEL INFO generating core.19579
- 1119715309 2005.06.25 R34-M0-N2-C:J12-U01 2005-06-25-09.01.49.568758 R34-M0-N2-C:J12-U01 RAS KERNEL INFO generating core.4501
- 1119715342 2005.06.25 R31-M1-NE-C:J06-U11 2005-06-25-09.02.22.976560 R31-M1-NE-C:J06-U11 RAS KERNEL INFO generating core.50230
- 1119715346 2005.06.25 R30-M1-NA-C:J14-U11 2005-06-25-09.02.26.738118 R30-M1-NA-C:J14-U11 RAS KERNEL INFO generating core.62516
- 1119715392 2005.06.25 R34-M1-ND-C:J10-U01 2005-06-25-09.03.12.849230 R34-M1-ND-C:J10-U01 RAS KERNEL INFO generating core.27921
- 1119715413 2005.06.25 R25-M0-N6-C:J08-U11 2005-06-25-09.03.33.373728 R25-M0-N6-C:J08-U11 RAS KERNEL INFO generating core.16830
- 1119715453 2005.06.25 R21-M1-N7-C:J10-U01 2005-06-25-09.04.13.083368 R21-M1-N7-C:J10-U01 RAS KERNEL INFO generating core.9369
- 1119715567 2005.06.25 R23-M1-N6-C:J14-U01 2005-06-25-09.06.07.234215 R23-M1-N6-C:J14-U01 RAS KERNEL INFO generating core.10140
- 1119715593 2005.06.25 R32-M0-NA-C:J13-U11 2005-06-25-09.06.33.323216 R32-M0-NA-C:J13-U11 RAS KERNEL INFO generating core.37749
- 1119715602 2005.06.25 R35-M0-N6-C:J06-U11 2005-06-25-09.06.42.210046 R35-M0-N6-C:J06-U11 RAS KERNEL INFO generating core.9654
- 1119715612 2005.06.25 R23-M0-N9-C:J05-U01 2005-06-25-09.06.52.855570 R23-M0-N9-C:J05-U01 RAS KERNEL INFO generating core.56155
- 1119715625 2005.06.25 R24-M1-N9-C:J06-U11 2005-06-25-09.07.05.564792 R24-M1-N9-C:J06-U11 RAS KERNEL INFO generating core.40250
- 1119715819 2005.06.25 R15-M1-NB-C:J12-U11 2005-06-25-09.10.19.844956 R15-M1-NB-C:J12-U11 RAS KERNEL INFO generating core.45353
- 1119715834 2005.06.25 R15-M1-NF-C:J07-U11 2005-06-25-09.10.34.515707 R15-M1-NF-C:J07-U11 RAS KERNEL INFO generating core.9578
- 1119715845 2005.06.25 R17-M1-N9-C:J16-U01 2005-06-25-09.10.45.117528 R17-M1-N9-C:J16-U01 RAS KERNEL INFO generating core.14856
- 1119715908 2005.06.25 R00-M1-NA-C:J14-U01 2005-06-25-09.11.48.847599 R00-M1-NA-C:J14-U01 RAS KERNEL INFO generating core.62468
- 1119715922 2005.06.25 R16-M1-NA-C:J02-U01 2005-06-25-09.12.02.795224 R16-M1-NA-C:J02-U01 RAS KERNEL INFO generating core.38415
- 1119715980 2005.06.25 R00-M0-N1-C:J09-U11 2005-06-25-09.13.00.933541 R00-M0-N1-C:J09-U11 RAS KERNEL INFO generating core.39138
- 1119725008 2005.06.25 R21-M1-N5-C:J08-U01 2005-06-25-11.43.28.548275 R21-M1-N5-C:J08-U01 RAS KERNEL INFO generating core.10394
- 1119725040 2005.06.25 R05-M0-N7-C:J04-U01 2005-06-25-11.44.00.166788 R05-M0-N7-C:J04-U01 RAS KERNEL INFO generating core.41347
- 1119725071 2005.06.25 R04-M0-N1-C:J03-U01 2005-06-25-11.44.31.893401 R04-M0-N1-C:J03-U01 RAS KERNEL INFO generating core.7619
- 1119725148 2005.06.25 R31-M1-N2-C:J07-U01 2005-06-25-11.45.48.948077 R31-M1-N2-C:J07-U01 RAS KERNEL INFO generating core.54486
- 1119725160 2005.06.25 R33-M1-N2-C:J10-U01 2005-06-25-11.46.00.991233 R33-M1-N2-C:J10-U01 RAS KERNEL INFO generating core.22421
- 1119725245 2005.06.25 R33-M1-NE-C:J11-U01 2005-06-25-11.47.25.436889 R33-M1-NE-C:J11-U01 RAS KERNEL INFO generating core.18261
- 1119725280 2005.06.25 R20-M0-N7-C:J15-U11 2005-06-25-11.48.00.361268 R20-M0-N7-C:J15-U11 RAS KERNEL INFO generating core.58616
- 1119725305 2005.06.25 R27-M1-N9-C:J17-U11 2005-06-25-11.48.25.313227 R27-M1-N9-C:J17-U11 RAS KERNEL INFO generating core.47736
- 1119725352 2005.06.25 R24-M0-N7-C:J17-U01 2005-06-25-11.49.12.068081 R24-M0-N7-C:J17-U01 RAS KERNEL INFO generating core.25048
- 1119725352 2005.06.25 R24-M0-N7-C:J10-U01 2005-06-25-11.49.12.823777 R24-M0-N7-C:J10-U01 RAS KERNEL INFO generating core.58777
- 1119725476 2005.06.25 R33-M0-N1-C:J11-U11 2005-06-25-11.51.16.800981 R33-M0-N1-C:J11-U11 RAS KERNEL INFO generating core.49137
- 1119725489 2005.06.25 R27-M1-NF-C:J10-U11 2005-06-25-11.51.29.486868 R27-M1-NF-C:J10-U11 RAS KERNEL INFO generating core.42553
- 1119726280 2005.06.25 R10-M1-NA-C:J13-U11 2005-06-25-12.04.40.064277 R10-M1-NA-C:J13-U11 RAS KERNEL INFO generating core.2109
- 1119726375 2005.06.25 R14-M1-N6-C:J06-U01 2005-06-25-12.06.15.851937 R14-M1-N6-C:J06-U01 RAS KERNEL INFO generating core.718
- 1119726549 2005.06.25 R01-M1-NB-C:J14-U01 2005-06-25-12.09.09.759272 R01-M1-NB-C:J14-U01 RAS KERNEL INFO generating core.10752
- 1119726634 2005.06.25 R06-M0-N7-C:J04-U01 2005-06-25-12.10.34.183984 R06-M0-N7-C:J04-U01 RAS KERNEL INFO generating core.323
- 1119736307 2005.06.25 R01-M1-N8-I:J18-U11 2005-06-25-14.51.47.575572 R01-M1-N8-I:J18-U11 RAS APP FATAL ciod: Error loading /home/glosli/src/ddcMD/ddcMD/1.1.13/ddcMDbglV: invalid or missing program image, No such file or directory
- 1119799504 2005.06.26 R01-M1-ND-C:J15-U11 2005-06-26-08.25.04.032782 R01-M1-ND-C:J15-U11 RAS KERNEL INFO CE sym 5, at 0x11042a80, mask 0x04
- 1119801659 2005.06.26 R02-M1-N0-C:J12-U11 2005-06-26-09.00.59.195050 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1119802755 2005.06.26 R03-M0-N9-C:J09-U01 2005-06-26-09.19.15.022089 R03-M0-N9-C:J09-U01 RAS KERNEL INFO generating core.21112
- 1119802781 2005.06.26 R07-M1-N0-C:J15-U11 2005-06-26-09.19.41.368208 R07-M1-N0-C:J15-U11 RAS KERNEL INFO program interrupt
- 1119802848 2005.06.26 R36-M1-NC-C:J06-U11 2005-06-26-09.20.48.858436 R36-M1-NC-C:J06-U11 RAS KERNEL INFO generating core.28370
- 1119803084 2005.06.26 R36-M0-ND-C:J13-U11 2005-06-26-09.24.44.819150 R36-M0-ND-C:J13-U11 RAS KERNEL INFO program interrupt: illegal instruction......1
- 1119803149 2005.06.26 R01-M1-N0-C:J04-U11 2005-06-26-09.25.49.706431 R01-M1-N0-C:J04-U11 RAS KERNEL INFO program interrupt: illegal instruction......1
- 1119803207 2005.06.26 R05-M1-N9-C:J14-U11 2005-06-26-09.26.47.879500 R05-M1-N9-C:J14-U11 RAS KERNEL INFO program interrupt: privileged instruction...0
- 1119803468 2005.06.26 R36-M1-ND-C:J16-U11 2005-06-26-09.31.08.715435 R36-M1-ND-C:J16-U11 RAS KERNEL INFO data store interrupt caused by dcbf.........0
- 1119803528 2005.06.26 R05-M0-NE-C:J05-U01 2005-06-26-09.32.08.442047 R05-M0-NE-C:J05-U01 RAS KERNEL INFO data store interrupt caused by icbi.........0
- 1119803550 2005.06.26 R35-M0-NB-C:J16-U01 2005-06-26-09.32.30.029388 R35-M0-NB-C:J16-U01 RAS KERNEL INFO data store interrupt caused by icbi.........0
- 1119803599 2005.06.26 R01-M0-N2-C:J08-U01 2005-06-26-09.33.19.958512 R01-M0-N2-C:J08-U01 RAS KERNEL INFO auxiliary processor.........................0
- 1119803602 2005.06.26 R00-M0-NB-C:J09-U01 2005-06-26-09.33.22.839312 R00-M0-NB-C:J09-U01 RAS KERNEL INFO auxiliary processor.........................0
- 1119803608 2005.06.26 R06-M0-NE-C:J12-U11 2005-06-26-09.33.28.642706 R06-M0-NE-C:J12-U11 RAS KERNEL INFO auxiliary processor.........................0
- 1119803693 2005.06.26 R00-M1-N3-C:J11-U01 2005-06-26-09.34.53.251232 R00-M1-N3-C:J11-U01 RAS KERNEL INFO program interrupt: unimplemented operation..0
- 1119803805 2005.06.26 R36-M0-N9-C:J12-U01 2005-06-26-09.36.45.237515 R36-M0-N9-C:J12-U01 RAS KERNEL INFO program interrupt: imprecise exception......0
- 1119803856 2005.06.26 R02-M1-N4-C:J12-U01 2005-06-26-09.37.36.335874 R02-M1-N4-C:J12-U01 RAS KERNEL INFO program interrupt: imprecise exception......0
- 1119806904 2005.06.26 R05-M0-N3-C:J11-U11 2005-06-26-10.28.24.350248 R05-M0-N3-C:J11-U11 RAS KERNEL INFO generating core.14700
- 1119806939 2005.06.26 R05-M0-N7-C:J13-U01 2005-06-26-10.28.59.279042 R05-M0-N7-C:J13-U01 RAS KERNEL INFO program interrupt
- 1119806955 2005.06.26 R03-M1-NC-C:J05-U11 2005-06-26-10.29.15.890436 R03-M1-NC-C:J05-U11 RAS KERNEL INFO program interrupt
- 1119806971 2005.06.26 R05-M1-N8-C:J15-U11 2005-06-26-10.29.31.819506 R05-M1-N8-C:J15-U11 RAS KERNEL INFO instruction address: 0x0062fe04
- 1119807051 2005.06.26 R05-M0-N6-C:J06-U11 2005-06-26-10.30.51.645473 R05-M0-N6-C:J06-U11 RAS KERNEL INFO exception syndrome register: 0x08000000
- 1119807165 2005.06.26 R00-M0-N4-C:J02-U01 2005-06-26-10.32.45.810547 R00-M0-N4-C:J02-U01 RAS KERNEL INFO program interrupt: illegal instruction......1
- 1119807218 2005.06.26 R35-M1-NF-C:J15-U01 2005-06-26-10.33.38.646025 R35-M1-NF-C:J15-U01 RAS KERNEL INFO program interrupt: privileged instruction...0
- 1119807482 2005.06.26 R05-M0-N5-C:J09-U11 2005-06-26-10.38.02.680379 R05-M0-N5-C:J09-U11 RAS KERNEL INFO data store interrupt caused by dcbf.........0
- 1119807491 2005.06.26 R02-M0-N9-C:J06-U01 2005-06-26-10.38.11.390037 R02-M0-N9-C:J06-U01 RAS KERNEL INFO data store interrupt caused by dcbf.........0
- 1119807588 2005.06.26 R33-M1-N3-C:J16-U11 2005-06-26-10.39.48.944209 R33-M1-N3-C:J16-U11 RAS KERNEL INFO auxiliary processor.........................0
- 1119859191 2005.06.27 R16-M1-N2-C:J17-U01 2005-06-27-00.59.51.127175 R16-M1-N2-C:J17-U01 RAS KERNEL INFO CE sym 0, at 0x1b8594e0, mask 0x40
- 1119977619 2005.06.28 R02-M1-NE 2005-06-28-09.53.39.479164 R02-M1-NE NULL DISCOVERY WARNING Node card is not fully functional
- 1119989828 2005.06.28 R26-M0-NB-C:J07-U01 2005-06-28-13.17.08.922455 R26-M0-NB-C:J07-U01 RAS KERNEL INFO CE sym 10, at 0x08e70580, mask 0x08
- 1120091427 2005.06.29 R05-M1-N3-C:J03-U11 2005-06-29-17.30.27.216695 R05-M1-N3-C:J03-U11 RAS KERNEL INFO generating core.1786
- 1120091562 2005.06.29 R37-M0-NA-C:J15-U01 2005-06-29-17.32.42.585032 R37-M0-NA-C:J15-U01 RAS KERNEL INFO generating core.26918
- 1120091766 2005.06.29 R32-M1-ND-C:J04-U01 2005-06-29-17.36.06.730195 R32-M1-ND-C:J04-U01 RAS KERNEL INFO generating core.10125
- 1120091839 2005.06.29 R15-M1-NB-C:J16-U01 2005-06-29-17.37.19.696868 R15-M1-NB-C:J16-U01 RAS KERNEL INFO generating core.4230
- 1120091979 2005.06.29 R04-M0-N9-C:J15-U11 2005-06-29-17.39.39.542921 R04-M0-N9-C:J15-U11 RAS KERNEL INFO generating core.179
- 1120092016 2005.06.29 R05-M1-ND-C:J06-U11 2005-06-29-17.40.16.155721 R05-M1-ND-C:J06-U11 RAS KERNEL INFO generating core.1177
- 1120092072 2005.06.29 R23-M1-NC-C:J05-U11 2005-06-29-17.41.12.397279 R23-M1-NC-C:J05-U11 RAS KERNEL INFO generating core.32693
- 1120092194 2005.06.29 R12-M0-N0-C:J13-U01 2005-06-29-17.43.14.852439 R12-M0-N0-C:J13-U01 RAS KERNEL INFO generating core.7151
- 1120092215 2005.06.29 R16-M0-N0-C:J13-U01 2005-06-29-17.43.35.753036 R16-M0-N0-C:J13-U01 RAS KERNEL INFO generating core.7023
- 1120093988 2005.06.29 R36-M1-N1-C:J05-U01 2005-06-29-18.13.08.381268 R36-M1-N1-C:J05-U01 RAS KERNEL INFO 344040 double-hummer alignment exceptions
- 1120094424 2005.06.29 R10-M0-N9-C:J05-U01 2005-06-29-18.20.24.553240 R10-M0-N9-C:J05-U01 RAS KERNEL INFO 344040 double-hummer alignment exceptions
- 1120094724 2005.06.29 R03-M0-N2-C:J02-U01 2005-06-29-18.25.24.062292 R03-M0-N2-C:J02-U01 RAS KERNEL INFO 344040 double-hummer alignment exceptions
- 1120145091 2005.06.30 R26-M0-NA-C:J17-U11 2005-06-30-08.24.51.228685 R26-M0-NA-C:J17-U11 RAS KERNEL INFO generating core.29308
- 1120145216 2005.06.30 R26-M1-ND-C:J08-U01 2005-06-30-08.26.56.428983 R26-M1-ND-C:J08-U01 RAS KERNEL INFO generating core.2586
- 1120145227 2005.06.30 R22-M1-N3-C:J14-U01 2005-06-30-08.27.07.043050 R22-M1-N3-C:J14-U01 RAS KERNEL INFO generating core.6040
- 1120145530 2005.06.30 R17-M0-NF-C:J16-U01 2005-06-30-08.32.10.323350 R17-M0-NF-C:J16-U01 RAS KERNEL INFO generating core.16904
- 1120145605 2005.06.30 R16-M0-NF-C:J07-U01 2005-06-30-08.33.25.225978 R16-M0-NF-C:J07-U01 RAS KERNEL INFO generating core.26186
- 1120145611 2005.06.30 R16-M0-NE-C:J14-U11 2005-06-30-08.33.31.578264 R16-M0-NE-C:J14-U11 RAS KERNEL INFO generating core.26156
- 1120145615 2005.06.30 R00-M0-ND-C:J17-U01 2005-06-30-08.33.35.910212 R00-M0-ND-C:J17-U01 RAS KERNEL INFO generating core.2112
- 1120145688 2005.06.30 R11-M0-N9-C:J03-U11 2005-06-30-08.34.48.113948 R11-M0-N9-C:J03-U11 RAS KERNEL INFO generating core.23659
- 1120145763 2005.06.30 R06-M0-NE-C:J11-U01 2005-06-30-08.36.03.800737 R06-M0-NE-C:J11-U01 RAS KERNEL INFO generating core.1605
- 1120145774 2005.06.30 R01-M1-NF-C:J05-U11 2005-06-30-08.36.14.017177 R01-M1-NF-C:J05-U11 RAS KERNEL INFO generating core.16483
- 1120145915 2005.06.30 R04-M0-NF-C:J12-U01 2005-06-30-08.38.35.958902 R04-M0-NF-C:J12-U01 RAS KERNEL INFO generating core.257
- 1120146038 2005.06.30 R25-M0-NC-C:J04-U11 2005-06-30-08.40.38.831619 R25-M0-NC-C:J04-U11 RAS KERNEL INFO generating core.18751
- 1120146042 2005.06.30 R21-M0-N0-C:J05-U11 2005-06-30-08.40.42.786067 R21-M0-N0-C:J05-U11 RAS KERNEL INFO generating core.22783
- 1120146085 2005.06.30 R32-M0-N8-C:J11-U01 2005-06-30-08.41.25.502105 R32-M0-N8-C:J11-U01 RAS KERNEL INFO generating core.8021
- 1120146112 2005.06.30 R15-M1-N4-C:J09-U11 2005-06-30-08.41.52.056302 R15-M1-N4-C:J09-U11 RAS KERNEL INFO generating core.10734
- 1120148315 2005.06.30 R00-M1-NA-C:J13-U11 2005-06-30-09.18.35.440546 R00-M1-NA-C:J13-U11 RAS KERNEL INFO generating core.3613
- 1120148424 2005.06.30 R06-M1-N2-C:J15-U11 2005-06-30-09.20.24.416584 R06-M1-N2-C:J15-U11 RAS KERNEL INFO generating core.1916
- 1120148578 2005.06.30 R17-M0-NE-C:J09-U01 2005-06-30-09.22.58.210658 R17-M0-NE-C:J09-U01 RAS KERNEL INFO generating core.4246
- 1120148687 2005.06.30 R14-M0-N1-C:J12-U01 2005-06-30-09.24.47.505143 R14-M0-N1-C:J12-U01 RAS KERNEL INFO generating core.7777
- 1120148743 2005.06.30 R11-M0-N0-C:J02-U11 2005-06-30-09.25.43.591205 R11-M0-N0-C:J02-U11 RAS KERNEL INFO generating core.5935
- 1120148771 2005.06.30 R33-M1-N7-C:J05-U11 2005-06-30-09.26.11.626795 R33-M1-N7-C:J05-U11 RAS KERNEL INFO generating core.8691
- 1120149021 2005.06.30 R23-M1-ND-C:J16-U01 2005-06-30-09.30.21.891142 R23-M1-ND-C:J16-U01 RAS KERNEL INFO generating core.5512
- 1120150857 2005.06.30 R05-M0-N7-C:J04-U01 2005-06-30-10.00.57.531545 R05-M0-N7-C:J04-U01 RAS KERNEL INFO generating core.1123
- 1120150859 2005.06.30 R05-M0-N3-C:J12-U11 2005-06-30-10.00.59.776334 R05-M0-N3-C:J12-U11 RAS KERNEL INFO generating core.1641
- 1120155328 2005.06.30 R16-M1-N2-C:J17-U01 2005-06-30-11.15.28.804669 R16-M1-N2-C:J17-U01 RAS KERNEL INFO CE sym 0, at 0x1b858ca0, mask 0x40
- 1120173883 2005.06.30 R36-M1-N4-I:J18-U01 2005-06-30-16.24.43.164196 R36-M1-N4-I:J18-U01 RAS APP FATAL ciod: Error creating node map from file /p/gb2/welcome3/32k_128x256x1_8x4x4.map: Bad file descriptor
- 1120177846 2005.06.30 R16-M1-N2-C:J17-U01 2005-06-30-17.30.46.535444 R16-M1-N2-C:J17-U01 RAS KERNEL INFO CE sym 0, at 0x1b858280, mask 0x40
- 1120201391 2005.07.01 R01-M1-ND-C:J03-U01 2005-07-01-00.03.11.095704 R01-M1-ND-C:J03-U01 RAS KERNEL INFO CE sym 22, at 0x049af800, mask 0x02
- 1120209808 2005.07.01 R07-M1-NC-C:J12-U11 2005-07-01-02.23.28.948917 R07-M1-NC-C:J12-U11 RAS KERNEL INFO generating core.1165
- 1120216069 2005.07.01 R21-M1-N8-C:J05-U11 2005-07-01-04.07.49.783918 R21-M1-N8-C:J05-U11 RAS KERNEL INFO generating core.23615
- 1120216139 2005.07.01 R36-M0-N7-C:J11-U01 2005-07-01-04.08.59.340805 R36-M0-N7-C:J11-U01 RAS KERNEL INFO generating core.865
- 1120216148 2005.07.01 R20-M1-N5-C:J06-U11 2005-07-01-04.09.08.743891 R20-M1-N5-C:J06-U11 RAS KERNEL INFO generating core.1626
- 1120216152 2005.07.01 R34-M0-N7-C:J08-U11 2005-07-01-04.09.12.043520 R34-M0-N7-C:J08-U11 RAS KERNEL INFO generating core.16594
- 1120216232 2005.07.01 R26-M0-N5-C:J07-U11 2005-07-01-04.10.32.786113 R26-M0-N5-C:J07-U11 RAS KERNEL INFO generating core.30586
- 1120216240 2005.07.01 R20-M1-N2-C:J17-U01 2005-07-01-04.10.40.232817 R20-M1-N2-C:J17-U01 RAS KERNEL INFO generating core.18540
- 1120216246 2005.07.01 R31-M1-N5-C:J11-U01 2005-07-01-04.10.46.426573 R31-M1-N5-C:J11-U01 RAS KERNEL INFO generating core.9825
- 1120216290 2005.07.01 R21-M1-NB-C:J07-U01 2005-07-01-04.11.30.534564 R21-M1-NB-C:J07-U01 RAS KERNEL INFO generating core.6698
- 1120216479 2005.07.01 R23-M1-NE-C:J13-U01 2005-07-01-04.14.39.846764 R23-M1-NE-C:J13-U01 RAS KERNEL INFO generating core.4525
- 1120216591 2005.07.01 R33-M1-N8-C:J17-U11 2005-07-01-04.16.31.056361 R33-M1-N8-C:J17-U11 RAS KERNEL INFO generating core.28084
- 1120216621 2005.07.01 R23-M1-NC-C:J14-U01 2005-07-01-04.17.01.697115 R23-M1-NC-C:J14-U01 RAS KERNEL INFO generating core.6028
- 1120216963 2005.07.01 R10-M1-ND-C:J17-U11 2005-07-01-04.22.43.382322 R10-M1-ND-C:J17-U11 RAS KERNEL INFO generating core.536
- 1120216974 2005.07.01 R10-M0-N2-C:J02-U01 2005-07-01-04.22.54.950859 R10-M0-N2-C:J02-U01 RAS KERNEL INFO generating core.15655
- 1120217012 2005.07.01 R16-M1-N6-C:J15-U01 2005-07-01-04.23.32.012882 R16-M1-N6-C:J15-U01 RAS KERNEL INFO generating core.8628
- 1120217049 2005.07.01 R15-M1-N9-C:J07-U11 2005-07-01-04.24.09.678810 R15-M1-N9-C:J07-U11 RAS KERNEL INFO generating core.12122
- 1120217052 2005.07.01 R17-M1-N7-C:J08-U01 2005-07-01-04.24.12.881758 R17-M1-N7-C:J08-U01 RAS KERNEL INFO generating core.2210
- 1120217064 2005.07.01 R15-M1-NF-C:J16-U11 2005-07-01-04.24.24.044042 R15-M1-NF-C:J16-U11 RAS KERNEL INFO generating core.10312
- 1120217117 2005.07.01 R12-M1-N6-C:J16-U01 2005-07-01-04.25.17.269721 R12-M1-N6-C:J16-U01 RAS KERNEL INFO generating core.228
- 1120217126 2005.07.01 R14-M1-NE-C:J14-U11 2005-07-01-04.25.26.994401 R14-M1-NE-C:J14-U11 RAS KERNEL INFO generating core.332
- 1120217169 2005.07.01 R14-M0-N2-C:J15-U11 2005-07-01-04.26.09.649848 R14-M0-N2-C:J15-U11 RAS KERNEL INFO generating core.15740
- 1120217184 2005.07.01 R11-M1-N2-C:J11-U11 2005-07-01-04.26.24.856056 R11-M1-N2-C:J11-U11 RAS KERNEL INFO generating core.3389
- 1120231520 2005.07.01 UNKNOWN_LOCATION 2005-07-01-08.25.20.466953 UNKNOWN_LOCATION NULL DISCOVERY INFO Ido chip status changed: FF:F2:9F:16:E2:23:00:0D:60:E9:1D:DC ip=10.0.0.151 v=9 t=4 status=M Fri Jul 01 08:16:53 PDT 2005
- 1120241131 2005.07.01 R37-M1-N4 2005-07-01-11.05.31.120732 R37-M1-N4 NULL DISCOVERY SEVERE Can not get assembly information for node card
- 1120259710 2005.07.01 R33-M1-N2-C:J17-U11 2005-07-01-16.15.10.056728 R33-M1-N2-C:J17-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120259762 2005.07.01 R37-M0-NE-C:J10-U01 2005-07-01-16.16.02.658866 R37-M0-NE-C:J10-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120259871 2005.07.01 R33-M0-N6-C:J07-U01 2005-07-01-16.17.51.960797 R33-M0-N6-C:J07-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120259888 2005.07.01 R25-M0-N7-C:J11-U01 2005-07-01-16.18.08.272925 R25-M0-N7-C:J11-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120259941 2005.07.01 R25-M1-N2-C:J08-U01 2005-07-01-16.19.01.600033 R25-M1-N2-C:J08-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120260065 2005.07.01 R27-M1-N8-C:J17-U01 2005-07-01-16.21.05.938269 R27-M1-N8-C:J17-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120260098 2005.07.01 R33-M0-N8-C:J07-U01 2005-07-01-16.21.38.233353 R33-M0-N8-C:J07-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120260870 2005.07.01 R20-M0-N5-C:J15-U01 2005-07-01-16.34.30.861885 R20-M0-N5-C:J15-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120260976 2005.07.01 R30-M1-NE-C:J02-U01 2005-07-01-16.36.16.829568 R30-M1-NE-C:J02-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120261034 2005.07.01 R23-M0-N1-C:J02-U11 2005-07-01-16.37.14.984043 R23-M0-N1-C:J02-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120262099 2005.07.01 R24-M1-NB-C:J14-U11 2005-07-01-16.54.59.334330 R24-M1-NB-C:J14-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120262185 2005.07.01 R35-M1-N7-C:J04-U01 2005-07-01-16.56.25.711911 R35-M1-N7-C:J04-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120262310 2005.07.01 R32-M1-ND-C:J11-U11 2005-07-01-16.58.30.209233 R32-M1-ND-C:J11-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120262314 2005.07.01 R32-M1-NB-C:J04-U11 2005-07-01-16.58.34.595623 R32-M1-NB-C:J04-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120262332 2005.07.01 R36-M0-N4-C:J15-U01 2005-07-01-16.58.52.572817 R36-M0-N4-C:J15-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120262354 2005.07.01 R24-M1-NC-C:J07-U11 2005-07-01-16.59.14.798327 R24-M1-NC-C:J07-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120263044 2005.07.01 R36-M0-N5-C:J16-U11 2005-07-01-17.10.44.021307 R36-M0-N5-C:J16-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120263061 2005.07.01 R31-M1-N2-C:J14-U01 2005-07-01-17.11.01.016985 R31-M1-N2-C:J14-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120263168 2005.07.01 R24-M0-N3-C:J09-U01 2005-07-01-17.12.48.485919 R24-M0-N3-C:J09-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120263355 2005.07.01 R35-M0-N6-C:J17-U11 2005-07-01-17.15.55.227258 R35-M0-N6-C:J17-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120265452 2005.07.01 R33-M1-NE-C:J11-U01 2005-07-01-17.50.52.851643 R33-M1-NE-C:J11-U01 RAS KERNEL INFO 458720 double-hummer alignment exceptions
- 1120265503 2005.07.01 R24-M1-NB-C:J06-U01 2005-07-01-17.51.43.600692 R24-M1-NB-C:J06-U01 RAS KERNEL INFO 458720 double-hummer alignment exceptions
- 1120265529 2005.07.01 R27-M1-N2-C:J03-U01 2005-07-01-17.52.09.980967 R27-M1-N2-C:J03-U01 RAS KERNEL INFO 458720 double-hummer alignment exceptions
- 1120265540 2005.07.01 R24-M1-NA-C:J14-U01 2005-07-01-17.52.20.409124 R24-M1-NA-C:J14-U01 RAS KERNEL INFO 458720 double-hummer alignment exceptions
- 1120265568 2005.07.01 R34-M0-N9-C:J08-U01 2005-07-01-17.52.48.085606 R34-M0-N9-C:J08-U01 RAS KERNEL INFO 458720 double-hummer alignment exceptions
- 1120265591 2005.07.01 R30-M1-NB-C:J17-U11 2005-07-01-17.53.11.746402 R30-M1-NB-C:J17-U11 RAS KERNEL INFO 458720 double-hummer alignment exceptions
- 1120265616 2005.07.01 R35-M1-N1-C:J09-U01 2005-07-01-17.53.36.360670 R35-M1-N1-C:J09-U01 RAS KERNEL INFO 458720 double-hummer alignment exceptions
- 1120265656 2005.07.01 R23-M0-N6-C:J11-U01 2005-07-01-17.54.16.064998 R23-M0-N6-C:J11-U01 RAS KERNEL INFO 458720 double-hummer alignment exceptions
- 1120266813 2005.07.01 R21-M0-ND-C:J07-U11 2005-07-01-18.13.33.739748 R21-M0-ND-C:J07-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120266847 2005.07.01 R33-M0-N2-C:J10-U01 2005-07-01-18.14.07.256483 R33-M0-N2-C:J10-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120266866 2005.07.01 R30-M1-N9-C:J08-U11 2005-07-01-18.14.26.319216 R30-M1-N9-C:J08-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120266887 2005.07.01 R35-M1-N5-C:J04-U01 2005-07-01-18.14.47.324202 R35-M1-N5-C:J04-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120266925 2005.07.01 R25-M1-N2-C:J10-U01 2005-07-01-18.15.25.287847 R25-M1-N2-C:J10-U01 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120266974 2005.07.01 R32-M0-NF-C:J17-U11 2005-07-01-18.16.14.553770 R32-M0-NF-C:J17-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120267083 2005.07.01 R33-M0-N8-C:J10-U11 2005-07-01-18.18.03.046965 R33-M0-N8-C:J10-U11 RAS KERNEL INFO 1146800 double-hummer alignment exceptions
- 1120351622 2005.07.02 R05-M0-NF-C:J09-U11 2005-07-02-17.47.02.835483 R05-M0-NF-C:J09-U11 RAS KERNEL INFO generating core.1114
- 1120351629 2005.07.02 R04-M0-N9-C:J17-U01 2005-07-02-17.47.09.816142 R04-M0-N9-C:J17-U01 RAS KERNEL INFO generating core.848
- 1120351671 2005.07.02 R05-M1-NE-C:J12-U01 2005-07-02-17.47.51.984240 R05-M1-NE-C:J12-U01 RAS KERNEL INFO generating core.2117
- 1120351682 2005.07.02 R04-M1-NE-C:J11-U11 2005-07-02-17.48.02.615045 R04-M1-NE-C:J11-U11 RAS KERNEL INFO generating core.3293
- 1120351719 2005.07.02 R05-M0-NC-C:J04-U01 2005-07-02-17.48.39.742843 R05-M0-NC-C:J04-U01 RAS KERNEL INFO generating core.1351
- 1120446972 2005.07.03 R36-M0-N6-C:J06-U01 2005-07-03-20.16.12.324256 R36-M0-N6-C:J06-U01 RAS KERNEL INFO generating core.17222
- 1120446979 2005.07.03 R22-M1-N9-C:J15-U01 2005-07-03-20.16.19.279199 R22-M1-N9-C:J15-U01 RAS KERNEL INFO generating core.20392
- 1120447005 2005.07.03 R26-M0-NA-C:J08-U11 2005-07-03-20.16.45.820915 R26-M0-NA-C:J08-U11 RAS KERNEL INFO generating core.31006
- 1120447017 2005.07.03 R20-M1-N1-C:J15-U11 2005-07-03-20.16.57.187615 R20-M1-N1-C:J15-U11 RAS KERNEL INFO generating core.20088
- 1120447042 2005.07.03 R37-M1-N3-C:J13-U11 2005-07-03-20.17.22.293433 R37-M1-N3-C:J13-U11 RAS KERNEL INFO generating core.26993
- 1120447058 2005.07.03 R23-M1-N7-C:J08-U11 2005-07-03-20.17.38.727762 R23-M1-N7-C:J08-U11 RAS KERNEL INFO generating core.20954
- 1120447377 2005.07.03 R23-M1-N5-C:J03-U11 2005-07-03-20.22.57.506170 R23-M1-N5-C:J03-U11 RAS KERNEL INFO generating core.6139
- 1120447387 2005.07.03 R23-M0-N5-C:J10-U01 2005-07-03-20.23.07.488351 R23-M0-N5-C:J10-U01 RAS KERNEL INFO generating core.10185
- 1120447451 2005.07.03 R32-M1-N5-C:J16-U11 2005-07-03-20.24.11.514321 R32-M1-N5-C:J16-U11 RAS KERNEL INFO generating core.13776
- 1120447561 2005.07.03 R24-M1-N8-C:J16-U01 2005-07-03-20.26.01.608125 R24-M1-N8-C:J16-U01 RAS KERNEL INFO generating core.19596
- 1120447592 2005.07.03 R34-M1-N0-C:J12-U01 2005-07-03-20.26.32.492671 R34-M1-N0-C:J12-U01 RAS KERNEL INFO generating core.15557
- 1120449671 2005.07.03 R31-M0-N7-C:J13-U01 2005-07-03-21.01.11.603590 R31-M0-N7-C:J13-U01 RAS KERNEL INFO generating core.4193
- 1120449678 2005.07.03 R37-M0-N1-C:J05-U11 2005-07-03-21.01.18.368546 R37-M0-N1-C:J05-U11 RAS KERNEL INFO generating core.7539
- 1120449800 2005.07.03 R21-M1-ND-C:J14-U01 2005-07-03-21.03.20.655726 R21-M1-ND-C:J14-U01 RAS KERNEL INFO generating core.22024
- 1120449840 2005.07.03 R27-M1-N3-C:J13-U11 2005-07-03-21.04.00.795038 R27-M1-N3-C:J13-U11 RAS KERNEL INFO generating core.22905
- 1120449964 2005.07.03 R21-M0-N6-C:J04-U11 2005-07-03-21.06.04.793853 R21-M0-N6-C:J04-U11 RAS KERNEL INFO generating core.24671
- 1120450102 2005.07.03 R22-M1-NC-C:J13-U11 2005-07-03-21.08.22.110006 R22-M1-NC-C:J13-U11 RAS KERNEL INFO generating core.17853
- 1120469475 2005.07.04 R33-M1-N7-C:J15-U01 2005-07-04-02.31.15.977001 R33-M1-N7-C:J15-U01 RAS KERNEL INFO generating core.25568
- 1120469605 2005.07.04 R21-M1-NB-C:J15-U01 2005-07-04-02.33.25.435620 R21-M1-NB-C:J15-U01 RAS KERNEL INFO generating core.23080
- 1120469660 2005.07.04 R21-M0-ND-C:J02-U01 2005-07-04-02.34.20.029089 R21-M0-ND-C:J02-U01 RAS KERNEL INFO generating core.9739
- 1120469698 2005.07.04 R31-M0-N9-C:J04-U11 2005-07-04-02.34.58.039555 R31-M0-N9-C:J04-U11 RAS KERNEL INFO generating core.7187
- 1120469722 2005.07.04 R23-M1-NA-C:J06-U01 2005-07-04-02.35.22.626530 R23-M1-NA-C:J06-U01 RAS KERNEL INFO generating core.23438
- 1120469778 2005.07.04 R25-M1-N1-C:J10-U01 2005-07-04-02.36.18.807780 R25-M1-N1-C:J10-U01 RAS KERNEL INFO generating core.7881
- 1120486467 2005.07.04 R10-M1-N1-C:J09-U01 2005-07-04-07.14.27.442953 R10-M1-N1-C:J09-U01 RAS KERNEL INFO generating core.9778
- 1120486586 2005.07.04 R12-M0-N1-C:J04-U11 2005-07-04-07.16.26.811512 R12-M0-N1-C:J04-U11 RAS KERNEL INFO generating core.7915
- 1120486728 2005.07.04 R17-M1-N4-C:J07-U11 2005-07-04-07.18.48.526515 R17-M1-N4-C:J07-U11 RAS KERNEL INFO generating core.3006
- 1120487288 2005.07.04 R36-M1-NB-C:J05-U01 2005-07-04-07.28.08.559194 R36-M1-NB-C:J05-U01 RAS KERNEL INFO generating core.14627
- 1120487397 2005.07.04 R26-M1-NA-C:J17-U01 2005-07-04-07.29.57.172516 R26-M1-NA-C:J17-U01 RAS KERNEL INFO generating core.2348
- 1120487464 2005.07.04 R34-M0-N7-C:J09-U01 2005-07-04-07.31.04.927617 R34-M0-N7-C:J09-U01 RAS KERNEL INFO generating core.226
- 1120487466 2005.07.04 R34-M0-N5-C:J04-U01 2005-07-04-07.31.06.190628 R34-M0-N5-C:J04-U01 RAS KERNEL INFO generating core.1219
- 1120487503 2005.07.04 R25-M0-NE-C:J11-U01 2005-07-04-07.31.43.800074 R25-M0-NE-C:J11-U01 RAS KERNEL INFO generating core.8877
- 1120487708 2005.07.04 R34-M1-N4-C:J09-U11 2005-07-04-07.35.08.890879 R34-M1-N4-C:J09-U11 RAS KERNEL INFO generating core.13558
- 1120487796 2005.07.04 R02-M1-N8-C:J17-U01 2005-07-04-07.36.36.338885 R02-M1-N8-C:J17-U01 RAS KERNEL INFO generating core.404
- 1120591331 2005.07.05 R32-M0-N4-I:J18-U01 2005-07-05-12.22.11.843446 R32-M0-N4-I:J18-U01 RAS KERNEL INFO ciod: Missing or invalid fields on line 1 of node map file /home/auselton/bgl/64mps.sequential.mapfile
- 1120658382 2005.07.06 R27-M1-N1-C:J02-U11 2005-07-06-06.59.42.777345 R27-M1-N1-C:J02-U11 RAS KERNEL INFO 1 torus receiver z+ input pipe error(s) (dcr 0x02f0) detected and corrected
- 1120746917 2005.07.07 R07-M0-N6-C:J03-U01 2005-07-07-07.35.17.360203 R07-M0-N6-C:J03-U01 RAS KERNEL INFO generating core.631
- 1120746970 2005.07.07 R07-M0-N0-C:J06-U01 2005-07-07-07.36.10.216237 R07-M0-N0-C:J06-U01 RAS KERNEL INFO generating core.998
- 1120748233 2005.07.07 R07-M1-N7-C:J10-U11 2005-07-07-07.57.13.121223 R07-M1-N7-C:J10-U11 RAS KERNEL INFO generating core.1129
- 1120748250 2005.07.07 R06-M0-N1-C:J16-U11 2005-07-07-07.57.30.872998 R06-M0-N1-C:J16-U11 RAS KERNEL INFO generating core.424
- 1120772478 2005.07.07 R01-M0-N6-C:J11-U11 2005-07-07-14.41.18.953461 R01-M0-N6-C:J11-U11 RAS KERNEL INFO generating core.1213
- 1120772499 2005.07.07 R01-M0-ND-C:J11-U11 2005-07-07-14.41.39.016623 R01-M0-ND-C:J11-U11 RAS KERNEL INFO generating core.1433
- 1120772538 2005.07.07 R01-M1-N8-C:J06-U01 2005-07-07-14.42.18.270771 R01-M1-N8-C:J06-U01 RAS KERNEL INFO generating core.2950
- 1120772792 2005.07.07 R00-M0-N4-C:J05-U11 2005-07-07-14.46.32.754236 R00-M0-N4-C:J05-U11 RAS KERNEL INFO generating core.319
- 1120772793 2005.07.07 R00-M0-N4-C:J04-U11 2005-07-07-14.46.33.219615 R00-M0-N4-C:J04-U11 RAS KERNEL INFO generating core.303
- 1120772805 2005.07.07 R05-M0-N8-C:J06-U11 2005-07-07-14.46.45.171689 R05-M0-N8-C:J06-U11 RAS KERNEL INFO generating core.1998
- 1120776001 2005.07.07 R26-M0-N5-C:J12-U01 2005-07-07-15.40.01.608873 R26-M0-N5-C:J12-U01 RAS KERNEL INFO 632431728 double-hummer alignment exceptions
- 1120776057 2005.07.07 R37-M0-NF-C:J17-U11 2005-07-07-15.40.57.457814 R37-M0-NF-C:J17-U11 RAS KERNEL INFO 634368144 double-hummer alignment exceptions
- 1120776186 2005.07.07 R24-M0-NE-C:J03-U11 2005-07-07-15.43.06.075797 R24-M0-NE-C:J03-U11 RAS KERNEL INFO 632254896 double-hummer alignment exceptions
- 1120776206 2005.07.07 R20-M0-N5-C:J16-U01 2005-07-07-15.43.26.852210 R20-M0-N5-C:J16-U01 RAS KERNEL INFO 633283776 double-hummer alignment exceptions
- 1120786826 2005.07.07 R12-M1-NF-C:J13-U01 2005-07-07-18.40.26.584600 R12-M1-NF-C:J13-U01 RAS KERNEL INFO generating core.841
- 1120786879 2005.07.07 R12-M1-NE-C:J04-U11 2005-07-07-18.41.19.670878 R12-M1-NE-C:J04-U11 RAS KERNEL INFO generating core.815
- 1120786954 2005.07.07 R04-M1-N7-C:J08-U11 2005-07-07-18.42.34.152599 R04-M1-N7-C:J08-U11 RAS KERNEL INFO generating core.57762
- 1120787008 2005.07.07 R14-M1-NC-C:J15-U11 2005-07-07-18.43.28.027906 R14-M1-NC-C:J15-U11 RAS KERNEL INFO generating core.3436
- 1120788451 2005.07.07 R34-M0-N3-C:J17-U11 2005-07-07-19.07.31.240935 R34-M0-N3-C:J17-U11 RAS KERNEL INFO generating core.4592
- 1120788495 2005.07.07 R30-M0-N6-C:J12-U01 2005-07-07-19.08.15.840409 R30-M0-N6-C:J12-U01 RAS KERNEL INFO generating core.149
- 1120788544 2005.07.07 R10-M0-NB-C:J02-U11 2005-07-07-19.09.04.167736 R10-M0-NB-C:J02-U11 RAS KERNEL INFO generating core.62507
- 1120788590 2005.07.07 R12-M1-NE-C:J02-U01 2005-07-07-19.09.50.668802 R12-M1-NE-C:J02-U01 RAS KERNEL INFO generating core.1807
- 1120889703 2005.07.08 R05-M0-N2 2005-07-08-23.15.03.798449 R05-M0-N2 NULL DISCOVERY WARNING Node card is not fully functional
- 1120908821 2005.07.09 R20-M0-N8-I:J18-U01 2005-07-09-04.33.41.091472 R20-M0-N8-I:J18-U01 RAS KERNEL INFO ciod: duplicate canonical-rank 31 to logical-rank 0 mapping at line 3 of node map file /p/gb2/pakin1/sweep3d-5x5x400-10mk-3mmi-1024pes-sweep/sweep.map
- 1120909906 2005.07.09 R16-M1-NC-I:J18-U11 2005-07-09-04.51.46.753675 R16-M1-NC-I:J18-U11 RAS APP FATAL ciod: Error creating node map from file /p/gb2/pakin1/sweep3d-5x5x400-10mk-3mmi-1024pes-xyzt/xyzt.map: Block device required
- 1120917279 2005.07.09 R16-M1-N2-C:J15-U01 2005-07-09-06.54.39.086324 R16-M1-N2-C:J15-U01 RAS KERNEL INFO generating core.9652
- 1120917299 2005.07.09 R17-M1-ND-C:J07-U01 2005-07-09-06.54.59.161891 R17-M1-ND-C:J07-U01 RAS KERNEL INFO generating core.11154
- 1120923499 2005.07.09 R33-M0-N9-C:J04-U11 2005-07-09-08.38.19.481016 R33-M0-N9-C:J04-U11 RAS KERNEL INFO generating core.7571
- 1120923521 2005.07.09 R20-M0-N6-C:J16-U01 2005-07-09-08.38.41.325656 R20-M0-N6-C:J16-U01 RAS KERNEL INFO generating core.12364
- 1120924431 2005.07.09 R22-M0-N5-C:J13-U11 2005-07-09-08.53.51.797283 R22-M0-N5-C:J13-U11 RAS KERNEL INFO generating core.13817
- 1120924519 2005.07.09 R20-M0-NA-C:J12-U01 2005-07-09-08.55.19.014398 R20-M0-NA-C:J12-U01 RAS KERNEL INFO generating core.14349
- 1120924566 2005.07.09 R35-M1-N7-C:J10-U01 2005-07-09-08.56.06.557539 R35-M1-N7-C:J10-U01 RAS KERNEL INFO generating core.8897
- 1120928196 2005.07.09 R27-M1-N9-C:J06-U01 2005-07-09-09.56.36.409760 R27-M1-N9-C:J06-U01 RAS KERNEL INFO generating core.7946
- 1120928280 2005.07.09 R21-M0-NB-C:J05-U11 2005-07-09-09.58.00.188544 R21-M0-NB-C:J05-U11 RAS KERNEL INFO generating core.10299
- 1120928332 2005.07.09 R25-M0-N4-C:J10-U11 2005-07-09-09.58.52.758252 R25-M0-N4-C:J10-U11 RAS KERNEL INFO generating core.9949
- 1120928349 2005.07.09 R25-M0-N0-C:J15-U01 2005-07-09-09.59.09.533825 R25-M0-N0-C:J15-U01 RAS KERNEL INFO generating core.12012
- 1120929103 2005.07.09 R14-M1-ND-C:J03-U11 2005-07-09-10.11.43.673631 R14-M1-ND-C:J03-U11 RAS KERNEL INFO generating core.9051
- 1120929257 2005.07.09 R14-M0-N9-C:J16-U11 2005-07-09-10.14.17.214294 R14-M0-N9-C:J16-U11 RAS KERNEL INFO generating core.7752
- 1120929262 2005.07.09 R11-M1-N5-C:J12-U11 2005-07-09-10.14.22.332626 R11-M1-N5-C:J12-U11 RAS KERNEL INFO generating core.10793
- 1120931195 2005.07.09 R20-M0-NE-C:J05-U01 2005-07-09-10.46.35.976265 R20-M0-NE-C:J05-U01 RAS KERNEL INFO generating core.24671
- 1120931199 2005.07.09 R31-M0-NF-C:J04-U01 2005-07-09-10.46.39.218896 R31-M0-NF-C:J04-U01 RAS KERNEL INFO generating core.40979
- 1120931244 2005.07.09 R37-M1-N7-C:J11-U01 2005-07-09-10.47.24.204096 R37-M1-N7-C:J11-U01 RAS KERNEL INFO generating core.18129
- 1120931305 2005.07.09 R27-M1-N9-C:J09-U11 2005-07-09-10.48.25.280918 R27-M1-N9-C:J09-U11 RAS KERNEL INFO generating core.47738
- 1120931338 2005.07.09 R21-M1-NE-C:J11-U11 2005-07-09-10.48.58.719782 R21-M1-NE-C:J11-U11 RAS KERNEL INFO generating core.42109
- 1120931379 2005.07.09 R26-M1-N9-C:J08-U01 2005-07-09-10.49.39.479084 R26-M1-N9-C:J08-U01 RAS KERNEL INFO generating core.6682
- 1120931429 2005.07.09 R24-M1-N1-C:J15-U11 2005-07-09-10.50.29.748945 R24-M1-N1-C:J15-U11 RAS KERNEL INFO generating core.7672
- 1120931465 2005.07.09 R31-M0-N9-C:J08-U01 2005-07-09-10.51.05.319040 R31-M0-N9-C:J08-U01 RAS KERNEL INFO generating core.47122
- 1120931504 2005.07.09 R31-M1-NE-C:J11-U01 2005-07-09-10.51.44.555371 R31-M1-NE-C:J11-U01 RAS KERNEL INFO generating core.17493
- 1120932365 2005.07.09 R31-M1-N9-C:J15-U11 2005-07-09-11.06.05.052923 R31-M1-N9-C:J15-U11 RAS KERNEL INFO generating core.23664
- 1120932577 2005.07.09 R03-M1-ND-C:J17-U11 2005-07-09-11.09.37.100403 R03-M1-ND-C:J17-U11 RAS KERNEL INFO generating core.19296
- 1120932589 2005.07.09 R24-M1-N1-C:J12-U01 2005-07-09-11.09.49.772584 R24-M1-N1-C:J12-U01 RAS KERNEL INFO generating core.6553
- 1120932594 2005.07.09 R01-M1-N9-C:J14-U01 2005-07-09-11.09.54.062141 R01-M1-N9-C:J14-U01 RAS KERNEL INFO generating core.56320
- 1120932606 2005.07.09 R36-M0-N5-C:J15-U11 2005-07-09-11.10.06.424632 R36-M0-N5-C:J15-U11 RAS KERNEL INFO generating core.3824
- 1120932622 2005.07.09 R36-M0-N2-C:J10-U01 2005-07-09-11.10.22.118843 R36-M0-N2-C:J10-U01 RAS KERNEL INFO generating core.5781
- 1120932658 2005.07.09 R33-M1-N2-C:J02-U01 2005-07-09-11.10.58.820525 R33-M1-N2-C:J02-U01 RAS KERNEL INFO generating core.55191
- 1120932709 2005.07.09 R20-M1-N2-C:J10-U11 2005-07-09-11.11.49.090815 R20-M1-N2-C:J10-U11 RAS KERNEL INFO generating core.38077
- 1120932758 2005.07.09 R37-M0-NE-C:J13-U01 2005-07-09-11.12.38.927629 R37-M0-NE-C:J13-U01 RAS KERNEL INFO generating core.41557
- 1120932797 2005.07.09 R26-M1-N5-C:J02-U11 2005-07-09-11.13.17.589543 R26-M1-N5-C:J02-U11 RAS KERNEL INFO generating core.36539
- 1120933688 2005.07.09 R27-M0-N4-C:J16-U11 2005-07-09-11.28.08.835803 R27-M0-N4-C:J16-U11 RAS KERNEL INFO generating core.51900
- 1120933818 2005.07.09 R20-M1-N7-C:J09-U11 2005-07-09-11.30.18.267700 R20-M1-N7-C:J09-U11 RAS KERNEL INFO generating core.33018
- 1120933839 2005.07.09 R26-M0-N3-C:J13-U11 2005-07-09-11.30.39.600360 R26-M0-N3-C:J13-U11 RAS KERNEL INFO generating core.29433
- 1120934002 2005.07.09 R24-M1-N5-C:J14-U01 2005-07-09-11.33.22.478541 R24-M1-N5-C:J14-U01 RAS KERNEL INFO generating core.36248
- 1120934069 2005.07.09 R21-M1-NF-C:J17-U11 2005-07-09-11.34.29.833311 R21-M1-NF-C:J17-U11 RAS KERNEL INFO generating core.41080
- 1120934108 2005.07.09 R36-M1-NE-C:J08-U01 2005-07-09-11.35.08.564220 R36-M1-NE-C:J08-U01 RAS KERNEL INFO generating core.57878
- 1120934392 2005.07.09 R30-M0-N6-C:J13-U11 2005-07-09-11.39.52.674271 R30-M0-N6-C:J13-U11 RAS KERNEL INFO generating core.245
- 1120934665 2005.07.09 R14-M1-ND-C:J04-U01 2005-07-09-11.44.25.948722 R14-M1-ND-C:J04-U01 RAS KERNEL INFO generating core.2315
- 1120934747 2005.07.09 R12-M0-N3-C:J02-U01 2005-07-09-11.45.47.078028 R12-M0-N3-C:J02-U01 RAS KERNEL INFO generating core.30603
- 1120934788 2005.07.09 R11-M0-NE-C:J11-U11 2005-07-09-11.46.28.950400 R11-M0-NE-C:J11-U11 RAS KERNEL INFO generating core.17517
- 1120934845 2005.07.09 R16-M1-ND-C:J13-U01 2005-07-09-11.47.25.418111 R16-M1-ND-C:J13-U01 RAS KERNEL INFO generating core.2633
- 1120934940 2005.07.09 R01-M1-NF-C:J15-U01 2005-07-09-11.49.00.675162 R01-M1-NF-C:J15-U01 RAS KERNEL INFO generating core.50240
- 1120934979 2005.07.09 R03-M0-N7-C:J03-U11 2005-07-09-11.49.39.697036 R03-M0-N7-C:J03-U11 RAS KERNEL INFO generating core.10211
- 1120935001 2005.07.09 R06-M1-NE-C:J04-U01 2005-07-09-11.50.01.726751 R06-M1-NE-C:J04-U01 RAS KERNEL INFO generating core.57863
- 1120935040 2005.07.09 R07-M1-ND-C:J17-U11 2005-07-09-11.50.40.338378 R07-M1-ND-C:J17-U11 RAS KERNEL INFO generating core.51808
- 1120935068 2005.07.09 R06-M1-ND-C:J04-U01 2005-07-09-11.51.08.686968 R06-M1-ND-C:J04-U01 RAS KERNEL INFO generating core.59907
- 1120935076 2005.07.09 R02-M1-NE-C:J08-U11 2005-07-09-11.51.16.012286 R02-M1-NE-C:J08-U11 RAS KERNEL INFO generating core.25382
- 1120935133 2005.07.09 R05-M0-N6-C:J09-U01 2005-07-09-11.52.13.812405 R05-M0-N6-C:J09-U01 RAS KERNEL INFO generating core.8646
- 1120935238 2005.07.09 R22-M1-N0-C:J05-U11 2005-07-09-11.53.58.430277 R22-M1-N0-C:J05-U11 RAS KERNEL INFO generating core.7167
- 1120935334 2005.07.09 R21-M1-N0-C:J13-U11 2005-07-09-11.55.34.841282 R21-M1-N0-C:J13-U11 RAS KERNEL INFO generating core.14589
- 1120935388 2005.07.09 R20-M1-N4-C:J17-U11 2005-07-09-11.56.28.685591 R20-M1-N4-C:J17-U11 RAS KERNEL INFO generating core.2300
- 1120935398 2005.07.09 R25-M1-N8-C:J16-U11 2005-07-09-11.56.38.939515 R25-M1-N8-C:J16-U11 RAS KERNEL INFO generating core.14652
- 1120935446 2005.07.09 R35-M0-N0-C:J05-U01 2005-07-09-11.57.26.222399 R35-M0-N0-C:J05-U01 RAS KERNEL INFO generating core.14807
- 1120935483 2005.07.09 R10-M0-N8-C:J06-U01 2005-07-09-11.58.03.523153 R10-M0-N8-C:J06-U01 RAS KERNEL INFO generating core.31758
- 1120935590 2005.07.09 R00-M0-N8-C:J10-U11 2005-07-09-11.59.50.155121 R00-M0-N8-C:J10-U11 RAS KERNEL INFO generating core.7205
- 1120935660 2005.07.09 R01-M0-NC-C:J04-U01 2005-07-09-12.01.00.789241 R01-M0-NC-C:J04-U01 RAS KERNEL INFO generating core.10247
- 1120937467 2005.07.09 R36-M1-NA-C:J06-U01 2005-07-09-12.31.07.283740 R36-M1-NA-C:J06-U01 RAS KERNEL INFO generating core.30230
- 1120937630 2005.07.09 R20-M1-N5-C:J05-U11 2005-07-09-12.33.50.353460 R20-M1-N5-C:J05-U11 RAS KERNEL INFO generating core.2299
- 1120937817 2005.07.09 R22-M0-N5-C:J07-U11 2005-07-09-12.36.57.597431 R22-M0-N5-C:J07-U11 RAS KERNEL INFO generating core.28666
- 1120937836 2005.07.09 R34-M0-N5-C:J12-U11 2005-07-09-12.37.16.494245 R34-M0-N5-C:J12-U11 RAS KERNEL INFO generating core.2481
- 1120938039 2005.07.09 R21-M0-N5-C:J14-U01 2005-07-09-12.40.39.809863 R21-M0-N5-C:J14-U01 RAS KERNEL INFO generating core.52376
- 1120938041 2005.07.09 R37-M0-N5-C:J08-U11 2005-07-09-12.40.41.552838 R37-M0-N5-C:J08-U11 RAS KERNEL INFO generating core.10930
- 1120938043 2005.07.09 R23-M0-N6-C:J06-U11 2005-07-09-12.40.43.695648 R23-M0-N6-C:J06-U11 RAS KERNEL INFO generating core.51134
- 1120938048 2005.07.09 R30-M0-NB-C:J03-U11 2005-07-09-12.40.48.888712 R30-M0-NB-C:J03-U11 RAS KERNEL INFO generating core.5235
- 1120938093 2005.07.09 R27-M0-N7-C:J10-U01 2005-07-09-12.41.33.279413 R27-M0-N7-C:J10-U01 RAS KERNEL INFO generating core.50841
- 1120938234 2005.07.09 R35-M0-N9-C:J10-U11 2005-07-09-12.43.54.033821 R35-M0-N9-C:J10-U11 RAS KERNEL INFO generating core.48433
- 1120938246 2005.07.09 R10-M1-N5-C:J04-U01 2005-07-09-12.44.06.253512 R10-M1-N5-C:J04-U01 RAS KERNEL INFO generating core.34955
- 1120938346 2005.07.09 R15-M1-NB-C:J15-U01 2005-07-09-12.45.46.326910 R15-M1-NB-C:J15-U01 RAS KERNEL INFO generating core.46408
- 1120938410 2005.07.09 R00-M0-ND-C:J14-U11 2005-07-09-12.46.50.657088 R00-M0-ND-C:J14-U11 RAS KERNEL INFO generating core.35872
- 1120938420 2005.07.09 R00-M0-N6-C:J12-U01 2005-07-09-12.47.00.644304 R00-M0-N6-C:J12-U01 RAS KERNEL INFO generating core.133
- 1120938451 2005.07.09 R16-M1-NA-C:J08-U01 2005-07-09-12.47.31.928637 R16-M1-NA-C:J08-U01 RAS KERNEL INFO generating core.4622
- 1120938541 2005.07.09 R12-M0-N6-C:J05-U11 2005-07-09-12.49.01.731883 R12-M0-N6-C:J05-U11 RAS KERNEL INFO generating core.58351
- 1120938585 2005.07.09 R03-M0-N6-C:J09-U01 2005-07-09-12.49.45.827456 R03-M0-N6-C:J09-U01 RAS KERNEL INFO generating core.9158
- 1120938606 2005.07.09 R13-M0-NB-C:J15-U11 2005-07-09-12.50.06.846867 R13-M0-NB-C:J15-U11 RAS KERNEL INFO generating core.22376
- 1120938631 2005.07.09 R11-M1-NB-C:J12-U11 2005-07-09-12.50.31.927917 R11-M1-NB-C:J12-U11 RAS KERNEL INFO generating core.12329
- 1120938674 2005.07.09 R02-M0-N5-C:J03-U01 2005-07-09-12.51.14.943043 R02-M0-N5-C:J03-U01 RAS KERNEL INFO generating core.4035
- 1120938682 2005.07.09 R01-M0-N9-C:J03-U11 2005-07-09-12.51.22.917676 R01-M0-N9-C:J03-U11 RAS KERNEL INFO generating core.15459
- 1120939508 2005.07.09 R21-M0-N9-C:J17-U01 2005-07-09-13.05.08.134359 R21-M0-N9-C:J17-U01 RAS KERNEL INFO generating core.55384
- 1120939518 2005.07.09 R20-M1-N2-C:J07-U01 2005-07-09-13.05.18.662491 R20-M1-N2-C:J07-U01 RAS KERNEL INFO generating core.5342
- 1120939553 2005.07.09 R25-M1-N2-C:J03-U01 2005-07-09-13.05.53.029625 R25-M1-N2-C:J03-U01 RAS KERNEL INFO generating core.46559
- 1120939573 2005.07.09 R00-M1-NE-C:J14-U01 2005-07-09-13.06.13.254915 R00-M1-NE-C:J14-U01 RAS KERNEL INFO generating core.25604
- 1120939626 2005.07.09 R36-M1-N4-C:J15-U11 2005-07-09-13.07.06.107122 R36-M1-N4-C:J15-U11 RAS KERNEL INFO generating core.61172
- 1120939761 2005.07.09 R36-M1-N6-C:J04-U11 2005-07-09-13.09.21.726410 R36-M1-N6-C:J04-U11 RAS KERNEL INFO generating core.25271
- 1120939783 2005.07.09 R20-M1-N6-C:J03-U01 2005-07-09-13.09.43.917357 R20-M1-N6-C:J03-U01 RAS KERNEL INFO generating core.34015
- 1120939798 2005.07.09 R26-M0-NA-C:J07-U01 2005-07-09-13.09.58.154321 R26-M0-NA-C:J07-U01 RAS KERNEL INFO generating core.63070
- 1120939974 2005.07.09 R24-M0-N1-C:J17-U01 2005-07-09-13.12.54.616018 R24-M0-N1-C:J17-U01 RAS KERNEL INFO generating core.31192
- 1120940044 2005.07.09 R24-M1-N1-C:J14-U01 2005-07-09-13.14.04.443048 R24-M1-N1-C:J14-U01 RAS KERNEL INFO generating core.40344
- 1120940045 2005.07.09 R22-M0-N2-C:J09-U11 2005-07-09-13.14.05.050608 R22-M0-N2-C:J09-U11 RAS KERNEL INFO generating core.62462
- 1120940057 2005.07.09 R36-M0-N9-C:J15-U01 2005-07-09-13.14.17.907696 R36-M0-N9-C:J15-U01 RAS KERNEL INFO generating core.40528
- 1120940204 2005.07.09 R21-M1-N7-C:J07-U11 2005-07-09-13.16.44.901016 R21-M1-N7-C:J07-U11 RAS KERNEL INFO generating core.42234
- 1120940220 2005.07.09 R27-M0-N1-C:J05-U01 2005-07-09-13.17.00.152550 R27-M0-N1-C:J05-U01 RAS KERNEL INFO generating core.23259
- 1120940220 2005.07.09 R27-M0-N1-C:J07-U11 2005-07-09-13.17.00.392088 R27-M0-N1-C:J07-U11 RAS KERNEL INFO generating core.24314
- 1120940284 2005.07.09 R30-M0-NA-C:J17-U01 2005-07-09-13.18.04.050308 R30-M0-NA-C:J17-U01 RAS KERNEL INFO generating core.4180
- 1120940300 2005.07.09 R23-M0-N1-C:J07-U01 2005-07-09-13.18.20.009738 R23-M0-N1-C:J07-U01 RAS KERNEL INFO generating core.24538
- 1120940346 2005.07.09 R24-M0-N6-C:J07-U01 2005-07-09-13.19.06.210518 R24-M0-N6-C:J07-U01 RAS KERNEL INFO generating core.26078
- 1120940349 2005.07.09 R32-M1-N3-C:J07-U11 2005-07-09-13.19.09.983269 R32-M1-N3-C:J07-U11 RAS KERNEL INFO generating core.30706
- 1120940357 2005.07.09 R32-M0-N1-C:J03-U01 2005-07-09-13.19.17.680850 R32-M0-N1-C:J03-U01 RAS KERNEL INFO generating core.8147
- 1120940402 2005.07.09 R10-M0-NE-C:J04-U11 2005-07-09-13.20.02.961250 R10-M0-NE-C:J04-U11 RAS KERNEL INFO generating core.57391
- 1120940548 2005.07.09 R17-M1-ND-C:J14-U01 2005-07-09-13.22.28.887665 R17-M1-ND-C:J14-U01 RAS KERNEL INFO generating core.11784
- 1120940599 2005.07.09 R16-M0-NB-C:J04-U01 2005-07-09-13.23.19.130183 R16-M0-NB-C:J04-U01 RAS KERNEL INFO generating core.61963
- 1120940618 2005.07.09 R14-M1-N7-C:J04-U01 2005-07-09-13.23.38.492029 R14-M1-N7-C:J04-U01 RAS KERNEL INFO generating core.33163
- 1120940634 2005.07.09 R06-M1-NB-C:J03-U01 2005-07-09-13.23.54.723073 R06-M1-NB-C:J03-U01 RAS KERNEL INFO generating core.30275
- 1120940659 2005.07.09 R15-M1-N1-C:J10-U01 2005-07-09-13.24.19.559994 R15-M1-N1-C:J10-U01 RAS KERNEL INFO generating core.15753
- 1120940700 2005.07.09 R00-M0-N1-C:J07-U01 2005-07-09-13.25.00.218073 R00-M0-N1-C:J07-U01 RAS KERNEL INFO generating core.7362
- 1120953127 2005.07.09 R27-M0-N6-C:J05-U11 2005-07-09-16.52.07.005910 R27-M0-N6-C:J05-U11 RAS KERNEL INFO generating core.8575
- 1120953142 2005.07.09 R21-M0-ND-C:J15-U11 2005-07-09-16.52.22.961118 R21-M0-ND-C:J15-U11 RAS KERNEL INFO generating core.9784
- 1120953217 2005.07.09 R35-M1-N9-C:J10-U11 2005-07-09-16.53.37.285437 R35-M1-N9-C:J10-U11 RAS KERNEL INFO generating core.11921
- 1120953260 2005.07.09 R31-M1-N0-C:J03-U01 2005-07-09-16.54.20.026465 R31-M1-N0-C:J03-U01 RAS KERNEL INFO generating core.11879
- 1120955479 2005.07.09 R26-M0-NF-C:J02-U01 2005-07-09-17.31.19.922132 R26-M0-NF-C:J02-U01 RAS KERNEL INFO generating core.13067
- 1120955633 2005.07.09 R26-M0-N7-C:J02-U01 2005-07-09-17.33.53.777163 R26-M0-N7-C:J02-U01 RAS KERNEL INFO generating core.13131
- 1120955677 2005.07.09 R34-M1-N4-C:J03-U11 2005-07-09-17.34.37.516117 R34-M1-N4-C:J03-U11 RAS KERNEL INFO generating core.14071
- 1120956526 2005.07.09 R37-M1-N7-C:J17-U01 2005-07-09-17.48.46.717674 R37-M1-N7-C:J17-U01 RAS KERNEL INFO generating core.8544
- 1120956527 2005.07.09 R37-M1-N7-C:J04-U11 2005-07-09-17.48.47.234245 R37-M1-N7-C:J04-U11 RAS KERNEL INFO generating core.8531
- 1120956557 2005.07.09 R20-M0-N5-C:J07-U11 2005-07-09-17.49.17.958580 R20-M0-N5-C:J07-U11 RAS KERNEL INFO generating core.13946
- 1120956704 2005.07.09 R24-M0-N0-C:J02-U11 2005-07-09-17.51.44.447891 R24-M0-N0-C:J02-U11 RAS KERNEL INFO generating core.16095
- 1120956736 2005.07.09 R21-M1-N8-C:J04-U01 2005-07-09-17.52.16.641372 R21-M1-N8-C:J04-U01 RAS KERNEL INFO generating core.7183
- 1120956863 2005.07.09 R06-M1-N8-C:J14-U01 2005-07-09-17.54.23.740575 R06-M1-N8-C:J14-U01 RAS KERNEL INFO generating core.1988
- 1120957066 2005.07.09 R01-M1-N5-C:J04-U11 2005-07-09-17.57.46.798518 R01-M1-N5-C:J04-U11 RAS KERNEL INFO generating core.2347
- 1120957404 2005.07.09 R05-M1-N2-C:J06-U01 2005-07-09-18.03.24.488333 R05-M1-N2-C:J06-U01 RAS KERNEL INFO generating core.2790
- 1120957410 2005.07.09 R01-M1-NA-C:J12-U11 2005-07-09-18.03.30.488263 R01-M1-NA-C:J12-U11 RAS KERNEL INFO generating core.2573
- 1120957717 2005.07.09 R00-M1-N9-C:J11-U01 2005-07-09-18.08.37.270465 R00-M1-N9-C:J11-U01 RAS KERNEL INFO generating core.3985
- 1120958362 2005.07.09 R26-M0-ND-C:J16-U11 2005-07-09-18.19.22.664176 R26-M0-ND-C:J16-U11 RAS KERNEL INFO generating core.13592
- 1120958385 2005.07.09 R37-M1-ND-C:J03-U01 2005-07-09-18.19.45.976122 R37-M1-ND-C:J03-U01 RAS KERNEL INFO generating core.10019
- 1120958403 2005.07.09 R21-M0-N3-C:J06-U11 2005-07-09-18.20.03.745197 R21-M0-N3-C:J06-U11 RAS KERNEL INFO generating core.10842
- 1120958502 2005.07.09 R23-M1-N1-C:J06-U01 2005-07-09-18.21.42.804580 R23-M1-N1-C:J06-U01 RAS KERNEL INFO generating core.8138
- 1120967799 2005.07.09 R17-M0-NF-C:J13-U11 2005-07-09-20.56.39.594043 R17-M0-NF-C:J13-U11 RAS KERNEL INFO 1365301360 double-hummer alignment exceptions
- 1120967919 2005.07.09 R14-M0-NB-C:J13-U11 2005-07-09-20.58.39.414224 R14-M0-NB-C:J13-U11 RAS KERNEL INFO 1315337824 double-hummer alignment exceptions
- 1120967922 2005.07.09 R14-M0-N9-C:J14-U01 2005-07-09-20.58.42.972274 R14-M0-N9-C:J14-U01 RAS KERNEL INFO 1401390016 double-hummer alignment exceptions
- 1120967952 2005.07.09 R32-M1-N9-C:J10-U11 2005-07-09-20.59.12.577698 R32-M1-N9-C:J10-U11 RAS KERNEL INFO generating core.32657
- 1120967983 2005.07.09 R35-M1-N3-C:J04-U11 2005-07-09-20.59.43.240343 R35-M1-N3-C:J04-U11 RAS KERNEL INFO generating core.26835
- 1120968079 2005.07.09 R31-M1-N1-C:J06-U11 2005-07-09-21.01.19.043600 R31-M1-N1-C:J06-U11 RAS KERNEL INFO generating core.28242
- 1120968107 2005.07.09 R23-M1-N7-C:J09-U01 2005-07-09-21.01.47.593602 R23-M1-N7-C:J09-U01 RAS KERNEL INFO generating core.20970
- 1120968120 2005.07.09 R30-M0-N3-C:J08-U11 2005-07-09-21.02.00.766575 R30-M0-N3-C:J08-U11 RAS KERNEL INFO generating core.18514
- 1120968134 2005.07.09 R26-M1-N3-C:J11-U11 2005-07-09-21.02.14.211419 R26-M1-N3-C:J11-U11 RAS KERNEL INFO generating core.19321
- 1120968137 2005.07.09 R22-M0-ND-C:J13-U11 2005-07-09-21.02.17.017814 R22-M0-ND-C:J13-U11 RAS KERNEL INFO generating core.13753
- 1120968273 2005.07.09 R33-M0-N2-C:J07-U11 2005-07-09-21.04.33.975556 R33-M0-N2-C:J07-U11 RAS KERNEL INFO generating core.23542
- 1120968310 2005.07.09 R34-M1-N6-C:J09-U11 2005-07-09-21.05.10.098884 R34-M1-N6-C:J09-U11 RAS KERNEL INFO generating core.28918
- 1120968439 2005.07.09 R25-M1-NF-C:J10-U11 2005-07-09-21.07.19.074851 R25-M1-NF-C:J10-U11 RAS KERNEL INFO generating core.21145
- 1120968530 2005.07.09 R35-M1-NF-C:J14-U01 2005-07-09-21.08.50.003067 R35-M1-NF-C:J14-U01 RAS KERNEL INFO generating core.8832
- 1120968585 2005.07.09 R20-M1-N8-C:J03-U01 2005-07-09-21.09.45.813839 R20-M1-N8-C:J03-U01 RAS KERNEL INFO generating core.20015
- 1120968609 2005.07.09 R27-M0-N8-C:J17-U01 2005-07-09-21.10.09.468130 R27-M0-N8-C:J17-U01 RAS KERNEL INFO generating core.11564
- 1120968690 2005.07.09 R30-M0-NC-C:J10-U11 2005-07-09-21.11.30.337616 R30-M0-NC-C:J10-U11 RAS KERNEL INFO generating core.1557
- 1120968838 2005.07.09 R11-M0-N2-C:J04-U11 2005-07-09-21.13.58.391696 R11-M0-N2-C:J04-U11 RAS KERNEL INFO generating core.5167
- 1120968857 2005.07.09 R11-M1-N2-C:J04-U01 2005-07-09-21.14.17.975757 R11-M1-N2-C:J04-U01 RAS KERNEL INFO generating core.3111
- 1120968887 2005.07.09 R10-M0-NB-C:J13-U11 2005-07-09-21.14.47.953574 R10-M0-NB-C:J13-U11 RAS KERNEL INFO generating core.7193
- 1120968941 2005.07.09 R13-M0-N0-C:J12-U01 2005-07-09-21.15.41.885081 R13-M0-N0-C:J12-U01 RAS KERNEL INFO generating core.5861
- 1120969022 2005.07.09 R30-M1-N1-C:J12-U01 2005-07-09-21.17.02.803088 R30-M1-N1-C:J12-U01 RAS KERNEL INFO generating core.15425
- 1120969071 2005.07.09 R30-M0-NF-C:J15-U11 2005-07-09-21.17.51.730570 R30-M0-NF-C:J15-U11 RAS KERNEL INFO generating core.560
- 1120969099 2005.07.09 R35-M0-ND-C:J14-U01 2005-07-09-21.18.19.960414 R35-M0-ND-C:J14-U01 RAS KERNEL INFO generating core.5760
- 1120969138 2005.07.09 R22-M0-ND-C:J17-U11 2005-07-09-21.18.58.127449 R22-M0-ND-C:J17-U11 RAS KERNEL INFO generating core.13752
- 1120969145 2005.07.09 R21-M0-N2-C:J15-U01 2005-07-09-21.19.05.376937 R21-M0-N2-C:J15-U01 RAS KERNEL INFO generating core.10860
- 1120969149 2005.07.09 R24-M0-N7-C:J17-U11 2005-07-09-21.19.09.892450 R24-M0-N7-C:J17-U11 RAS KERNEL INFO generating core.12536
- 1120969163 2005.07.09 R36-M0-N9-C:J17-U01 2005-07-09-21.19.23.537422 R36-M0-N9-C:J17-U01 RAS KERNEL INFO generating core.19744
- 1120969179 2005.07.09 R31-M1-N7-C:J03-U01 2005-07-09-21.19.39.649600 R31-M1-N7-C:J03-U01 RAS KERNEL INFO generating core.8803
- 1120969258 2005.07.09 R21-M0-N0-C:J06-U01 2005-07-09-21.20.58.981208 R21-M0-N0-C:J06-U01 RAS KERNEL INFO generating core.28238
- 1120969308 2005.07.09 R36-M1-N7-C:J05-U01 2005-07-09-21.21.48.751878 R36-M1-N7-C:J05-U01 RAS KERNEL INFO generating core.12643
- 1120969332 2005.07.09 R20-M0-N6-C:J03-U11 2005-07-09-21.22.12.020466 R20-M0-N6-C:J03-U11 RAS KERNEL INFO generating core.12927
- 1120969348 2005.07.09 R37-M0-NA-C:J10-U01 2005-07-09-21.22.28.245426 R37-M0-NA-C:J10-U01 RAS KERNEL INFO generating core.6917
- 1120969377 2005.07.09 R27-M1-N7-C:J11-U01 2005-07-09-21.22.57.518045 R27-M1-N7-C:J11-U01 RAS KERNEL INFO generating core.21353
- 1120969383 2005.07.09 R26-M1-N6-C:J15-U11 2005-07-09-21.23.03.768684 R26-M1-N6-C:J15-U11 RAS KERNEL INFO generating core.17276
- 1120969432 2005.07.09 R27-M0-N3-C:J04-U01 2005-07-09-21.23.52.155670 R27-M0-N3-C:J04-U01 RAS KERNEL INFO generating core.26955
- 1120969436 2005.07.09 R27-M0-N2-C:J03-U11 2005-07-09-21.23.56.916963 R27-M0-N2-C:J03-U11 RAS KERNEL INFO generating core.11135
- 1120969467 2005.07.09 R31-M0-NA-C:J03-U11 2005-07-09-21.24.27.695037 R31-M0-NA-C:J03-U11 RAS KERNEL INFO generating core.6711
- 1120969497 2005.07.09 R27-M1-ND-C:J02-U01 2005-07-09-21.24.57.796889 R27-M1-ND-C:J02-U01 RAS KERNEL INFO generating core.22283
- 1120969508 2005.07.09 R35-M0-NE-C:J03-U11 2005-07-09-21.25.08.918888 R35-M0-NE-C:J03-U11 RAS KERNEL INFO generating core.21175
- 1120969527 2005.07.09 R25-M0-N9-C:J11-U01 2005-07-09-21.25.27.506107 R25-M0-N9-C:J11-U01 RAS KERNEL INFO generating core.28329
- 1120969674 2005.07.09 R22-M1-N8-C:J02-U01 2005-07-09-21.27.54.261842 R22-M1-N8-C:J02-U01 RAS KERNEL INFO generating core.20367
- 1120969704 2005.07.09 R27-M1-N8-C:J08-U11 2005-07-09-21.28.24.603741 R27-M1-N8-C:J08-U11 RAS KERNEL INFO generating core.23838
- 1120969709 2005.07.09 R22-M0-N8-C:J15-U01 2005-07-09-21.28.29.361442 R22-M0-N8-C:J15-U01 RAS KERNEL INFO generating core.16300
- 1120969793 2005.07.09 R33-M0-N0-C:J04-U01 2005-07-09-21.29.53.355653 R33-M0-N0-C:J04-U01 RAS KERNEL INFO generating core.7623
- 1120969840 2005.07.09 R01-M0-N0-C:J10-U01 2005-07-09-21.30.40.884135 R01-M0-N0-C:J10-U01 RAS KERNEL INFO generating core.1957
- 1120970168 2005.07.09 R04-M1-NE-C:J15-U11 2005-07-09-21.36.08.377661 R04-M1-NE-C:J15-U11 RAS KERNEL INFO generating core.3292
- 1120970941 2005.07.09 R00-M1-N7-C:J11-U01 2005-07-09-21.49.01.198976 R00-M1-N7-C:J11-U01 RAS KERNEL INFO generating core.3249
- 1120970946 2005.07.09 R01-M1-N2-C:J08-U11 2005-07-09-21.49.06.101225 R01-M1-N2-C:J08-U11 RAS KERNEL INFO generating core.2606
- 1120972565 2005.07.09 R01-M1-N7-C:J11-U11 2005-07-09-22.16.05.307549 R01-M1-N7-C:J11-U11 RAS KERNEL INFO generating core.2233
- 1120972589 2005.07.09 R04-M1-N0-C:J05-U01 2005-07-09-22.16.29.497623 R04-M1-N0-C:J05-U01 RAS KERNEL INFO generating core.3959
- 1120972998 2005.07.09 R05-M1-N9-C:J16-U01 2005-07-09-22.23.18.517961 R05-M1-N9-C:J16-U01 RAS KERNEL INFO generating core.2880
- 1120973021 2005.07.09 R04-M1-NB-C:J15-U01 2005-07-09-22.23.41.495412 R04-M1-NB-C:J15-U01 RAS KERNEL INFO generating core.3792
- 1120974004 2005.07.09 R01-M1-N9-C:J03-U11 2005-07-09-22.40.04.247442 R01-M1-N9-C:J03-U11 RAS KERNEL INFO generating core.2971
- 1120975821 2005.07.09 R04-M1-NE-C:J10-U01 2005-07-09-23.10.21.207634 R04-M1-NE-C:J10-U01 RAS KERNEL INFO generating core.3269
- 1121006513 2005.07.10 R27-M1-N9-C:J15-U11 2005-07-10-07.41.53.770739 R27-M1-N9-C:J15-U11 RAS KERNEL INFO generating core.24376
- 1121006557 2005.07.10 R24-M0-N3-C:J02-U01 2005-07-10-07.42.37.036562 R24-M0-N3-C:J02-U01 RAS KERNEL INFO generating core.31435
- 1121006570 2005.07.10 R26-M1-N9-C:J02-U01 2005-07-10-07.42.50.396450 R26-M1-N9-C:J02-U01 RAS KERNEL INFO generating core.3851
- 1121006621 2005.07.10 R36-M0-N9-C:J11-U01 2005-07-10-07.43.41.076829 R36-M0-N9-C:J11-U01 RAS KERNEL INFO generating core.20257
- 1121006676 2005.07.10 R30-M1-N9-C:J10-U01 2005-07-10-07.44.36.306387 R30-M1-N9-C:J10-U01 RAS KERNEL INFO generating core.32257
- 1121006677 2005.07.10 R23-M1-NA-C:J14-U11 2005-07-10-07.44.37.462311 R23-M1-NA-C:J14-U11 RAS KERNEL INFO generating core.7068
- 1121006797 2005.07.10 R25-M1-N7-C:J13-U01 2005-07-10-07.46.37.300865 R25-M1-N7-C:J13-U01 RAS KERNEL INFO generating core.4329
- 1121006800 2005.07.10 R24-M1-N7-C:J16-U11 2005-07-10-07.46.40.398797 R24-M1-N7-C:J16-U11 RAS KERNEL INFO generating core.16600
- 1121006825 2005.07.10 R37-M0-N2-C:J05-U11 2005-07-10-07.47.05.910036 R37-M0-N2-C:J05-U11 RAS KERNEL INFO generating core.6519
- 1121006882 2005.07.10 R32-M1-N3-C:J10-U11 2005-07-10-07.48.02.079501 R32-M1-N3-C:J10-U11 RAS KERNEL INFO generating core.15313
- 1121006900 2005.07.10 R24-M1-N6-C:J05-U01 2005-07-10-07.48.20.756795 R24-M1-N6-C:J05-U01 RAS KERNEL INFO generating core.16623
- 1121006926 2005.07.10 R25-M0-NA-C:J13-U11 2005-07-10-07.48.46.695024 R25-M0-NA-C:J13-U11 RAS KERNEL INFO generating core.10429
- 1121007023 2005.07.10 R36-M1-N0-C:J17-U11 2005-07-10-07.50.23.201865 R36-M1-N0-C:J17-U11 RAS KERNEL INFO generating core.32116
- 1121007046 2005.07.10 R30-M1-N8-C:J15-U11 2005-07-10-07.50.46.048460 R30-M1-N8-C:J15-U11 RAS KERNEL INFO generating core.15924
- 1121091173 2005.07.11 R30-M0-NB-C:J09-U01 2005-07-11-07.12.53.947417 R30-M0-NB-C:J09-U01 RAS KERNEL INFO CE sym 2, at 0x0d09f1c0, mask 0x80
- 1121111691 2005.07.11 R22-M0-N0-I:J18-U11 2005-07-11-12.54.51.738880 R22-M0-N0-I:J18-U11 RAS APP FATAL ciod: Error loading /home/greeno/raptor.new.dev.3d.BGL.CXX_XL.MPI.ex: invalid or missing program image, No such file or directory
KERNRTSP 1121115817 2005.07.11 R01-M0-N1-C:J09-U11 2005-07-11-14.03.37.675251 R01-M0-N1-C:J09-U11 RAS KERNEL FATAL rts panic! - stopping execution
KERNRTSP 1121115819 2005.07.11 R04-M0-N1-C:J02-U01 2005-07-11-14.03.39.101778 R04-M0-N1-C:J02-U01 RAS KERNEL FATAL rts panic! - stopping execution
- 1121172088 2005.07.12 R37-M1-N6-C:J03-U11 2005-07-12-05.41.28.215496 R37-M1-N6-C:J03-U11 RAS KERNEL INFO generating core.9079
- 1121172217 2005.07.12 R27-M1-NB-C:J12-U11 2005-07-12-05.43.37.421103 R27-M1-NB-C:J12-U11 RAS KERNEL INFO generating core.6425
- 1121172239 2005.07.12 R26-M1-N1-C:J16-U01 2005-07-12-05.43.59.502921 R26-M1-N1-C:J16-U01 RAS KERNEL INFO generating core.3400
- 1121172258 2005.07.12 R31-M0-NA-C:J03-U11 2005-07-12-05.44.18.854186 R31-M0-NA-C:J03-U11 RAS KERNEL INFO generating core.6711
- 1121172266 2005.07.12 R34-M0-N2-C:J04-U11 2005-07-12-05.44.26.319459 R34-M0-N2-C:J04-U11 RAS KERNEL INFO generating core.2263
- 1121172311 2005.07.12 R25-M0-ND-C:J09-U01 2005-07-12-05.45.11.854624 R25-M0-ND-C:J09-U01 RAS KERNEL INFO generating core.9386
- 1121172452 2005.07.12 R22-M1-N0-C:J10-U11 2005-07-12-05.47.32.414558 R22-M1-N0-C:J10-U11 RAS KERNEL INFO generating core.4061
- 1121172512 2005.07.12 R34-M1-N4-C:J07-U11 2005-07-12-05.48.32.532637 R34-M1-N4-C:J07-U11 RAS KERNEL INFO generating core.14070
- 1121172512 2005.07.12 R34-M1-N4-C:J10-U01 2005-07-12-05.48.32.925986 R34-M1-N4-C:J10-U01 RAS KERNEL INFO generating core.14021
- 1121172822 2005.07.12 R16-M1-N1-C:J05-U01 2005-07-12-05.53.42.889632 R16-M1-N1-C:J05-U01 RAS KERNEL INFO generating core.1715
- 1121172833 2005.07.12 R17-M0-NF-C:J10-U11 2005-07-12-05.53.53.558095 R17-M0-NF-C:J10-U11 RAS KERNEL INFO generating core.4489
- 1121172957 2005.07.12 R11-M1-N6-C:J16-U11 2005-07-12-05.55.57.631026 R11-M1-N6-C:J16-U11 RAS KERNEL INFO generating core.2092
- 1121172988 2005.07.12 R15-M0-N8-C:J02-U11 2005-07-12-05.56.28.419299 R15-M0-N8-C:J02-U11 RAS KERNEL INFO generating core.5967
- 1121238463 2005.07.13 R11-M1-N3-C:J07-U11 2005-07-13-00.07.43.091930 R11-M1-N3-C:J07-U11 RAS KERNEL INFO total of 1 ddr error(s) detected and corrected
- 1121252281 2005.07.13 R33-M1-NE-C:J07-U01 2005-07-13-03.58.01.460537 R33-M1-NE-C:J07-U01 RAS KERNEL INFO 118661512 double-hummer alignment exceptions
- 1121252310 2005.07.13 R24-M0-N1-C:J03-U01 2005-07-13-03.58.30.362009 R24-M0-N1-C:J03-U01 RAS KERNEL INFO 112099344 double-hummer alignment exceptions
- 1121252323 2005.07.13 R22-M1-N3-C:J02-U01 2005-07-13-03.58.43.479838 R22-M1-N3-C:J02-U01 RAS KERNEL INFO 113170696 double-hummer alignment exceptions
- 1121252326 2005.07.13 R26-M1-N1-C:J11-U01 2005-07-13-03.58.46.179005 R26-M1-N1-C:J11-U01 RAS KERNEL INFO 117400328 double-hummer alignment exceptions
- 1121252377 2005.07.13 R30-M1-N2-C:J11-U11 2005-07-13-03.59.37.615097 R30-M1-N2-C:J11-U11 RAS KERNEL INFO 112762744 double-hummer alignment exceptions
- 1121252397 2005.07.13 R23-M1-N6-C:J14-U11 2005-07-13-03.59.57.142168 R23-M1-N6-C:J14-U11 RAS KERNEL INFO 116130784 double-hummer alignment exceptions
- 1121252450 2005.07.13 R11-M1-NE-C:J06-U01 2005-07-13-04.00.50.209028 R11-M1-NE-C:J06-U01 RAS KERNEL INFO 114147592 double-hummer alignment exceptions
- 1121252452 2005.07.13 R00-M1-N9-C:J16-U01 2005-07-13-04.00.52.200218 R00-M1-N9-C:J16-U01 RAS KERNEL INFO 113622512 double-hummer alignment exceptions
- 1121252465 2005.07.13 R13-M0-N7-C:J09-U01 2005-07-13-04.01.05.366712 R13-M0-N7-C:J09-U01 RAS KERNEL INFO 114059360 double-hummer alignment exceptions
- 1121252541 2005.07.13 R03-M0-N7-C:J12-U01 2005-07-13-04.02.21.078387 R03-M0-N7-C:J12-U01 RAS KERNEL INFO 114091808 double-hummer alignment exceptions
- 1121252570 2005.07.13 R01-M0-NE-C:J05-U11 2005-07-13-04.02.50.779636 R01-M0-NE-C:J05-U11 RAS KERNEL INFO 115325776 double-hummer alignment exceptions
- 1121252594 2005.07.13 R36-M0-N4-C:J04-U01 2005-07-13-04.03.14.251132 R36-M0-N4-C:J04-U01 RAS KERNEL INFO 113982272 double-hummer alignment exceptions
- 1121252594 2005.07.13 R22-M1-N8-C:J09-U01 2005-07-13-04.03.14.945951 R22-M1-N8-C:J09-U01 RAS KERNEL INFO 113355952 double-hummer alignment exceptions
- 1121252628 2005.07.13 R34-M0-N0-C:J17-U11 2005-07-13-04.03.48.756221 R34-M0-N0-C:J17-U11 RAS KERNEL INFO 115193048 double-hummer alignment exceptions
- 1121252729 2005.07.13 R26-M0-N3-C:J08-U01 2005-07-13-04.05.29.216867 R26-M0-N3-C:J08-U01 RAS KERNEL INFO 111990296 double-hummer alignment exceptions
- 1121252775 2005.07.13 R21-M0-N2-C:J07-U01 2005-07-13-04.06.15.639557 R21-M0-N2-C:J07-U01 RAS KERNEL INFO 112625368 double-hummer alignment exceptions
- 1121252884 2005.07.13 R30-M1-N6-C:J14-U11 2005-07-13-04.08.04.789912 R30-M1-N6-C:J14-U11 RAS KERNEL INFO 116249832 double-hummer alignment exceptions
- 1121252899 2005.07.13 R23-M0-N3-C:J12-U11 2005-07-13-04.08.19.327373 R23-M0-N3-C:J12-U11 RAS KERNEL INFO 112481672 double-hummer alignment exceptions
- 1121252955 2005.07.13 R11-M1-N3-C:J10-U11 2005-07-13-04.09.15.514952 R11-M1-N3-C:J10-U11 RAS KERNEL INFO 116332728 double-hummer alignment exceptions
- 1121256179 2005.07.13 R36-M0-N7-C:J04-U11 2005-07-13-05.02.59.701827 R36-M0-N7-C:J04-U11 RAS KERNEL INFO generating core.691
- 1121256302 2005.07.13 R31-M1-N5-C:J02-U11 2005-07-13-05.05.02.446625 R31-M1-N5-C:J02-U11 RAS KERNEL INFO generating core.52403
- 1121256361 2005.07.13 R37-M0-NB-C:J05-U01 2005-07-13-05.06.01.792263 R37-M0-NB-C:J05-U01 RAS KERNEL INFO generating core.12883
- 1121256367 2005.07.13 R20-M1-NA-C:J04-U11 2005-07-13-05.06.07.030871 R20-M1-NA-C:J04-U11 RAS KERNEL INFO generating core.4159
- 1121256377 2005.07.13 R22-M1-N2-C:J08-U11 2005-07-13-05.06.17.839446 R22-M1-N2-C:J08-U11 RAS KERNEL INFO generating core.5054
- 1121256399 2005.07.13 R27-M0-N6-C:J09-U11 2005-07-13-05.06.39.748124 R27-M0-N6-C:J09-U11 RAS KERNEL INFO generating core.17150
- 1121256538 2005.07.13 R31-M0-N1-C:J09-U01 2005-07-13-05.08.58.429347 R31-M0-N1-C:J09-U01 RAS KERNEL INFO generating core.47314
- 1121256570 2005.07.13 R34-M0-N3-C:J14-U01 2005-07-13-05.09.30.620888 R34-M0-N3-C:J14-U01 RAS KERNEL INFO generating core.5520
- 1121256613 2005.07.13 R25-M0-N7-C:J10-U01 2005-07-13-05.10.13.046702 R25-M0-N7-C:J10-U01 RAS KERNEL INFO generating core.17817
- 1121256768 2005.07.13 R30-M0-ND-C:J09-U01 2005-07-13-05.12.48.222029 R30-M0-ND-C:J09-U01 RAS KERNEL INFO generating core.34898
- 1121256778 2005.07.13 R23-M1-N2-C:J02-U01 2005-07-13-05.12.58.717336 R23-M1-N2-C:J02-U01 RAS KERNEL INFO generating core.47007
- 1121256780 2005.07.13 R23-M0-N7-C:J02-U11 2005-07-13-05.13.00.634737 R23-M0-N7-C:J02-U11 RAS KERNEL INFO generating core.18363
- 1121256804 2005.07.13 R23-M0-N2-C:J07-U01 2005-07-13-05.13.24.457117 R23-M0-N2-C:J07-U01 RAS KERNEL INFO generating core.22494
- 1121268169 2005.07.13 R32-M1-N8-C:J05-U01 2005-07-13-08.22.49.590817 R32-M1-N8-C:J05-U01 RAS KERNEL INFO CE sym 16, at 0x0456cd40, mask 0x04
- 1121308028 2005.07.13 R23-M0-N5-C:J04-U01 2005-07-13-19.27.08.790721 R23-M0-N5-C:J04-U01 RAS KERNEL INFO generating core.26059
- 1121308164 2005.07.13 R20-M1-N4-C:J02-U01 2005-07-13-19.29.24.480958 R20-M1-N4-C:J02-U01 RAS KERNEL INFO generating core.17999
- 1121308207 2005.07.13 R22-M0-N4-C:J02-U01 2005-07-13-19.30.07.825519 R22-M0-N4-C:J02-U01 RAS KERNEL INFO generating core.30671
- 1121308254 2005.07.13 R20-M1-NB-C:J15-U01 2005-07-13-19.30.54.128655 R20-M1-NB-C:J15-U01 RAS KERNEL INFO generating core.2600
- 1121308274 2005.07.13 R26-M0-N3-C:J14-U11 2005-07-13-19.31.14.143065 R26-M0-N3-C:J14-U11 RAS KERNEL INFO generating core.15192
- 1121308295 2005.07.13 R31-M1-N5-C:J15-U11 2005-07-13-19.31.35.829394 R31-M1-N5-C:J15-U11 RAS KERNEL INFO generating core.26224
- 1121308295 2005.07.13 R31-M1-N5-C:J17-U11 2005-07-13-19.31.35.927185 R31-M1-N5-C:J17-U11 RAS KERNEL INFO generating core.25712
- 1121308306 2005.07.13 R37-M0-NA-C:J04-U11 2005-07-13-19.31.46.716108 R37-M0-NA-C:J04-U11 RAS KERNEL INFO generating core.22807
- 1121308531 2005.07.13 R00-M0-NC-C:J17-U11 2005-07-13-19.35.31.224305 R00-M0-NC-C:J17-U11 RAS KERNEL INFO generating core.284
- 1121308565 2005.07.13 R01-M0-NC-C:J12-U01 2005-07-13-19.36.05.649243 R01-M0-NC-C:J12-U01 RAS KERNEL INFO generating core.5381
- 1121308655 2005.07.13 R23-M0-NB-C:J05-U01 2005-07-13-19.37.35.130931 R23-M0-NB-C:J05-U01 RAS KERNEL INFO generating core.27051
- 1121308677 2005.07.13 R24-M0-N6-C:J15-U01 2005-07-13-19.37.57.813607 R24-M0-N6-C:J15-U01 RAS KERNEL INFO generating core.29420
- 1121308707 2005.07.13 R35-M1-NE-C:J09-U11 2005-07-13-19.38.27.805883 R35-M1-NE-C:J09-U11 RAS KERNEL INFO generating core.8374
- 1121308757 2005.07.13 R20-M0-N0-C:J06-U11 2005-07-13-19.39.17.229626 R20-M0-N0-C:J06-U11 RAS KERNEL INFO generating core.15966
- 1121309100 2005.07.13 R00-M1-N2-C:J02-U11 2005-07-13-19.45.00.645776 R00-M1-N2-C:J02-U11 RAS KERNEL INFO generating core.7855
- 1121309138 2005.07.13 R05-M1-N7-C:J15-U11 2005-07-13-19.45.38.909318 R05-M1-N7-C:J15-U11 RAS KERNEL INFO generating core.2296
- 1121309141 2005.07.13 R05-M0-NE-C:J04-U11 2005-07-13-19.45.41.015068 R05-M0-NE-C:J04-U11 RAS KERNEL INFO generating core.1103
- 1121309169 2005.07.13 R05-M0-N9-C:J07-U11 2005-07-13-19.46.09.666973 R05-M0-N9-C:J07-U11 RAS KERNEL INFO generating core.2010
- 1121309186 2005.07.13 R04-M1-N6-C:J12-U11 2005-07-13-19.46.26.896284 R04-M1-N6-C:J12-U11 RAS KERNEL INFO generating core.3181
- 1121310867 2005.07.13 R26-M0-N9-C:J08-U01 2005-07-13-20.14.27.766914 R26-M0-N9-C:J08-U01 RAS KERNEL INFO generating core.32010
- 1121310909 2005.07.13 R33-M1-ND-C:J05-U11 2005-07-13-20.15.09.106661 R33-M1-ND-C:J05-U11 RAS KERNEL INFO generating core.9651
- 1121310915 2005.07.13 R27-M0-N5-C:J10-U01 2005-07-13-20.15.15.724512 R27-M0-N5-C:J10-U01 RAS KERNEL INFO generating core.10057
- 1121310926 2005.07.13 R37-M0-ND-C:J03-U11 2005-07-13-20.15.26.655051 R37-M0-ND-C:J03-U11 RAS KERNEL INFO generating core.5939
- 1121311022 2005.07.13 R24-M0-NF-C:J15-U11 2005-07-13-20.17.02.167067 R24-M0-NF-C:J15-U11 RAS KERNEL INFO generating core.12984
- 1121311028 2005.07.13 R20-M1-ND-C:J05-U11 2005-07-13-20.17.08.205986 R20-M1-ND-C:J05-U11 RAS KERNEL INFO generating core.17467
- 1121311109 2005.07.13 R34-M1-NE-C:J03-U11 2005-07-13-20.18.29.526143 R34-M1-NE-C:J03-U11 RAS KERNEL INFO generating core.29367
- 1121311111 2005.07.13 R30-M1-NA-C:J14-U01 2005-07-13-20.18.31.598175 R30-M1-NA-C:J14-U01 RAS KERNEL INFO generating core.14852
- 1121311141 2005.07.13 R25-M1-N6-C:J07-U11 2005-07-13-20.19.01.767232 R25-M1-N6-C:J07-U11 RAS KERNEL INFO generating core.4862
- 1121311157 2005.07.13 R25-M0-NB-C:J03-U11 2005-07-13-20.19.17.435123 R25-M0-NB-C:J03-U11 RAS KERNEL INFO generating core.27323
- 1121311176 2005.07.13 R31-M1-NA-C:J08-U11 2005-07-13-20.19.36.355702 R31-M1-NA-C:J08-U11 RAS KERNEL INFO generating core.26646
- 1121311390 2005.07.13 R37-M0-NC-C:J17-U11 2005-07-13-20.23.10.257361 R37-M0-NC-C:J17-U11 RAS KERNEL INFO generating core.5428
- 1121311410 2005.07.13 R33-M1-N8-C:J15-U11 2005-07-13-20.23.30.263483 R33-M1-N8-C:J15-U11 RAS KERNEL INFO generating core.12212
- 1121311425 2005.07.13 R26-M1-N8-C:J04-U01 2005-07-13-20.23.45.306783 R26-M1-N8-C:J04-U01 RAS KERNEL INFO generating core.3343
- 1121311483 2005.07.13 R32-M0-N0-C:J09-U01 2005-07-13-20.24.43.430319 R32-M0-N0-C:J09-U01 RAS KERNEL INFO generating core.19942
- 1121311518 2005.07.13 R32-M0-NC-C:J14-U11 2005-07-13-20.25.18.627085 R32-M0-NC-C:J14-U11 RAS KERNEL INFO generating core.18324
- 1121311526 2005.07.13 R32-M0-N8-C:J09-U11 2005-07-13-20.25.26.300247 R32-M0-N8-C:J09-U11 RAS KERNEL INFO generating core.3510
- 1121311849 2005.07.13 R22-M1-ND-C:J05-U11 2005-07-13-20.30.49.413470 R22-M1-ND-C:J05-U11 RAS KERNEL INFO generating core.17851
- 1121311900 2005.07.13 R22-M0-ND-C:J15-U01 2005-07-13-20.31.40.606506 R22-M0-ND-C:J15-U01 RAS KERNEL INFO generating core.30632
- 1121311928 2005.07.13 R21-M0-N2-C:J05-U01 2005-07-13-20.32.08.455087 R21-M0-N2-C:J05-U01 RAS KERNEL INFO generating core.26735
- 1121312018 2005.07.13 R36-M0-NB-C:J12-U01 2005-07-13-20.33.38.333720 R36-M0-NB-C:J12-U01 RAS KERNEL INFO generating core.18689
- 1121312027 2005.07.13 R31-M1-N9-C:J09-U11 2005-07-13-20.33.47.220912 R31-M1-N9-C:J09-U11 RAS KERNEL INFO generating core.27698
- 1121312054 2005.07.13 R33-M1-NA-C:J12-U11 2005-07-13-20.34.14.390511 R33-M1-NA-C:J12-U11 RAS KERNEL INFO generating core.10645
- 1121312104 2005.07.13 R33-M0-NF-C:J10-U11 2005-07-13-20.35.04.592115 R33-M0-NF-C:J10-U11 RAS KERNEL INFO generating core.5009
- 1121312172 2005.07.13 R25-M1-ND-C:J10-U11 2005-07-13-20.36.12.659925 R25-M1-ND-C:J10-U11 RAS KERNEL INFO generating core.5785
- 1121312220 2005.07.13 R23-M1-NF-C:J12-U11 2005-07-13-20.37.00.827496 R23-M1-NF-C:J12-U11 RAS KERNEL INFO generating core.4505
- 1121312231 2005.07.13 R23-M0-NB-C:J11-U11 2005-07-13-20.37.11.588497 R23-M0-NB-C:J11-U11 RAS KERNEL INFO generating core.27577
- 1121312262 2005.07.13 R32-M1-N1-C:J03-U11 2005-07-13-20.37.42.283997 R32-M1-N1-C:J03-U11 RAS KERNEL INFO generating core.32755
- 1121312282 2005.07.13 R32-M1-N5-C:J02-U11 2005-07-13-20.38.02.068017 R32-M1-N5-C:J02-U11 RAS KERNEL INFO generating core.30675
- 1121312306 2005.07.13 R35-M1-N0-C:J13-U01 2005-07-13-20.38.26.141724 R35-M1-N0-C:J13-U01 RAS KERNEL INFO generating core.11493
- 1121312337 2005.07.13 R31-M0-NC-C:J03-U01 2005-07-13-20.38.57.050498 R31-M0-NC-C:J03-U01 RAS KERNEL INFO generating core.5671
- 1121312351 2005.07.13 R37-M1-N8-C:J08-U11 2005-07-13-20.39.11.473210 R37-M1-N8-C:J08-U11 RAS KERNEL INFO generating core.27926
- 1121312380 2005.07.13 R20-M0-N0-C:J02-U11 2005-07-13-20.39.40.465411 R20-M0-N0-C:J02-U11 RAS KERNEL INFO generating core.15967
- 1121312396 2005.07.13 R21-M1-N0-C:J15-U01 2005-07-13-20.39.56.708943 R21-M1-N0-C:J15-U01 RAS KERNEL INFO generating core.24172
- 1121312412 2005.07.13 R30-M1-NC-C:J17-U01 2005-07-13-20.40.12.834348 R30-M1-NC-C:J17-U01 RAS KERNEL INFO generating core.29732
- 1121312448 2005.07.13 R30-M0-N4-C:J09-U11 2005-07-13-20.40.48.847134 R30-M0-N4-C:J09-U11 RAS KERNEL INFO generating core.17526
- 1121313534 2005.07.13 R20-M1-N9-C:J12-U11 2005-07-13-20.58.54.994815 R20-M1-N9-C:J12-U11 RAS KERNEL INFO generating core.19481
- 1121313545 2005.07.13 R37-M0-N1-C:J12-U01 2005-07-13-20.59.05.561390 R37-M0-N1-C:J12-U01 RAS KERNEL INFO generating core.23873
- 1121313580 2005.07.13 R26-M0-NB-C:J04-U01 2005-07-13-20.59.40.239374 R26-M0-NB-C:J04-U01 RAS KERNEL INFO generating core.14603
- 1121313655 2005.07.13 R26-M0-N1-C:J13-U11 2005-07-13-21.00.55.502877 R26-M0-N1-C:J13-U11 RAS KERNEL INFO generating core.15737
- 1121313817 2005.07.13 R21-M1-N2-C:J02-U01 2005-07-13-21.03.37.422278 R21-M1-N2-C:J02-U01 RAS KERNEL INFO generating core.23119
- 1121313824 2005.07.13 R22-M0-N3-C:J17-U01 2005-07-13-21.03.44.525664 R22-M0-N3-C:J17-U01 RAS KERNEL INFO generating core.14824
- 1121313853 2005.07.13 R36-M1-N9-C:J17-U01 2005-07-13-21.04.13.288105 R36-M1-N9-C:J17-U01 RAS KERNEL INFO generating core.15648
- 1121313856 2005.07.13 R33-M0-N2-C:J15-U11 2005-07-13-21.04.16.708949 R33-M0-N2-C:J15-U11 RAS KERNEL INFO generating core.7156
- 1121313915 2005.07.13 R33-M0-NA-C:J05-U11 2005-07-13-21.05.15.201453 R33-M0-NA-C:J05-U11 RAS KERNEL INFO generating core.22967
- 1121313927 2005.07.13 R06-M1-N8-C:J16-U11 2005-07-13-21.05.27.499424 R06-M1-N8-C:J16-U11 RAS KERNEL INFO generating core.3980
- 1121313946 2005.07.13 R35-M1-N1-C:J12-U11 2005-07-13-21.05.46.397052 R35-M1-N1-C:J12-U11 RAS KERNEL INFO generating core.11473
- 1121313953 2005.07.13 R34-M1-N7-C:J03-U11 2005-07-13-21.05.53.702684 R34-M1-N7-C:J03-U11 RAS KERNEL INFO generating core.13043
- 1121313970 2005.07.13 R27-M0-N1-C:J05-U01 2005-07-13-21.06.10.081119 R27-M0-N1-C:J05-U01 RAS KERNEL INFO generating core.28011
- 1121313971 2005.07.13 R21-M0-NA-C:J11-U11 2005-07-13-21.06.11.111463 R21-M0-NA-C:J11-U11 RAS KERNEL INFO generating core.27197
- 1121313974 2005.07.13 R21-M0-N9-C:J17-U11 2005-07-13-21.06.14.855951 R21-M0-N9-C:J17-U11 RAS KERNEL INFO generating core.27704
- 1121314001 2005.07.13 R21-M0-N5-C:J13-U11 2005-07-13-21.06.41.876909 R21-M0-N5-C:J13-U11 RAS KERNEL INFO generating core.9337
- 1121314022 2005.07.13 R37-M0-N2-C:J09-U01 2005-07-13-21.07.02.007176 R37-M0-N2-C:J09-U01 RAS KERNEL INFO generating core.22886
- 1121314138 2005.07.13 R37-M0-NC-C:J11-U11 2005-07-13-21.08.58.817169 R37-M0-NC-C:J11-U11 RAS KERNEL INFO generating core.22325
- 1121314248 2005.07.13 R23-M1-N8-C:J09-U11 2005-07-13-21.10.48.693447 R23-M1-N8-C:J09-U11 RAS KERNEL INFO generating core.23998
- 1121383222 2005.07.14 R01-M0-N0-I:J18-U11 2005-07-14-16.20.22.672549 R01-M0-N0-I:J18-U11 RAS APP FATAL ciod: Error creating node map from file /home/pakin1/sweep3d-2.2b/results/random1-8x32x32x2.map: Permission denied
- 1121408299 2005.07.14 R23-M0-N0-I:J18-U01 2005-07-14-23.18.19.684786 R23-M0-N0-I:J18-U01 RAS KERNEL INFO ciod: Z coordinate 32 exceeds physical dimension 32 at line 33 of node map file /p/gb2/pakin1/contention-32768cpes-torus/xyzt-1x1x32768x1.map
- 1121442061 2005.07.15 R00-M0-NA-C:J04-U01 2005-07-15-08.41.01.843236 R00-M0-NA-C:J04-U01 RAS KERNEL INFO generating core.4615
- 1121442070 2005.07.15 R01-M0-NB-C:J07-U01 2005-07-15-08.41.10.966443 R01-M0-NB-C:J07-U01 RAS KERNEL INFO generating core.1682
- 1121442075 2005.07.15 R00-M0-N9-C:J14-U01 2005-07-15-08.41.15.738617 R00-M0-N9-C:J14-U01 RAS KERNEL INFO generating core.896
- 1121442115 2005.07.15 R04-M0-NB-C:J04-U01 2005-07-15-08.41.55.151875 R04-M0-NB-C:J04-U01 RAS KERNEL INFO generating core.4675
- 1121442124 2005.07.15 R04-M0-NE-C:J17-U01 2005-07-15-08.42.04.042901 R04-M0-NE-C:J17-U01 RAS KERNEL INFO generating core.84
- 1121451271 2005.07.15 R07-M1-ND-C:J02-U11 2005-07-15-11.14.31.993348 R07-M1-ND-C:J02-U11 RAS KERNEL INFO generating core.1227
- 1121484659 2005.07.15 R33-M0-N6-C:J13-U11 2005-07-15-20.30.59.176883 R33-M0-N6-C:J13-U11 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 33, bit 7
- 1121493806 2005.07.15 R36-M1-ND-C:J04-U11 2005-07-15-23.03.26.814988 R36-M1-ND-C:J04-U11 RAS KERNEL INFO generating core.59955
- 1121493886 2005.07.15 R20-M1-N2-C:J03-U11 2005-07-15-23.04.46.601583 R20-M1-N2-C:J03-U11 RAS KERNEL INFO generating core.38143
- 1121493913 2005.07.15 R37-M0-N6-C:J09-U11 2005-07-15-23.05.13.065015 R37-M0-N6-C:J09-U11 RAS KERNEL INFO generating core.41718
- 1121494125 2005.07.15 R36-M1-N2-C:J05-U11 2005-07-15-23.08.45.113982 R36-M1-N2-C:J05-U11 RAS KERNEL INFO generating core.29431
- 1121494129 2005.07.15 R36-M0-NE-C:J05-U11 2005-07-15-23.08.49.027626 R36-M0-NE-C:J05-U11 RAS KERNEL INFO generating core.33399
- 1121494144 2005.07.15 R31-M0-ND-C:J03-U11 2005-07-15-23.09.04.924538 R31-M0-ND-C:J03-U11 RAS KERNEL INFO generating core.11379
- 1121494164 2005.07.15 R34-M0-N5-C:J03-U11 2005-07-15-23.09.24.725815 R34-M0-N5-C:J03-U11 RAS KERNEL INFO generating core.36339
- 1121494252 2005.07.15 R33-M0-NF-C:J11-U01 2005-07-15-23.10.52.087816 R33-M0-NF-C:J11-U01 RAS KERNEL INFO generating core.42833
- 1121494263 2005.07.15 R31-M1-NB-C:J05-U11 2005-07-15-23.11.03.449718 R31-M1-NB-C:J05-U11 RAS KERNEL INFO generating core.20595
- 1121494308 2005.07.15 R21-M0-N7-C:J14-U01 2005-07-15-23.11.48.831797 R21-M0-N7-C:J14-U01 RAS KERNEL INFO generating core.17560
- 1121494314 2005.07.15 R23-M0-NE-C:J16-U11 2005-07-15-23.11.54.188138 R23-M0-NE-C:J16-U11 RAS KERNEL INFO generating core.17212
- 1121495246 2005.07.15 R36-M1-NF-C:J02-U01 2005-07-15-23.27.26.925798 R36-M1-NF-C:J02-U01 RAS KERNEL INFO generating core.26131
- 1121495342 2005.07.15 R37-M0-N3-C:J13-U01 2005-07-15-23.29.02.491468 R37-M0-N3-C:J13-U01 RAS KERNEL INFO generating core.45777
- 1121495479 2005.07.15 R24-M0-N2-C:J15-U01 2005-07-15-23.31.19.487402 R24-M0-N2-C:J15-U01 RAS KERNEL INFO generating core.62940
- 1121495496 2005.07.15 R20-M0-N5-C:J03-U01 2005-07-15-23.31.36.540561 R20-M0-N5-C:J03-U01 RAS KERNEL INFO generating core.27867
- 1121495532 2005.07.15 R27-M0-NB-C:J16-U01 2005-07-15-23.32.12.622085 R27-M0-NB-C:J16-U01 RAS KERNEL INFO generating core.21016
- 1121495594 2005.07.15 R34-M0-N7-C:J13-U01 2005-07-15-23.33.14.550781 R34-M0-N7-C:J13-U01 RAS KERNEL INFO generating core.465
- 1121495754 2005.07.15 R21-M0-N9-C:J16-U11 2005-07-15-23.35.54.639346 R21-M0-N9-C:J16-U11 RAS KERNEL INFO generating core.55352
- 1121495798 2005.07.15 R37-M0-N5-C:J02-U11 2005-07-15-23.36.38.092419 R37-M0-N5-C:J02-U11 RAS KERNEL INFO generating core.44723
- 1121495911 2005.07.15 R32-M0-N5-C:J02-U11 2005-07-15-23.38.31.246747 R32-M0-N5-C:J02-U11 RAS KERNEL INFO generating core.36787
- 1121495968 2005.07.15 R10-M0-N2-C:J03-U01 2005-07-15-23.39.28.079463 R10-M0-N2-C:J03-U01 RAS KERNEL INFO generating core.29903
- 1121495976 2005.07.15 R12-M1-N7-C:J08-U11 2005-07-15-23.39.36.355775 R12-M1-N7-C:J08-U11 RAS KERNEL INFO generating core.33706
- 1121496041 2005.07.15 R17-M1-N6-C:J04-U11 2005-07-15-23.40.41.176362 R17-M1-N6-C:J04-U11 RAS KERNEL INFO generating core.8879
- 1121496047 2005.07.15 R15-M0-NF-C:J11-U01 2005-07-15-23.40.47.857118 R15-M0-NF-C:J11-U01 RAS KERNEL INFO generating core.50505
- 1121496062 2005.07.15 R10-M0-N3-C:J03-U01 2005-07-15-23.41.02.923439 R10-M0-N3-C:J03-U01 RAS KERNEL INFO generating core.29899
- 1121496080 2005.07.15 R15-M1-N3-C:J15-U01 2005-07-15-23.41.20.270704 R15-M1-N3-C:J15-U01 RAS KERNEL INFO generating core.46536
- 1121496335 2005.07.15 R13-M0-NE-C:J14-U11 2005-07-15-23.45.35.927063 R13-M0-NE-C:J14-U11 RAS KERNEL INFO generating core.50988
- 1121496365 2005.07.15 R03-M1-NB-C:J10-U01 2005-07-15-23.46.05.574350 R03-M1-NB-C:J10-U01 RAS KERNEL INFO generating core.22273
- 1121496382 2005.07.15 R11-M1-NA-C:J11-U01 2005-07-15-23.46.22.277125 R11-M1-NA-C:J11-U01 RAS KERNEL INFO generating core.46157
- 1121496460 2005.07.15 R06-M1-N7-C:J06-U11 2005-07-15-23.47.40.002166 R06-M1-N7-C:J06-U11 RAS KERNEL INFO generating core.59042
- 1121496542 2005.07.15 R02-M1-NA-C:J05-U11 2005-07-15-23.49.02.062268 R02-M1-NA-C:J05-U11 RAS KERNEL INFO generating core.29543
- 1121496570 2005.07.15 R07-M1-NA-C:J03-U11 2005-07-15-23.49.30.116304 R07-M1-NA-C:J03-U11 RAS KERNEL INFO generating core.22119
- 1121496582 2005.07.15 R02-M0-NA-C:J16-U11 2005-07-15-23.49.42.308319 R02-M0-NA-C:J16-U11 RAS KERNEL INFO generating core.4900
- 1121597832 2005.07.17 R36-M1-NB-C:J14-U11 2005-07-17-03.57.12.463575 R36-M1-NB-C:J14-U11 RAS KERNEL INFO generating core.30256
- 1121597851 2005.07.17 R36-M1-N1-C:J08-U01 2005-07-17-03.57.31.777381 R36-M1-N1-C:J08-U01 RAS KERNEL INFO generating core.64146
- 1121597882 2005.07.17 R22-M0-N7-C:J17-U01 2005-07-17-03.58.02.994632 R22-M0-N7-C:J17-U01 RAS KERNEL INFO generating core.58328
- 1121597962 2005.07.17 R33-M1-ND-C:J10-U01 2005-07-17-03.59.22.582679 R33-M1-ND-C:J10-U01 RAS KERNEL INFO generating core.20241
- 1121597978 2005.07.17 R33-M1-N9-C:J10-U01 2005-07-17-03.59.38.285272 R33-M1-N9-C:J10-U01 RAS KERNEL INFO generating core.24337
- 1121597997 2005.07.17 R37-M1-NA-C:J12-U11 2005-07-17-03.59.57.544786 R37-M1-NA-C:J12-U11 RAS KERNEL INFO generating core.21045
- 1121598024 2005.07.17 R21-M1-N1-C:J16-U01 2005-07-17-04.00.24.540747 R21-M1-N1-C:J16-U01 RAS KERNEL INFO generating core.47256
- 1121598039 2005.07.17 R21-M1-NA-C:J05-U11 2005-07-17-04.00.39.450366 R21-M1-NA-C:J05-U11 RAS KERNEL INFO generating core.45183
- 1121598051 2005.07.17 R27-M1-NA-C:J06-U11 2005-07-17-04.00.51.583517 R27-M1-NA-C:J06-U11 RAS KERNEL INFO generating core.46654
- 1121598150 2005.07.17 R23-M0-NA-C:J09-U11 2005-07-17-04.02.30.872103 R23-M0-NA-C:J09-U11 RAS KERNEL INFO generating core.21374
- 1121598237 2005.07.17 R23-M1-NB-C:J16-U11 2005-07-17-04.03.57.280027 R23-M1-NB-C:J16-U11 RAS KERNEL INFO generating core.45880
- 1121598257 2005.07.17 R31-M1-N3-C:J05-U01 2005-07-17-04.04.17.544035 R31-M1-N3-C:J05-U01 RAS KERNEL INFO generating core.20691
- 1121598278 2005.07.17 R33-M0-NA-C:J05-U11 2005-07-17-04.04.38.873517 R33-M0-NA-C:J05-U11 RAS KERNEL INFO generating core.45943
- 1121598391 2005.07.17 R25-M1-NB-C:J11-U01 2005-07-17-04.06.31.496101 R25-M1-NB-C:J11-U01 RAS KERNEL INFO generating core.46425
- 1121598603 2005.07.17 R13-M1-ND-C:J13-U11 2005-07-17-04.10.03.965790 R13-M1-ND-C:J13-U11 RAS KERNEL INFO generating core.11113
- 1121598608 2005.07.17 R16-M1-NB-C:J07-U11 2005-07-17-04.10.08.166749 R16-M1-NB-C:J07-U11 RAS KERNEL INFO generating core.5738
- 1121598678 2005.07.17 R10-M1-N6-C:J13-U11 2005-07-17-04.11.18.510363 R10-M1-N6-C:J13-U11 RAS KERNEL INFO generating core.237
- 1121598731 2005.07.17 R17-M1-N9-C:J09-U01 2005-07-17-04.12.11.971953 R17-M1-N9-C:J09-U01 RAS KERNEL INFO generating core.47690
- 1121598788 2005.07.17 R12-M0-NF-C:J16-U01 2005-07-17-04.13.08.062527 R12-M0-NF-C:J16-U01 RAS KERNEL INFO generating core.58120
- 1121598991 2005.07.17 R11-M1-NA-C:J05-U11 2005-07-17-04.16.31.772786 R11-M1-NA-C:J05-U11 RAS KERNEL INFO generating core.45167
- 1121599038 2005.07.17 R01-M1-N6-C:J07-U11 2005-07-17-04.17.18.609719 R01-M1-N6-C:J07-U11 RAS KERNEL INFO generating core.17638
- 1121599136 2005.07.17 R07-M1-ND-C:J11-U11 2005-07-17-04.18.56.608263 R07-M1-ND-C:J11-U11 RAS KERNEL INFO generating core.52833
- 1121599161 2005.07.17 R07-M1-N1-C:J10-U01 2005-07-17-04.19.21.452071 R07-M1-N1-C:J10-U01 RAS KERNEL INFO generating core.56961
- 1121599219 2005.07.17 R01-M0-ND-C:J16-U01 2005-07-17-04.20.19.372170 R01-M0-ND-C:J16-U01 RAS KERNEL INFO generating core.43008
- 1121599282 2005.07.17 R04-M1-NB-C:J13-U11 2005-07-17-04.21.22.640231 R04-M1-NB-C:J13-U11 RAS KERNEL INFO generating core.29025
- 1121599303 2005.07.17 R06-M0-N9-C:J06-U01 2005-07-17-04.21.43.559173 R06-M0-N9-C:J06-U01 RAS KERNEL INFO generating core.7682
- 1121599333 2005.07.17 R31-M0-N0-C:J04-U11 2005-07-17-04.22.13.319842 R31-M0-N0-C:J04-U11 RAS KERNEL INFO generating core.14519
- 1121599491 2005.07.17 R21-M0-N4-C:J13-U01 2005-07-17-04.24.51.040618 R21-M0-N4-C:J13-U01 RAS KERNEL INFO generating core.51421
- 1121599560 2005.07.17 R10-M0-NC-C:J09-U11 2005-07-17-04.26.00.533212 R10-M0-NC-C:J09-U11 RAS KERNEL INFO generating core.26734
- 1121599608 2005.07.17 R14-M1-N0-C:J09-U11 2005-07-17-04.26.48.055689 R14-M1-N0-C:J09-U11 RAS KERNEL INFO generating core.39406
- 1121599612 2005.07.17 R00-M1-N0-C:J08-U11 2005-07-17-04.26.52.789085 R00-M1-N0-C:J08-U11 RAS KERNEL INFO generating core.30886
- 1121599720 2005.07.17 R17-M1-N4-C:J14-U01 2005-07-17-04.28.40.989032 R17-M1-N4-C:J14-U01 RAS KERNEL INFO generating core.11916
- 1121706993 2005.07.18 R37-M1-NE-C:J02-U11 2005-07-18-10.16.33.614707 R37-M1-NE-C:J02-U11 RAS KERNEL INFO generating core.25367
- 1121707096 2005.07.18 R21-M1-N2-C:J07-U01 2005-07-18-10.18.16.381095 R21-M1-N2-C:J07-U01 RAS KERNEL INFO generating core.23150
- 1121707109 2005.07.18 R22-M0-N2-C:J12-U11 2005-07-18-10.18.29.909939 R22-M0-N2-C:J12-U11 RAS KERNEL INFO generating core.31197
- 1121707209 2005.07.18 R30-M1-NE-C:J02-U11 2005-07-18-10.20.09.218913 R30-M1-NE-C:J02-U11 RAS KERNEL INFO generating core.29207
- 1121707280 2005.07.18 R23-M0-N2-C:J15-U01 2005-07-18-10.21.20.444216 R23-M0-N2-C:J15-U01 RAS KERNEL INFO generating core.11244
- 1121707362 2005.07.18 R36-M1-N8-C:J06-U01 2005-07-18-10.22.42.290042 R36-M1-N8-C:J06-U01 RAS KERNEL INFO generating core.16134
- 1121707363 2005.07.18 R26-M0-NC-C:J07-U11 2005-07-18-10.22.43.700974 R26-M0-NC-C:J07-U11 RAS KERNEL INFO generating core.14142
- 1121707390 2005.07.18 R27-M0-N0-C:J12-U11 2005-07-18-10.23.10.943573 R27-M0-N0-C:J12-U11 RAS KERNEL INFO generating core.27997
- 1121707427 2005.07.18 R34-M1-N8-C:J07-U01 2005-07-18-10.23.47.339006 R34-M1-N8-C:J07-U01 RAS KERNEL INFO generating core.16038
- 1121707460 2005.07.18 R23-M1-N0-C:J05-U01 2005-07-18-10.24.20.440509 R23-M1-N0-C:J05-U01 RAS KERNEL INFO generating core.7663
KERNTERM 1121727206 2005.07.18 R20-M1-N8-C:J12-U01 2005-07-18-15.53.26.229596 R20-M1-N8-C:J12-U01 RAS KERNEL FATAL rts: kernel terminated for reason 1001rts: bad message header: invalid cpu, type=42315, cpu=105, index=1207960804, total=2691015
- 1121727746 2005.07.18 R00-M1-N5-C:J03-U11 2005-07-18-16.02.26.154455 R00-M1-N5-C:J03-U11 RAS KERNEL FATAL debug wait enable.................0
- 1121736053 2005.07.18 R02-M1-N5-C:J17-U01 2005-07-18-18.20.53.293303 R02-M1-N5-C:J17-U01 RAS KERNEL INFO generating core.176
- 1121900469 2005.07.20 R33-M1-N3-C:J02-U01 2005-07-20-16.01.09.861175 R33-M1-N3-C:J02-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121900596 2005.07.20 R24-M0-NB-C:J06-U11 2005-07-20-16.03.16.028154 R24-M0-NB-C:J06-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121900682 2005.07.20 R30-M1-N9-C:J14-U11 2005-07-20-16.04.42.418104 R30-M1-N9-C:J14-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121900708 2005.07.20 R35-M1-N5-C:J08-U11 2005-07-20-16.05.08.320371 R35-M1-N5-C:J08-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121900851 2005.07.20 R32-M1-N5-C:J06-U01 2005-07-20-16.07.31.195707 R32-M1-N5-C:J06-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121900875 2005.07.20 R10-M0-NF-C:J04-U01 2005-07-20-16.07.55.072864 R10-M0-NF-C:J04-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901038 2005.07.20 R16-M0-N3-C:J11-U01 2005-07-20-16.10.38.843234 R16-M0-N3-C:J11-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901046 2005.07.20 R00-M1-NF-C:J04-U11 2005-07-20-16.10.46.120814 R00-M1-NF-C:J04-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901110 2005.07.20 R15-M0-N2-C:J04-U11 2005-07-20-16.11.50.364727 R15-M0-N2-C:J04-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901115 2005.07.20 R11-M1-NB-C:J15-U11 2005-07-20-16.11.55.030054 R11-M1-NB-C:J15-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901136 2005.07.20 R04-M1-N1-C:J12-U01 2005-07-20-16.12.16.427147 R04-M1-N1-C:J12-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901193 2005.07.20 R02-M0-NF-C:J10-U11 2005-07-20-16.13.13.815662 R02-M0-NF-C:J10-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901286 2005.07.20 R04-M1-NE-C:J02-U11 2005-07-20-16.14.46.720905 R04-M1-NE-C:J02-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901372 2005.07.20 R35-M1-N4-C:J17-U01 2005-07-20-16.16.12.601313 R35-M1-N4-C:J17-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901493 2005.07.20 R05-M1-NC-C:J14-U11 2005-07-20-16.18.13.877286 R05-M1-NC-C:J14-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901515 2005.07.20 R14-M0-N8-C:J13-U11 2005-07-20-16.18.35.920838 R14-M0-N8-C:J13-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901541 2005.07.20 R11-M1-N0-C:J08-U01 2005-07-20-16.19.01.457880 R11-M1-N0-C:J08-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121901551 2005.07.20 R17-M0-N4-C:J06-U01 2005-07-20-16.19.11.825942 R17-M0-N4-C:J06-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121902597 2005.07.20 R11-M1-NE-C:J09-U01 2005-07-20-16.36.37.976854 R11-M1-NE-C:J09-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121902788 2005.07.20 R31-M0-NC-C:J14-U11 2005-07-20-16.39.48.713007 R31-M0-NC-C:J14-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121902797 2005.07.20 R27-M1-NC-C:J15-U01 2005-07-20-16.39.57.764472 R27-M1-NC-C:J15-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121902827 2005.07.20 R25-M1-N8-C:J03-U01 2005-07-20-16.40.27.205362 R25-M1-N8-C:J03-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121902859 2005.07.20 R10-M1-N0-C:J09-U11 2005-07-20-16.40.59.622276 R10-M1-N0-C:J09-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121902860 2005.07.20 R10-M1-N0-C:J16-U11 2005-07-20-16.41.00.180072 R10-M1-N0-C:J16-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121902868 2005.07.20 R00-M0-N4-C:J17-U01 2005-07-20-16.41.08.898577 R00-M0-N4-C:J17-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121903649 2005.07.20 R34-M0-N1-C:J08-U01 2005-07-20-16.54.09.929292 R34-M0-N1-C:J08-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121903678 2005.07.20 R32-M0-N3-C:J09-U11 2005-07-20-16.54.38.831021 R32-M0-N3-C:J09-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121903679 2005.07.20 R32-M0-N3-C:J08-U11 2005-07-20-16.54.39.433528 R32-M0-N3-C:J08-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121903735 2005.07.20 R12-M1-N5-C:J15-U11 2005-07-20-16.55.35.876056 R12-M1-N5-C:J15-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121903795 2005.07.20 R06-M0-N7-C:J10-U11 2005-07-20-16.56.35.232612 R06-M0-N7-C:J10-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121903856 2005.07.20 R02-M0-N8-C:J12-U01 2005-07-20-16.57.36.030657 R02-M0-N8-C:J12-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121904520 2005.07.20 R36-M0-N7-C:J08-U01 2005-07-20-17.08.40.559526 R36-M0-N7-C:J08-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121904541 2005.07.20 R24-M1-N5-C:J03-U01 2005-07-20-17.09.01.109294 R24-M1-N5-C:J03-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121904572 2005.07.20 R32-M0-N5-C:J10-U11 2005-07-20-17.09.32.073144 R32-M0-N5-C:J10-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121904574 2005.07.20 R10-M1-ND-C:J17-U11 2005-07-20-17.09.34.845947 R10-M1-ND-C:J17-U11 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121905368 2005.07.20 R02-M0-NF-C:J13-U01 2005-07-20-17.22.48.249852 R02-M0-NF-C:J13-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1121906028 2005.07.20 R36-M0-NE-C:J17-U01 2005-07-20-17.33.48.094973 R36-M0-NE-C:J17-U01 RAS KERNEL INFO 8 floating point alignment exceptions
- 1122126854 2005.07.23 R27-M0-N5-C:J11-U11 2005-07-23-06.54.14.284728 R27-M0-N5-C:J11-U11 RAS KERNEL INFO total of 1 ddr error(s) detected and corrected
- 1122132688 2005.07.23 R16-M0-N8-C:J09-U01 2005-07-23-08.31.28.200982 R16-M0-N8-C:J09-U01 RAS KERNEL INFO generating core.7830
- 1122132705 2005.07.23 R12-M1-N0-C:J13-U01 2005-07-23-08.31.45.382020 R12-M1-N0-C:J13-U01 RAS KERNEL INFO generating core.1781
- 1122132718 2005.07.23 R10-M1-ND-C:J11-U01 2005-07-23-08.31.58.764442 R10-M1-ND-C:J11-U01 RAS KERNEL INFO generating core.8977
- 1122132758 2005.07.23 R17-M0-N7-C:J11-U01 2005-07-23-08.32.38.872396 R17-M0-N7-C:J11-U01 RAS KERNEL INFO generating core.12721
- 1122132819 2005.07.23 R14-M0-ND-C:J11-U11 2005-07-23-08.33.39.102357 R14-M0-ND-C:J11-U11 RAS KERNEL INFO generating core.15193
- 1122132870 2005.07.23 R16-M1-N5-C:J15-U01 2005-07-23-08.34.30.223226 R16-M1-N5-C:J15-U01 RAS KERNEL INFO generating core.9136
- 1122133251 2005.07.23 R27-M0-NE-C:J11-U11 2005-07-23-08.40.51.684790 R27-M0-NE-C:J11-U11 RAS KERNEL INFO generating core.9021
- 1122133459 2005.07.23 R37-M0-NC-C:J03-U01 2005-07-23-08.44.19.668771 R37-M0-NC-C:J03-U01 RAS KERNEL INFO generating core.5927
- 1122133483 2005.07.23 R26-M1-N0-C:J05-U11 2005-07-23-08.44.43.722542 R26-M1-N0-C:J05-U11 RAS KERNEL INFO generating core.3455
- 1122133520 2005.07.23 R30-M0-N8-C:J12-U11 2005-07-23-08.45.20.908335 R30-M0-N8-C:J12-U11 RAS KERNEL INFO generating core.3093
- 1122139627 2005.07.23 R01-M0-NE-C:J17-U01 2005-07-23-10.27.07.224287 R01-M0-NE-C:J17-U01 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122139629 2005.07.23 R05-M1-N5-C:J14-U11 2005-07-23-10.27.09.450287 R05-M1-N5-C:J14-U11 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122139727 2005.07.23 R04-M0-N4-C:J12-U01 2005-07-23-10.28.47.226151 R04-M0-N4-C:J12-U01 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122139731 2005.07.23 R01-M1-N8-C:J13-U11 2005-07-23-10.28.51.576312 R01-M1-N8-C:J13-U11 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122143012 2005.07.23 R14-M1-N3-C:J04-U01 2005-07-23-11.23.32.715770 R14-M1-N3-C:J04-U01 RAS KERNEL INFO generating core.9315
- 1122143037 2005.07.23 R10-M0-N2-C:J12-U01 2005-07-23-11.23.57.554984 R10-M0-N2-C:J12-U01 RAS KERNEL INFO generating core.7205
- 1122143084 2005.07.23 R13-M1-NF-C:J12-U01 2005-07-23-11.24.44.659166 R13-M1-NF-C:J12-U01 RAS KERNEL INFO generating core.2241
- 1122143119 2005.07.23 R15-M1-N6-C:J02-U11 2005-07-23-11.25.19.391258 R15-M1-N6-C:J02-U11 RAS KERNEL INFO generating core.10607
- 1122143241 2005.07.23 R10-M0-NC-C:J15-U11 2005-07-23-11.27.21.304353 R10-M0-NC-C:J15-U11 RAS KERNEL INFO generating core.6940
- 1122143251 2005.07.23 R10-M0-N4-C:J05-U11 2005-07-23-11.27.31.167078 R10-M0-N4-C:J05-U11 RAS KERNEL INFO generating core.14911
- 1122143827 2005.07.23 R33-M1-N5-C:J13-U11 2005-07-23-11.37.07.526139 R33-M1-N5-C:J13-U11 RAS KERNEL INFO generating core.9713
- 1122143917 2005.07.23 R37-M1-N1-C:J11-U11 2005-07-23-11.38.37.267951 R37-M1-N1-C:J11-U11 RAS KERNEL INFO generating core.28529
- 1122143971 2005.07.23 R24-M1-N3-C:J16-U01 2005-07-23-11.39.31.111761 R24-M1-N3-C:J16-U01 RAS KERNEL INFO generating core.18632
- 1122144001 2005.07.23 R22-M1-N3-C:J12-U01 2005-07-23-11.40.01.453782 R22-M1-N3-C:J12-U01 RAS KERNEL INFO generating core.2505
- 1122144004 2005.07.23 R21-M0-NF-C:J09-U11 2005-07-23-11.40.04.831495 R21-M0-NF-C:J09-U11 RAS KERNEL INFO generating core.24634
- 1122144022 2005.07.23 R22-M0-N3-C:J02-U01 2005-07-23-11.40.22.071233 R22-M0-N3-C:J02-U01 RAS KERNEL INFO generating core.31691
- 1122144024 2005.07.23 R24-M1-N2-C:J02-U01 2005-07-23-11.40.24.966850 R24-M1-N2-C:J02-U01 RAS KERNEL INFO generating core.2767
- 1122144042 2005.07.23 R36-M1-NE-C:J11-U11 2005-07-23-11.40.42.753000 R36-M1-NE-C:J11-U11 RAS KERNEL INFO generating core.29493
- 1122144072 2005.07.23 R33-M0-NB-C:J10-U01 2005-07-23-11.41.12.568550 R33-M0-NB-C:J10-U01 RAS KERNEL INFO generating core.23425
- 1122147215 2005.07.23 R01-M1-N1-C:J03-U11 2005-07-23-12.33.35.436731 R01-M1-N1-C:J03-U11 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122160892 2005.07.23 R01-M1-NB-C:J03-U11 2005-07-23-16.21.32.749114 R01-M1-NB-C:J03-U11 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 20, bit 3
- 1122160898 2005.07.23 R04-M0-N1-C:J10-U01 2005-07-23-16.21.38.272915 R04-M0-N1-C:J10-U01 RAS KERNEL INFO total of 23 ddr error(s) detected and corrected
- 1122162130 2005.07.23 R24-M1-ND-C:J09-U11 2005-07-23-16.42.10.500605 R24-M1-ND-C:J09-U11 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122162200 2005.07.23 R33-M1-N1-C:J09-U01 2005-07-23-16.43.20.845234 R33-M1-N1-C:J09-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122162364 2005.07.23 R27-M0-N9-C:J07-U11 2005-07-23-16.46.04.171597 R27-M0-N9-C:J07-U11 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122164883 2005.07.23 R17-M1-NF-C:J10-U11 2005-07-23-17.28.03.140418 R17-M1-NF-C:J10-U11 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122164887 2005.07.23 R17-M1-N9-C:J05-U01 2005-07-23-17.28.07.326889 R17-M1-N9-C:J05-U01 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122164950 2005.07.23 R11-M0-N9-C:J17-U01 2005-07-23-17.29.10.970499 R11-M0-N9-C:J17-U01 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122164957 2005.07.23 R15-M0-N6-C:J12-U01 2005-07-23-17.29.17.315417 R15-M0-N6-C:J12-U01 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122164972 2005.07.23 R13-M1-N2-C:J06-U11 2005-07-23-17.29.32.058080 R13-M1-N2-C:J06-U11 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122165031 2005.07.23 R14-M1-N4-C:J04-U01 2005-07-23-17.30.31.565397 R14-M1-N4-C:J04-U01 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122165037 2005.07.23 R12-M0-N0-C:J12-U11 2005-07-23-17.30.37.160243 R12-M0-N0-C:J12-U11 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122165073 2005.07.23 R13-M1-N4-C:J04-U11 2005-07-23-17.31.13.824418 R13-M1-N4-C:J04-U11 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122165076 2005.07.23 R13-M0-N0-C:J15-U11 2005-07-23-17.31.16.270211 R13-M0-N0-C:J15-U11 RAS KERNEL INFO 15 floating point alignment exceptions
- 1122165233 2005.07.23 R16-M0-N9-C:J08-U11 2005-07-23-17.33.53.667330 R16-M0-N9-C:J08-U11 RAS KERNEL INFO generating core.7818
- 1122165293 2005.07.23 R12-M0-NC-C:J05-U01 2005-07-23-17.34.53.711297 R12-M0-NC-C:J05-U01 RAS KERNEL INFO generating core.15063
- 1122165304 2005.07.23 R16-M0-N0-C:J10-U01 2005-07-23-17.35.04.461170 R16-M0-N0-C:J10-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165425 2005.07.23 R14-M1-N3-C:J15-U01 2005-07-23-17.37.05.399108 R14-M1-N3-C:J15-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165429 2005.07.23 R16-M0-NE-C:J06-U01 2005-07-23-17.37.09.528299 R16-M0-NE-C:J06-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165437 2005.07.23 R16-M0-ND-C:J02-U11 2005-07-23-17.37.17.280587 R16-M0-ND-C:J02-U11 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165453 2005.07.23 R12-M1-N6-C:J06-U11 2005-07-23-17.37.33.651480 R12-M1-N6-C:J06-U11 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165503 2005.07.23 R16-M1-N3-C:J15-U11 2005-07-23-17.38.23.229235 R16-M1-N3-C:J15-U11 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165588 2005.07.23 R17-M0-N4-C:J06-U11 2005-07-23-17.39.48.305148 R17-M0-N4-C:J06-U11 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165615 2005.07.23 R15-M0-N6-C:J10-U11 2005-07-23-17.40.15.015021 R15-M0-N6-C:J10-U11 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165629 2005.07.23 R14-M1-N0-C:J06-U01 2005-07-23-17.40.29.371632 R14-M1-N0-C:J06-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165682 2005.07.23 R10-M1-N4-C:J06-U01 2005-07-23-17.41.22.125837 R10-M1-N4-C:J06-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165850 2005.07.23 R17-M0-NF-C:J04-U01 2005-07-23-17.44.10.520225 R17-M0-NF-C:J04-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165876 2005.07.23 R10-M1-NE-C:J05-U01 2005-07-23-17.44.36.808433 R10-M1-NE-C:J05-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122165882 2005.07.23 R11-M1-NE-C:J10-U11 2005-07-23-17.44.42.945213 R11-M1-NE-C:J10-U11 RAS KERNEL INFO generating core.2317
- 1122165888 2005.07.23 R15-M0-NE-C:J07-U01 2005-07-23-17.44.48.654569 R15-M0-NE-C:J07-U01 RAS KERNEL INFO generating core.12630
- 1122165890 2005.07.23 R15-M1-NA-C:J13-U11 2005-07-23-17.44.50.608907 R15-M1-NA-C:J13-U11 RAS KERNEL INFO generating core.11357
- 1122165915 2005.07.23 R17-M0-N1-C:J06-U11 2005-07-23-17.45.15.142194 R17-M0-N1-C:J06-U11 RAS KERNEL INFO generating core.14250
- 1122166026 2005.07.23 R11-M1-NF-C:J07-U01 2005-07-23-17.47.06.550385 R11-M1-NF-C:J07-U01 RAS KERNEL INFO generating core.10514
- 1122166057 2005.07.23 R16-M1-N0-C:J10-U01 2005-07-23-17.47.37.233460 R16-M1-N0-C:J10-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122166096 2005.07.23 R17-M0-N4-C:J12-U11 2005-07-23-17.48.16.382766 R17-M0-N4-C:J12-U11 RAS KERNEL INFO generating core.4781
- 1122166538 2005.07.23 R15-M1-N3-C:J11-U11 2005-07-23-17.55.38.692000 R15-M1-N3-C:J11-U11 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122166574 2005.07.23 R14-M1-ND-C:J17-U01 2005-07-23-17.56.14.409667 R14-M1-ND-C:J17-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122166579 2005.07.23 R14-M0-NF-C:J09-U01 2005-07-23-17.56.19.153654 R14-M0-NF-C:J09-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122166602 2005.07.23 R16-M1-NE-C:J10-U11 2005-07-23-17.56.42.172094 R16-M1-NE-C:J10-U11 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122166655 2005.07.23 R13-M0-NB-C:J03-U11 2005-07-23-17.57.35.683632 R13-M0-NB-C:J03-U11 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122166749 2005.07.23 R13-M1-NC-C:J04-U01 2005-07-23-17.59.09.578130 R13-M1-NC-C:J04-U01 RAS KERNEL INFO 5 floating point alignment exceptions
- 1122167255 2005.07.23 R16-M1-NB-C:J06-U01 2005-07-23-18.07.35.329193 R16-M1-NB-C:J06-U01 RAS KERNEL INFO 685 floating point alignment exceptions
- 1122167266 2005.07.23 R17-M0-NF-C:J09-U01 2005-07-23-18.07.46.273016 R17-M0-NF-C:J09-U01 RAS KERNEL INFO 685 floating point alignment exceptions
- 1122167432 2005.07.23 R11-M1-N9-C:J07-U11 2005-07-23-18.10.32.223048 R11-M1-N9-C:J07-U11 RAS KERNEL INFO 685 floating point alignment exceptions
- 1122167438 2005.07.23 R11-M1-N5-C:J14-U11 2005-07-23-18.10.38.305020 R11-M1-N5-C:J14-U11 RAS KERNEL INFO 685 floating point alignment exceptions
- 1122168559 2005.07.23 R13-M0-N2-C:J02-U01 2005-07-23-18.29.19.795230 R13-M0-N2-C:J02-U01 RAS KERNEL INFO 1005 floating point alignment exceptions
- 1122168606 2005.07.23 R16-M1-NA-C:J09-U11 2005-07-23-18.30.06.965137 R16-M1-NA-C:J09-U11 RAS KERNEL INFO 1005 floating point alignment exceptions
- 1122168668 2005.07.23 R14-M0-N1-C:J12-U11 2005-07-23-18.31.08.281027 R14-M0-N1-C:J12-U11 RAS KERNEL INFO 1005 floating point alignment exceptions
- 1122168756 2005.07.23 R17-M0-N0-C:J10-U01 2005-07-23-18.32.36.821809 R17-M0-N0-C:J10-U01 RAS KERNEL INFO 1005 floating point alignment exceptions
- 1122168766 2005.07.23 R13-M1-N4-C:J14-U01 2005-07-23-18.32.46.116065 R13-M1-N4-C:J14-U01 RAS KERNEL INFO 1005 floating point alignment exceptions
- 1122244714 2005.07.24 R36-M1-N9-C:J06-U01 2005-07-24-15.38.34.839211 R36-M1-N9-C:J06-U01 RAS KERNEL INFO CE sym 25, at 0x155e28e0, mask 0x08
- 1122255910 2005.07.24 R37-M0-N1-C:J02-U01 2005-07-24-18.45.10.049310 R37-M0-N1-C:J02-U01 RAS KERNEL INFO generating core.16019
- 1122255994 2005.07.24 R31-M0-NB-C:J17-U11 2005-07-24-18.46.34.093592 R31-M0-NB-C:J17-U11 RAS KERNEL INFO generating core.45168
- 1122256043 2005.07.24 R24-M1-NF-C:J05-U01 2005-07-24-18.47.23.101939 R24-M1-NF-C:J05-U01 RAS KERNEL INFO generating core.33115
- 1122256252 2005.07.24 R34-M0-N2-C:J04-U11 2005-07-24-18.50.52.804496 R34-M0-N2-C:J04-U11 RAS KERNEL INFO generating core.4535
- 1122256376 2005.07.24 R31-M1-NA-C:J04-U01 2005-07-24-18.52.56.307046 R31-M1-NA-C:J04-U01 RAS KERNEL INFO generating core.53271
- 1122256379 2005.07.24 R34-M1-N9-C:J17-U01 2005-07-24-18.52.59.208597 R34-M1-N9-C:J17-U01 RAS KERNEL INFO generating core.31056
- 1122256630 2005.07.24 R10-M0-N2-C:J10-U11 2005-07-24-18.57.10.497267 R10-M0-N2-C:J10-U11 RAS KERNEL INFO generating core.62637
- 1122256784 2005.07.24 R17-M1-N9-C:J11-U11 2005-07-24-18.59.44.109317 R17-M1-N9-C:J11-U11 RAS KERNEL INFO generating core.15977
- 1122256860 2005.07.24 R12-M1-N2-C:J04-U11 2005-07-24-19.01.00.580526 R12-M1-N2-C:J04-U11 RAS KERNEL INFO generating core.37807
- 1122256902 2005.07.24 R00-M0-N2-C:J06-U01 2005-07-24-19.01.42.801712 R00-M0-N2-C:J06-U01 RAS KERNEL INFO generating core.38022
- 1122334240 2005.07.25 R01-M1-NB-C:J15-U01 2005-07-25-16.30.40.590097 R01-M1-NB-C:J15-U01 RAS KERNEL INFO generating core.2704
- 1122334241 2005.07.25 R00-M0-N7-C:J09-U01 2005-07-25-16.30.41.390181 R00-M0-N7-C:J09-U01 RAS KERNEL INFO generating core.50
- 1122334272 2005.07.25 R00-M1-N0-C:J14-U11 2005-07-25-16.31.12.207957 R00-M1-N0-C:J14-U11 RAS KERNEL INFO generating core.4012
- 1122334283 2005.07.25 R05-M1-N0-C:J12-U01 2005-07-25-16.31.23.780557 R05-M1-N0-C:J12-U01 RAS KERNEL INFO generating core.2917
- 1122400971 2005.07.26 R06-M0-N7-C:J07-U01 2005-07-26-11.02.51.140194 R06-M0-N7-C:J07-U01 RAS KERNEL INFO generating core.114
- 1122403352 2005.07.26 R06-M0-N5-C:J15-U11 2005-07-26-11.42.32.680498 R06-M0-N5-C:J15-U11 RAS KERNEL INFO generating core.248
- 1122405365 2005.07.26 R04-M0-N6-C:J09-U01 2005-07-26-12.16.05.063905 R04-M0-N6-C:J09-U01 RAS KERNEL INFO generating core.118
- 1122405838 2005.07.26 R06-M0-N8-C:J11-U01 2005-07-26-12.23.58.178894 R06-M0-N8-C:J11-U01 RAS KERNEL INFO generating core.469
- 1122430461 2005.07.26 R20-M0-N9-C:J06-U01 2005-07-26-19.14.21.706235 R20-M0-N9-C:J06-U01 RAS KERNEL INFO generating core.64538
- 1122430482 2005.07.26 R22-M1-NB-C:J08-U11 2005-07-26-19.14.42.115150 R22-M1-NB-C:J08-U11 RAS KERNEL INFO generating core.37690
- 1122430512 2005.07.26 R23-M1-N7-C:J10-U01 2005-07-26-19.15.12.085226 R23-M1-N7-C:J10-U01 RAS KERNEL INFO generating core.42905
- 1122430586 2005.07.26 R21-M0-N1-C:J05-U01 2005-07-26-19.16.26.413910 R21-M0-N1-C:J05-U01 RAS KERNEL INFO generating core.55515
- 1122430589 2005.07.26 R27-M1-NA-C:J02-U11 2005-07-26-19.16.29.748932 R27-M1-NA-C:J02-U11 RAS KERNEL INFO generating core.46655
- 1122430659 2005.07.26 R22-M1-N3-C:J02-U11 2005-07-26-19.17.39.082056 R22-M1-N3-C:J02-U11 RAS KERNEL INFO generating core.38843
- 1122430660 2005.07.26 R20-M1-ND-C:J06-U01 2005-07-26-19.17.40.775429 R20-M1-ND-C:J06-U01 RAS KERNEL INFO generating core.3098
- 1122430675 2005.07.26 R21-M1-N2-C:J12-U11 2005-07-26-19.17.55.987174 R21-M1-N2-C:J12-U11 RAS KERNEL INFO generating core.45245
- 1122430739 2005.07.26 R34-M0-N7-C:J15-U01 2005-07-26-19.18.59.568031 R34-M0-N7-C:J15-U01 RAS KERNEL INFO generating core.34256
- 1122430783 2005.07.26 R20-M0-NB-C:J02-U11 2005-07-26-19.19.43.766607 R20-M0-NB-C:J02-U11 RAS KERNEL INFO generating core.29755
- 1122430797 2005.07.26 R35-M1-N7-C:J02-U01 2005-07-26-19.19.57.499754 R35-M1-N7-C:J02-U01 RAS KERNEL INFO generating core.50579
- 1122431105 2005.07.26 R13-M1-NE-C:J10-U01 2005-07-26-19.25.05.763740 R13-M1-NE-C:J10-U01 RAS KERNEL INFO generating core.9997
- 1122431133 2005.07.26 R16-M0-N9-C:J17-U01 2005-07-26-19.25.33.847429 R16-M0-N9-C:J17-U01 RAS KERNEL INFO generating core.64072
- 1122431171 2005.07.26 R16-M0-N6-C:J12-U01 2005-07-26-19.26.11.720872 R16-M0-N6-C:J12-U01 RAS KERNEL INFO generating core.25229
- 1122431178 2005.07.26 R17-M1-NB-C:J09-U11 2005-07-26-19.26.18.003421 R17-M1-NB-C:J09-U11 RAS KERNEL INFO generating core.45674
- 1122431204 2005.07.26 R10-M1-NB-C:J16-U11 2005-07-26-19.26.44.167957 R10-M1-NB-C:J16-U11 RAS KERNEL INFO generating core.36904
- 1122431319 2005.07.26 R16-M0-ND-C:J17-U01 2005-07-26-19.28.39.681757 R16-M0-ND-C:J17-U01 RAS KERNEL INFO generating core.59976
- 1122431319 2005.07.26 R16-M0-ND-C:J13-U01 2005-07-26-19.28.39.863604 R16-M0-ND-C:J13-U01 RAS KERNEL INFO generating core.59977
- 1122431396 2005.07.26 R07-M0-N6-C:J07-U11 2005-07-26-19.29.56.539285 R07-M0-N6-C:J07-U11 RAS KERNEL INFO generating core.9958
- 1122431408 2005.07.26 R11-M0-N5-C:J14-U01 2005-07-26-19.30.08.728161 R11-M0-N5-C:J14-U01 RAS KERNEL INFO generating core.19592
- 1122431485 2005.07.26 R03-M1-N6-C:J08-U11 2005-07-26-19.31.25.816198 R03-M1-N6-C:J08-U11 RAS KERNEL INFO generating core.50086
- 1122431514 2005.07.26 R03-M1-N2-C:J11-U01 2005-07-26-19.31.54.482487 R03-M1-N2-C:J11-U01 RAS KERNEL INFO generating core.22469
- 1122431540 2005.07.26 R14-M0-N9-C:J04-U11 2005-07-26-19.32.20.863604 R14-M0-N9-C:J04-U11 RAS KERNEL INFO generating core.63787
- 1122431669 2005.07.26 R07-M1-N6-C:J14-U11 2005-07-26-19.34.29.061235 R07-M1-N6-C:J14-U11 RAS KERNEL INFO generating core.18084
- 1122431732 2005.07.26 R05-M1-N9-C:J11-U01 2005-07-26-19.35.32.608060 R05-M1-N9-C:J11-U01 RAS KERNEL INFO generating core.23873
- 1122492300 2005.07.27 R36-M0-N7-C:J13-U11 2005-07-27-12.25.00.990788 R36-M0-N7-C:J13-U11 RAS KERNEL INFO generating core.2105
- 1122492307 2005.07.27 R36-M0-N3-C:J09-U01 2005-07-27-12.25.07.297896 R36-M0-N3-C:J09-U01 RAS KERNEL INFO generating core.306
- 1122574113 2005.07.28 R31-M1-N1-C:J08-U11 2005-07-28-11.08.33.510063 R31-M1-N1-C:J08-U11 RAS KERNEL INFO generating core.11346
- 1122574115 2005.07.28 R31-M0-NB-C:J12-U11 2005-07-28-11.08.35.403371 R31-M0-NB-C:J12-U11 RAS KERNEL INFO generating core.6161
- 1122627016 2005.07.29 R36-M0-NA-C:J13-U01 2005-07-29-01.50.16.378824 R36-M0-NA-C:J13-U01 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 11, bit 6
- 1123021638 2005.08.02 UNKNOWN_LOCATION 2005-08-02-15.27.18.161989 UNKNOWN_LOCATION NULL DISCOVERY INFO New ido chip inserted into the database: FF:F2:9F:15:1F:72:00:0D:60:EA:E0:8D ip=10.7.0.13 v=9 t=2
- 1123025540 2005.08.02 UNKNOWN_LOCATION 2005-08-02-16.32.20.358039 UNKNOWN_LOCATION NULL DISCOVERY INFO New ido chip inserted into the database: FF:F2:9F:16:BF:6C:00:0D:60:E9:40:93 ip=10.5.0.46 v=13 t=4
- 1123025954 2005.08.02 R23-M0-N7 2005-08-02-16.39.14.159918 R23-M0-N7 NULL HARDWARE SEVERE NodeCard is not fully functional
- 1123030687 2005.08.02 R00-M0-N2 2005-08-02-17.58.07.084868 R00-M0-N2 NULL DISCOVERY ERROR Node card status: ALERT 0, ALERT 1, ALERT 2, ALERT 3 is (are) active. Clock Mode is Low. Clock Select is Midplane. Phy JTAG Reset is asserted. ASIC JTAG Reset is not asserted. Temperature Mask is not active. No temperature error. Temperature Limit Error Latch is clear. PGOOD is asserted. PGOOD error latch is clear. MPGOOD is OK. MPGOOD error latch is clear. The 2.5 volt rail is OK. The 1.5 volt rail is OK.
- 1123030693 2005.08.02 R00-M0-ND 2005-08-02-17.58.13.994269 R00-M0-ND NULL DISCOVERY INFO Node card VPD check: U11 node in processor card slot J16 do not match. VPD ecid 04DF80A7942FFFFF0C081AE08CD2, found 0000000000000000000000000000
- 1123042536 2005.08.02 UNKNOWN_LOCATION 2005-08-02-21.15.36.811548 UNKNOWN_LOCATION NULL DISCOVERY SEVERE Can not get assembly information for node card
- 1123043313 2005.08.02 UNKNOWN_LOCATION 2005-08-02-21.28.33.869729 UNKNOWN_LOCATION NULL DISCOVERY INFO New ido chip inserted into the database: FF:F2:9F:16:C4:C2:00:0D:60:E9:3B:3D ip=10.2.1.37 v=13 t=4
- 1123060215 2005.08.03 UNKNOWN_LOCATION 2005-08-03-02.10.15.206558 UNKNOWN_LOCATION NULL DISCOVERY SEVERE Can not get assembly information for node card
- 1123110662 2005.08.03 NULL 2005-08-03-16.11.02.839771 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1123110668 2005.08.03 NULL 2005-08-03-16.11.08.572137 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1123110754 2005.08.03 NULL 2005-08-03-16.12.34.240816 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1123110774 2005.08.03 NULL 2005-08-03-16.12.54.271533 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1123110830 2005.08.03 NULL 2005-08-03-16.13.50.570973 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1123110843 2005.08.03 NULL 2005-08-03-16.14.03.884634 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1123110844 2005.08.03 NULL 2005-08-03-16.14.04.704650 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1123110860 2005.08.03 NULL 2005-08-03-16.14.20.667595 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1123110898 2005.08.03 NULL 2005-08-03-16.14.58.668464 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1123176466 2005.08.04 R71-M0-NC 2005-08-04-10.27.46.284345 R71-M0-NC NULL DISCOVERY ERROR Node card status: no ALERTs are active. Clock Mode is Low. Clock Select is Midplane. Phy JTAG Reset is asserted. ASIC JTAG Reset is asserted. Temperature Mask is not active. No temperature error. Temperature Limit Error Latch is clear. PGOOD IS NOT ASSERTED. PGOOD ERROR LATCH IS ACTIVE. MPGOOD IS NOT OK. MPGOOD ERROR LATCH IS ACTIVE. The 2.5 volt rail is OK. The 1.5 volt rail is OK.
- 1123176833 2005.08.04 R10-M1-N7 2005-08-04-10.33.53.378606 R10-M1-N7 NULL DISCOVERY INFO Node card VPD check: U01 node in processor card slot J14 do not match. VPD ecid 074C04C10F7BFFFF08051AE08ED2, found 04C180A5D12FFFFF05081BD088E2
- 1123178302 2005.08.04 R06-M1-ND 2005-08-04-10.58.22.010246 R06-M1-ND NULL DISCOVERY WARNING Node card is not fully functional
- 1123183618 2005.08.04 R76-M1-N4 2005-08-04-12.26.58.275812 R76-M1-N4 NULL DISCOVERY ERROR Node card status: no ALERTs are active. Clock Mode is Low. Clock Select is Midplane. Phy JTAG Reset is asserted. ASIC JTAG Reset is asserted. Temperature Mask is not active. No temperature error. Temperature Limit Error Latch is clear. PGOOD IS NOT ASSERTED. PGOOD ERROR LATCH IS ACTIVE. MPGOOD IS NOT OK. MPGOOD ERROR LATCH IS ACTIVE. The 2.5 volt rail is OK. The 1.5 volt rail is OK.
- 1123195083 2005.08.04 UNKNOWN_LOCATION 2005-08-04-15.38.03.475517 UNKNOWN_LOCATION NULL DISCOVERY INFO Ido chip status changed: FF:F2:9F:16:EB:27:00:0D:60:E9:14:D8 ip=10.0.2.192 v=13 t=4 status=M Thu Aug 04 15:29:45 PDT 2005
- 1123195190 2005.08.04 UNKNOWN_LOCATION 2005-08-04-15.39.50.374720 UNKNOWN_LOCATION NULL DISCOVERY INFO Ido chip status changed: FF:F2:9F:15:7E:6E:00:0D:60:EA:81:91 ip=10.0.1.155 v=13 t=1 status=M Thu Aug 04 15:31:25 PDT 2005
- 1123240786 2005.08.05 R23-M1-N1-C:J13-U11 2005-08-05-04.19.46.879304 R23-M1-N1-C:J13-U11 RAS KERNEL INFO CE sym 27, at 0x11b3f3c0, mask 0x10
- 1123262593 2005.08.05 R25-M1-N2 2005-08-05-10.23.13.029850 R25-M1-N2 NULL HARDWARE WARNING PrepareForService shutting down NodeCard(mLctn(R25-M1-N2), mCardSernum(203231503833343000000000594c31304b34333431303158), mLp(FF:F2:9F:16:CF:0F:00:0D:60:E9:30:F0), mIp(10.2.2.80), mType(4)) as part of Service Action 310
- 1123607801 2005.08.09 UNKNOWN_LOCATION 2005-08-09-10.16.41.431406 UNKNOWN_LOCATION NULL DISCOVERY INFO Ido chip status changed: FF:F2:9F:16:DC:81:00:0D:60:E9:23:7E ip=10.6.1.207 v=13 t=4 status=M Tue Aug 09 10:08:23 PDT 2005
- 1123608612 2005.08.09 R40-M1-N5 2005-08-09-10.30.12.637004 R40-M1-N5 NULL DISCOVERY SEVERE Can not get assembly information for node card
- 1123608679 2005.08.09 R70-M0-N0 2005-08-09-10.31.19.671420 R70-M0-N0 NULL DISCOVERY SEVERE Can not get assembly information for node card
- 1123609246 2005.08.09 R51-M1-ND 2005-08-09-10.40.46.749252 R51-M1-ND NULL DISCOVERY WARNING Node card is not fully functional
- 1123609672 2005.08.09 R44-M0-N3 2005-08-09-10.47.52.703182 R44-M0-N3 NULL DISCOVERY SEVERE Can not get assembly information for node card
- 1123609997 2005.08.09 R74-M0-N1 2005-08-09-10.53.17.485009 R74-M0-N1 NULL DISCOVERY WARNING Node card is not fully functional
- 1123610468 2005.08.09 R26-M0-N7 2005-08-09-11.01.08.922516 R26-M0-N7 NULL DISCOVERY ERROR Node card status: no ALERTs are active. Clock Mode is Low. Clock Select is Midplane. Phy JTAG Reset is asserted. ASIC JTAG Reset is asserted. Temperature Mask is not active. No temperature error. Temperature Limit Error Latch is clear. PGOOD IS NOT ASSERTED. PGOOD ERROR LATCH IS ACTIVE. MPGOOD IS NOT OK. MPGOOD ERROR LATCH IS ACTIVE. The 2.5 volt rail is OK. The 1.5 volt rail is OK.
- 1123614572 2005.08.09 R00-M0-N6-C:J03-U11 2005-08-09-12.09.32.252222 R00-M0-N6-C:J03-U11 RAS KERNEL FATAL rts tree/torus link training failed: wanted: B C X+ X- Y+ Y- Z+ Z- got: B C X- Y- Z+ Z-
- 1123637155 2005.08.09 R23-M0-N0-C:J10-U01 2005-08-09-18.25.55.113195 R23-M0-N0-C:J10-U01 RAS KERNEL INFO CE sym 18, at 0x0e0272e0, mask 0x10
- 1123652286 2005.08.09 R15-M0-NC-C:J04-U11 2005-08-09-22.38.06.850691 R15-M0-NC-C:J04-U11 RAS KERNEL INFO 1 tree receiver 2 in re-synch state event(s) (dcr 0x019a) detected
- 1123685937 2005.08.10 R14-M1-N0-C:J07-U11 2005-08-10-07.58.57.502279 R14-M1-N0-C:J07-U11 RAS KERNEL INFO total of 1 ddr error(s) detected and corrected
- 1123709473 2005.08.10 R47-M0-N4-C:J07-U01 2005-08-10-14.31.13.463274 R47-M0-N4-C:J07-U01 RAS KERNEL INFO CE sym 8, at 0x0a2ae600, mask 0x10
- 1123759596 2005.08.11 R35-M0-N6-C:J13-U01 2005-08-11-04.26.36.236761 R35-M0-N6-C:J13-U01 RAS KERNEL INFO CE sym 28, at 0x0fe65820, mask 0x04
- 1123850737 2005.08.12 R61-M0-NB-C:J05-U01 2005-08-12-05.45.37.457186 R61-M0-NB-C:J05-U01 RAS KERNEL INFO CE sym 25, at 0x10e1b8a0, mask 0x10
- 1123914312 2005.08.12 R44-M0-N8-I:J18-U01 2005-08-12-23.25.12.135766 R44-M0-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123914469 2005.08.12 R46-M0-N0-I:J18-U01 2005-08-12-23.27.49.807991 R46-M0-N0-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123914543 2005.08.12 R52-M1-N8-I:J18-U11 2005-08-12-23.29.03.099851 R52-M1-N8-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123914551 2005.08.12 R57-M1-N0-I:J18-U01 2005-08-12-23.29.11.430269 R57-M1-N0-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123914577 2005.08.12 R61-M1-N0-I:J18-U01 2005-08-12-23.29.37.454616 R61-M1-N0-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123914638 2005.08.12 R24-M0-N0-I:J18-U01 2005-08-12-23.30.38.435583 R24-M0-N0-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123914727 2005.08.12 R55-M1-N0-I:J18-U11 2005-08-12-23.32.07.413454 R55-M1-N0-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123914754 2005.08.12 R31-M1-N4-I:J18-U11 2005-08-12-23.32.34.800803 R31-M1-N4-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123914842 2005.08.12 R66-M0-N8-I:J18-U01 2005-08-12-23.34.02.183112 R66-M0-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123914894 2005.08.12 R15-M0-NC-I:J18-U11 2005-08-12-23.34.54.272486 R15-M0-NC-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123914895 2005.08.12 R16-M1-N0-I:J18-U11 2005-08-12-23.34.55.201666 R16-M1-N0-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123915332 2005.08.12 R01-M0-NC-I:J18-U11 2005-08-12-23.42.12.603889 R01-M0-NC-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123915673 2005.08.12 R66-M1-N4-I:J18-U01 2005-08-12-23.47.53.914235 R66-M1-N4-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123915807 2005.08.12 R74-M1-N8-I:J18-U01 2005-08-12-23.50.07.561913 R74-M1-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123915920 2005.08.12 R73-M0-N4-I:J18-U11 2005-08-12-23.52.00.785387 R73-M0-N4-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/performance/MINIBEN/mb_243_0810/allreduce.rts: invalid or missing program image, Exec format error
- 1123966456 2005.08.13 R27-M1-NC-I:J18-U11 2005-08-13-13.54.16.386775 R27-M1-NC-I:J18-U11 RAS APP FATAL ciod: LOGIN chdir(pwd) failed: No such file or directory
- 1123966497 2005.08.13 R61-M1-NC-I:J18-U01 2005-08-13-13.54.57.564471 R61-M1-NC-I:J18-U01 RAS APP FATAL ciod: LOGIN chdir(pwd) failed: No such file or directory
- 1123976348 2005.08.13 R60-M1-NC-I:J18-U01 2005-08-13-16.39.08.734879 R60-M1-NC-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/scaletest/stability/MDCASK/WORK/65576/inferno: invalid or missing program image, No such file or directory
APPSEV 1124071359 2005.08.14 R21-M0-N8-I:J18-U11 2005-08-14-19.02.39.467287 R21-M0-N8-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix after LOAD_MESSAGE on CioStream socket to 172.16.96.116:42213: Link has been severed
- 1124131896 2005.08.15 R40-M1-N0-I:J18-U11 2005-08-15-11.51.36.375271 R40-M1-N0-I:J18-U11 RAS KERNEL INFO ciod: Missing or invalid fields on line 1 of node map file /home/auselton/bgl/mapfiles/bgl.128mps.f64.map
KERNTERM 1124167519 2005.08.15 R71-M0-NA-C:J12-U11 2005-08-15-21.45.19.228173 R71-M0-NA-C:J12-U11 RAS KERNEL FATAL rts: kernel terminated for reason 1003
KERNTERM 1124167540 2005.08.15 R63-M0-N4-C:J16-U01 2005-08-15-21.45.40.395707 R63-M0-N4-C:J16-U01 RAS KERNEL FATAL rts: kernel terminated for reason 1004
- 1124225022 2005.08.16 R06-M0-NB-C:J16-U01 2005-08-16-13.43.42.931133 R06-M0-NB-C:J16-U01 RAS KERNEL INFO CE sym 22, at 0x009c14e0, mask 0x20
- 1124233921 2005.08.16 R06-M0-NB-C:J16-U01 2005-08-16-16.12.01.257432 R06-M0-NB-C:J16-U01 RAS KERNEL INFO total of 1 ddr error(s) detected and corrected
- 1124306491 2005.08.17 R12-M0-N4-I:J18-U01 2005-08-17-12.21.31.579167 R12-M0-N4-I:J18-U01 RAS APP FATAL ciod: LOGIN chdir(/p/bg1/da) failed: No such file or directory
- 1124394179 2005.08.18 R30-M0-N0-I:J18-U01 2005-08-18-12.42.59.378202 R30-M0-N0-I:J18-U01 RAS KERNEL INFO ciod: generated 64 core files for program /bgl/apps/swl-prep/ibm-swl/functional/sppm_chkpt/run/sppm
- 1124394227 2005.08.18 R71-M1-N8-I:J18-U01 2005-08-18-12.43.47.536288 R71-M1-N8-I:J18-U01 RAS KERNEL INFO ciod: generated 64 core files for program /bgl/apps/swl-prep/ibm-swl/functional/sppm_chkpt/run/sppm
- 1124394255 2005.08.18 R02-M0-N4-I:J18-U11 2005-08-18-12.44.15.155220 R02-M0-N4-I:J18-U11 RAS KERNEL INFO ciod: generated 64 core files for program /bgl/apps/swl-prep/ibm-swl/functional/sppm_chkpt/run/sppm
- 1124519271 2005.08.19 R60-M1-N7-C:J16-U11 2005-08-19-23.27.51.734783 R60-M1-N7-C:J16-U11 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 10, bit 0
- 1124546691 2005.08.20 R44-M1-N4-I:J18-U11 2005-08-20-07.04.51.651202 R44-M1-N4-I:J18-U11 RAS KERNEL INFO ciod: generated 1 core files for program /bgl/apps/swl-prep/ibm-swl/functional/sppm_chkpt/run/sppm
- 1124547186 2005.08.20 R65-M0-NC-I:J18-U01 2005-08-20-07.13.06.997141 R65-M0-NC-I:J18-U01 RAS KERNEL INFO ciod: generated 64 core files for program /bgl/apps/swl-prep/ibm-swl/functional/sppm_chkpt/run/sppm
- 1124611136 2005.08.21 R11-M1-N2-C:J12-U01 2005-08-21-00.58.56.344729 R11-M1-N2-C:J12-U01 RAS KERNEL INFO CE sym 3, at 0x0b19b8a0, mask 0x08
- 1124611166 2005.08.21 R30-M1-NC-C:J02-U01 2005-08-21-00.59.26.072269 R30-M1-NC-C:J02-U01 RAS KERNEL INFO CE sym 20, at 0x180462c0, mask 0x02
- 1124722015 2005.08.22 R24-M1-N4-I:J18-U11 2005-08-22-07.46.55.044922 R24-M1-N4-I:J18-U11 RAS APP FATAL ciod: LOGIN chdir(/home/germann2/SPaSM_static) failed: No such file or directory
- 1124736611 2005.08.22 R34-M1-N8-I:J18-U01 2005-08-22-11.50.11.486431 R34-M1-N8-I:J18-U01 RAS KERNEL INFO ciod: generated 128 core files for program /home/germann2/BGL-demo/SPaSM_mpi
- 1124738105 2005.08.22 R70-M1-N0-I:J18-U01 2005-08-22-12.15.05.947393 R70-M1-N0-I:J18-U01 RAS KERNEL INFO ciod: generated 128 core files for program /home/germann2/BGL-demo/SPaSM_mpi
- 1124738119 2005.08.22 R16-M0-N4-I:J18-U11 2005-08-22-12.15.19.318427 R16-M0-N4-I:J18-U11 RAS KERNEL INFO ciod: generated 128 core files for program /home/germann2/BGL-demo/SPaSM_mpi
- 1124741118 2005.08.22 R21-M1-NC-I:J18-U01 2005-08-22-13.05.18.840938 R21-M1-NC-I:J18-U01 RAS KERNEL INFO ciod: generated 128 core files for program /home/germann2/BGL-demo/SPaSM_mpi
- 1124741126 2005.08.22 R65-M0-NC-I:J18-U11 2005-08-22-13.05.26.526291 R65-M0-NC-I:J18-U11 RAS KERNEL INFO ciod: generated 128 core files for program /home/germann2/BGL-demo/SPaSM_mpi
- 1124741161 2005.08.22 R04-M1-N8-I:J18-U11 2005-08-22-13.06.01.351555 R04-M1-N8-I:J18-U11 RAS KERNEL INFO ciod: generated 128 core files for program /home/germann2/BGL-demo/SPaSM_mpi
- 1124798068 2005.08.23 R50-M1-N0-I:J18-U11 2005-08-23-04.54.28.532357 R50-M1-N0-I:J18-U11 RAS APP FATAL ciod: LOGIN chdir(/home/germann2/BGL-demo) failed: No such file or directory
- 1124818171 2005.08.23 R14-M0-N0-I:J18-U11 2005-08-23-10.29.31.137434 R14-M0-N0-I:J18-U11 RAS KERNEL INFO ciod: generated 128 core files for program /home/germann2/BGL-demo/SPaSM_mpi
- 1124818189 2005.08.23 R06-M0-N0-I:J18-U01 2005-08-23-10.29.49.877845 R06-M0-N0-I:J18-U01 RAS KERNEL INFO ciod: generated 128 core files for program /home/germann2/BGL-demo/SPaSM_mpi
- 1124972563 2005.08.25 R71-M0-NF-C:J04-U01 2005-08-25-05.22.43.295317 R71-M0-NF-C:J04-U01 RAS KERNEL INFO total of 1 ddr error(s) detected and corrected
- 1125082053 2005.08.26 R62-M1-N3-C:J04-U01 2005-08-26-11.47.33.903841 R62-M1-N3-C:J04-U01 RAS KERNEL INFO iar 003a90fc dear 00b360e8
- 1125082080 2005.08.26 R63-M1-NF-C:J03-U11 2005-08-26-11.48.00.152812 R63-M1-NF-C:J03-U11 RAS KERNEL INFO iar 003a90dc dear 00b35148
- 1125082154 2005.08.26 R62-M1-NA-C:J14-U01 2005-08-26-11.49.14.423511 R62-M1-NA-C:J14-U01 RAS KERNEL INFO iar 003a90fc dear 00b35db8
- 1125082191 2005.08.26 R62-M1-NF-C:J09-U11 2005-08-26-11.49.51.360139 R62-M1-NF-C:J09-U11 RAS KERNEL INFO iar 003a9108 dear 00c07348
- 1125082746 2005.08.26 R63-M1-N6-C:J17-U01 2005-08-26-11.59.06.032773 R63-M1-N6-C:J17-U01 RAS KERNEL INFO 2201500 double-hummer alignment exceptions
- 1125082766 2005.08.26 R62-M1-N6-C:J06-U11 2005-08-26-11.59.26.069334 R62-M1-N6-C:J06-U11 RAS KERNEL INFO 2201500 double-hummer alignment exceptions
- 1125082778 2005.08.26 R63-M1-NB-C:J09-U01 2005-08-26-11.59.38.574254 R63-M1-NB-C:J09-U01 RAS KERNEL INFO iar 003a9294 dear 00c06c48
- 1125083016 2005.08.26 R63-M1-NA-C:J03-U01 2005-08-26-12.03.36.439157 R63-M1-NA-C:J03-U01 RAS KERNEL INFO iar 003a92a0 dear 00b36898
- 1125083240 2005.08.26 R61-M0-N8-C:J05-U01 2005-08-26-12.07.20.308532 R61-M0-N8-C:J05-U01 RAS KERNEL INFO 1547520 double-hummer alignment exceptions
- 1125083293 2005.08.26 R60-M1-N1-C:J11-U11 2005-08-26-12.08.13.538298 R60-M1-N1-C:J11-U11 RAS KERNEL INFO iar 003a9260 dear 00efe838
- 1125083309 2005.08.26 R65-M1-N9-C:J16-U11 2005-08-26-12.08.29.680955 R65-M1-N9-C:J16-U11 RAS KERNEL INFO iar 003a9260 dear 00efcbc8
- 1125083411 2005.08.26 R64-M1-N3-C:J07-U11 2005-08-26-12.10.11.586133 R64-M1-N3-C:J07-U11 RAS KERNEL INFO iar 003a929c dear 00efc768
- 1125083438 2005.08.26 R60-M0-NE-C:J10-U11 2005-08-26-12.10.38.958016 R60-M0-NE-C:J10-U11 RAS KERNEL INFO iar 003a929c dear 00efd448
- 1125083510 2005.08.26 R65-M1-N5-C:J14-U11 2005-08-26-12.11.50.217747 R65-M1-N5-C:J14-U11 RAS KERNEL INFO iar 003a92a0 dear 00efd358
- 1125083592 2005.08.26 R61-M1-ND-C:J17-U01 2005-08-26-12.13.12.965338 R61-M1-ND-C:J17-U01 RAS KERNEL INFO iar 003a92a0 dear 00fcc6b8
- 1125083604 2005.08.26 R65-M1-N4-C:J06-U01 2005-08-26-12.13.24.227502 R65-M1-N4-C:J06-U01 RAS KERNEL INFO iar 003a92a0 dear 00efd518
- 1125083702 2005.08.26 R65-M0-ND-C:J12-U01 2005-08-26-12.15.02.887927 R65-M0-ND-C:J12-U01 RAS KERNEL INFO iar 003a9254 dear 00f05688
- 1125083731 2005.08.26 R61-M0-N3-C:J11-U11 2005-08-26-12.15.31.374008 R61-M0-N3-C:J11-U11 RAS KERNEL INFO iar 003a9254 dear 00f07a88
- 1125083978 2005.08.26 R66-M1-NC-C:J14-U11 2005-08-26-12.19.38.583415 R66-M1-NC-C:J14-U11 RAS KERNEL INFO 3095040 double-hummer alignment exceptions
- 1125084029 2005.08.26 R67-M1-NB-C:J11-U11 2005-08-26-12.20.29.691318 R67-M1-NB-C:J11-U11 RAS KERNEL INFO iar 003a9260 dear 012924f8
- 1125084030 2005.08.26 R67-M1-NB-C:J10-U11 2005-08-26-12.20.30.217174 R67-M1-NB-C:J10-U11 RAS KERNEL INFO iar 003a9260 dear 0128f2f8
- 1125084057 2005.08.26 R67-M1-N3-C:J16-U01 2005-08-26-12.20.57.286694 R67-M1-N3-C:J16-U01 RAS KERNEL INFO iar 003a9260 dear 0128fc88
- 1125084072 2005.08.26 R66-M1-N4-C:J06-U01 2005-08-26-12.21.12.175889 R66-M1-N4-C:J06-U01 RAS KERNEL INFO iar 003a9260 dear 01290878
- 1125084124 2005.08.26 R76-M0-N1-C:J02-U11 2005-08-26-12.22.04.891042 R76-M0-N1-C:J02-U11 RAS KERNEL INFO 773760 double-hummer alignment exceptions
- 1125084127 2005.08.26 R66-M1-ND-C:J15-U01 2005-08-26-12.22.07.689504 R66-M1-ND-C:J15-U01 RAS KERNEL INFO iar 003a929c dear 01291228
- 1125084143 2005.08.26 R67-M1-N8-C:J02-U11 2005-08-26-12.22.23.952539 R67-M1-N8-C:J02-U11 RAS KERNEL INFO iar 003a929c dear 01291708
- 1125084172 2005.08.26 R67-M1-N6-C:J02-U11 2005-08-26-12.22.52.773900 R67-M1-N6-C:J02-U11 RAS KERNEL INFO iar 003a92a0 dear 013611f8
- 1125084224 2005.08.26 R67-M1-N1-C:J03-U11 2005-08-26-12.23.44.724823 R67-M1-N1-C:J03-U11 RAS KERNEL INFO iar 003a92a0 dear 012902f8
- 1125084274 2005.08.26 R77-M0-N2-C:J06-U01 2005-08-26-12.24.34.310422 R77-M0-N2-C:J06-U01 RAS KERNEL INFO iar 003a9260 dear 00ef83b8
- 1125084296 2005.08.26 R67-M0-ND-C:J02-U01 2005-08-26-12.24.56.862775 R67-M0-ND-C:J02-U01 RAS KERNEL INFO iar 003a9254 dear 01299f08
- 1125084314 2005.08.26 R74-M0-ND-C:J09-U11 2005-08-26-12.25.14.255206 R74-M0-ND-C:J09-U11 RAS KERNEL INFO iar 003a9260 dear 00ef6c38
- 1125084379 2005.08.26 R66-M0-N0-C:J16-U01 2005-08-26-12.26.19.940191 R66-M0-N0-C:J16-U01 RAS KERNEL INFO iar 003a9258 dear 0136bd38
- 1125084396 2005.08.26 R64-M0-NB-C:J15-U01 2005-08-26-12.26.36.200859 R64-M0-NB-C:J15-U01 RAS KERNEL INFO 1524480 double-hummer alignment exceptions
- 1125084400 2005.08.26 R65-M0-N3-C:J04-U01 2005-08-26-12.26.40.236812 R65-M0-N3-C:J04-U01 RAS KERNEL INFO 1524480 double-hummer alignment exceptions
- 1125084423 2005.08.26 R60-M0-N6-C:J09-U11 2005-08-26-12.27.03.476538 R60-M0-N6-C:J09-U11 RAS KERNEL INFO 1524480 double-hummer alignment exceptions
- 1125084431 2005.08.26 R66-M1-ND-C:J09-U11 2005-08-26-12.27.11.743183 R66-M1-ND-C:J09-U11 RAS KERNEL INFO iar 003a9260 dear 012979b8
- 1125084437 2005.08.26 R67-M0-NB-C:J12-U11 2005-08-26-12.27.17.234946 R67-M0-NB-C:J12-U11 RAS KERNEL INFO iar 003a9260 dear 0129a9a8
- 1125084446 2005.08.26 R67-M0-N8-C:J06-U11 2005-08-26-12.27.26.648114 R67-M0-N8-C:J06-U11 RAS KERNEL INFO iar 003a9260 dear 01299e48
- 1125084490 2005.08.26 R66-M0-N9-C:J10-U01 2005-08-26-12.28.10.329248 R66-M0-N9-C:J10-U01 RAS KERNEL INFO iar 003a929c dear 0136b2d8
- 1125084517 2005.08.26 R77-M0-N8-C:J09-U01 2005-08-26-12.28.37.922676 R77-M0-N8-C:J09-U01 RAS KERNEL INFO iar 003a9260 dear 00e24bd8
- 1125084553 2005.08.26 R60-M0-NF-C:J10-U11 2005-08-26-12.29.13.250807 R60-M0-NF-C:J10-U11 RAS KERNEL INFO iar 003a9260 dear 00f2a468
- 1125084580 2005.08.26 R67-M1-N1-C:J05-U11 2005-08-26-12.29.40.731213 R67-M1-N1-C:J05-U11 RAS KERNEL INFO iar 003a92a0 dear 0129a488
- 1125084639 2005.08.26 R65-M1-NC-C:J09-U01 2005-08-26-12.30.39.328425 R65-M1-NC-C:J09-U01 RAS KERNEL INFO iar 003a9260 dear 00f29b68
- 1125084695 2005.08.26 R64-M0-N1-C:J02-U01 2005-08-26-12.31.35.708190 R64-M0-N1-C:J02-U01 RAS KERNEL INFO iar 003a926c dear 00f2b078
- 1125087561 2005.08.26 R14-M1-N5-C:J12-U11 2005-08-26-13.19.21.725812 R14-M1-N5-C:J12-U11 RAS KERNEL INFO CE sym 0, at 0x12af93a0, mask 0x08
- 1125114716 2005.08.26 R74-M1-N0-C:J12-U01 2005-08-26-20.51.56.845739 R74-M1-N0-C:J12-U01 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 28, bit 3
- 1125149105 2005.08.27 R36-M0-N5-C:J03-U01 2005-08-27-06.25.05.684576 R36-M0-N5-C:J03-U01 RAS KERNEL INFO CE sym 23, at 0x03f89c00, mask 0x10
- 1125193792 2005.08.27 R11-M1-N7 2005-08-27-18.49.52.912954 R11-M1-N7 NULL DISCOVERY ERROR Node card status: no ALERTs are active. Clock Mode is Low. Clock Select is Midplane. Phy JTAG Reset is asserted. ASIC JTAG Reset is asserted. Temperature Mask is not active. No temperature error. Temperature Limit Error Latch is clear. PGOOD IS NOT ASSERTED. PGOOD ERROR LATCH IS ACTIVE. MPGOOD IS NOT OK. MPGOOD ERROR LATCH IS ACTIVE. The 2.5 volt rail is OK. The 1.5 volt rail is OK.
- 1125209512 2005.08.27 R14-M1-N0-C:J07-U11 2005-08-27-23.11.52.680924 R14-M1-N0-C:J07-U11 RAS KERNEL INFO CE sym 10, at 0x01d7a160, mask 0x10
- 1125219517 2005.08.28 R77-M1-NF-C:J02-U11 2005-08-28-01.58.37.175646 R77-M1-NF-C:J02-U11 RAS KERNEL INFO 156055738 double-hummer alignment exceptions
- 1125219610 2005.08.28 R72-M1-NA-C:J04-U11 2005-08-28-02.00.10.847914 R72-M1-NA-C:J04-U11 RAS KERNEL INFO 147337410 double-hummer alignment exceptions
- 1125220160 2005.08.28 R73-M1-N8-C:J06-U01 2005-08-28-02.09.20.654913 R73-M1-N8-C:J06-U01 RAS KERNEL INFO 139632470 double-hummer alignment exceptions
- 1125220268 2005.08.28 R75-M1-N8-C:J13-U11 2005-08-28-02.11.08.660202 R75-M1-N8-C:J13-U11 RAS KERNEL INFO 155918940 double-hummer alignment exceptions
- 1125220412 2005.08.28 R74-M0-NB-C:J17-U01 2005-08-28-02.13.32.663293 R74-M0-NB-C:J17-U01 RAS KERNEL INFO iar 00149fc0 dear 00739d98
- 1125220525 2005.08.28 R76-M1-ND-C:J11-U11 2005-08-28-02.15.25.781153 R76-M1-ND-C:J11-U11 RAS KERNEL INFO iar 0014a150 dear 0072b9f8
- 1125220578 2005.08.28 R70-M0-NF-C:J06-U01 2005-08-28-02.16.18.871630 R70-M0-NF-C:J06-U01 RAS KERNEL INFO iar 0014a150 dear 00719fc8
- 1125221152 2005.08.28 R41-M0-N2-C:J10-U01 2005-08-28-02.25.52.876786 R41-M0-N2-C:J10-U01 RAS KERNEL INFO 28298332 double-hummer alignment exceptions
- 1125221429 2005.08.28 R55-M0-N2-C:J08-U01 2005-08-28-02.30.29.169747 R55-M0-N2-C:J08-U01 RAS KERNEL INFO 23768676 double-hummer alignment exceptions
- 1125221591 2005.08.28 R44-M0-NF-C:J13-U11 2005-08-28-02.33.11.392223 R44-M0-NF-C:J13-U11 RAS KERNEL INFO 34253552 double-hummer alignment exceptions
- 1125221625 2005.08.28 R44-M0-NE-C:J10-U01 2005-08-28-02.33.45.151094 R44-M0-NE-C:J10-U01 RAS KERNEL INFO 35050330 double-hummer alignment exceptions
- 1125221765 2005.08.28 R40-M0-N1-C:J10-U01 2005-08-28-02.36.05.474124 R40-M0-N1-C:J10-U01 RAS KERNEL INFO 35418742 double-hummer alignment exceptions
- 1125221810 2005.08.28 R40-M1-N6-C:J12-U11 2005-08-28-02.36.50.012310 R40-M1-N6-C:J12-U11 RAS KERNEL INFO 47332148 double-hummer alignment exceptions
- 1125221900 2005.08.28 R50-M1-NB-C:J07-U11 2005-08-28-02.38.20.600044 R50-M1-NB-C:J07-U11 RAS KERNEL INFO 39190074 double-hummer alignment exceptions
- 1125221957 2005.08.28 R57-M1-NF-C:J11-U11 2005-08-28-02.39.17.369798 R57-M1-NF-C:J11-U11 RAS KERNEL INFO 30646810 double-hummer alignment exceptions
- 1125222100 2005.08.28 R47-M0-N5-C:J15-U01 2005-08-28-02.41.40.753192 R47-M0-N5-C:J15-U01 RAS KERNEL INFO 36833864 double-hummer alignment exceptions
- 1125222337 2005.08.28 R45-M0-NA-C:J04-U11 2005-08-28-02.45.37.895456 R45-M0-NA-C:J04-U11 RAS KERNEL INFO 24669022 double-hummer alignment exceptions
- 1125222384 2005.08.28 R50-M1-N1-C:J05-U11 2005-08-28-02.46.24.315745 R50-M1-N1-C:J05-U11 RAS KERNEL INFO 38029258 double-hummer alignment exceptions
- 1125222491 2005.08.28 R42-M1-NE-C:J15-U11 2005-08-28-02.48.11.570833 R42-M1-NE-C:J15-U11 RAS KERNEL INFO 39309752 double-hummer alignment exceptions
- 1125222526 2005.08.28 R54-M0-NB-C:J07-U01 2005-08-28-02.48.46.001360 R54-M0-NB-C:J07-U01 RAS KERNEL INFO 27901852 double-hummer alignment exceptions
- 1125222743 2005.08.28 R40-M1-N4-C:J17-U01 2005-08-28-02.52.23.119441 R40-M1-N4-C:J17-U01 RAS KERNEL INFO 41097026 double-hummer alignment exceptions
- 1125222925 2005.08.28 R52-M0-N4-C:J17-U11 2005-08-28-02.55.25.760111 R52-M0-N4-C:J17-U11 RAS KERNEL INFO 54600944 double-hummer alignment exceptions
- 1125223026 2005.08.28 R54-M0-NC-C:J09-U11 2005-08-28-02.57.06.184496 R54-M0-NC-C:J09-U11 RAS KERNEL INFO 32888202 double-hummer alignment exceptions
- 1125223076 2005.08.28 R41-M1-N3-C:J15-U11 2005-08-28-02.57.56.219310 R41-M1-N3-C:J15-U11 RAS KERNEL INFO iar 0014a150 dear 0098d458
- 1125223213 2005.08.28 R54-M1-ND-C:J08-U11 2005-08-28-03.00.13.685722 R54-M1-ND-C:J08-U11 RAS KERNEL INFO iar 0014a150 dear 0098e178
- 1125223360 2005.08.28 R51-M1-N9-C:J10-U01 2005-08-28-03.02.40.130543 R51-M1-N9-C:J10-U01 RAS KERNEL INFO iar 0014a150 dear 00998588
- 1125223505 2005.08.28 R45-M1-ND-C:J16-U11 2005-08-28-03.05.05.139218 R45-M1-ND-C:J16-U11 RAS KERNEL INFO iar 0014a150 dear 00991c88
- 1125223673 2005.08.28 R40-M0-N1-C:J16-U01 2005-08-28-03.07.53.456194 R40-M0-N1-C:J16-U01 RAS KERNEL INFO iar 00149ee0 dear 009a2788
- 1125223882 2005.08.28 R57-M0-N1-C:J16-U01 2005-08-28-03.11.22.587537 R57-M0-N1-C:J16-U01 RAS KERNEL INFO iar 00149f88 dear 0099be48
- 1125223894 2005.08.28 R56-M1-N9-C:J14-U01 2005-08-28-03.11.34.105667 R56-M1-N9-C:J14-U01 RAS KERNEL INFO iar 0014a150 dear 00a53af8
- 1125223914 2005.08.28 R51-M0-N6-C:J15-U01 2005-08-28-03.11.54.456031 R51-M0-N6-C:J15-U01 RAS KERNEL INFO iar 0014a150 dear 009b9288
- 1125224157 2005.08.28 R52-M0-N6-C:J10-U11 2005-08-28-03.15.57.488722 R52-M0-N6-C:J10-U11 RAS KERNEL INFO iar 0014a150 dear 009a50f8
- 1125224175 2005.08.28 R52-M1-N6-C:J13-U01 2005-08-28-03.16.15.509778 R52-M1-N6-C:J13-U01 RAS KERNEL INFO iar 0014a150 dear 0098cdc8
- 1125224247 2005.08.28 R45-M0-N1-C:J14-U11 2005-08-28-03.17.27.804260 R45-M0-N1-C:J14-U11 RAS KERNEL INFO iar 00149ee0 dear 0099e5e8
- 1125224255 2005.08.28 R43-M1-NE-C:J16-U01 2005-08-28-03.17.35.313713 R43-M1-NE-C:J16-U01 RAS KERNEL INFO iar 0014a150 dear 00991c88
- 1125224269 2005.08.28 R45-M0-ND-C:J09-U01 2005-08-28-03.17.49.107299 R45-M0-ND-C:J09-U01 RAS KERNEL INFO iar 0014a150 dear 009829b8
- 1125224445 2005.08.28 R45-M0-N6-C:J14-U01 2005-08-28-03.20.45.069861 R45-M0-N6-C:J14-U01 RAS KERNEL INFO iar 00149f88 dear 00996f88
- 1125224451 2005.08.28 R44-M1-NB-C:J13-U01 2005-08-28-03.20.51.894497 R44-M1-NB-C:J13-U01 RAS KERNEL INFO iar 0014a1e4 dear 0099e6c8
- 1125224452 2005.08.28 R44-M1-NB-C:J14-U11 2005-08-28-03.20.52.331600 R44-M1-NB-C:J14-U11 RAS KERNEL INFO iar 0014a150 dear 009a2998
- 1125225358 2005.08.28 R54-M0-NC-I:J18-U01 2005-08-28-03.35.58.673640 R54-M0-NC-I:J18-U01 RAS KERNEL INFO NFS Mount failed on bglio716, slept 15 seconds, retrying (1)
- 1125225361 2005.08.28 R05-M0-NC-I:J18-U11 2005-08-28-03.36.01.417658 R05-M0-NC-I:J18-U11 RAS KERNEL INFO NFS Mount failed on bglio91, slept 15 seconds, retrying (1)
- 1125238008 2005.08.28 R30-M0-N7-C:J09-U01 2005-08-28-07.06.48.138974 R30-M0-N7-C:J09-U01 RAS KERNEL INFO CE sym 26, at 0x07a95e80, mask 0x10
- 1125248393 2005.08.28 R00-M0-N4-C:J06-U11 2005-08-28-09.59.53.063681 R00-M0-N4-C:J06-U11 RAS KERNEL INFO 3 ddr errors(s) detected and corrected on rank 0, symbol 18, bit 0
- 1125379628 2005.08.29 R26-M0-NC-I:J18-U01 2005-08-29-22.27.08.904529 R26-M0-NC-I:J18-U01 RAS KERNEL INFO ciod: generated 64 core files for program IMB-MPI1.2MB_perf
- 1125479181 2005.08.31 R00-M1-NB-C:J02-U01 2005-08-31-02.06.21.134545 R00-M1-NB-C:J02-U01 RAS KERNEL INFO CE sym 35, at 0x1317cee0, mask 0x10
KERNMNTF 1125552593 2005.08.31 R42-M1-NC-I:J18-U11 2005-08-31-22.29.53.058583 R42-M1-NC-I:J18-U11 RAS KERNEL FATAL Lustre mount FAILED : bglio559 : point /p/gb1
KERNMNTF 1125552643 2005.08.31 R25-M1-N4-I:J18-U01 2005-08-31-22.30.43.752965 R25-M1-N4-I:J18-U01 RAS KERNEL FATAL Lustre mount FAILED : bglio344 : point /p/gb1
- 1125593559 2005.09.01 R14-M1-N7-C:J10-U01 2005-09-01-09.52.39.403870 R14-M1-N7-C:J10-U01 RAS KERNEL INFO 4 ddr errors(s) detected and corrected on rank 0, symbol 29, bit 4
- 1125668117 2005.09.02 R72-M1-N4-I:J18-U01 2005-09-02-06.35.17.592760 R72-M1-N4-I:J18-U01 RAS APP FATAL ciod: LOGIN chdir(/bglscratch/bwallen/SWL/SYS-CALLS/testcases/kernel/syscalls/truncate) failed: No such file or directory
KERNMNTF 1125690043 2005.09.02 R02-M1-N8-I:J18-U01 2005-09-02-12.40.43.759217 R02-M1-N8-I:J18-U01 RAS KERNEL FATAL Lustre mount FAILED : bglio46 : point /p/gb1
KERNMNTF 1125692642 2005.09.02 R10-M1-N4-I:J18-U01 2005-09-02-13.24.02.308650 R10-M1-N4-I:J18-U01 RAS KERNEL FATAL Lustre mount FAILED : bglio136 : point /p/gb1
KERNMNTF 1125694345 2005.09.02 R04-M0-NC-I:J18-U11 2005-09-02-13.52.25.806989 R04-M0-NC-I:J18-U11 RAS KERNEL FATAL Lustre mount FAILED : bglio75 : point /p/gb1
- 1125701454 2005.09.02 R63-M1-N0 2005-09-02-15.50.54.754016 R63-M1-N0 NULL DISCOVERY INFO Node card VPD check: U11 node in processor card slot J18 do not match. VPD ecid 04CC81389C2FFFFF03071B7048DF, found 04D780871F2FFFFF08031BD046E2
- 1125730913 2005.09.03 R06-M1-N6-C:J15-U01 2005-09-03-00.01.53.125988 R06-M1-N6-C:J15-U01 RAS KERNEL INFO 1 torus receiver x+ input pipe error(s) (dcr 0x02ec) detected and corrected
- 1125749350 2005.09.03 R62-M0-NC-I:J18-U11 2005-09-03-05.09.10.001351 R62-M0-NC-I:J18-U11 RAS APP FATAL ciod: LOGIN chdir(/LAPACK_440d/BLAS) failed: No such file or directory
KERNMNTF 1125759503 2005.09.03 R55-M1-NC-I:J18-U01 2005-09-03-07.58.23.476703 R55-M1-NC-I:J18-U01 RAS KERNEL FATAL Lustre mount FAILED : bglio736 : point /p/gb1
- 1125797779 2005.09.03 R03-M0-N3-C:J02-U11 2005-09-03-18.36.19.663579 R03-M0-N3-C:J02-U11 RAS KERNEL INFO CE sym 33, at 0x00dbd200, mask 0x80
- 1125817576 2005.09.04 R70-M1-NC-C:J09-U11 2005-09-04-00.06.16.089830 R70-M1-NC-C:J09-U11 RAS KERNEL INFO CE sym 26, at 0x146ad540, mask 0x10
KERNMNTF 1125871271 2005.09.04 R24-M1-NC-I:J18-U01 2005-09-04-15.01.11.038078 R24-M1-NC-I:J18-U01 RAS KERNEL FATAL Lustre mount FAILED : bglio336 : point /p/gb1
- 1126180040 2005.09.08 R75-M0-N8-C:J10-U01 2005-09-08-04.47.20.324073 R75-M0-N8-C:J10-U01 RAS KERNEL INFO CE sym 21, at 0x04d64dc0, mask 0x80
KERNMNTF 1126202752 2005.09.08 R01-M1-N4-I:J18-U11 2005-09-08-11.05.52.800796 R01-M1-N4-I:J18-U11 RAS KERNEL FATAL Lustre mount FAILED : bglio23 : point /p/gb1
- 1126211891 2005.09.08 R63-M1-NC-I:J18-U11 2005-09-08-13.38.11.467298 R63-M1-NC-I:J18-U11 RAS KERNEL INFO ciod: pollControlDescriptors: Detected the debugger died.
- 1126211906 2005.09.08 R62-M1-NC-I:J18-U11 2005-09-08-13.38.26.477794 R62-M1-NC-I:J18-U11 RAS KERNEL INFO ciod: In packet from node 91.0 (R62-M1-Nf-C:J03-U11), message code 2 is not 3 or 4294967295 (softheader=003b005b 00030000 00000001 00000000)
- 1126267442 2005.09.09 R13-M0-N2-C:J09-U01 2005-09-09-05.04.02.346866 R13-M0-N2-C:J09-U01 RAS KERNEL INFO 1 torus receiver y+ input pipe error(s) (dcr 0x02ee) detected and corrected
- 1126272208 2005.09.09 R50-M0-NF-C:J10-U01 2005-09-09-06.23.28.274924 R50-M0-NF-C:J10-U01 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 3, bit 6
- 1126275477 2005.09.09 R03-M1-N6-C:J11-U11 2005-09-09-07.17.57.768677 R03-M1-N6-C:J11-U11 RAS KERNEL INFO CE sym 6, at 0x1880a8e0, mask 0x40
- 1126421175 2005.09.10 R16-M0-NC-I:J18-U01 2005-09-10-23.46.15.250215 R16-M0-NC-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/swl-prep/rky-swl/MPI-PERF/IMB/perf_tests/IMB-MPI1.5124KB: invalid or missing program image, No such file or directory
APPSEV 1126434160 2005.09.11 R43-M0-N8-I:J18-U11 2005-09-11-03.22.40.852080 R43-M0-N8-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix after LOAD_MESSAGE on CioStream socket to 172.16.96.116:45713: Link has been severed
- 1126526629 2005.09.12 R20-M1-N8-C:J13-U11 2005-09-12-05.03.49.910676 R20-M1-N8-C:J13-U11 RAS KERNEL INFO suppressing further interrupts of same type
APPSEV 1126539103 2005.09.12 R41-M1-N4-I:J18-U11 2005-09-12-08.31.43.214677 R41-M1-N4-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix after LOAD_MESSAGE on CioStream socket to 172.16.96.116:41304: Link has been severed
APPSEV 1126539106 2005.09.12 R53-M1-N0-I:J18-U01 2005-09-12-08.31.46.757073 R53-M1-N0-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix after LOAD_MESSAGE on CioStream socket to 172.16.96.116:41217: Link has been severed
- 1126587645 2005.09.12 R63-M1-NC-C:J06-U01 2005-09-12-22.00.45.953966 R63-M1-NC-C:J06-U01 RAS KERNEL INFO total of 1 ddr error(s) detected and corrected
KERNMNTF 1126627325 2005.09.13 R30-M0-N4-I:J18-U01 2005-09-13-09.02.05.602211 R30-M0-N4-I:J18-U01 RAS KERNEL FATAL Lustre mount FAILED : bglio388 : point /p/gb1
APPSEV 1126798111 2005.09.15 R72-M1-N0-I:J18-U01 2005-09-15-08.28.31.789429 R72-M1-N0-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix after LOAD_MESSAGE on CioStream socket to 172.16.96.116:37916: Link has been severed
- 1126894078 2005.09.16 R37-M0-NF-C:J07-U01 2005-09-16-11.07.58.039535 R37-M0-NF-C:J07-U01 RAS KERNEL INFO CE sym 25, at 0x1a544020, mask 0x40
- 1126969026 2005.09.17 UNKNOWN_LOCATION 2005-09-17-07.57.06.422022 UNKNOWN_LOCATION NULL DISCOVERY INFO Ido chip status changed: FF:F2:9F:15:7B:E0:00:0D:60:EA:84:1F ip=10.7.0.130 v=13 t=1 status=M Sat Sep 17 07:49:04 PDT 2005
- 1126979315 2005.09.17 R76-M1-N8 2005-09-17-10.48.35.944995 R76-M1-N8 NULL DISCOVERY ERROR Node card status: no ALERTs are active. Clock Mode is Low. Clock Select is Midplane. Phy JTAG Reset is asserted. ASIC JTAG Reset is asserted. Temperature Mask is not active. No temperature error. Temperature Limit Error Latch is clear. PGOOD IS NOT ASSERTED. PGOOD ERROR LATCH IS ACTIVE. MPGOOD IS NOT OK. MPGOOD ERROR LATCH IS ACTIVE. The 2.5 volt rail is OK. The 1.5 volt rail is OK.
- 1127078365 2005.09.18 R71-M1-N8-C:J17-U11 2005-09-18-14.19.25.940731 R71-M1-N8-C:J17-U11 RAS KERNEL INFO CE sym 9, at 0x123b6ca0, mask 0x20
- 1127147534 2005.09.19 R01-M0-NC-I:J18-U01 2005-09-19-09.32.14.136187 R01-M0-NC-I:J18-U01 RAS APP FATAL ciod: LOGIN chdir(/home/spelce1/UMT2K/umt2k/ckpt_umt2k_src/TEST/NEW_TEST) failed: No such file or directory
- 1127147540 2005.09.19 R77-M0-NC-I:J18-U11 2005-09-19-09.32.20.314194 R77-M0-NC-I:J18-U11 RAS APP FATAL ciod: LOGIN chdir(/home/spelce1/UMT2K/umt2k/ckpt_umt2k_src/TEST/NEW_TEST) failed: No such file or directory
- 1127154489 2005.09.19 R47-M0-N4-C:J07-U01 2005-09-19-11.28.09.693945 R47-M0-N4-C:J07-U01 RAS KERNEL INFO CE sym 8, at 0x0a2ae600, mask 0x10
- 1127163833 2005.09.19 R32-M1-N0-I:J18-U01 2005-09-19-14.03.53.806037 R32-M1-N0-I:J18-U01 RAS APP FATAL ciod: LOGIN chdir(/home/yates/SWL_tests/BGL64k_SWL_tests_develop/MPI-VAL/MPITs_v050902/rundir) failed: No such file or directory
- 1127242650 2005.09.20 R05-M0-NA 2005-09-20-11.57.30.636832 R05-M0-NA NULL DISCOVERY INFO Node card VPD check: U01 node in processor card slot J15 do not match. VPD ecid 04DE7DB80D7BFFFF04051B70D8D9, found 04DE7DF2D37BFFFF09081B6088D9
- 1127243163 2005.09.20 NULL 2005-09-20-12.06.03.683009 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243213 2005.09.20 NULL 2005-09-20-12.06.53.608772 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243214 2005.09.20 NULL 2005-09-20-12.06.54.769367 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243216 2005.09.20 NULL 2005-09-20-12.06.56.253999 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243219 2005.09.20 NULL 2005-09-20-12.06.59.554854 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243219 2005.09.20 NULL 2005-09-20-12.06.59.601269 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243221 2005.09.20 NULL 2005-09-20-12.07.01.621806 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243228 2005.09.20 NULL 2005-09-20-12.07.08.329482 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243349 2005.09.20 NULL 2005-09-20-12.09.09.807842 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243352 2005.09.20 NULL 2005-09-20-12.09.12.363370 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243360 2005.09.20 NULL 2005-09-20-12.09.20.750312 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243361 2005.09.20 NULL 2005-09-20-12.09.21.408989 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243367 2005.09.20 NULL 2005-09-20-12.09.27.065172 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243377 2005.09.20 NULL 2005-09-20-12.09.37.578989 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243379 2005.09.20 NULL 2005-09-20-12.09.39.030884 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243411 2005.09.20 NULL 2005-09-20-12.10.11.325034 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243412 2005.09.20 NULL 2005-09-20-12.10.12.752571 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243509 2005.09.20 NULL 2005-09-20-12.11.49.638167 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243510 2005.09.20 NULL 2005-09-20-12.11.50.889778 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243514 2005.09.20 NULL 2005-09-20-12.11.54.288690 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243528 2005.09.20 NULL 2005-09-20-12.12.08.547906 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243531 2005.09.20 NULL 2005-09-20-12.12.11.388103 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243536 2005.09.20 NULL 2005-09-20-12.12.16.117396 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127243560 2005.09.20 NULL 2005-09-20-12.12.40.943077 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127244289 2005.09.20 R05-M0-N6 2005-09-20-12.24.49.324668 R05-M0-N6 NULL DISCOVERY INFO Node card VPD check: U11 node in processor card slot J17 do not match. VPD ecid 04D97DB7937BFFFF05071B7054DA, found 04D081A3892FFFFF0D0B1C505AF3
- 1127245515 2005.09.20 R26-M0-N7 2005-09-20-12.45.15.754325 R26-M0-N7 NULL DISCOVERY INFO Node card VPD check: U01 node in processor card slot J08 do not match. VPD ecid 075F04E8A27BFFFF07021C3096ED, found 04D78137922FFFFF040E1C3052ED
- 1127248868 2005.09.20 NULL 2005-09-20-13.41.08.107703 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127248870 2005.09.20 NULL 2005-09-20-13.41.10.218619 NULL RAS MMCS ERROR idoproxydb hit ASSERT condition: ASSERT expression=0 Source file=idotransportmgr.cpp Source line=1043 Function=int IdoTransportMgr::SendPacket(IdoUdpMgr*, BglCtlPavTrace*)
- 1127264122 2005.09.20 R27-M0-N8-I:J18-U11 2005-09-20-17.55.22.058829 R27-M0-N8-I:J18-U11 RAS APP FATAL ciod: Error loading /home/yates//bandwidth.rts: invalid or missing program image, No such file or directory
- 1127264333 2005.09.20 R45-M0-N0-I:J18-U01 2005-09-20-17.58.53.385609 R45-M0-N0-I:J18-U01 RAS APP FATAL ciod: Error loading /home/yates//torus-latency.rts: invalid or missing program image, No such file or directory
- 1127264347 2005.09.20 R72-M0-N8-I:J18-U01 2005-09-20-17.59.07.131519 R72-M0-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /home/yates//torus-latency.rts: invalid or missing program image, No such file or directory
- 1127264397 2005.09.20 R34-M1-N8-I:J18-U01 2005-09-20-17.59.57.105133 R34-M1-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /home/yates//torus-latency.rts: invalid or missing program image, No such file or directory
- 1127264475 2005.09.20 R44-M1-NC-I:J18-U01 2005-09-20-18.01.15.836848 R44-M1-NC-I:J18-U01 RAS APP FATAL ciod: Error loading /home/yates//broadcast.rts: invalid or missing program image, No such file or directory
- 1127264516 2005.09.20 R05-M0-N1-C:J06-U11 2005-09-20-18.01.56.398119 R05-M0-N1-C:J06-U11 RAS KERNEL FATAL rts: bad message header: expecting type 57 instead of type 3 (softheader=00131db8 81aa0003 00000002 00000000) PSR0=00001f01 PSR1=00000000 PRXF=00000002 PIXF=00000007
- 1127264866 2005.09.20 R73-M1-NC-I:J18-U11 2005-09-20-18.07.46.603480 R73-M1-NC-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/SWL/performance/MINIBEN//bandwidth.rts: invalid or missing program image, No such file or directory
- 1127265034 2005.09.20 R26-M1-N8-I:J18-U01 2005-09-20-18.10.34.811726 R26-M1-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/SWL/performance/MINIBEN//torus-latency.rts: invalid or missing program image, No such file or directory
- 1127265314 2005.09.20 R27-M1-N4-I:J18-U01 2005-09-20-18.15.14.037829 R27-M1-N4-I:J18-U01 RAS APP FATAL ciod: LOGIN chdir(/bgl/apps/SWL/performance/MINIBEN///2005.09.20-18.06.05) failed: No such file or directory
- 1127265535 2005.09.20 R23-M1-N8-I:J18-U11 2005-09-20-18.18.55.570301 R23-M1-N8-I:J18-U11 RAS APP FATAL ciod: LOGIN chdir(/bgl/apps/SWL/performance/MINIBEN///2005.09.20-18.06.05) failed: No such file or directory
- 1127265666 2005.09.20 R54-M1-N4-I:J18-U11 2005-09-20-18.21.06.263282 R54-M1-N4-I:J18-U11 RAS APP FATAL ciod: LOGIN chdir(/bgl/apps/SWL/performance/MINIBEN///2005.09.20-18.06.01) failed: No such file or directory
- 1127265731 2005.09.20 R16-M1-N4-I:J18-U11 2005-09-20-18.22.11.721083 R16-M1-N4-I:J18-U11 RAS APP FATAL ciod: LOGIN chdir(/bgl/apps/SWL/performance/MINIBEN///2005.09.20-18.06.08) failed: No such file or directory
- 1127265777 2005.09.20 R62-M1-N8-I:J18-U01 2005-09-20-18.22.57.490109 R62-M1-N8-I:J18-U01 RAS APP FATAL ciod: LOGIN chdir(/bgl/apps/SWL/performance/MINIBEN///2005.09.20-18.06.01) failed: No such file or directory
- 1127356175 2005.09.21 R23-M0-N5-C:J12-U01 2005-09-21-19.29.35.484182 R23-M0-N5-C:J12-U01 RAS KERNEL INFO CE sym 29, at 0x14123c20, mask 0x08
- 1127380615 2005.09.22 R63-M1-NA-C:J14-U11 2005-09-22-02.16.55.345135 R63-M1-NA-C:J14-U11 RAS KERNEL INFO total of 1 ddr error(s) detected and corrected
- 1127456570 2005.09.22 R57-M0-NB-C:J07-U11 2005-09-22-23.22.50.037439 R57-M0-NB-C:J07-U11 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 35, bit 1
- 1127464994 2005.09.23 R35-M1-NE-C:J09-U01 2005-09-23-01.43.14.910997 R35-M1-NE-C:J09-U01 RAS KERNEL INFO CE sym 26, at 0x022c3fc0, mask 0x40
- 1127464998 2005.09.23 R10-M0-N0-C:J06-U01 2005-09-23-01.43.18.801088 R10-M0-N0-C:J06-U01 RAS KERNEL INFO total of 4 ddr error(s) detected and corrected
- 1127515927 2005.09.23 R07-M0-ND-C:J16-U01 2005-09-23-15.52.07.594229 R07-M0-ND-C:J16-U01 RAS KERNEL INFO CE sym 27, at 0x166a9f20, mask 0x02
- 1127592240 2005.09.24 R06-M1-N4-C:J08-U01 2005-09-24-13.04.00.023717 R06-M1-N4-C:J08-U01 RAS KERNEL INFO CE sym 21, at 0x13d38aa0, mask 0x40
- 1127668543 2005.09.25 R66-M1-N1-C:J09-U11 2005-09-25-10.15.43.266674 R66-M1-N1-C:J09-U11 RAS KERNEL INFO CE sym 15, at 0x0bde77e0, mask 0x04
- 1127756818 2005.09.26 R64-M0-NB-C:J14-U01 2005-09-26-10.46.58.701612 R64-M0-NB-C:J14-U01 RAS KERNEL INFO CE sym 16, at 0x0149a040, mask 0x80
APPSEV 1127780591 2005.09.26 R27-M0-NC-I:J18-U11 2005-09-26-17.23.11.155275 R27-M0-NC-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix after LOAD_MESSAGE on CioStream socket to 172.16.96.116:34903: Link has been severed
- 1127781971 2005.09.26 R34-M0-N8-I:J18-U11 2005-09-26-17.46.11.092617 R34-M0-N8-I:J18-U11 RAS KERNEL INFO ciod: generated 128 core files for program /bgl/apps/SWL/stability/DDCMD//ddcMDbglV
- 1127871900 2005.09.27 R01-M0-N8-C:J02-U11 2005-09-27-18.45.00.453947 R01-M0-N8-C:J02-U11 RAS KERNEL INFO CE sym 2, at 0x0fa3d060, mask 0x08
- 1127960557 2005.09.28 R12-M1-N4-I:J18-U01 2005-09-28-19.22.37.311035 R12-M1-N4-I:J18-U01 RAS KERNEL INFO ciod: generated 64 core files for program /home/spelce1/HPCC_IBM/TomAndJeff/bin/tested_copro/tested_copro.rts
- 1128027755 2005.09.29 R03-M1-ND-C:J14-U01 2005-09-29-14.02.35.872055 R03-M1-ND-C:J14-U01 RAS KERNEL INFO critical input interrupt (unit=0x0b bit=0x06): warning for torus y+ wire
- 1128034565 2005.09.29 R20-M1-NB-C:J16-U11 2005-09-29-15.56.05.077911 R20-M1-NB-C:J16-U11 RAS KERNEL INFO CE sym 5, at 0x0e37bbe0, mask 0x40
- 1128074784 2005.09.30 R07-M0-N0-C:J09-U01 2005-09-30-03.06.24.049908 R07-M0-N0-C:J09-U01 RAS KERNEL INFO CE sym 4, at 0x18f28be0, mask 0x40
- 1128090751 2005.09.30 R63-M1-NC-C:J06-U01 2005-09-30-07.32.31.742095 R63-M1-NC-C:J06-U01 RAS KERNEL INFO CE sym 13, at 0x0397e0e0, mask 0x08
- 1128114748 2005.09.30 R11-M1-NE-C:J17-U01 2005-09-30-14.12.28.114880 R11-M1-NE-C:J17-U01 RAS KERNEL INFO CE sym 9, at 0x11b9cc60, mask 0x04
- 1128170317 2005.10.01 R73-M0-N4-I:J18-U11 2005-10-01-05.38.37.246285 R73-M0-N4-I:J18-U11 RAS APP FATAL ciod: Error loading /home/spelce1/HPCC_IBM/Urgent/Gunnels/VNM64/vnm.rts: invalid or missing program image, Permission denied
- 1128375498 2005.10.03 R23-M0-N8-C:J15-U11 2005-10-03-14.38.18.573730 R23-M0-N8-C:J15-U11 RAS KERNEL INFO CE sym 6, at 0x06ec7a20, mask 0x20
- 1128382008 2005.10.03 R63-M0-N4-I:J18-U01 2005-10-03-16.26.48.791418 R63-M0-N4-I:J18-U01 RAS KERNEL INFO ciod: generated 108 core files for program /home/spelce1/HPCC_IBM/Urgent/VNM/32K/vnm.rts
- 1128457224 2005.10.04 R53-M1-ND-C:J09-U11 2005-10-04-13.20.24.392522 R53-M1-ND-C:J09-U11 RAS KERNEL INFO CE sym 9, at 0x11f82720, mask 0x02
- 1128458085 2005.10.04 R23-M0-N8-C:J15-U11 2005-10-04-13.34.45.062257 R23-M0-N8-C:J15-U11 RAS KERNEL INFO CE sym 6, at 0x0148b640, mask 0x10
- 1128510395 2005.10.05 R62-M1-N8-I:J18-U01 2005-10-05-04.06.35.463665 R62-M1-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /home/spelce1/HPCC_IBM/Urgent/VNM/64K/vnm.rts: invalid or missing program image, Permission denied
APPSEV 1128597888 2005.10.06 R56-M0-N0-I:J18-U01 2005-10-06-04.24.48.021229 R56-M0-N0-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix after LOAD_MESSAGE on CioStream socket to 172.16.96.116:53591: Link has been severed
- 1128719294 2005.10.07 R60-M1-N5-C:J09-U11 2005-10-07-14.08.14.709280 R60-M1-N5-C:J09-U11 RAS KERNEL INFO total of 2 ddr error(s) detected and corrected
- 1128832666 2005.10.08 R42-M1-ND-C:J07-U11 2005-10-08-21.37.46.422377 R42-M1-ND-C:J07-U11 RAS KERNEL INFO CE sym 20, at 0x0a508740, mask 0x02
- 1128832724 2005.10.08 R03-M1-N9-C:J09-U11 2005-10-08-21.38.44.248283 R03-M1-N9-C:J09-U11 RAS KERNEL INFO CE sym 25, at 0x06c27c60, mask 0x40
- 1128832742 2005.10.08 R53-M1-NB-C:J13-U01 2005-10-08-21.39.02.618861 R53-M1-NB-C:J13-U01 RAS KERNEL INFO total of 12 ddr error(s) detected and corrected
- 1128844166 2005.10.09 R43-M0-N9-C:J08-U11 2005-10-09-00.49.26.875181 R43-M0-N9-C:J08-U11 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 28, bit 0
APPSEV 1128970806 2005.10.10 R05-M0-N8-I:J18-U11 2005-10-10-12.00.06.181123 R05-M0-N8-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix after LOAD_MESSAGE on CioStream socket to 172.16.96.116:57673: Link has been severed
- 1129158022 2005.10.12 R23-M0-N8-C:J15-U11 2005-10-12-16.00.22.364012 R23-M0-N8-C:J15-U11 RAS KERNEL INFO CE sym 5, at 0x13253280, mask 0x04
- 1129166647 2005.10.12 R34-M1-N8-I:J18-U01 2005-10-12-18.24.07.100662 R34-M1-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /home/spelce1/HPCC_IBM/Urgent/COP/64K/vnm.rts: invalid or missing program image, Permission denied
- 1129166670 2005.10.12 R73-M1-NC-I:J18-U01 2005-10-12-18.24.30.314474 R73-M1-NC-I:J18-U01 RAS APP FATAL ciod: Error loading /home/spelce1/HPCC_IBM/Urgent/COP/64K/vnm.rts: invalid or missing program image, Permission denied
- 1129166708 2005.10.12 R17-M1-N0-I:J18-U11 2005-10-12-18.25.08.543706 R17-M1-N0-I:J18-U11 RAS APP FATAL ciod: Error loading /home/spelce1/HPCC_IBM/Urgent/COP/64K/vnm.rts: invalid or missing program image, Permission denied
- 1129204934 2005.10.13 R62-M1-N4-I:J18-U11 2005-10-13-05.02.14.653341 R62-M1-N4-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/followup/SPASM/spasm.254: invalid or missing program image, Permission denied
- 1129204989 2005.10.13 R44-M0-N4-I:J18-U01 2005-10-13-05.03.09.377689 R44-M0-N4-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/followup/SPASM/spasm.254: invalid or missing program image, Permission denied
- 1129208349 2005.10.13 R73-M0-NC-I:J18-U11 2005-10-13-05.59.09.874956 R73-M0-NC-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/followup/MINIBEN/MB_254_051007/torus-latency.rts: invalid or missing program image, No such file or directory
- 1129270727 2005.10.13 R30-M0-N5-C:J05-U01 2005-10-13-23.18.47.295678 R30-M0-N5-C:J05-U01 RAS KERNEL INFO CE sym 14, at 0x17086be0, mask 0x02
- 1129271439 2005.10.13 R06-M1-NA-C:J08-U11 2005-10-13-23.30.39.273604 R06-M1-NA-C:J08-U11 RAS KERNEL INFO suppressing further interrupts of same type
- 1129273479 2005.10.14 R07-M1-NA-C:J09-U01 2005-10-14-00.04.39.329001 R07-M1-NA-C:J09-U01 RAS KERNEL INFO 4 ddr errors(s) detected and corrected on rank 0, symbol 13, bit 5
- 1129273482 2005.10.14 R53-M1-N0-C:J04-U11 2005-10-14-00.04.42.389631 R53-M1-N0-C:J04-U11 RAS KERNEL INFO 4 tree receiver 2 in re-synch state event(s) (dcr 0x019a) detected
- 1129352994 2005.10.14 R57-M1-NE-C:J12-U01 2005-10-14-22.09.54.782506 R57-M1-NE-C:J12-U01 RAS KERNEL INFO CE sym 23, at 0x1793c560, mask 0x01
- 1129353114 2005.10.14 R13-M1-N3-C:J05-U11 2005-10-14-22.11.54.142836 R13-M1-N3-C:J05-U11 RAS KERNEL INFO CE sym 14, at 0x08d4e740, mask 0x01
- 1129437983 2005.10.15 R25-M1-N0-I:J18-U01 2005-10-15-21.46.23.436761 R25-M1-N0-I:J18-U01 RAS KERNEL INFO ciod: generated 64 core files for program /home/spelce1/HPCC_IBM/Urgent/COP/64K/opt_co_dc.rts
- 1129456912 2005.10.16 R24-M0-N4-I:J18-U01 2005-10-16-03.01.52.266920 R24-M0-N4-I:J18-U01 RAS KERNEL INFO ciod: generated 58 core files for program /home/spelce1/HPCC_IBM/Urgent/COP/64K/opt_co_dc.rts
APPSEV 1129501476 2005.10.16 R71-M1-N0-I:J18-U01 2005-10-16-15.24.36.889002 R71-M1-N0-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix after LOAD_MESSAGE on CioStream socket to 172.16.96.116:47696: Link has been severed
- 1129550547 2005.10.17 R02-M0-N4-C:J04-U11 2005-10-17-05.02.27.161242 R02-M0-N4-C:J04-U11 RAS KERNEL INFO data cache search parity error detected. attempting to correct
- 1129550629 2005.10.17 R02-M0-N4-C:J04-U11 2005-10-17-05.03.49.800918 R02-M0-N4-C:J04-U11 RAS KERNEL INFO data cache search parity error detected. attempting to correct
- 1129551549 2005.10.17 R02-M0-N4-C:J04-U11 2005-10-17-05.19.09.715915 R02-M0-N4-C:J04-U11 RAS KERNEL INFO data cache search parity error detected. attempting to correct
- 1129551731 2005.10.17 R02-M0-N4-C:J04-U11 2005-10-17-05.22.11.645917 R02-M0-N4-C:J04-U11 RAS KERNEL INFO data cache search parity error detected. attempting to correct
- 1129552305 2005.10.17 R02-M0-N4-C:J04-U11 2005-10-17-05.31.45.435674 R02-M0-N4-C:J04-U11 RAS KERNEL INFO data cache search parity error detected. attempting to correct
- 1129552512 2005.10.17 R02-M0-N4-C:J04-U11 2005-10-17-05.35.12.825443 R02-M0-N4-C:J04-U11 RAS KERNEL INFO data cache search parity error detected. attempting to correct
- 1129724002 2005.10.19 R21-M1-N4-C:J10-U01 2005-10-19-05.13.22.183386 R21-M1-N4-C:J10-U01 RAS KERNEL INFO CE sym 12, at 0x18e33e80, mask 0x04
- 1129724004 2005.10.19 R13-M1-N8-C:J02-U11 2005-10-19-05.13.24.054893 R13-M1-N8-C:J02-U11 RAS KERNEL INFO CE sym 13, at 0x00f254e0, mask 0x20
- 1129734520 2005.10.19 R17-M0-N0-I:J18-U01 2005-10-19-08.08.40.058960 R17-M0-N0-I:J18-U01 RAS KERNEL INFO shutdown complete
- 1129820589 2005.10.20 R62-M0-N8-I:J18-U01 2005-10-20-08.03.09.612112 R62-M0-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/followup/RAPTOR/new.raptor.trace: invalid or missing program image, Exec format error
- 1129820611 2005.10.20 R01-M1-NC-I:J18-U01 2005-10-20-08.03.31.975086 R01-M1-NC-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/followup/RAPTOR/new.raptor.trace: invalid or missing program image, Exec format error
- 1129864798 2005.10.20 R04-M0-NC-I:J18-U11 2005-10-20-20.19.58.037602 R04-M0-NC-I:J18-U11 RAS KERNEL INFO ciod: generated 64 core files for program /home/rfisher/sodscaling/flash2
- 1129897177 2005.10.21 R07-M1-N0-C:J17-U01 2005-10-21-05.19.37.093437 R07-M1-N0-C:J17-U01 RAS KERNEL INFO total of 1 ddr error(s) detected and corrected
- 1130123101 2005.10.23 R21-M1-N0-C:J10-U11 2005-10-23-20.05.01.813190 R21-M1-N0-C:J10-U11 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 17, bit 7
- 1130251536 2005.10.25 R46-M0-NC-I:J18-U11 2005-10-25-07.45.36.124897 R46-M0-NC-I:J18-U11 RAS APP FATAL ciod: Error loading allreduce_int_V1R1.rts: invalid or missing program image, No such file or directory
- 1130255740 2005.10.25 R11-M1-N2-C:J09-U11 2005-10-25-08.55.40.146416 R11-M1-N2-C:J09-U11 RAS KERNEL INFO critical input interrupt (unit=0x0b bit=0x0b): warning for torus z- wire
- 1130479310 2005.10.27 R55-M0-N9-C:J06-U11 2005-10-27-23.01.50.720520 R55-M0-N9-C:J06-U11 RAS KERNEL INFO 16 tree receiver 2 in re-synch state event(s) (dcr 0x019a) detected
KERNTERM 1130529580 2005.10.28 R63-M1-NF-C:J15-U11 2005-10-28-12.59.40.203147 R63-M1-NF-C:J15-U11 RAS KERNEL FATAL rts: kernel terminated for reason 1004
KERNREC 1130529611 2005.10.28 R63-M1-N5-C:J17-U01 2005-10-28-13.00.11.719498 R63-M1-N5-C:J17-U01 RAS KERNEL FATAL Error receiving packet on tree network, expecting type 57 instead of type 3 (softheader=0064588e 8aff0003 00000002 00000000) PSR0=00001f01 PSR1=00000000 PRXF=00000002 PIXF=00000007
- 1130645609 2005.10.29 R06-M0-N9-C:J02-U01 2005-10-29-21.13.29.183062 R06-M0-N9-C:J02-U01 RAS KERNEL INFO CE sym 29, at 0x1b719940, mask 0x02
- 1130675804 2005.10.30 R65-M1-NA-C:J14-U01 2005-10-30-04.36.44.005858 R65-M1-NA-C:J14-U01 RAS KERNEL INFO CE sym 27, at 0x1b70b860, mask 0x80
- 1130688037 2005.10.30 R26-M0-N4-I:J18-U11 2005-10-30-08.00.37.655705 R26-M0-N4-I:J18-U11 RAS APP FATAL ciod: Error loading /home/spelce1/HPCC_IBM/Urgent/COP/64K/hpcc-1.0.0.102905_opt_essl_cpm: invalid or missing program image, Permission denied
- 1130766712 2005.10.31 R10-M0-N3-C:J07-U01 2005-10-31-05.51.52.778656 R10-M0-N3-C:J07-U01 RAS KERNEL INFO CE sym 16, at 0x18fa5d40, mask 0x80
- 1130770293 2005.10.31 R74-M1-NC-C:J15-U11 2005-10-31-06.51.33.260350 R74-M1-NC-C:J15-U11 RAS KERNEL INFO 4 ddr errors(s) detected and corrected on rank 0, symbol 24, bit 1
- 1130840276 2005.11.01 R20-M1-NF-C:J10-U01 2005-11-01-02.17.56.853469 R20-M1-NF-C:J10-U01 RAS KERNEL INFO instruction cache parity error corrected
- 1131019038 2005.11.03 R27-M1-N8-C:J16-U11 2005-11-03-03.57.18.815377 R27-M1-N8-C:J16-U11 RAS KERNEL INFO CE sym 30, at 0x1bd2b700, mask 0x04
- 1131030168 2005.11.03 R54-M1-N8-I:J18-U01 2005-11-03-07.02.48.152436 R54-M1-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /g/g0/spelce1/HPCC_IBM/Urgent/COP/64K/RandomAccess.64R.rts: invalid or missing program image, Permission denied
- 1131036240 2005.11.03 R06-M0-NC-I:J18-U11 2005-11-03-08.44.00.733074 R06-M0-NC-I:J18-U11 RAS APP FATAL ciod: Error loading /home.old/glosli/src/ddcMD/ddcMD1.1.17/bin/ddcMDGbglV: invalid or missing program image, No such file or directory
- 1131036466 2005.11.03 R15-M0-NC-I:J18-U11 2005-11-03-08.47.46.669663 R15-M0-NC-I:J18-U11 RAS APP FATAL ciod: Error loading /home.old/glosli/src/ddcMD/ddcMD1.1.18a/bin/ddcMDGbglV: invalid or missing program image, No such file or directory
- 1131057830 2005.11.03 R60-M0-NE-C:J08-U11 2005-11-03-14.43.50.771087 R60-M0-NE-C:J08-U11 RAS KERNEL INFO iar 00106210 dear 0244c1dc
- 1131057844 2005.11.03 R61-M1-NB-C:J17-U11 2005-11-03-14.44.04.612936 R61-M1-NB-C:J17-U11 RAS KERNEL INFO 640404 floating point alignment exceptions
- 1131057860 2005.11.03 R61-M0-N3-C:J12-U11 2005-11-03-14.44.20.274649 R61-M0-N3-C:J12-U11 RAS KERNEL INFO 640404 floating point alignment exceptions
- 1131057877 2005.11.03 R60-M1-NC-C:J10-U11 2005-11-03-14.44.37.862831 R60-M1-NC-C:J10-U11 RAS KERNEL INFO iar 00106210 dear 0244c1dc
- 1131057899 2005.11.03 R61-M0-NC-C:J03-U11 2005-11-03-14.44.59.509719 R61-M0-NC-C:J03-U11 RAS KERNEL INFO iar 00106210 dear 0244c20c
- 1131057913 2005.11.03 R65-M1-NB-C:J14-U01 2005-11-03-14.45.13.427392 R65-M1-NB-C:J14-U01 RAS KERNEL INFO iar 00106228 dear 0244c1ec
- 1131057950 2005.11.03 R64-M0-N6-C:J17-U11 2005-11-03-14.45.50.094903 R64-M0-N6-C:J17-U11 RAS KERNEL INFO iar 00106210 dear 0244c20c
- 1131057996 2005.11.03 R61-M1-N5-C:J13-U11 2005-11-03-14.46.36.274147 R61-M1-N5-C:J13-U11 RAS KERNEL INFO iar 00106210 dear 0244c20c
- 1131058041 2005.11.03 R65-M1-N6-C:J11-U01 2005-11-03-14.47.21.714965 R65-M1-N6-C:J11-U01 RAS KERNEL INFO iar 00106228 dear 0244c1ec
- 1131058087 2005.11.03 R60-M1-N3-C:J11-U01 2005-11-03-14.48.07.486751 R60-M1-N3-C:J11-U01 RAS KERNEL INFO iar 00106228 dear 0244c1ec
- 1131058115 2005.11.03 R66-M0-NF-C:J12-U01 2005-11-03-14.48.35.040947 R66-M0-NF-C:J12-U01 RAS KERNEL INFO 640404 floating point alignment exceptions
- 1131058125 2005.11.03 R61-M0-N1-C:J05-U11 2005-11-03-14.48.45.760239 R61-M0-N1-C:J05-U11 RAS KERNEL INFO iar 00106228 dear 0244c21c
- 1131058168 2005.11.03 R64-M1-N5-C:J10-U11 2005-11-03-14.49.28.162764 R64-M1-N5-C:J10-U11 RAS KERNEL INFO iar 001061dc dear 0244c22c
- 1131058170 2005.11.03 R64-M1-N9-C:J15-U11 2005-11-03-14.49.30.411659 R64-M1-N9-C:J15-U11 RAS KERNEL INFO iar 001061dc dear 0244c22c
- 1131058219 2005.11.03 R60-M0-N1-C:J14-U01 2005-11-03-14.50.19.824448 R60-M0-N1-C:J14-U01 RAS KERNEL INFO iar 001061dc dear 0244c25c
- 1131058229 2005.11.03 R61-M0-N9-C:J04-U01 2005-11-03-14.50.29.243970 R61-M0-N9-C:J04-U01 RAS KERNEL INFO iar 001061dc dear 0244c22c
- 1131058249 2005.11.03 R66-M1-N8-C:J12-U01 2005-11-03-14.50.49.805068 R66-M1-N8-C:J12-U01 RAS KERNEL INFO iar 00106228 dear 0244c1ec
- 1131058316 2005.11.03 R65-M0-N1-C:J04-U01 2005-11-03-14.51.56.723449 R65-M0-N1-C:J04-U01 RAS KERNEL INFO iar 00106210 dear 0244c20c
- 1131058319 2005.11.03 R67-M0-ND-C:J03-U01 2005-11-03-14.51.59.126492 R67-M0-ND-C:J03-U01 RAS KERNEL INFO iar 00106228 dear 0244c1ec
- 1131058388 2005.11.03 R66-M0-N5-C:J04-U11 2005-11-03-14.53.08.510509 R66-M0-N5-C:J04-U11 RAS KERNEL INFO iar 001061dc dear 0244c22c
- 1131058399 2005.11.03 R64-M1-N7-C:J16-U11 2005-11-03-14.53.19.794119 R64-M1-N7-C:J16-U11 RAS KERNEL INFO iar 00105ea0 dear 0244c28c
- 1131058409 2005.11.03 R60-M1-NF-C:J12-U11 2005-11-03-14.53.29.339977 R60-M1-NF-C:J12-U11 RAS KERNEL INFO iar 00105e94 dear 02f5883c
- 1131058417 2005.11.03 R66-M0-N8-C:J11-U01 2005-11-03-14.53.37.642554 R66-M0-N8-C:J11-U01 RAS KERNEL INFO iar 001061dc dear 0244c22c
- 1131058477 2005.11.03 R65-M0-N4-C:J11-U11 2005-11-03-14.54.37.592074 R65-M0-N4-C:J11-U11 RAS KERNEL INFO iar 001061dc dear 0244c28c
- 1131059142 2005.11.03 R66-M1-N3-C:J06-U11 2005-11-03-15.05.42.754713 R66-M1-N3-C:J06-U11 RAS KERNEL INFO 640764 floating point alignment exceptions
- 1131059158 2005.11.03 R67-M0-N8-C:J02-U01 2005-11-03-15.05.58.736742 R67-M0-N8-C:J02-U01 RAS KERNEL INFO 640404 floating point alignment exceptions
- 1131059209 2005.11.03 R66-M0-N5-C:J12-U11 2005-11-03-15.06.49.830610 R66-M0-N5-C:J12-U11 RAS KERNEL INFO 640404 floating point alignment exceptions
- 1131059244 2005.11.03 R67-M0-NC-C:J16-U01 2005-11-03-15.07.24.046955 R67-M0-NC-C:J16-U01 RAS KERNEL INFO iar 00106210 dear 0244c20c
- 1131059251 2005.11.03 R66-M0-NC-C:J13-U01 2005-11-03-15.07.31.811477 R66-M0-NC-C:J13-U01 RAS KERNEL INFO iar 00106210 dear 0244c1dc
- 1131059283 2005.11.03 R67-M1-ND-C:J10-U11 2005-11-03-15.08.03.331945 R67-M1-ND-C:J10-U11 RAS KERNEL INFO iar 00106228 dear 0244c21c
- 1131059308 2005.11.03 R66-M1-N5-C:J14-U11 2005-11-03-15.08.28.067991 R66-M1-N5-C:J14-U11 RAS KERNEL INFO iar 00106210 dear 0244c1dc
- 1131059401 2005.11.03 R66-M0-N0-C:J07-U01 2005-11-03-15.10.01.251362 R66-M0-N0-C:J07-U01 RAS KERNEL INFO iar 001061dc dear 0244c25c
- 1131059444 2005.11.03 R66-M1-N9-C:J03-U11 2005-11-03-15.10.44.315723 R66-M1-N9-C:J03-U11 RAS KERNEL INFO iar 001061dc dear 0244c22c
- 1131059530 2005.11.03 R66-M1-N0-C:J17-U01 2005-11-03-15.12.10.253142 R66-M1-N0-C:J17-U01 RAS KERNEL INFO iar 00106228 dear 0244c21c
- 1131059594 2005.11.03 R66-M0-N4-C:J02-U01 2005-11-03-15.13.14.342761 R66-M0-N4-C:J02-U01 RAS KERNEL INFO iar 001061dc dear 0244c28c
- 1131059607 2005.11.03 R67-M0-N5-C:J04-U11 2005-11-03-15.13.27.784212 R67-M0-N5-C:J04-U11 RAS KERNEL INFO iar 001061dc dear 0244c28c
- 1131059636 2005.11.03 R66-M1-ND-C:J14-U11 2005-11-03-15.13.56.350902 R66-M1-ND-C:J14-U11 RAS KERNEL INFO iar 00106210 dear 0244c23c
- 1131059653 2005.11.03 R67-M1-N8-C:J16-U01 2005-11-03-15.14.13.999389 R67-M1-N8-C:J16-U01 RAS KERNEL INFO iar 00106210 dear 0244c23c
- 1131059658 2005.11.03 R66-M0-N4-C:J16-U01 2005-11-03-15.14.18.909332 R66-M0-N4-C:J16-U01 RAS KERNEL INFO iar 00106210 dear 0244c26c
- 1131059703 2005.11.03 R67-M0-N2-C:J07-U11 2005-11-03-15.15.03.617956 R67-M0-N2-C:J07-U11 RAS KERNEL INFO iar 00106228 dear 0244c24c
- 1131059744 2005.11.03 R67-M1-N8-C:J10-U11 2005-11-03-15.15.44.150398 R67-M1-N8-C:J10-U11 RAS KERNEL INFO iar 00106228 dear 0244c27c
- 1131059757 2005.11.03 R67-M1-N0-C:J09-U11 2005-11-03-15.15.57.061077 R67-M1-N0-C:J09-U11 RAS KERNEL INFO iar 00106228 dear 0244c27c
- 1131059774 2005.11.03 R67-M0-N1-C:J03-U11 2005-11-03-15.16.14.637454 R67-M0-N1-C:J03-U11 RAS KERNEL INFO iar 00106228 dear 0244c27c
- 1131060122 2005.11.03 R67-M1-NA-C:J13-U01 2005-11-03-15.22.02.358932 R67-M1-NA-C:J13-U01 RAS KERNEL INFO iar 00106568 dear 02494bbc
- 1131060130 2005.11.03 R67-M1-NF-C:J10-U01 2005-11-03-15.22.10.405323 R67-M1-NF-C:J10-U01 RAS KERNEL INFO 487476 floating point alignment exceptions
- 1131060157 2005.11.03 R66-M1-NB-C:J17-U01 2005-11-03-15.22.37.393511 R66-M1-NB-C:J17-U01 RAS KERNEL INFO 487476 floating point alignment exceptions
- 1131060196 2005.11.03 R67-M1-NA-C:J03-U11 2005-11-03-15.23.16.963770 R67-M1-NA-C:J03-U11 RAS KERNEL INFO iar 00106580 dear 02494bcc
- 1131060202 2005.11.03 R67-M0-N6-C:J04-U01 2005-11-03-15.23.22.213145 R67-M0-N6-C:J04-U01 RAS KERNEL INFO iar 00106568 dear 02494bbc
- 1131060268 2005.11.03 R67-M0-N1-C:J11-U01 2005-11-03-15.24.28.446530 R67-M0-N1-C:J11-U01 RAS KERNEL INFO iar 00106580 dear 02494bcc
- 1131060284 2005.11.03 R66-M0-NA-C:J03-U11 2005-11-03-15.24.44.675679 R66-M0-NA-C:J03-U11 RAS KERNEL INFO iar 00106534 dear 02494c0c
- 1131060319 2005.11.03 R66-M0-N7-C:J08-U01 2005-11-03-15.25.19.549536 R66-M0-N7-C:J08-U01 RAS KERNEL INFO iar 00106534 dear 02494c0c
- 1131060387 2005.11.03 R67-M0-N6-C:J03-U11 2005-11-03-15.26.27.238003 R67-M0-N6-C:J03-U11 RAS KERNEL INFO iar 00106580 dear 02494bfc
- 1131060429 2005.11.03 R66-M1-NA-C:J15-U11 2005-11-03-15.27.09.727930 R66-M1-NA-C:J15-U11 RAS KERNEL INFO iar 00106568 dear 02494bec
- 1131060431 2005.11.03 R66-M1-ND-C:J12-U11 2005-11-03-15.27.11.996914 R66-M1-ND-C:J12-U11 RAS KERNEL INFO iar 00106568 dear 02494bec
- 1131060537 2005.11.03 R67-M0-N4-C:J10-U01 2005-11-03-15.28.57.931792 R67-M0-N4-C:J10-U01 RAS KERNEL INFO iar 00106534 dear 02494c3c
- 1131060539 2005.11.03 R67-M0-N0-C:J08-U11 2005-11-03-15.28.59.009148 R67-M0-N0-C:J08-U11 RAS KERNEL INFO iar 00106580 dear 02494bfc
- 1131060633 2005.11.03 R67-M1-N8-C:J09-U11 2005-11-03-15.30.33.871473 R67-M1-N8-C:J09-U11 RAS KERNEL INFO iar 00106534 dear 02494c3c
- 1131060644 2005.11.03 R66-M1-NC-C:J04-U11 2005-11-03-15.30.44.679329 R66-M1-NC-C:J04-U11 RAS KERNEL INFO iar 00106534 dear 02494c3c
- 1131060739 2005.11.03 R67-M1-NA-C:J03-U11 2005-11-03-15.32.19.271963 R67-M1-NA-C:J03-U11 RAS KERNEL INFO iar 00106580 dear 02494c2c
- 1131060806 2005.11.03 R67-M0-N0-C:J03-U01 2005-11-03-15.33.26.605133 R67-M0-N0-C:J03-U01 RAS KERNEL INFO iar 00106580 dear 02494c2c
- 1131061480 2005.11.03 R61-M0-N8-C:J03-U01 2005-11-03-15.44.40.306019 R61-M0-N8-C:J03-U01 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131061513 2005.11.03 R60-M0-NA-C:J13-U01 2005-11-03-15.45.13.710540 R60-M0-NA-C:J13-U01 RAS KERNEL INFO iar 00106568 dear 045e140c
- 1131061592 2005.11.03 R65-M1-N4-C:J10-U01 2005-11-03-15.46.32.715061 R65-M1-N4-C:J10-U01 RAS KERNEL INFO iar 00106580 dear 045e141c
- 1131061693 2005.11.03 R60-M0-N1-C:J04-U01 2005-11-03-15.48.13.958179 R60-M0-N1-C:J04-U01 RAS KERNEL INFO iar 00106580 dear 045e141c
- 1131061732 2005.11.03 R65-M0-N8-C:J15-U01 2005-11-03-15.48.52.839931 R65-M0-N8-C:J15-U01 RAS KERNEL INFO iar 00106580 dear 045e141c
- 1131061783 2005.11.03 R64-M1-N3-C:J05-U11 2005-11-03-15.49.43.212690 R64-M1-N3-C:J05-U11 RAS KERNEL INFO iar 00106534 dear 045e145c
- 1131061828 2005.11.03 R62-M0-N3-C:J14-U01 2005-11-03-15.50.28.449895 R62-M0-N3-C:J14-U01 RAS KERNEL INFO iar 00106568 dear 045e147c
- 1131061838 2005.11.03 R62-M0-N9-C:J12-U01 2005-11-03-15.50.38.592985 R62-M0-N9-C:J12-U01 RAS KERNEL INFO iar 00106580 dear 045e148c
- 1131063447 2005.11.03 R51-M1-NA-C:J17-U11 2005-11-03-16.17.27.446763 R51-M1-NA-C:J17-U11 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131063531 2005.11.03 R56-M0-N7-C:J02-U01 2005-11-03-16.18.51.107407 R56-M0-N7-C:J02-U01 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131063627 2005.11.03 R42-M0-N0-C:J14-U01 2005-11-03-16.20.27.921728 R42-M0-N0-C:J14-U01 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131063678 2005.11.03 R41-M1-NE-C:J13-U11 2005-11-03-16.21.18.099781 R41-M1-NE-C:J13-U11 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131063752 2005.11.03 R41-M0-N9-C:J16-U11 2005-11-03-16.22.32.185151 R41-M0-N9-C:J16-U11 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131063778 2005.11.03 R41-M0-N5-C:J06-U01 2005-11-03-16.22.58.761391 R41-M0-N5-C:J06-U01 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131063785 2005.11.03 R54-M1-N2-C:J06-U01 2005-11-03-16.23.05.085001 R54-M1-N2-C:J06-U01 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131063954 2005.11.03 R51-M1-NA-C:J05-U01 2005-11-03-16.25.54.660204 R51-M1-NA-C:J05-U01 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131064150 2005.11.03 R02-M1-NE-C:J02-U11 2005-11-03-16.29.10.392116 R02-M1-NE-C:J02-U11 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131064168 2005.11.03 R12-M0-NB-C:J13-U01 2005-11-03-16.29.28.796896 R12-M0-NB-C:J13-U01 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131064315 2005.11.03 R27-M1-N2-C:J10-U01 2005-11-03-16.31.55.074652 R27-M1-N2-C:J10-U01 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131064518 2005.11.03 R06-M1-N0-C:J12-U01 2005-11-03-16.35.18.503121 R06-M1-N0-C:J12-U01 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131065141 2005.11.03 R36-M1-N2-C:J08-U11 2005-11-03-16.45.41.838424 R36-M1-N2-C:J08-U11 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131065341 2005.11.03 R25-M1-N5-C:J16-U01 2005-11-03-16.49.01.163874 R25-M1-N5-C:J16-U01 RAS KERNEL INFO 2354412 floating point alignment exceptions
- 1131066692 2005.11.03 R36-M1-NB-C:J09-U01 2005-11-03-17.11.32.221132 R36-M1-NB-C:J09-U01 RAS KERNEL INFO 62500 floating point alignment exceptions
- 1131066789 2005.11.03 R24-M1-ND-C:J12-U11 2005-11-03-17.13.09.263119 R24-M1-ND-C:J12-U11 RAS KERNEL INFO 62500 floating point alignment exceptions
- 1131067982 2005.11.03 R35-M1-NF-C:J13-U01 2005-11-03-17.33.02.056235 R35-M1-NF-C:J13-U01 RAS KERNEL INFO 62500 floating point alignment exceptions
- 1131123526 2005.11.04 R62-M0-N1-C:J08-U11 2005-11-04-08.58.46.231312 R62-M0-N1-C:J08-U11 RAS KERNEL INFO iar 00106570 dear 0245a10c
- 1131123589 2005.11.04 R62-M0-N6-C:J03-U01 2005-11-04-08.59.49.668674 R62-M0-N6-C:J03-U01 RAS KERNEL INFO iar 001061fc dear 02f6676c
- 1131123637 2005.11.04 R62-M0-NB-C:J09-U11 2005-11-04-09.00.37.126852 R62-M0-NB-C:J09-U11 RAS KERNEL INFO iar 00106588 dear 0245a17c
- 1131125457 2005.11.04 R62-M0-NE-C:J08-U01 2005-11-04-09.30.57.389794 R62-M0-NE-C:J08-U01 RAS KERNEL INFO iar 0010662c dear 0245ccfc
- 1131125519 2005.11.04 R62-M0-N0-C:J14-U01 2005-11-04-09.31.59.900525 R62-M0-N0-C:J14-U01 RAS KERNEL INFO iar 00106bd8 dear 0245ce0c
- 1131125543 2005.11.04 R62-M0-N4-C:J12-U11 2005-11-04-09.32.23.060745 R62-M0-N4-C:J12-U11 RAS KERNEL INFO iar 00106834 dear 0245ccfc
- 1131125932 2005.11.04 R62-M0-NE-C:J15-U01 2005-11-04-09.38.52.887110 R62-M0-NE-C:J15-U01 RAS KERNEL INFO iar 00106bd8 dear 0245cd8c
- 1131125954 2005.11.04 R62-M0-N2-C:J11-U11 2005-11-04-09.39.14.407511 R62-M0-N2-C:J11-U11 RAS KERNEL INFO iar 0010682c dear 0245ccfc
- 1131125989 2005.11.04 R62-M0-N8-C:J13-U11 2005-11-04-09.39.49.409481 R62-M0-N8-C:J13-U11 RAS KERNEL INFO iar 00106b8c dear 0245ce8c
- 1131126038 2005.11.04 R62-M0-N6-C:J10-U01 2005-11-04-09.40.38.701063 R62-M0-N6-C:J10-U01 RAS KERNEL INFO iar 00106bc0 dear 0245cd7c
- 1131126144 2005.11.04 R62-M0-NB-C:J16-U11 2005-11-04-09.42.24.810298 R62-M0-NB-C:J16-U11 RAS KERNEL INFO iar 00106bd8 dear 0245cd9c
- 1131126148 2005.11.04 R62-M0-N1-C:J02-U01 2005-11-04-09.42.28.862256 R62-M0-N1-C:J02-U01 RAS KERNEL INFO iar 00106bd8 dear 0245ceec
- 1131126178 2005.11.04 R62-M0-ND-C:J09-U01 2005-11-04-09.42.58.025371 R62-M0-ND-C:J09-U01 RAS KERNEL INFO iar 0010681c dear 02f692fc
- 1131126200 2005.11.04 R62-M0-NA-C:J15-U11 2005-11-04-09.43.20.670728 R62-M0-NA-C:J15-U11 RAS KERNEL INFO iar 00106b8c dear 0245cf5c
- 1131126215 2005.11.04 R62-M0-N6-C:J09-U11 2005-11-04-09.43.35.668773 R62-M0-N6-C:J09-U11 RAS KERNEL INFO iar 00106bc0 dear 0245cdec
- 1131126633 2005.11.04 R62-M0-N2-C:J14-U01 2005-11-04-09.50.33.808350 R62-M0-N2-C:J14-U01 RAS KERNEL INFO iar 00106600 dear 0245ccfc
- 1131126699 2005.11.04 R62-M0-N1-C:J02-U11 2005-11-04-09.51.39.563817 R62-M0-N1-C:J02-U11 RAS KERNEL INFO iar 00106bd8 dear 0245cf0c
- 1131126706 2005.11.04 R62-M0-N6-C:J16-U01 2005-11-04-09.51.46.538961 R62-M0-N6-C:J16-U01 RAS KERNEL INFO iar 00106834 dear 0245cd0c
- 1131126736 2005.11.04 R62-M0-N6-C:J15-U11 2005-11-04-09.52.16.767608 R62-M0-N6-C:J15-U11 RAS KERNEL INFO iar 00106bd8 dear 0245cf3c
- 1131126738 2005.11.04 R62-M0-N2-C:J07-U01 2005-11-04-09.52.18.968277 R62-M0-N2-C:J07-U01 RAS KERNEL INFO iar 0010686c dear 02f6930c
- 1131126747 2005.11.04 R62-M0-NC-C:J04-U01 2005-11-04-09.52.27.280593 R62-M0-NC-C:J04-U01 RAS KERNEL INFO iar 0010686c dear 02f6951c
- 1131127186 2005.11.04 R62-M0-N9-C:J08-U11 2005-11-04-09.59.46.036595 R62-M0-N9-C:J08-U11 RAS KERNEL INFO 45684 floating point alignment exceptions
- 1131127207 2005.11.04 R62-M0-N1-C:J10-U01 2005-11-04-10.00.07.211412 R62-M0-N1-C:J10-U01 RAS KERNEL INFO iar 00106704 dear 0245b80c
- 1131127212 2005.11.04 R62-M0-NF-C:J03-U01 2005-11-04-10.00.12.875334 R62-M0-NF-C:J03-U01 RAS KERNEL INFO iar 0010639c dear 0245b71c
- 1131127257 2005.11.04 R62-M0-N9-C:J10-U11 2005-11-04-10.00.57.413635 R62-M0-N9-C:J10-U11 RAS KERNEL INFO iar 00106704 dear 0245b83c
- 1131127267 2005.11.04 R62-M0-N8-C:J14-U01 2005-11-04-10.01.07.918137 R62-M0-N8-C:J14-U01 RAS KERNEL INFO iar 00106704 dear 0245d57c
- 1131127308 2005.11.04 R62-M0-NB-C:J12-U01 2005-11-04-10.01.48.158698 R62-M0-NB-C:J12-U01 RAS KERNEL INFO iar 001063a0 dear 02f67e4c
- 1131127313 2005.11.04 R62-M0-ND-C:J12-U11 2005-11-04-10.01.53.184425 R62-M0-ND-C:J12-U11 RAS KERNEL INFO iar 001064fc dear 0245b64c
- 1131127319 2005.11.04 R62-M0-N6-C:J05-U11 2005-11-04-10.01.59.090723 R62-M0-N6-C:J05-U11 RAS KERNEL INFO iar 0010671c dear 0245b72c
- 1131127543 2005.11.04 R62-M0-NC-C:J11-U01 2005-11-04-10.05.43.243815 R62-M0-NC-C:J11-U01 RAS KERNEL INFO 16604010 floating point alignment exceptions
- 1131127543 2005.11.04 R62-M0-NC-C:J06-U11 2005-11-04-10.05.43.570607 R62-M0-NC-C:J06-U11 RAS KERNEL INFO 16469355 floating point alignment exceptions
- 1131127560 2005.11.04 R62-M0-N7-C:J05-U11 2005-11-04-10.06.00.288313 R62-M0-N7-C:J05-U11 RAS KERNEL INFO iar 00106390 dear 02f67cfc
- 1131127565 2005.11.04 R62-M0-N0-C:J13-U11 2005-11-04-10.06.05.176180 R62-M0-N0-C:J13-U11 RAS KERNEL INFO iar 00106704 dear 0245b6bc
- 1131127588 2005.11.04 R62-M0-N4-C:J16-U11 2005-11-04-10.06.28.496633 R62-M0-N4-C:J16-U11 RAS KERNEL INFO iar 00106168 dear 0245b64c
- 1131127609 2005.11.04 R62-M0-N4-C:J04-U01 2005-11-04-10.06.49.824350 R62-M0-N4-C:J04-U01 RAS KERNEL INFO iar 0010635c dear 0245b62c
- 1131127613 2005.11.04 R62-M0-NE-C:J13-U01 2005-11-04-10.06.53.324365 R62-M0-NE-C:J13-U01 RAS KERNEL INFO iar 00106704 dear 0245b77c
- 1131127672 2005.11.04 R62-M0-NE-C:J02-U11 2005-11-04-10.07.52.715008 R62-M0-NE-C:J02-U11 RAS KERNEL INFO iar 001064fc dear 0245b64c
- 1131128050 2005.11.04 R62-M0-N4-C:J03-U01 2005-11-04-10.14.10.540522 R62-M0-N4-C:J03-U01 RAS KERNEL INFO 16736445 floating point alignment exceptions
- 1131128061 2005.11.04 R62-M0-N7-C:J16-U01 2005-11-04-10.14.21.410156 R62-M0-N7-C:J16-U01 RAS KERNEL INFO iar 001061cc dear 02f6667c
- 1131128074 2005.11.04 R62-M0-N9-C:J08-U11 2005-11-04-10.14.34.823505 R62-M0-N9-C:J08-U11 RAS KERNEL INFO iar 00105fd4 dear 0245a09c
- 1131128083 2005.11.04 R62-M0-N4-C:J14-U01 2005-11-04-10.14.43.761466 R62-M0-N4-C:J14-U01 RAS KERNEL INFO iar 00105fd4 dear 0245a09c
- 1131128104 2005.11.04 R62-M0-N0-C:J17-U01 2005-11-04-10.15.04.044405 R62-M0-N0-C:J17-U01 RAS KERNEL INFO iar 001061e4 dear 0245a09c
- 1131128125 2005.11.04 R62-M0-N0-C:J16-U01 2005-11-04-10.15.25.666952 R62-M0-N0-C:J16-U01 RAS KERNEL INFO iar 001061ec dear 02f6668c
- 1131128169 2005.11.04 R62-M0-N7-C:J06-U01 2005-11-04-10.16.09.870195 R62-M0-N7-C:J06-U01 RAS KERNEL INFO iar 001061ec dear 02f6668c
- 1131129646 2005.11.04 R62-M0-N6-C:J16-U11 2005-11-04-10.40.46.576314 R62-M0-N6-C:J16-U11 RAS KERNEL INFO 74350836 floating point alignment exceptions
- 1131129716 2005.11.04 R62-M0-NC-C:J15-U11 2005-11-04-10.41.56.879911 R62-M0-NC-C:J15-U11 RAS KERNEL INFO iar 0010650c dear 0246a95c
- 1131130552 2005.11.04 R62-M0-NC-C:J05-U01 2005-11-04-10.55.52.550384 R62-M0-NC-C:J05-U01 RAS KERNEL INFO iar 00106988 dear 02f7bd6c
- 1131130593 2005.11.04 R62-M0-N0-C:J15-U01 2005-11-04-10.56.33.280199 R62-M0-N0-C:J15-U01 RAS KERNEL INFO iar 00106cf8 dear 0246f9fc
- 1131131501 2005.11.04 R63-M0-N8-C:J03-U11 2005-11-04-11.11.41.412257 R63-M0-N8-C:J03-U11 RAS KERNEL INFO 54004554 floating point alignment exceptions
- 1131131553 2005.11.04 R63-M0-NE-C:J11-U11 2005-11-04-11.12.33.858917 R63-M0-NE-C:J11-U11 RAS KERNEL INFO iar 001069a8 dear 02f7bd7c
- 1131139987 2005.11.04 R63-M0-N8-C:J16-U11 2005-11-04-13.33.07.115361 R63-M0-N8-C:J16-U11 RAS KERNEL INFO iar 001066ac dear 0252524c
- 1131140000 2005.11.04 R63-M0-ND-C:J15-U01 2005-11-04-13.33.20.655126 R63-M0-ND-C:J15-U01 RAS KERNEL INFO iar 001066c4 dear 0252528c
- 1131141199 2005.11.04 R63-M0-NB-C:J05-U11 2005-11-04-13.53.19.475674 R63-M0-NB-C:J05-U11 RAS KERNEL INFO iar 001066c4 dear 025251fc
- 1131141205 2005.11.04 R63-M0-N3-C:J16-U11 2005-11-04-13.53.25.955263 R63-M0-N3-C:J16-U11 RAS KERNEL INFO iar 00106678 dear 0252523c
- 1131141902 2005.11.04 R62-M0-N1-C:J04-U01 2005-11-04-14.05.02.591917 R62-M0-N1-C:J04-U01 RAS KERNEL INFO iar 001007b8 dear 065b4c7c
- 1131141926 2005.11.04 R62-M0-NB-C:J02-U01 2005-11-04-14.05.26.394289 R62-M0-NB-C:J02-U01 RAS KERNEL INFO iar 001007a8 dear 065b4cac
- 1131142179 2005.11.04 R60-M1-N0-I:J18-U01 2005-11-04-14.09.39.179254 R60-M1-N0-I:J18-U01 RAS KERNEL INFO ciod: generated 128 core files for program /g/g20/valone1/SPaSM-shock/SPaSM_mpi
- 1131142353 2005.11.04 R62-M0-NC-C:J16-U01 2005-11-04-14.12.33.760945 R62-M0-NC-C:J16-U01 RAS KERNEL INFO iar 001066ac dear 0252523c
- 1131142377 2005.11.04 R62-M0-N4-C:J15-U11 2005-11-04-14.12.57.989477 R62-M0-N4-C:J15-U11 RAS KERNEL INFO iar 001066c4 dear 0252527c
- 1131142767 2005.11.04 R62-M0-N0-C:J04-U11 2005-11-04-14.19.27.963289 R62-M0-N0-C:J04-U11 RAS KERNEL INFO iar 001066c4 dear 0252523c
- 1131142811 2005.11.04 R62-M0-NC-C:J06-U11 2005-11-04-14.20.11.135552 R62-M0-NC-C:J06-U11 RAS KERNEL INFO iar 001066ac dear 025252bc
- 1131142817 2005.11.04 R62-M0-N5-C:J16-U11 2005-11-04-14.20.17.262659 R62-M0-N5-C:J16-U11 RAS KERNEL INFO iar 001066c4 dear 0252529c
- 1131147217 2005.11.04 R63-M0-NF-C:J05-U01 2005-11-04-15.33.37.855786 R63-M0-NF-C:J05-U01 RAS KERNEL INFO 2667384 floating point alignment exceptions
- 1131147227 2005.11.04 R63-M0-N1-C:J05-U11 2005-11-04-15.33.47.360607 R63-M0-N1-C:J05-U11 RAS KERNEL INFO iar 00106570 dear 0245a5bc
- 1131147251 2005.11.04 R63-M0-N1-C:J16-U01 2005-11-04-15.34.11.676048 R63-M0-N1-C:J16-U01 RAS KERNEL INFO iar 00106570 dear 0245a5ec
- 1131147273 2005.11.04 R63-M0-N4-C:J05-U11 2005-11-04-15.34.33.610222 R63-M0-N4-C:J05-U11 RAS KERNEL INFO iar 0010653c dear 0245a63c
- 1131148249 2005.11.04 R63-M0-N0-C:J03-U01 2005-11-04-15.50.49.414886 R63-M0-N0-C:J03-U01 RAS KERNEL INFO 407736 floating point alignment exceptions
- 1131148435 2005.11.04 R63-M0-N7-C:J09-U01 2005-11-04-15.53.55.739224 R63-M0-N7-C:J09-U01 RAS KERNEL INFO 428412 floating point alignment exceptions
- 1131148446 2005.11.04 R63-M0-N4-C:J02-U01 2005-11-04-15.54.06.214936 R63-M0-N4-C:J02-U01 RAS KERNEL INFO 441348 floating point alignment exceptions
- 1131148482 2005.11.04 R63-M0-N0-C:J17-U01 2005-11-04-15.54.42.089011 R63-M0-N0-C:J17-U01 RAS KERNEL INFO iar 001061d4 dear 024696cc
- 1131148486 2005.11.04 R63-M0-N2-C:J03-U11 2005-11-04-15.54.46.919787 R63-M0-N2-C:J03-U11 RAS KERNEL INFO iar 00106348 dear 024696bc
- 1131148496 2005.11.04 R63-M0-NF-C:J08-U11 2005-11-04-15.54.56.089471 R63-M0-NF-C:J08-U11 RAS KERNEL INFO iar 00106204 dear 02f75edc
- 1131156131 2005.11.04 R45-M1-NA-C:J11-U01 2005-11-04-18.02.11.252563 R45-M1-NA-C:J11-U01 RAS KERNEL INFO CE sym 21, at 0x111d8c60, mask 0x01
- 1131163283 2005.11.04 R63-M0-N3-C:J02-U11 2005-11-04-20.01.23.415003 R63-M0-N3-C:J02-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1131206251 2005.11.05 R17-M0-N8-C:J05-U01 2005-11-05-07.57.31.806929 R17-M0-N8-C:J05-U01 RAS KERNEL INFO CE sym 34, at 0x11e7ed80, mask 0x80
- 1131218683 2005.11.05 R11-M1-N4-C:J03-U01 2005-11-05-11.24.43.382408 R11-M1-N4-C:J03-U01 RAS KERNEL INFO 2 ddr errors(s) detected and corrected on rank 0, symbol 18, bit 6
- 1131347286 2005.11.06 R63-M0-N3-C:J02-U11 2005-11-06-23.08.06.640071 R63-M0-N3-C:J02-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1131366730 2005.11.07 R50-M0-N4-I:J18-U01 2005-11-07-04.32.10.481855 R50-M0-N4-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/followup/RAPTOR/pre-study/raptor.newcomp.r1: invalid or missing program image, Permission denied
- 1131366881 2005.11.07 R60-M0-N0-I:J18-U01 2005-11-07-04.34.41.779706 R60-M0-N0-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/followup/RAPTOR/pre-study/raptor.newcomp.r1: invalid or missing program image, Permission denied
- 1131408782 2005.11.07 R63-M0-N6-C:J15-U01 2005-11-07-16.13.02.108416 R63-M0-N6-C:J15-U01 RAS KERNEL INFO iar 00105e88 dear 02f6436c
- 1131408830 2005.11.07 R63-M0-N9-C:J10-U01 2005-11-07-16.13.50.630658 R63-M0-N9-C:J10-U01 RAS KERNEL INFO iar 00105e84 dear 0244f29c
- 1131458788 2005.11.08 R74-M0-N1-C:J05-U01 2005-11-08-06.06.28.982001 R74-M0-N1-C:J05-U01 RAS KERNEL INFO CE sym 30, at 0x042358a0, mask 0x20
- 1131470027 2005.11.08 R27-M0-N0-I:J18-U11 2005-11-08-09.13.47.353261 R27-M0-N0-I:J18-U11 RAS KERNEL INFO ciod: generated 128 core files for program /g/g24/germann2/BGL-demo/SPaSM_mpi
- 1131473353 2005.11.08 R62-M0-N2-C:J13-U11 2005-11-08-10.09.13.667082 R62-M0-N2-C:J13-U11 RAS KERNEL INFO iar 001061e8 dear 0247012c
- 1131473377 2005.11.08 R62-M0-NA-C:J04-U11 2005-11-08-10.09.37.180669 R62-M0-NA-C:J04-U11 RAS KERNEL INFO iar 001061e8 dear 0247015c
- 1131473384 2005.11.08 R62-M0-NA-C:J09-U01 2005-11-08-10.09.44.211204 R62-M0-NA-C:J09-U01 RAS KERNEL INFO iar 00106200 dear 0247016c
- 1131473391 2005.11.08 R62-M0-NE-C:J03-U11 2005-11-08-10.09.51.058276 R62-M0-NE-C:J03-U11 RAS KERNEL INFO iar 001061b4 dear 024701dc
- 1131473771 2005.11.08 R62-M0-N4-C:J11-U11 2005-11-08-10.16.11.798242 R62-M0-N4-C:J11-U11 RAS KERNEL INFO iar 001061e8 dear 0247015c
- 1131473802 2005.11.08 R62-M0-NC-C:J14-U01 2005-11-08-10.16.42.854605 R62-M0-NC-C:J14-U01 RAS KERNEL INFO iar 00106200 dear 0247019c
- 1131473805 2005.11.08 R62-M0-NE-C:J09-U01 2005-11-08-10.16.45.386821 R62-M0-NE-C:J09-U01 RAS KERNEL INFO iar 00105e84 dear 024701dc
- 1131473825 2005.11.08 R62-M0-N1-C:J10-U01 2005-11-08-10.17.05.439216 R62-M0-N1-C:J10-U01 RAS KERNEL INFO iar 00106200 dear 0247019c
- 1131474292 2005.11.08 R62-M0-NB-C:J12-U01 2005-11-08-10.24.52.743959 R62-M0-NB-C:J12-U01 RAS KERNEL INFO iar 001061e8 dear 0247015c
- 1131474344 2005.11.08 R62-M0-N8-C:J16-U01 2005-11-08-10.25.44.140702 R62-M0-N8-C:J16-U01 RAS KERNEL INFO iar 001061e8 dear 0247018c
- 1131477523 2005.11.08 R62-M0-N9-C:J09-U11 2005-11-08-11.18.43.884902 R62-M0-N9-C:J09-U11 RAS KERNEL INFO iar 001061e8 dear 0247015c
- 1131477528 2005.11.08 R62-M0-N8-C:J06-U11 2005-11-08-11.18.48.319519 R62-M0-N8-C:J06-U11 RAS KERNEL INFO iar 001061e8 dear 0247012c
- 1131477539 2005.11.08 R62-M0-N2-C:J05-U11 2005-11-08-11.18.59.141452 R62-M0-N2-C:J05-U11 RAS KERNEL INFO iar 001061b4 dear 0247017c
- 1131477542 2005.11.08 R62-M0-ND-C:J03-U11 2005-11-08-11.19.02.975234 R62-M0-ND-C:J03-U11 RAS KERNEL INFO iar 001061b4 dear 024701ac
- 1131477544 2005.11.08 R62-M0-N4-C:J14-U11 2005-11-08-11.19.04.849931 R62-M0-N4-C:J14-U11 RAS KERNEL INFO iar 001061b4 dear 0247017c
- 1131477559 2005.11.08 R62-M0-ND-C:J05-U01 2005-11-08-11.19.19.457791 R62-M0-ND-C:J05-U01 RAS KERNEL INFO iar 00106200 dear 0247019c
- 1131478044 2005.11.08 R63-M0-N3-C:J02-U11 2005-11-08-11.27.24.415997 R63-M0-N3-C:J02-U11 RAS KERNEL INFO iar 001061e8 dear 0247015c
- 1131478061 2005.11.08 R63-M0-NB-C:J06-U11 2005-11-08-11.27.41.288597 R63-M0-NB-C:J06-U11 RAS KERNEL INFO iar 001061b4 dear 0247017c
- 1131484346 2005.11.08 R63-M0-N6-C:J13-U01 2005-11-08-13.12.26.089394 R63-M0-N6-C:J13-U01 RAS KERNEL INFO 19220208 floating point alignment exceptions
KERNREC 1131499523 2005.11.08 R62-M0-N2-C:J09-U11 2005-11-08-17.25.23.131318 R62-M0-N2-C:J09-U11 RAS KERNEL FATAL Error receiving packet on tree network, expecting type 57 instead of type 3 (softheader=00589370 90990003 00000002 00000000) PSR0=00001f01 PSR1=00000000 PRXF=00000002 PIXF=00000007
- 1131525612 2005.11.09 R62-M0-N4-C:J13-U01 2005-11-09-00.40.12.350846 R62-M0-N4-C:J13-U01 RAS KERNEL FATAL program interrupt: privileged instruction...0
- 1131579563 2005.11.09 R62-M0-N1-C:J04-U11 2005-11-09-15.39.23.829694 R62-M0-N1-C:J04-U11 RAS KERNEL INFO iar 00106ba0 dear 0246de3c
- 1131579941 2005.11.09 R62-M0-N0-C:J02-U01 2005-11-09-15.45.41.245664 R62-M0-N0-C:J02-U01 RAS KERNEL INFO iar 00106b98 dear 0246dd9c
- 1131579963 2005.11.09 R62-M0-NC-C:J05-U11 2005-11-09-15.46.03.437898 R62-M0-NC-C:J05-U11 RAS KERNEL INFO iar 00106c0c dear 0246de1c
- 1131582295 2005.11.09 R63-M0-NA-C:J09-U01 2005-11-09-16.24.55.322546 R63-M0-NA-C:J09-U01 RAS KERNEL INFO 5302707 floating point alignment exceptions
- 1131582323 2005.11.09 R63-M0-N8-C:J14-U11 2005-11-09-16.25.23.800081 R63-M0-N8-C:J14-U11 RAS KERNEL INFO iar 00106448 dear 0246dbbc
- 1131582345 2005.11.09 R63-M0-N5-C:J05-U11 2005-11-09-16.25.45.329267 R63-M0-N5-C:J05-U11 RAS KERNEL INFO iar 00106448 dear 0246db5c
- 1131582346 2005.11.09 R63-M0-N0-C:J03-U11 2005-11-09-16.25.46.239765 R63-M0-N0-C:J03-U11 RAS KERNEL INFO iar 00106448 dear 0246dbec
- 1131582509 2005.11.09 R61-M0-NB-C:J05-U01 2005-11-09-16.28.29.127581 R61-M0-NB-C:J05-U01 RAS KERNEL INFO CE sym 25, at 0x10e1bce0, mask 0x40
- 1131583004 2005.11.09 R63-M0-NF-C:J05-U11 2005-11-09-16.36.44.235997 R63-M0-NF-C:J05-U11 RAS KERNEL INFO iar 00106288 dear 02f7a0fc
- 1131590854 2005.11.09 R63-M0-N6-C:J13-U01 2005-11-09-18.47.34.555338 R63-M0-N6-C:J13-U01 RAS KERNEL INFO iar 00113890 dear 0fee9c3c
- 1131590868 2005.11.09 R63-M0-N1-C:J07-U01 2005-11-09-18.47.48.543958 R63-M0-N1-C:J07-U01 RAS KERNEL INFO iar 00113898 dear 0fee9c4c
- 1131590881 2005.11.09 R63-M0-N4-C:J05-U11 2005-11-09-18.48.01.657230 R63-M0-N4-C:J05-U11 RAS KERNEL INFO iar 00113898 dear 0fee9c4c
- 1131601567 2005.11.09 R51-M1-N4-C:J07-U01 2005-11-09-21.46.07.597381 R51-M1-N4-C:J07-U01 RAS KERNEL INFO 5 torus receiver z+ input pipe error(s) (dcr 0x02f0) detected and corrected
- 1131601592 2005.11.09 R45-M1-NA-C:J10-U01 2005-11-09-21.46.32.491859 R45-M1-NA-C:J10-U01 RAS KERNEL INFO CE sym 9, at 0x12870760, mask 0x40
- 1131624435 2005.11.10 R22-M1-N1-C:J17-U11 2005-11-10-04.07.15.804295 R22-M1-N1-C:J17-U11 RAS KERNEL INFO 3 L3 EDRAM error(s) (dcr 0x0157) detected and corrected
- 1131624438 2005.11.10 R65-M0-N2-C:J05-U11 2005-11-10-04.07.18.981015 R65-M0-N2-C:J05-U11 RAS KERNEL INFO CE sym 6, at 0x00e462c0, mask 0x02
- 1131624456 2005.11.10 R61-M0-NB-C:J05-U01 2005-11-10-04.07.36.503737 R61-M0-NB-C:J05-U01 RAS KERNEL INFO CE sym 25, at 0x00e19cc0, mask 0x10
- 1131632923 2005.11.10 R34-M1-N4-C:J10-U01 2005-11-10-06.28.43.189795 R34-M1-N4-C:J10-U01 RAS KERNEL INFO 2 ddr errors(s) detected and corrected on rank 0, symbol 4, bit 4
- 1131653512 2005.11.10 R21-M1-N3-C:J03-U11 2005-11-10-12.11.52.704752 R21-M1-N3-C:J03-U11 RAS KERNEL INFO CE sym 15, at 0x1bb2fe80, mask 0x02
KERNTERM 1131680322 2005.11.10 R63-M0-N6-C:J12-U11 2005-11-10-19.38.42.369022 R63-M0-N6-C:J12-U11 RAS KERNEL FATAL rts: kernel terminated for reason 1004
- 1131692621 2005.11.10 R63-M0-NF-C:J06-U01 2005-11-10-23.03.41.120402 R63-M0-NF-C:J06-U01 RAS KERNEL INFO iar 00106274 dear 0246da8c
- 1131692641 2005.11.10 R63-M0-N4-C:J04-U11 2005-11-10-23.04.01.480413 R63-M0-N4-C:J04-U11 RAS KERNEL INFO iar 00106448 dear 0246dacc
- 1131692649 2005.11.10 R63-M0-N3-C:J14-U11 2005-11-10-23.04.09.760063 R63-M0-N3-C:J14-U11 RAS KERNEL INFO iar 00106298 dear 02f7a18c
- 1131693152 2005.11.10 R63-M0-N5-C:J12-U01 2005-11-10-23.12.32.269153 R63-M0-N5-C:J12-U01 RAS KERNEL INFO 488205 floating point alignment exceptions
- 1131712792 2005.11.11 R31-M0-N8-I:J18-U01 2005-11-11-04.39.52.183132 R31-M0-N8-I:J18-U01 RAS KERNEL INFO ciod: generated 128 core files for program /bgl/apps/followup/SPaSM_static/SPaSM_mpi.new_comp
- 1131720323 2005.11.11 R02-M1-N4-C:J06-U01 2005-11-11-06.45.23.635450 R02-M1-N4-C:J06-U01 RAS KERNEL INFO 1 ddr errors(s) detected and corrected on rank 0, symbol 8, bit 7
- 1131764302 2005.11.11 R23-M1-N0-C:J08-U11 2005-11-11-18.58.22.474164 R23-M1-N0-C:J08-U11 RAS KERNEL INFO CE sym 25, at 0x12127ee0, mask 0x10
KERNTERM 1132021523 2005.11.14 R46-M1-NE-C:J14-U11 2005-11-14-18.25.23.269634 R46-M1-NE-C:J14-U11 RAS KERNEL FATAL rts: kernel terminated for reason 1004
KERNREC 1132021541 2005.11.14 R46-M1-NF-C:J11-U01 2005-11-14-18.25.41.161796 R46-M1-NF-C:J11-U01 RAS KERNEL FATAL Error receiving packet on tree network, expecting type 57 instead of type 3 (softheader=00ce22e8 e6200003 00000002 00000000) PSR0=00001f01 PSR1=00000000 PRXF=00000002 PIXF=00000007
KERNREC 1132021542 2005.11.14 R46-M1-N1-C:J02-U01 2005-11-14-18.25.42.152117 R46-M1-N1-C:J02-U01 RAS KERNEL FATAL Error receiving packet on tree network, expecting type 57 instead of type 3 (softheader=00ce22e8 e6200003 00000002 00000000) PSR0=20021f01 PSR1=00000000 PRXF=00000002 PIXF=00000007
- 1132062083 2005.11.15 R05-M0-NC-C:J04-U11 2005-11-15-05.41.23.201247 R05-M0-NC-C:J04-U11 RAS KERNEL INFO critical input interrupt (unit=0x0b bit=0x0a): warning for torus z+ wire, suppressing further interrupts of same type
- 1132067079 2005.11.15 R17-M1-N0-I:J18-U01 2005-11-15-07.04.39.066903 R17-M1-N0-I:J18-U01 RAS KERNEL INFO ciod: generated 128 core files for program /bgl/apps/followup/SPaSM_static/SPaSM_mpi.rel2
- 1132069983 2005.11.15 R34-M1-N4-I:J18-U01 2005-11-15-07.53.03.697929 R34-M1-N4-I:J18-U01 RAS KERNEL INFO ciod: generated 64 core files for program /bgl/apps/followup/hellow/a.out.1111
- 1132070039 2005.11.15 R15-M0-NC-I:J18-U11 2005-11-15-07.53.59.004294 R15-M0-NC-I:J18-U11 RAS KERNEL INFO ciod: generated 64 core files for program /bgl/apps/followup/hellow/a.out.1111
- 1132076597 2005.11.15 R23-M0-N7 2005-11-15-09.43.17.181620 R23-M0-N7 NULL DISCOVERY INFO Node card VPD check: U11 node in processor card slot J04 do not match. VPD ecid 04D58088CB2FFFFF08031C104CE9, found 04CA80D8012FFFFF08061A8094CB
- 1132076952 2005.11.15 R67-M1-NA 2005-11-15-09.49.12.022636 R67-M1-NA NULL DISCOVERY INFO Node card VPD check: U11 node in processor card slot J10 do not match. VPD ecid 07570DD8002FFFFF09031B7090E2, found 04D480D8482FFFFF0A071BA052DE
KERNTERM 1132111168 2005.11.15 R57-M0-NA-C:J13-U11 2005-11-15-19.19.28.379713 R57-M0-NA-C:J13-U11 RAS KERNEL FATAL rts: kernel terminated for reason 1004
KERNREC 1132111278 2005.11.15 R57-M0-N8-C:J12-U01 2005-11-15-19.21.18.471861 R57-M0-N8-C:J12-U01 RAS KERNEL FATAL Error receiving packet on tree network, expecting type 57 instead of type 3 (softheader=009756d5 8bfa0003 00000002 00000000) PSR0=20021f01 PSR1=00000000 PRXF=00000002 PIXF=00000007
- 1132232535 2005.11.17 R64-M1-N4-I:J18-U01 2005-11-17-05.02.15.807696 R64-M1-N4-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/followup/SPaSM_static/SPaSM.460-1115: invalid or missing program image, Permission denied
- 1132234526 2005.11.17 R72-M1-NE-C:J04-U01 2005-11-17-05.35.26.782649 R72-M1-NE-C:J04-U01 RAS KERNEL INFO 18507114 torus sender z- retransmission error(s) (dcr 0x02f9) detected and corrected over 198 seconds
- 1132236972 2005.11.17 R72-M1-N6-C:J04-U01 2005-11-17-06.16.12.504899 R72-M1-N6-C:J04-U01 RAS KERNEL INFO 26741629 torus sender z- retransmission error(s) (dcr 0x02f9) detected and corrected over 268 seconds
- 1132320935 2005.11.18 R02-M1-NC-I:J18-U01 2005-11-18-05.35.35.752458 R02-M1-NC-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001b0
- 1132326853 2005.11.18 R63-M1-N4-I:J18-U01 2005-11-18-07.14.13.952254 R63-M1-N4-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001b0
- 1132328434 2005.11.18 R35-M0-NC-I:J18-U11 2005-11-18-07.40.34.306398 R35-M0-NC-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001b0
- 1132546059 2005.11.20 R25-M0-N2-C:J11-U11 2005-11-20-20.07.39.717185 R25-M0-N2-C:J11-U11 RAS KERNEL INFO CE sym 28, at 0x1efc7020, mask 0x20
- 1132549418 2005.11.20 R55-M1-N8-I:J18-U11 2005-11-20-21.03.38.769342 R55-M1-N8-I:J18-U11 RAS KERNEL INFO ciod: generated 128 core files for program /g/g90/glosli/src/ddcMD/ddcMD1.1.18a/bin/ddcMDbglV
- 1132549487 2005.11.20 R06-M1-N8-I:J18-U01 2005-11-20-21.04.47.799840 R06-M1-N8-I:J18-U01 RAS KERNEL INFO ciod: generated 128 core files for program /g/g90/glosli/src/ddcMD/ddcMD1.1.18a/bin/ddcMDbglV
KERNMNTF 1132575798 2005.11.21 R04-M1-N8-I:J18-U01 2005-11-21-04.23.18.419757 R04-M1-N8-I:J18-U01 RAS KERNEL FATAL Lustre mount FAILED : bglio78 : block_id : location
- 1132579592 2005.11.21 R10-M1-N0-I:J18-U11 2005-11-21-05.26.32.169664 R10-M1-N0-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001b0
- 1132584089 2005.11.21 R26-M0-N0-I:J18-U11 2005-11-21-06.41.29.787206 R26-M0-N0-I:J18-U11 RAS KERNEL INFO MACHINE CHECK DCR read timeout (mc=e08x iar 0x00000000 lr 0xc00045a4)
- 1132584495 2005.11.21 R26-M0-N0-I:J18-U11 2005-11-21-06.48.15.351050 R26-M0-N0-I:J18-U11 RAS KERNEL INFO MACHINE CHECK DCR read timeout (mc=e08x iar 0x00000000 lr 0xc00045a4)
- 1132584963 2005.11.21 R26-M0-N0-I:J18-U11 2005-11-21-06.56.03.369673 R26-M0-N0-I:J18-U11 RAS KERNEL INFO MACHINE CHECK DCR read timeout (mc=e08x iar 0x00000000 lr 0xc00045a4)
- 1132585070 2005.11.21 R26-M0-N0-I:J18-U11 2005-11-21-06.57.50.899596 R26-M0-N0-I:J18-U11 RAS KERNEL INFO MACHINE CHECK DCR read timeout (mc=e08x iar 0x00000000 lr 0xc00045a4)
- 1132585230 2005.11.21 R26-M0-N0-I:J18-U11 2005-11-21-07.00.30.773283 R26-M0-N0-I:J18-U11 RAS KERNEL INFO MACHINE CHECK DCR read timeout (mc=e08x iar 0x00000000 lr 0xc00045a4)
- 1132586440 2005.11.21 R26-M0-N0-I:J18-U11 2005-11-21-07.20.40.564558 R26-M0-N0-I:J18-U11 RAS KERNEL INFO MACHINE CHECK DCR read timeout (mc=e08x iar 0x00000000 lr 0xc00045a4)
- 1132586451 2005.11.21 R26-M0-N0-I:J18-U11 2005-11-21-07.20.51.661738 R26-M0-N0-I:J18-U11 RAS KERNEL INFO MACHINE CHECK DCR read timeout (mc=e08x iar 0x00000000 lr 0xc00045a4)
- 1132596009 2005.11.21 R16-M0-NC-I:J18-U01 2005-11-21-10.00.09.399765 R16-M0-NC-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001b2
APPTO 1132600084 2005.11.21 R14-M0-NC-I:J18-U11 2005-11-21-11.08.04.944948 R14-M0-NC-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:41554, Connection timed out
APPRES 1132609258 2005.11.21 R13-M1-N4-I:J18-U11 2005-11-21-13.40.58.052836 R13-M1-N4-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:52839, Connection reset by peer
- 1132609277 2005.11.21 R55-M0-NC-I:J18-U01 2005-11-21-13.41.17.863430 R55-M0-NC-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x0000044a
APPRES 1132609304 2005.11.21 R22-M0-N0-I:J18-U01 2005-11-21-13.41.44.752517 R22-M0-N0-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:52930, Connection reset by peer
- 1132609314 2005.11.21 R36-M1-N4-I:J18-U11 2005-11-21-13.41.54.797197 R36-M1-N4-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x0000044d
APPRES 1132609342 2005.11.21 R65-M1-NC-I:J18-U11 2005-11-21-13.42.22.905034 R65-M1-NC-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:53387, Connection reset by peer
APPTO 1132613223 2005.11.21 R16-M1-NC-I:J18-U11 2005-11-21-14.47.03.703578 R16-M1-NC-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:41060, Connection timed out
APPRES 1132613769 2005.11.21 R66-M1-N8-I:J18-U01 2005-11-21-14.56.09.042983 R66-M1-N8-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:41587, Connection reset by peer
- 1132675121 2005.11.22 R26-M1-N1-C:J02-U11 2005-11-22-07.58.41.090225 R26-M1-N1-C:J02-U11 RAS KERNEL INFO CE sym 14, at 0x136cd5e0, mask 0x04
- 1132841276 2005.11.24 R03-M1-NF-C:J07-U01 2005-11-24-06.07.56.407913 R03-M1-NF-C:J07-U01 RAS KERNEL FATAL dbcr0=0x00000000 dbsr=0x00000000 ccr0=0x40002000
APPSEV 1132853885 2005.11.24 R14-M0-N8-I:J18-U01 2005-11-24-09.38.05.459006 R14-M0-N8-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:41534, Link has been severed
- 1132870645 2005.11.24 R77-M0-N4-I:J18-U01 2005-11-24-14.17.25.200932 R77-M0-N4-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x00001a12
APPSEV 1132912103 2005.11.25 R45-M1-N0-I:J18-U01 2005-11-25-01.48.23.054823 R45-M1-N0-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:54780, Link has been severed
- 1132969138 2005.11.25 R76-M0-NC-I:J18-U11 2005-11-25-17.38.58.840897 R76-M0-NC-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f8
APPSEV 1133009333 2005.11.26 R54-M0-N0-I:J18-U01 2005-11-26-04.48.53.869394 R54-M0-N0-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:60243, Link has been severed
- 1133011728 2005.11.26 R72-M1-NC-I:J18-U11 2005-11-26-05.28.48.120288 R72-M1-NC-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f5
- 1133011767 2005.11.26 R01-M1-NC-I:J18-U11 2005-11-26-05.29.27.205468 R01-M1-NC-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2
APPSEV 1133044906 2005.11.26 R33-M1-N4-I:J18-U11 2005-11-26-14.41.46.876542 R33-M1-N4-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:47189, Link has been severed
- 1133051996 2005.11.26 R03-M1-NF-C:J07-U01 2005-11-26-16.39.56.330868 R03-M1-NF-C:J07-U01 RAS KERNEL FATAL r24=0x0ffea4c8 r25=0x00000003 r26=0x0000000f r27=0xffffd000
- 1133052171 2005.11.26 R14-M1-NC-I:J18-U01 2005-11-26-16.42.51.171566 R14-M1-NC-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2
- 1133150722 2005.11.27 R15-M0-N0-I:J18-U11 2005-11-27-20.05.22.588765 R15-M0-N0-I:J18-U11 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f5
- 1133280774 2005.11.29 R55-M0-N9-C:J06-U11 2005-11-29-08.12.54.614554 R55-M0-N9-C:J06-U11 RAS KERNEL INFO 13 tree receiver 1 in re-synch state event(s) (dcr 0x0185) detected over 4562 seconds
- 1133306285 2005.11.29 R60-M0-N4-I:J18-U01 2005-11-29-15.18.05.050463 R60-M0-N4-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2
- 1133306326 2005.11.29 R21-M0-N8-I:J18-U01 2005-11-29-15.18.46.642475 R21-M0-N8-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f2
- 1133310116 2005.11.29 R67-M1-N7 2005-11-29-16.21.56.855025 R67-M1-N7 NULL DISCOVERY INFO Node card VPD check: U01 node in processor card slot J05 do not match. VPD ecid 04D37DF2DE7BFFFF0D081AF0DAD2, found 04DD80740E2FFFFF0A0C19D0CEBD
- 1133317778 2005.11.29 R01-M1-NC-I:J18-U01 2005-11-29-18.29.38.531250 R01-M1-NC-I:J18-U01 RAS KERNEL INFO ciod: Received signal 15, code=0, errno=0, address=0x000001f5
- 1133367847 2005.11.30 R74-M1-NB-C:J02-U11 2005-11-30-08.24.07.024349 R74-M1-NB-C:J02-U11 RAS KERNEL INFO CE sym 1, at 0x01d7eaa0, mask 0x02
- 1133409475 2005.11.30 R15-M1-NC-I:J18-U01 2005-11-30-19.57.55.220367 R15-M1-NC-I:J18-U01 RAS APP FATAL ciod: LOGIN chdir(/g/g0/spelce1/Linpack_SWL) failed: No such file or directory
- 1133409779 2005.11.30 R13-M1-NC-I:J18-U01 2005-11-30-20.02.59.033978 R13-M1-NC-I:J18-U01 RAS APP FATAL ciod: LOGIN chdir(/g/g0/spelce1/Linpack_SWL) failed: No such file or directory
- 1133447694 2005.12.01 R11-M1-ND-C:J08-U01 2005-12-01-06.34.54.203650 R11-M1-ND-C:J08-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133447703 2005.12.01 R11-M1-NA-C:J13-U01 2005-12-01-06.35.03.376951 R11-M1-NA-C:J13-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133447722 2005.12.01 R11-M1-NF-C:J07-U01 2005-12-01-06.35.22.172118 R11-M1-NF-C:J07-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133447728 2005.12.01 R06-M0-N3-C:J13-U11 2005-12-01-06.35.28.126758 R06-M0-N3-C:J13-U11 RAS KERNEL INFO 10722 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133447738 2005.12.01 R10-M0-N7-C:J08-U11 2005-12-01-06.35.38.328553 R10-M0-N7-C:J08-U11 RAS KERNEL INFO 10740 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133447761 2005.12.01 R17-M0-N8-C:J09-U11 2005-12-01-06.36.01.057694 R17-M0-N8-C:J09-U11 RAS KERNEL INFO 10744 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133447763 2005.12.01 R06-M0-N7-C:J05-U01 2005-12-01-06.36.03.872611 R06-M0-N7-C:J05-U01 RAS KERNEL INFO 10700 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133447776 2005.12.01 R16-M0-N4-C:J07-U11 2005-12-01-06.36.16.876079 R16-M0-N4-C:J07-U11 RAS KERNEL INFO 10732 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133447777 2005.12.01 R16-M0-N4-C:J06-U11 2005-12-01-06.36.17.847421 R16-M0-N4-C:J06-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133447789 2005.12.01 R07-M1-N0-C:J07-U01 2005-12-01-06.36.29.326116 R07-M1-N0-C:J07-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133447940 2005.12.01 R36-M0-NB-C:J12-U01 2005-12-01-06.39.00.882940 R36-M0-NB-C:J12-U01 RAS KERNEL INFO 10655 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133447941 2005.12.01 R13-M0-NA-C:J08-U01 2005-12-01-06.39.01.729708 R13-M0-NA-C:J08-U01 RAS KERNEL INFO 10650 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133447958 2005.12.01 R13-M1-N9-C:J02-U11 2005-12-01-06.39.18.093254 R13-M1-N9-C:J02-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133447964 2005.12.01 R34-M0-N8-C:J06-U11 2005-12-01-06.39.24.979006 R34-M0-N8-C:J06-U11 RAS KERNEL INFO 10686 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448039 2005.12.01 R55-M1-N5-C:J04-U01 2005-12-01-06.40.39.460137 R55-M1-N5-C:J04-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448052 2005.12.01 R33-M1-N7-C:J12-U01 2005-12-01-06.40.52.287178 R33-M1-N7-C:J12-U01 RAS KERNEL INFO 10674 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448053 2005.12.01 R37-M0-N3-C:J11-U11 2005-12-01-06.40.53.513066 R37-M0-N3-C:J11-U11 RAS KERNEL INFO 10722 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448074 2005.12.01 R56-M0-NA-C:J12-U11 2005-12-01-06.41.14.286536 R56-M0-NA-C:J12-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448080 2005.12.01 R56-M0-N1-C:J05-U11 2005-12-01-06.41.20.180521 R56-M0-N1-C:J05-U11 RAS KERNEL INFO 10730 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448086 2005.12.01 R37-M1-NE-C:J04-U11 2005-12-01-06.41.26.523789 R37-M1-NE-C:J04-U11 RAS KERNEL INFO 10686 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448090 2005.12.01 R50-M0-N9-C:J11-U01 2005-12-01-06.41.30.114225 R50-M0-N9-C:J11-U01 RAS KERNEL INFO 10692 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448110 2005.12.01 R56-M0-N9-C:J08-U01 2005-12-01-06.41.50.888337 R56-M0-N9-C:J08-U01 RAS KERNEL INFO 10660 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448160 2005.12.01 R37-M1-N0-C:J16-U11 2005-12-01-06.42.40.072946 R37-M1-N0-C:J16-U11 RAS KERNEL INFO 10752 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448166 2005.12.01 R32-M0-N4-C:J17-U11 2005-12-01-06.42.46.338885 R32-M0-N4-C:J17-U11 RAS KERNEL INFO 10732 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448168 2005.12.01 R57-M1-N0-C:J02-U01 2005-12-01-06.42.48.326640 R57-M1-N0-C:J02-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448168 2005.12.01 R32-M1-NC-C:J16-U11 2005-12-01-06.42.48.626892 R32-M1-NC-C:J16-U11 RAS KERNEL INFO 10676 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448263 2005.12.01 R52-M0-N5-C:J10-U01 2005-12-01-06.44.23.178101 R52-M0-N5-C:J10-U01 RAS KERNEL INFO 10752 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448278 2005.12.01 R52-M0-N1-C:J10-U11 2005-12-01-06.44.38.098317 R52-M0-N1-C:J10-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448296 2005.12.01 R72-M0-NF-C:J15-U11 2005-12-01-06.44.56.029154 R72-M0-NF-C:J15-U11 RAS KERNEL INFO 10728 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448298 2005.12.01 R73-M1-N7-C:J04-U01 2005-12-01-06.44.58.829439 R73-M1-N7-C:J04-U01 RAS KERNEL INFO 10712 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448302 2005.12.01 R75-M0-N4-C:J12-U01 2005-12-01-06.45.02.528716 R75-M0-N4-C:J12-U01 RAS KERNEL INFO 10718 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448306 2005.12.01 R53-M0-N1-C:J11-U11 2005-12-01-06.45.06.711202 R53-M0-N1-C:J11-U11 RAS KERNEL INFO 10722 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448310 2005.12.01 R76-M1-NC-C:J16-U01 2005-12-01-06.45.10.211281 R76-M1-NC-C:J16-U01 RAS KERNEL INFO 10644 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448393 2005.12.01 R66-M0-N2-C:J17-U11 2005-12-01-06.46.33.760675 R66-M0-N2-C:J17-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448397 2005.12.01 R66-M0-NB-C:J17-U11 2005-12-01-06.46.37.568406 R66-M0-NB-C:J17-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448410 2005.12.01 R41-M0-NB-C:J05-U11 2005-12-01-06.46.50.571631 R41-M0-NB-C:J05-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448439 2005.12.01 R73-M0-N2-C:J11-U11 2005-12-01-06.47.19.872148 R73-M0-N2-C:J11-U11 RAS KERNEL INFO 10738 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448448 2005.12.01 R73-M0-NE-C:J14-U01 2005-12-01-06.47.28.616018 R73-M0-NE-C:J14-U01 RAS KERNEL INFO 10646 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448466 2005.12.01 R44-M0-N3-C:J06-U01 2005-12-01-06.47.46.598443 R44-M0-N3-C:J06-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448468 2005.12.01 R77-M0-N5-C:J04-U11 2005-12-01-06.47.48.847938 R77-M0-N5-C:J04-U11 RAS KERNEL INFO 10710 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448469 2005.12.01 R61-M0-N9-C:J16-U01 2005-12-01-06.47.49.100165 R61-M0-N9-C:J16-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448479 2005.12.01 R64-M0-N2-C:J16-U01 2005-12-01-06.47.59.953454 R64-M0-N2-C:J16-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448491 2005.12.01 R45-M0-N4-C:J13-U01 2005-12-01-06.48.11.378453 R45-M0-N4-C:J13-U01 RAS KERNEL INFO 10706 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448493 2005.12.01 R64-M0-N1-C:J06-U11 2005-12-01-06.48.13.253997 R64-M0-N1-C:J06-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448498 2005.12.01 R64-M0-N8-C:J05-U11 2005-12-01-06.48.18.151322 R64-M0-N8-C:J05-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448588 2005.12.01 R46-M1-N5-C:J16-U01 2005-12-01-06.49.48.883198 R46-M1-N5-C:J16-U01 RAS KERNEL INFO 10765 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448628 2005.12.01 R42-M0-NA-C:J09-U11 2005-12-01-06.50.28.913616 R42-M0-NA-C:J09-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448653 2005.12.01 R27-M1-N9-C:J09-U01 2005-12-01-06.50.53.822651 R27-M1-N9-C:J09-U01 RAS KERNEL INFO 10698 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448656 2005.12.01 R27-M1-NA-C:J05-U11 2005-12-01-06.50.56.045860 R27-M1-NA-C:J05-U11 RAS KERNEL INFO 10744 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448658 2005.12.01 R27-M0-NE-C:J17-U11 2005-12-01-06.50.58.300024 R27-M0-NE-C:J17-U11 RAS KERNEL INFO 10740 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448674 2005.12.01 R26-M1-NF-C:J09-U01 2005-12-01-06.51.14.927478 R26-M1-NF-C:J09-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448686 2005.12.01 R45-M1-N8-C:J14-U11 2005-12-01-06.51.26.654520 R45-M1-N8-C:J14-U11 RAS KERNEL INFO 10684 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448690 2005.12.01 R26-M1-N8-C:J08-U01 2005-12-01-06.51.30.715510 R26-M1-N8-C:J08-U01 RAS KERNEL INFO 10652 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133448762 2005.12.01 R25-M0-N4-C:J13-U01 2005-12-01-06.52.42.454848 R25-M0-N4-C:J13-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448791 2005.12.01 R22-M0-ND-C:J17-U01 2005-12-01-06.53.11.512681 R22-M0-ND-C:J17-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133448800 2005.12.01 R23-M1-NF-C:J15-U01 2005-12-01-06.53.20.715143 R23-M1-NF-C:J15-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133449350 2005.12.01 R20-M1-N5-C:J04-U01 2005-12-01-07.02.30.989473 R20-M1-N5-C:J04-U01 RAS KERNEL INFO 1966 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454553 2005.12.01 R16-M0-N2-C:J07-U11 2005-12-01-08.29.13.906911 R16-M0-N2-C:J07-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454582 2005.12.01 R14-M1-NA-C:J11-U11 2005-12-01-08.29.42.047463 R14-M1-NA-C:J11-U11 RAS KERNEL INFO 10738 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454584 2005.12.01 R17-M0-N6-C:J06-U01 2005-12-01-08.29.44.223227 R17-M0-N6-C:J06-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454588 2005.12.01 R06-M0-N2-C:J09-U01 2005-12-01-08.29.48.814199 R06-M0-N2-C:J09-U01 RAS KERNEL INFO 10714 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454591 2005.12.01 R15-M0-N3-C:J09-U11 2005-12-01-08.29.51.750461 R15-M0-N3-C:J09-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454594 2005.12.01 R04-M0-N6-C:J10-U01 2005-12-01-08.29.54.636421 R04-M0-N6-C:J10-U01 RAS KERNEL INFO 10718 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454601 2005.12.01 R14-M1-N1-C:J10-U01 2005-12-01-08.30.01.182162 R14-M1-N1-C:J10-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454611 2005.12.01 R16-M1-ND-C:J05-U11 2005-12-01-08.30.11.278697 R16-M1-ND-C:J05-U11 RAS KERNEL INFO 10698 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454624 2005.12.01 R00-M0-N4-C:J09-U11 2005-12-01-08.30.24.223851 R00-M0-N4-C:J09-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454630 2005.12.01 R00-M0-N0-C:J10-U01 2005-12-01-08.30.30.752844 R00-M0-N0-C:J10-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454634 2005.12.01 R01-M0-NC-C:J12-U01 2005-12-01-08.30.34.342940 R01-M0-NC-C:J12-U01 RAS KERNEL INFO 10644 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454715 2005.12.01 R11-M1-N3-C:J10-U01 2005-12-01-08.31.55.274517 R11-M1-N3-C:J10-U01 RAS KERNEL INFO 10752 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454722 2005.12.01 R11-M1-ND-C:J09-U01 2005-12-01-08.32.02.378980 R11-M1-ND-C:J09-U01 RAS KERNEL INFO 10698 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454732 2005.12.01 R07-M1-N7-C:J07-U01 2005-12-01-08.32.12.649613 R07-M1-N7-C:J07-U01 RAS KERNEL INFO 10666 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454734 2005.12.01 R07-M1-N7-C:J14-U01 2005-12-01-08.32.14.399124 R07-M1-N7-C:J14-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454737 2005.12.01 R50-M1-NA-C:J11-U11 2005-12-01-08.32.17.451081 R50-M1-NA-C:J11-U11 RAS KERNEL INFO 10738 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454739 2005.12.01 R05-M1-N2-C:J08-U01 2005-12-01-08.32.19.913638 R05-M1-N2-C:J08-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454748 2005.12.01 R36-M1-NA-C:J05-U11 2005-12-01-08.32.28.589350 R36-M1-NA-C:J05-U11 RAS KERNEL INFO 10744 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454756 2005.12.01 R07-M0-NF-C:J13-U11 2005-12-01-08.32.36.360568 R07-M0-NF-C:J13-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454783 2005.12.01 R05-M0-N5-C:J08-U11 2005-12-01-08.33.03.466525 R05-M0-N5-C:J08-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454886 2005.12.01 R32-M0-NB-C:J14-U01 2005-12-01-08.34.46.529035 R32-M0-NB-C:J14-U01 RAS KERNEL INFO 10663 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454886 2005.12.01 R35-M0-N5-C:J16-U01 2005-12-01-08.34.46.559803 R35-M0-N5-C:J16-U01 RAS KERNEL INFO 10766 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133454908 2005.12.01 R32-M1-N7-C:J02-U01 2005-12-01-08.35.08.088826 R32-M1-N7-C:J02-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454956 2005.12.01 R56-M1-NE-C:J11-U01 2005-12-01-08.35.56.512508 R56-M1-NE-C:J11-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454968 2005.12.01 R55-M1-NE-C:J16-U01 2005-12-01-08.36.08.709539 R55-M1-NE-C:J16-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133454977 2005.12.01 R35-M0-NC-C:J15-U11 2005-12-01-08.36.17.734289 R35-M0-NC-C:J15-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455006 2005.12.01 R55-M1-N8-C:J03-U11 2005-12-01-08.36.46.691884 R55-M1-N8-C:J03-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455058 2005.12.01 R53-M1-N9-C:J11-U11 2005-12-01-08.37.38.478575 R53-M1-N9-C:J11-U11 RAS KERNEL INFO 10722 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455064 2005.12.01 R52-M0-N5-C:J04-U01 2005-12-01-08.37.44.951513 R52-M0-N5-C:J04-U01 RAS KERNEL INFO 10680 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455066 2005.12.01 R77-M1-N7-C:J08-U11 2005-12-01-08.37.46.082541 R77-M1-N7-C:J08-U11 RAS KERNEL INFO 10740 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455087 2005.12.01 R73-M1-N5-C:J07-U01 2005-12-01-08.38.07.010836 R73-M1-N5-C:J07-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455110 2005.12.01 R71-M1-N3-C:J08-U01 2005-12-01-08.38.30.513495 R71-M1-N3-C:J08-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455122 2005.12.01 R53-M1-N8-C:J08-U11 2005-12-01-08.38.42.253020 R53-M1-N8-C:J08-U11 RAS KERNEL INFO 10684 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455125 2005.12.01 R71-M0-NF-C:J11-U11 2005-12-01-08.38.45.935400 R71-M0-NF-C:J11-U11 RAS KERNEL INFO CE sym 4, at 0x0589f8e0, mask 0x10
- 1133455130 2005.12.01 R74-M1-NB-C:J09-U01 2005-12-01-08.38.50.542867 R74-M1-NB-C:J09-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455140 2005.12.01 R71-M0-NC-C:J14-U01 2005-12-01-08.39.00.094017 R71-M0-NC-C:J14-U01 RAS KERNEL INFO 10616 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455156 2005.12.01 R76-M1-N4-C:J06-U11 2005-12-01-08.39.16.294851 R76-M1-N4-C:J06-U11 RAS KERNEL INFO 10754 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455159 2005.12.01 R60-M0-N4-C:J05-U11 2005-12-01-08.39.19.744712 R60-M0-N4-C:J05-U11 RAS KERNEL INFO 10668 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455262 2005.12.01 R61-M0-N7-C:J14-U11 2005-12-01-08.41.02.009575 R61-M0-N7-C:J14-U11 RAS KERNEL INFO 10708 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455272 2005.12.01 R65-M0-NE-C:J10-U11 2005-12-01-08.41.12.059696 R65-M0-NE-C:J10-U11 RAS KERNEL INFO 10676 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455288 2005.12.01 R41-M1-NF-C:J04-U01 2005-12-01-08.41.28.414439 R41-M1-NF-C:J04-U01 RAS KERNEL INFO 10640 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455300 2005.12.01 R66-M0-N9-C:J08-U11 2005-12-01-08.41.40.303515 R66-M0-N9-C:J08-U11 RAS KERNEL INFO 10666 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455302 2005.12.01 R70-M0-N8-C:J12-U11 2005-12-01-08.41.42.720184 R70-M0-N8-C:J12-U11 RAS KERNEL INFO 10676 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455305 2005.12.01 R66-M0-N5-C:J13-U11 2005-12-01-08.41.45.677601 R66-M0-N5-C:J13-U11 RAS KERNEL INFO 10722 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455319 2005.12.01 R40-M0-N8-C:J06-U11 2005-12-01-08.41.59.328614 R40-M0-N8-C:J06-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455322 2005.12.01 R66-M0-NC-C:J04-U11 2005-12-01-08.42.02.759879 R66-M0-NC-C:J04-U11 RAS KERNEL INFO 10684 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455455 2005.12.01 R44-M0-N3-C:J05-U01 2005-12-01-08.44.15.799524 R44-M0-N3-C:J05-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455467 2005.12.01 R40-M1-NE-C:J10-U11 2005-12-01-08.44.27.738774 R40-M1-NE-C:J10-U11 RAS KERNEL INFO 10676 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455483 2005.12.01 R43-M0-N2-C:J09-U01 2005-12-01-08.44.43.581956 R43-M0-N2-C:J09-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455488 2005.12.01 R25-M0-N7-C:J07-U01 2005-12-01-08.44.48.531704 R25-M0-N7-C:J07-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455494 2005.12.01 R44-M1-NC-C:J03-U01 2005-12-01-08.44.54.498009 R44-M1-NC-C:J03-U01 RAS KERNEL INFO 10714 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455498 2005.12.01 R45-M1-NC-C:J07-U01 2005-12-01-08.44.58.904349 R45-M1-NC-C:J07-U01 RAS KERNEL INFO 10712 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455511 2005.12.01 R25-M1-ND-C:J08-U01 2005-12-01-08.45.11.599274 R25-M1-ND-C:J08-U01 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455517 2005.12.01 R43-M1-ND-C:J08-U01 2005-12-01-08.45.17.404927 R43-M1-ND-C:J08-U01 RAS KERNEL INFO 10636 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455621 2005.12.01 R26-M0-N9-C:J06-U01 2005-12-01-08.47.01.109678 R26-M0-N9-C:J06-U01 RAS KERNEL INFO 10664 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455631 2005.12.01 R24-M0-N5-C:J03-U11 2005-12-01-08.47.11.934066 R24-M0-N5-C:J03-U11 RAS KERNEL INFO 10730 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455641 2005.12.01 R27-M1-N6-C:J11-U11 2005-12-01-08.47.21.734190 R27-M1-N6-C:J11-U11 RAS KERNEL INFO 10722 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455641 2005.12.01 R23-M0-ND-C:J11-U11 2005-12-01-08.47.21.966726 R23-M0-ND-C:J11-U11 RAS KERNEL INFO 10722 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455642 2005.12.01 R23-M0-ND-C:J08-U11 2005-12-01-08.47.22.963148 R23-M0-ND-C:J08-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455643 2005.12.01 R22-M0-N5-C:J06-U11 2005-12-01-08.47.23.541583 R22-M0-N5-C:J06-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455648 2005.12.01 R23-M0-N2-C:J09-U11 2005-12-01-08.47.28.894064 R23-M0-N2-C:J09-U11 RAS KERNEL INFO 10736 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133455649 2005.12.01 R23-M1-N5-C:J07-U11 2005-12-01-08.47.29.441454 R23-M1-N5-C:J07-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455654 2005.12.01 R24-M0-N6-C:J05-U11 2005-12-01-08.47.34.385432 R24-M0-N6-C:J05-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133455660 2005.12.01 R24-M0-N9-C:J10-U11 2005-12-01-08.47.40.003136 R24-M0-N9-C:J10-U11 RAS KERNEL INFO 0 microseconds spent in the rbs signal handler during 0 calls. 0 microseconds was the maximum time for a single instance of a correctable ddr.
- 1133458296 2005.12.01 R00-M0-NA-C:J14-U11 2005-12-01-09.31.36.346830 R00-M0-NA-C:J14-U11 RAS KERNEL INFO 720 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133458363 2005.12.01 R00-M0-N8-C:J17-U01 2005-12-01-09.32.43.947020 R00-M0-N8-C:J17-U01 RAS KERNEL INFO 720 total interrupts. 0 critical input interrupts. 0 microseconds total spent on critical input interrupts, 0 microseconds max time in a critical input interrupt.
- 1133474785 2005.12.01 R47-M1-N4-C:J17-U11 2005-12-01-14.06.25.612025 R47-M1-N4-C:J17-U11 RAS KERNEL INFO CE sym 27, at 0x00284720, mask 0x11
- 1133475318 2005.12.01 R20-M1-NB-C:J09-U01 2005-12-01-14.15.18.446368 R20-M1-NB-C:J09-U01 RAS KERNEL INFO CE sym 9, at 0x0021d9c0, mask 0x20
- 1133573986 2005.12.02 R66-M0-N1-C:J03-U01 2005-12-02-17.39.46.428006 R66-M0-N1-C:J03-U01 RAS KERNEL INFO CE sym 15, at 0x06a72120, mask 0x04
- 1133625592 2005.12.03 R06-M1-NF-C:J12-U01 2005-12-03-07.59.52.640827 R06-M1-NF-C:J12-U01 RAS KERNEL INFO 1 ddr error(s) detected and corrected on rank 0, symbol 24 over 335 seconds
- 1133630999 2005.12.03 R10-M1-N2 2005-12-03-09.29.59.645290 R10-M1-N2 NULL HARDWARE WARNING PrepareForService shutting down Node card(mLctn(R10-M1-N2), mCardSernum(203231503833343000000000594c31304b34323934303257), mLp(FF:F2:9F:16:E0:DA:00:0D:60:E9:1F:25), mIp(10.1.1.164), mType(4)) as part of Service Action 648
- 1133638373 2005.12.03 R56-M0-NA-C:J15-U11 2005-12-03-11.32.53.165837 R56-M0-NA-C:J15-U11 RAS KERNEL INFO Kernel detected 35591540 integer alignment exceptions (35591533) iar 0x0023f108, dear 0x1feaa260 (35591534) iar 0x00265564, dear 0x1feaa1c0 (35591535) iar 0x00265574, dear 0x1feaa1e0 (35591536) iar 0x00265578, dear 0x1feaa200 (35591537) iar 0x00265588, dear 0x1feaa220 (35591538) iar 0x0026558c, dear 0x1feaa240 (35591539) iar 0x00265594, dear 0x1feaa260 (35591540) iar 0x00265598, dear 0x1feaa280
- 1133646587 2005.12.03 R55-M0-N9-C:J06-U11 2005-12-03-13.49.47.382429 R55-M0-N9-C:J06-U11 RAS KERNEL INFO 1 tree receiver 1 in re-synch state event(s) (dcr 0x0185) detected over 244 seconds
- 1133691676 2005.12.04 R65-M1-NA-C:J14-U01 2005-12-04-02.21.16.848710 R65-M1-NA-C:J14-U01 RAS KERNEL INFO CE sym 27, at 0x1b70b860, mask 0x80
- 1133700378 2005.12.04 R60-M0-N2-C:J12-U11 2005-12-04-04.46.18.276872 R60-M0-N2-C:J12-U11 RAS KERNEL INFO CE sym 33, at 0x1ff2fc60, mask 0x04
- 1133700386 2005.12.04 R21-M1-N4-C:J10-U01 2005-12-04-04.46.26.869771 R21-M1-N4-C:J10-U01 RAS KERNEL INFO CE sym 12, at 0x18e33e80, mask 0x04
- 1133706305 2005.12.04 R74-M0-N1-C:J05-U01 2005-12-04-06.25.05.145493 R74-M0-N1-C:J05-U01 RAS KERNEL INFO CE sym 30, at 0x042358a0, mask 0x20
- 1133707549 2005.12.04 R60-M0-ND-C:J07-U11 2005-12-04-06.45.49.687761 R60-M0-ND-C:J07-U11 RAS KERNEL INFO CE sym 11, at 0x06366640, mask 0x04
- 1133709964 2005.12.04 R35-M1-N9-C:J14-U01 2005-12-04-07.26.04.629795 R35-M1-N9-C:J14-U01 RAS KERNEL INFO CE sym 19, at 0x057c7e00, mask 0x20
- 1133715641 2005.12.04 R23-M0-NC-C:J05-U01 2005-12-04-09.00.41.190112 R23-M0-NC-C:J05-U01 RAS KERNEL INFO 159 ddr error(s) detected and corrected on rank 0, symbol 29 over 2486 seconds
- 1133715641 2005.12.04 R23-M0-NC-C:J05-U01 2005-12-04-09.00.41.990496 R23-M0-NC-C:J05-U01 RAS KERNEL INFO CE sym 29, at 0x0ea9fd60, mask 0x40
- 1133715868 2005.12.04 R06-M1-N6-C:J08-U01 2005-12-04-09.04.28.582086 R06-M1-N6-C:J08-U01 RAS KERNEL INFO total of 1 ddr error(s) detected and corrected over 2487 seconds
- 1133721087 2005.12.04 R71-M0-N4-C:J14-U01 2005-12-04-10.31.27.742650 R71-M0-N4-C:J14-U01 RAS KERNEL FATAL minus normalized number..................0
KERNMNTF 1133726737 2005.12.04 R46-M0-N8-I:J18-U11 2005-12-04-12.05.37.993474 R46-M0-N8-I:J18-U11 RAS KERNEL FATAL Lustre mount FAILED : bglio617 : block_id : location
- 1133835041 2005.12.05 R52-M1-NA-C:J03-U01 2005-12-05-18.10.41.610617 R52-M1-NA-C:J03-U01 RAS KERNEL FATAL fraction rounded.........................0
- 1133892304 2005.12.06 R12-M0-NC 2005-12-06-10.05.04.300635 R12-M0-NC NULL DISCOVERY WARNING Node card is not fully functional
- 1133913870 2005.12.06 R01-M0-N0-C:J04-U01 2005-12-06-16.04.30.188673 R01-M0-N0-C:J04-U01 RAS KERNEL INFO critical input interrupt (unit=0x0b bit=0x17): warning for tree C1 wire, suppressing further interrupts of same type
- 1133919747 2005.12.06 R03-M0-N7-C:J07-U01 2005-12-06-17.42.27.383145 R03-M0-N7-C:J07-U01 RAS KERNEL FATAL size of scratchpad portion of L3.........0 (0M)
- 1133925845 2005.12.06 R36-M0-N2-C:J05-U01 2005-12-06-19.24.05.504553 R36-M0-N2-C:J05-U01 RAS KERNEL INFO Kernel detected 3830884 integer alignment exceptions (3830877) iar 0x00544ea8, dear 0x01fc1ba0 (3830878) iar 0x00544eb8, dear 0x01fc1bc0 (3830879) iar 0x00544ea8, dear 0x01fc1be0 (3830880) iar 0x00544eb8, dear 0x01fc1c00 (3830881) iar 0x00544ee0, dear 0x01fc1c20 (3830882) iar 0x00544ef0, dear 0x01fc1c40 (3830883) iar 0x00544ee0, dear 0x01fc1c60 (3830884) iar 0x00544ef0, dear 0x01fc1c80
- 1133926478 2005.12.06 R27-M0-NB-C:J08-U01 2005-12-06-19.34.38.822749 R27-M0-NB-C:J08-U01 RAS KERNEL INFO Kernel detected 3853444 integer alignment exceptions (3853437) iar 0x00544ea8, dear 0x01ef5e20 (3853438) iar 0x00544eb8, dear 0x01ef5e40 (3853439) iar 0x00544ea8, dear 0x01ef5e60 (3853440) iar 0x00544eb8, dear 0x01ef5e80 (3853441) iar 0x00544ee0, dear 0x01ef5ea0 (3853442) iar 0x00544ef0, dear 0x01ef5ec0 (3853443) iar 0x00544ee0, dear 0x01ef5ee0 (3853444) iar 0x00544ef0, dear 0x01ef5f00
- 1133926732 2005.12.06 R27-M1-NF-C:J17-U01 2005-12-06-19.38.52.768522 R27-M1-NF-C:J17-U01 RAS KERNEL INFO Kernel detected 3747792 integer alignment exceptions (3747785) iar 0x00544ea8, dear 0x01fc9220 (3747786) iar 0x00544eb8, dear 0x01fc9240 (3747787) iar 0x00544ea8, dear 0x01fc9260 (3747788) iar 0x00544eb8, dear 0x01fc9280 (3747789) iar 0x00544ee0, dear 0x01fc92a0 (3747790) iar 0x00544ef0, dear 0x01fc92c0 (3747791) iar 0x00544ee0, dear 0x01fc92e0 (3747792) iar 0x00544ef0, dear 0x01fc9300
- 1133928125 2005.12.06 R36-M0-N7-C:J09-U11 2005-12-06-20.02.05.596871 R36-M0-N7-C:J09-U11 RAS KERNEL INFO Kernel detected 3946290 integer alignment exceptions (3946283) iar 0x00544ea8, dear 0x01ef6080 (3946284) iar 0x00544eb8, dear 0x01ef60a0 (3946285) iar 0x00544ea8, dear 0x01ef60c0 (3946286) iar 0x00544eb8, dear 0x01ef60e0 (3946287) iar 0x00544ee0, dear 0x01ef6100 (3946288) iar 0x00544ef0, dear 0x01ef6120 (3946289) iar 0x00544ee0, dear 0x01ef6140 (3946290) iar 0x00544ef0, dear 0x01ef6160
- 1133928278 2005.12.06 R26-M0-N9-C:J11-U01 2005-12-06-20.04.38.539848 R26-M0-N9-C:J11-U01 RAS KERNEL INFO Kernel detected 4909176 integer alignment exceptions (4909169) iar 0x00544ea8, dear 0x01fc8a40 (4909170) iar 0x00544eb8, dear 0x01fc8a60 (4909171) iar 0x00544ea8, dear 0x01fc8a80 (4909172) iar 0x00544eb8, dear 0x01fc8aa0 (4909173) iar 0x00544ee0, dear 0x01fc8ac0 (4909174) iar 0x00544ef0, dear 0x01fc8ae0 (4909175) iar 0x00544ee0, dear 0x01fc8b00 (4909176) iar 0x00544ef0, dear 0x01fc8b20
- 1133928323 2005.12.06 R31-M0-NF-C:J07-U11 2005-12-06-20.05.23.094186 R31-M0-NF-C:J07-U11 RAS KERNEL INFO Kernel detected 4898338 integer alignment exceptions (4898331) iar 0x00544ea8, dear 0x049e75e0 (4898332) iar 0x00544eb8, dear 0x049e7600 (4898333) iar 0x00544ea8, dear 0x049e7620 (4898334) iar 0x00544eb8, dear 0x049e7640 (4898335) iar 0x00544ee0, dear 0x049e7660 (4898336) iar 0x00544ef0, dear 0x049e7680 (4898337) iar 0x00544ee0, dear 0x049e76a0 (4898338) iar 0x00544ef0, dear 0x049e76c0
- 1133928447 2005.12.06 R37-M1-N1-C:J02-U01 2005-12-06-20.07.27.653573 R37-M1-N1-C:J02-U01 RAS KERNEL INFO Kernel detected 3945740 integer alignment exceptions (3945733) iar 0x00544ea8, dear 0x01ef7960 (3945734) iar 0x00544eb8, dear 0x01ef7980 (3945735) iar 0x00544ea8, dear 0x01ef79a0 (3945736) iar 0x00544eb8, dear 0x01ef79c0 (3945737) iar 0x00544ee0, dear 0x01ef79e0 (3945738) iar 0x00544ef0, dear 0x01ef7a00 (3945739) iar 0x00544ee0, dear 0x01ef7a20 (3945740) iar 0x00544ef0, dear 0x01ef7a40
- 1133928743 2005.12.06 R36-M0-NE-C:J13-U01 2005-12-06-20.12.23.534341 R36-M0-NE-C:J13-U01 RAS KERNEL INFO Kernel detected 3486044 integer alignment exceptions (3486037) iar 0x00544ea8, dear 0x01ef7c60 (3486038) iar 0x00544eb8, dear 0x01ef7c80 (3486039) iar 0x00544ea8, dear 0x01ef7ca0 (3486040) iar 0x00544eb8, dear 0x01ef7cc0 (3486041) iar 0x00544ee0, dear 0x01ef7ce0 (3486042) iar 0x00544ef0, dear 0x01ef7d00 (3486043) iar 0x00544ee0, dear 0x01ef7d20 (3486044) iar 0x00544ef0, dear 0x01ef7d40
- 1134109056 2005.12.08 R64-M1-NC-I:J18-U01 2005-12-08-22.17.36.597185 R64-M1-NC-I:J18-U01 RAS APP FATAL ciod: Error loading /g/g90/glosli/src/ddcMD/ddcMD1.1.17/bin/ddcMDbglV: invalid or missing program image, No such file or directory
- 1134109138 2005.12.08 R20-M1-N0-I:J18-U11 2005-12-08-22.18.58.098120 R20-M1-N0-I:J18-U11 RAS APP FATAL ciod: Error loading /g/g90/glosli/src/ddcMD/ddcMD1.1.17/bin/ddcMDbglV: invalid or missing program image, No such file or directory
- 1134117490 2005.12.09 R70-M0-N7-C:J15-U01 2005-12-09-00.38.10.632331 R70-M0-N7-C:J15-U01 RAS KERNEL INFO 1 ddr error(s) detected and corrected on rank 0, symbol 9 over 986 seconds
- 1134130421 2005.12.09 R34-M1-NB-C:J17-U11 2005-12-09-04.13.41.810977 R34-M1-NB-C:J17-U11 RAS KERNEL INFO total of 5 ddr error(s) detected and corrected over 9108 seconds
- 1134159256 2005.12.09 R20-M1-N4-I:J18-U01 2005-12-09-12.14.16.349690 R20-M1-N4-I:J18-U01 RAS APP FATAL ciod: Error loading /p/gb1/stella/SPPM/1322/sppm_DD: invalid or missing program image, Permission denied
- 1134159495 2005.12.09 R27-M0-N4-I:J18-U11 2005-12-09-12.18.15.322330 R27-M0-N4-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/SWL/stability/MDCASK/WORK/1383/inferno: invalid or missing program image, Permission denied
- 1134159559 2005.12.09 R03-M0-N0-I:J18-U11 2005-12-09-12.19.19.226546 R03-M0-N0-I:J18-U11 RAS APP FATAL ciod: Error loading /p/gb1/stella/UMT2K/1372/umt2k_DD: invalid or missing program image, Permission denied
- 1134160405 2005.12.09 R06-M0-N4-I:J18-U01 2005-12-09-12.33.25.851921 R06-M0-N4-I:J18-U01 RAS APP FATAL ciod: Error loading /bgl/apps/SWL/stability/NEWS05/news05_DD: invalid or missing program image, Permission denied
- 1134160534 2005.12.09 R61-M1-N8-I:J18-U01 2005-12-09-12.35.34.641975 R61-M1-N8-I:J18-U01 RAS APP FATAL ciod: Error loading /p/gb1/stella/UMT2K/1325/umt2k_DD: invalid or missing program image, Permission denied
- 1134160697 2005.12.09 R64-M0-N8-I:J18-U11 2005-12-09-12.38.17.827307 R64-M0-N8-I:J18-U11 RAS APP FATAL ciod: Error loading /p/gb1/stella/UMT2K/1369/umt2k_DD: invalid or missing program image, Permission denied
- 1134161333 2005.12.09 R61-M0-N0-I:J18-U11 2005-12-09-12.48.53.105166 R61-M0-N0-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/SWL/stability/HPL/WORK/1498./hpl_DD: invalid or missing program image, No such file or directory
- 1134161388 2005.12.09 R07-M0-N8-I:J18-U11 2005-12-09-12.49.48.361835 R07-M0-N8-I:J18-U11 RAS APP FATAL ciod: Error loading /bgl/apps/SWL/stability/HPL/WORK/1501./hpl_DD: invalid or missing program image, No such file or directory
- 1134259774 2005.12.10 R15-M1-NE-C:J06-U01 2005-12-10-16.09.34.106950 R15-M1-NE-C:J06-U01 RAS KERNEL INFO 28 ddr error(s) detected and corrected on rank 0, symbol 21 over 11562 seconds
APPOUT 1134357549 2005.12.11 R63-M1-N0-I:J18-U11 2005-12-11-19.19.09.737509 R63-M1-N0-I:J18-U11 RAS APP FATAL ciod: LOGIN chdir(/p/gb1/stella/RAPTOR/2183) failed: Input/output error
APPSEV 1134496355 2005.12.13 R74-M1-N8-I:J18-U11 2005-12-13-09.52.35.527182 R74-M1-N8-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:55929, Link has been severed
APPSEV 1134496398 2005.12.13 R44-M0-N4-I:J18-U01 2005-12-13-09.53.18.260783 R44-M0-N4-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:56170, Link has been severed
- 1134530536 2005.12.13 R74-M1-N0-C:J12-U01 2005-12-13-19.22.16.368819 R74-M1-N0-C:J12-U01 RAS KERNEL INFO 2 ddr error(s) detected and corrected on rank 0, symbol 28 over 3365 seconds
- 1134599601 2005.12.14 R76-M1-N0-I:J18-U01 2005-12-14-14.33.21.188706 R76-M1-N0-I:J18-U01 RAS APP FATAL ciod: Error loading /g/g0/spelce1/Tuned/SPaSM-base/rundir/SPaSM.baseline: invalid or missing program image, Permission denied
- 1134601394 2005.12.14 R42-M1-N2-C:J03-U11 2005-12-14-15.03.14.212440 R42-M1-N2-C:J03-U11 RAS KERNEL INFO critical input interrupt (unit=0x0b bit=0x0a): warning for torus z+ wire, suppressing further interrupts of same type
- 1134623156 2005.12.14 R61-M0-N8-C:J17-U11 2005-12-14-21.05.56.145306 R61-M0-N8-C:J17-U11 RAS KERNEL INFO 2 ddr error(s) detected and corrected on rank 0, symbol 34 over 2738 seconds
- 1134626183 2005.12.14 R74-M1-N0-C:J12-U01 2005-12-14-21.56.23.782819 R74-M1-N0-C:J12-U01 RAS KERNEL INFO 5 ddr error(s) detected and corrected on rank 0, symbol 28 over 2946 seconds
APPSEV 1134630981 2005.12.14 R37-M1-N8-I:J18-U11 2005-12-14-23.16.21.740107 R37-M1-N8-I:J18-U11 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:49934, Link has been severed
APPSEV 1134631019 2005.12.14 R32-M0-NC-I:J18-U01 2005-12-14-23.16.59.947676 R32-M0-NC-I:J18-U01 RAS APP FATAL ciod: Error reading message prefix on CioStream socket to 172.16.96.116:50288, Link has been severed
- 1134748483 2005.12.16 R60-M1-N7-C:J17-U01 2005-12-16-07.54.43.245870 R60-M1-N7-C:J17-U01 RAS KERNEL INFO total of 20 ddr error(s) detected and corrected over 22070 seconds
- 1134784680 2005.12.16 R66-M0-N8-I:J18-U01 2005-12-16-17.58.00.577715 R66-M0-N8-I:J18-U01 RAS KERNEL INFO ciod: pollControlDescriptors: Detected the debugger died.
- 1134929456 2005.12.18 R30-M1-N3-C:J13-U01 2005-12-18-10.10.56.366499 R30-M1-N3-C:J13-U01 RAS KERNEL INFO 2 L3 EDRAM error(s) (dcr 0x0157) detected and corrected over 282 seconds
- 1134954903 2005.12.18 R13-M1-N1-C:J03-U01 2005-12-18-17.15.03.147145 R13-M1-N1-C:J03-U01 RAS KERNEL INFO total of 3 ddr error(s) detected and corrected over 8732 seconds
- 1135050698 2005.12.19 R77-M1-NC-I:J18-U01 2005-12-19-19.51.38.151363 R77-M1-NC-I:J18-U01 RAS KERNEL INFO ciod: pollControlDescriptors: Detected the debugger died.
- 1135095320 2005.12.20 R45-M0-N4-I:J18-U01 2005-12-20-08.15.20.957109 R45-M0-N4-I:J18-U01 RAS KERNEL INFO ciod: generated 128 core files for program /g/g24/germann2/SPaSM_static/SPaSM_mpi
- 1135178837 2005.12.21 R62-M0-N4-I:J18-U01 2005-12-21-07.27.17.749308 R62-M0-N4-I:J18-U01 RAS APP FATAL ciod: Error loading /g/g24/buber/Yunsic/BlueGene/partad.develf/taddriver.32.exe: program image too big, 361544528 > 266076160
- 1135579635 2005.12.25 R62-M1-NA-C:J04-U01 2005-12-25-22.47.15.963417 R62-M1-NA-C:J04-U01 RAS KERNEL FATAL fpr29=0xffffffff ffffffff ffffffff ffffffff
- 1135602839 2005.12.26 R61-M0-N3-C:J11-U11 2005-12-26-05.13.59.265193 R61-M0-N3-C:J11-U11 RAS KERNEL FATAL Machine State Register: 0x0002f900
2015-10-18 18:01:47,978 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1445144423722_0020_000001
2015-10-18 18:01:48,963 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Executing with tokens:
2015-10-18 18:01:48,963 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 20 cluster_timestamp: 1445144423722 } attemptId: 1 } keyId: -127633188)
2015-10-18 18:01:49,228 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Using mapred newApiCommitter.
2015-10-18 18:01:50,353 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter set in config null
2015-10-18 18:01:50,509 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2015-10-18 18:01:50,556 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.jobhistory.EventType for class org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler
2015-10-18 18:01:50,556 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher
2015-10-18 18:01:50,556 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher
2015-10-18 18:01:50,556 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher
2015-10-18 18:01:50,572 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventType for class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler
2015-10-18 18:01:50,572 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.speculate.Speculator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher
2015-10-18 18:01:50,572 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocator$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter
2015-10-18 18:01:50,588 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncher$EventType for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter
2015-10-18 18:01:50,634 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
2015-10-18 18:01:50,666 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
2015-10-18 18:01:50,713 INFO [main] org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils: Default file system [hdfs://msra-sa-41:9000]
2015-10-18 18:01:50,728 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Emitting job history data to the timeline server is not enabled
2015-10-18 18:01:50,806 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.job.event.JobFinishEvent$Type for class org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler
2015-10-18 18:01:51,197 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-18 18:01:51,306 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-18 18:01:51,306 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MRAppMaster metrics system started
2015-10-18 18:01:51,322 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Adding job token for job_1445144423722_0020 to jobTokenSecretManager
2015-10-18 18:01:51,619 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Not uberizing job_1445144423722_0020 because: not enabled; too many maps; too much input;
2015-10-18 18:01:51,650 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Input size for job job_1445144423722_0020 = 1256521728. Number of splits = 10
2015-10-18 18:01:51,650 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Number of reduces for job job_1445144423722_0020 = 1
2015-10-18 18:01:51,650 INFO [main] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1445144423722_0020Job Transitioned from NEW to INITED
2015-10-18 18:01:51,650 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster launching normal, non-uberized, multi-container job job_1445144423722_0020.
2015-10-18 18:01:51,713 INFO [main] org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-18 18:01:51,775 INFO [Socket Reader #1 for port 62260] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 62260
2015-10-18 18:01:51,791 INFO [main] org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server
2015-10-18 18:01:51,791 INFO [main] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Instantiated MRClientService at MININT-FNANLI5.fareast.corp.microsoft.com/10.86.169.121:62260
2015-10-18 18:01:51,806 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-18 18:01:51,806 INFO [IPC Server listener on 62260] org.apache.hadoop.ipc.Server: IPC Server listener on 62260: starting
2015-10-18 18:01:51,885 INFO [main] org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-18 18:01:51,900 INFO [main] org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.mapreduce is not defined
2015-10-18 18:01:51,916 INFO [main] org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-18 18:01:51,916 INFO [main] org.apache.hadoop.http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce
2015-10-18 18:01:51,916 INFO [main] org.apache.hadoop.http.HttpServer2: Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static
2015-10-18 18:01:51,947 INFO [main] org.apache.hadoop.http.HttpServer2: adding path spec: /mapreduce/*
2015-10-18 18:01:51,947 INFO [main] org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2015-10-18 18:01:51,963 INFO [main] org.apache.hadoop.http.HttpServer2: Jetty bound to port 62267
2015-10-18 18:01:51,963 INFO [main] org.mortbay.log: jetty-6.1.26
2015-10-18 18:01:52,088 INFO [main] org.mortbay.log: Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\Users\msrabi\AppData\Local\Temp\Jetty_0_0_0_0_62267_mapreduce____.8n7xum\webapp
2015-10-18 18:01:52,728 INFO [main] org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:62267
2015-10-18 18:01:52,728 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Web app /mapreduce started at 62267
2015-10-18 18:01:53,400 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2015-10-18 18:01:53,400 INFO [main] org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-18 18:01:53,447 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: JOB_CREATE job_1445144423722_0020
2015-10-18 18:01:53,447 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true
2015-10-18 18:01:53,447 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3
2015-10-18 18:01:53,447 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33
2015-10-18 18:01:53,510 INFO [Socket Reader #1 for port 62270] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 62270
2015-10-18 18:01:53,510 INFO [IPC Server listener on 62270] org.apache.hadoop.ipc.Server: IPC Server listener on 62270: starting
2015-10-18 18:01:53,510 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-18 18:01:53,557 INFO [main] org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at msra-sa-41/10.190.173.170:8030
2015-10-18 18:01:53,713 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: maxContainerCapability: <memory:8192, vCores:32>
2015-10-18 18:01:53,713 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: queue: default
2015-10-18 18:01:53,713 INFO [main] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Upper limit on the thread pool size is 500
2015-10-18 18:01:53,713 INFO [main] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
2015-10-18 18:01:53,744 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1445144423722_0020Job Transitioned from INITED to SETUP
2015-10-18 18:01:53,775 INFO [CommitterEvent Processor #0] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: JOB_SETUP
2015-10-18 18:01:53,822 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1445144423722_0020Job Transitioned from SETUP to RUNNING
2015-10-18 18:01:53,838 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000000 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000001 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000002 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000003 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000004 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,869 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000005 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000006 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000007 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000008 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000009 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_r_000000 Task Transitioned from NEW to SCHEDULED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000001_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000002_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000003_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000004_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000005_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000006_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000007_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000008_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000009_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_r_000000_0 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:01:53,885 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: mapResourceRequest:<memory:1024, vCores:1>
2015-10-18 18:01:53,900 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1445144423722_0020, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job_1445144423722_0020_1.jhist
2015-10-18 18:01:53,900 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: reduceResourceRequest:<memory:1024, vCores:1>
2015-10-18 18:01:54,791 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:10 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0
2015-10-18 18:01:54,838 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:10240, vCores:-17> knownNMs=4
2015-10-18 18:01:54,853 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:10240, vCores:-17>
2015-10-18 18:01:54,853 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:01:55,854 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:10240, vCores:-17>
2015-10-18 18:01:55,854 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:01:56,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
2015-10-18 18:01:56,916 INFO [RMCommunicator Allocator] org.apache.hadoop.yarn.util.RackResolver: Resolved 04DN8IQ.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:56,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445144423722_0020_01_000002 to attempt_1445144423722_0020_m_000000_0
2015-10-18 18:01:56,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:8192, vCores:-19>
2015-10-18 18:01:56,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:01:56,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:9 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:1 ContRel:0 HostLocal:0 RackLocal:1
2015-10-18 18:01:57,072 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved 04DN8IQ.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:57,104 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-jar file on the remote FS is hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job.jar
2015-10-18 18:01:57,119 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job.xml
2015-10-18 18:01:57,119 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Adding #0 tokens and #1 secret keys for NM use for launching container
2015-10-18 18:01:57,119 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Size of containertokens_dob is 1
2015-10-18 18:01:57,119 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Putting shuffle token in serviceData
2015-10-18 18:01:57,260 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2015-10-18 18:01:57,291 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000002 taskAttempt attempt_1445144423722_0020_m_000000_0
2015-10-18 18:01:57,291 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445144423722_0020_m_000000_0
2015-10-18 18:01:57,291 INFO [ContainerLauncher #0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : 04DN8IQ.fareast.corp.microsoft.com:54883
2015-10-18 18:01:57,447 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445144423722_0020_m_000000_0 : 13562
2015-10-18 18:01:57,447 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445144423722_0020_m_000000_0] using containerId: [container_1445144423722_0020_01_000002 on NM: [04DN8IQ.fareast.corp.microsoft.com:54883]
2015-10-18 18:01:57,447 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2015-10-18 18:01:57,447 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445144423722_0020_m_000000
2015-10-18 18:01:57,447 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000000 Task Transitioned from SCHEDULED to RUNNING
2015-10-18 18:01:57,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:7168, vCores:-20> knownNMs=4
2015-10-18 18:01:57,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:7168, vCores:-20>
2015-10-18 18:01:57,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:01:58,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
2015-10-18 18:01:58,916 INFO [RMCommunicator Allocator] org.apache.hadoop.yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:58,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445144423722_0020_01_000003 to attempt_1445144423722_0020_m_000001_0
2015-10-18 18:01:58,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:5120, vCores:-22>
2015-10-18 18:01:58,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:01:58,916 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:8 ScheduledReds:0 AssignedMaps:2 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:2 ContRel:0 HostLocal:0 RackLocal:2
2015-10-18 18:01:58,916 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:01:58,916 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000001_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2015-10-18 18:01:58,916 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000003 taskAttempt attempt_1445144423722_0020_m_000001_0
2015-10-18 18:01:58,916 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445144423722_0020_m_000001_0
2015-10-18 18:01:58,916 INFO [ContainerLauncher #1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:52368
2015-10-18 18:01:58,963 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445144423722_0020_m_000001_0 : 13562
2015-10-18 18:01:58,963 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445144423722_0020_m_000001_0] using containerId: [container_1445144423722_0020_01_000003 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:52368]
2015-10-18 18:01:58,963 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000001_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2015-10-18 18:01:58,963 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445144423722_0020_m_000001
2015-10-18 18:01:58,963 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000001 Task Transitioned from SCHEDULED to RUNNING
2015-10-18 18:01:59,948 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:4096, vCores:-23> knownNMs=4
2015-10-18 18:01:59,948 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:4096, vCores:-23>
2015-10-18 18:01:59,948 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:00,963 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
2015-10-18 18:02:00,963 INFO [RMCommunicator Allocator] org.apache.hadoop.yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:02:00,963 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445144423722_0020_01_000004 to attempt_1445144423722_0020_m_000002_0
2015-10-18 18:02:00,963 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:3072, vCores:-24>
2015-10-18 18:02:00,963 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:00,963 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:7 ScheduledReds:0 AssignedMaps:3 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:3 ContRel:0 HostLocal:0 RackLocal:3
2015-10-18 18:02:00,963 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MININT-FNANLI5.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:02:00,963 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000002_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2015-10-18 18:02:00,979 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000004 taskAttempt attempt_1445144423722_0020_m_000002_0
2015-10-18 18:02:00,979 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445144423722_0020_m_000002_0
2015-10-18 18:02:00,979 INFO [ContainerLauncher #2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:52368
2015-10-18 18:02:01,041 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445144423722_0020_m_000002_0 : 13562
2015-10-18 18:02:01,041 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445144423722_0020_m_000002_0] using containerId: [container_1445144423722_0020_01_000004 on NM: [MININT-FNANLI5.fareast.corp.microsoft.com:52368]
2015-10-18 18:02:01,041 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000002_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2015-10-18 18:02:01,041 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445144423722_0020_m_000002
2015-10-18 18:02:01,041 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000002 Task Transitioned from SCHEDULED to RUNNING
2015-10-18 18:02:02,026 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:1024, vCores:-26> knownNMs=4
2015-10-18 18:02:02,026 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:1024, vCores:-26>
2015-10-18 18:02:02,026 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:02,104 INFO [Socket Reader #1 for port 62270] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1445144423722_0020 (auth:SIMPLE)
2015-10-18 18:02:02,510 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445144423722_0020_m_000002 asked for a task
2015-10-18 18:02:02,510 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445144423722_0020_m_000002 given task: attempt_1445144423722_0020_m_000000_0
2015-10-18 18:02:03,041 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:1024, vCores:-26>
2015-10-18 18:02:03,041 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:04,167 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:04,167 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:05,229 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:05,229 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:05,651 INFO [Socket Reader #1 for port 62270] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1445144423722_0020 (auth:SIMPLE)
2015-10-18 18:02:05,870 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445144423722_0020_m_000003 asked for a task
2015-10-18 18:02:05,870 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445144423722_0020_m_000003 given task: attempt_1445144423722_0020_m_000001_0
2015-10-18 18:02:06,557 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:06,557 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:07,573 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:07,573 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:08,636 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:08,636 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:09,667 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:09,667 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:10,667 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:10,667 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:11,714 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:11,714 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:12,745 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:12,745 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:12,808 INFO [Socket Reader #1 for port 62270] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1445144423722_0020 (auth:SIMPLE)
2015-10-18 18:02:12,855 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445144423722_0020_m_000004 asked for a task
2015-10-18 18:02:12,855 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445144423722_0020_m_000004 given task: attempt_1445144423722_0020_m_000002_0
2015-10-18 18:02:13,902 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:13,902 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:14,933 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:14,933 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:15,949 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:15,949 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:17,027 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.023958297
2015-10-18 18:02:17,058 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:17,058 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:18,105 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:18,105 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:19,152 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:19,152 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:20,168 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:20,168 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:20,558 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.065791264
2015-10-18 18:02:21,261 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:21,261 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:21,699 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.05713628
2015-10-18 18:02:22,402 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:22,402 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:23,465 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:23,465 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:24,527 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:24,527 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:24,543 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10004553
2015-10-18 18:02:24,949 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.079464614
2015-10-18 18:02:25,058 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.06741504
2015-10-18 18:02:25,590 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:25,590 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:26,668 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:26,668 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:27,746 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:27,746 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:27,965 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10635664
2015-10-18 18:02:28,856 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:28,856 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:28,965 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.10501281
2015-10-18 18:02:29,090 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.103304505
2015-10-18 18:02:29,918 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:29,918 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:30,949 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:30,949 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:31,668 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10635664
2015-10-18 18:02:31,996 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:31,996 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:32,434 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.1066108
2015-10-18 18:02:32,715 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.10660437
2015-10-18 18:02:33,012 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:33,012 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:34,043 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:34,043 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:35,090 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:35,090 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:35,231 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10635664
2015-10-18 18:02:35,809 INFO [IPC Server handler 11 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.1066108
2015-10-18 18:02:36,137 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:36,137 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:36,215 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.10660437
2015-10-18 18:02:37,169 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:37,169 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:38,200 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:38,200 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:38,762 INFO [IPC Server handler 28 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10635664
2015-10-18 18:02:39,200 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.1066108
2015-10-18 18:02:39,216 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:39,216 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:39,622 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.10660437
2015-10-18 18:02:40,247 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:40,247 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:41,309 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:41,309 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:42,200 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10635664
2015-10-18 18:02:42,341 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:42,341 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:43,060 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.1066108
2015-10-18 18:02:43,263 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.10660437
2015-10-18 18:02:43,388 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:43,388 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:44,419 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:44,419 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:45,450 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:45,450 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:45,841 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10635664
2015-10-18 18:02:46,482 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:46,482 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:46,575 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.1066108
2015-10-18 18:02:46,685 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.10660437
2015-10-18 18:02:47,497 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
2015-10-18 18:02:47,497 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445144423722_0020_01_000005 to attempt_1445144423722_0020_m_000003_0
2015-10-18 18:02:47,497 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:47,513 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:47,513 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:6 ScheduledReds:0 AssignedMaps:4 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:4 ContRel:0 HostLocal:1 RackLocal:3
2015-10-18 18:02:47,513 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:02:47,513 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000003_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2015-10-18 18:02:47,685 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000005 taskAttempt attempt_1445144423722_0020_m_000003_0
2015-10-18 18:02:47,685 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445144423722_0020_m_000003_0
2015-10-18 18:02:47,685 INFO [ContainerLauncher #3] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:7109
2015-10-18 18:02:47,841 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445144423722_0020_m_000003_0 : 13562
2015-10-18 18:02:47,841 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445144423722_0020_m_000003_0] using containerId: [container_1445144423722_0020_01_000005 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:7109]
2015-10-18 18:02:47,841 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000003_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2015-10-18 18:02:47,841 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445144423722_0020_m_000003
2015-10-18 18:02:47,841 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000003 Task Transitioned from SCHEDULED to RUNNING
2015-10-18 18:02:48,529 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
2015-10-18 18:02:48,544 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:48,544 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:49,372 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10635664
2015-10-18 18:02:49,576 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:49,576 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:49,997 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.1066108
2015-10-18 18:02:50,216 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.10660437
2015-10-18 18:02:50,513 INFO [Socket Reader #1 for port 62270] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1445144423722_0020 (auth:SIMPLE)
2015-10-18 18:02:50,560 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445144423722_0020_m_000005 asked for a task
2015-10-18 18:02:50,560 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445144423722_0020_m_000005 given task: attempt_1445144423722_0020_m_000003_0
2015-10-18 18:02:50,622 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:50,622 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:51,654 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:51,654 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:52,716 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:52,716 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:52,841 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10635664
2015-10-18 18:02:53,513 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.1066108
2015-10-18 18:02:53,685 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.10660437
2015-10-18 18:02:53,795 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:53,795 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:54,826 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:54,826 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:55,857 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:55,857 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:56,185 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10635664
2015-10-18 18:02:56,951 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:56,951 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:57,060 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.10875365
2015-10-18 18:02:57,139 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.10660437
2015-10-18 18:02:57,639 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.106493875
2015-10-18 18:02:58,014 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:58,014 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:59,061 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:02:59,061 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:02:59,654 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.10635664
2015-10-18 18:03:00,123 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:00,123 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:00,248 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.14981231
2015-10-18 18:03:00,264 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.13841225
2015-10-18 18:03:00,717 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.106493875
2015-10-18 18:03:01,201 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:01,201 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:02,279 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:02,279 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:03,154 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.1430594
2015-10-18 18:03:03,373 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:03,373 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:03,576 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.19084874
2015-10-18 18:03:03,795 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.106493875
2015-10-18 18:03:03,795 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.17293417
2015-10-18 18:03:04,451 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:04,451 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:05,514 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:05,514 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:06,561 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:06,561 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:06,702 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.1795899
2015-10-18 18:03:06,827 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.17982168
2015-10-18 18:03:07,077 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.19211523
2015-10-18 18:03:07,358 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.19212553
2015-10-18 18:03:07,608 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:07,608 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:08,655 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:08,655 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:09,702 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:09,702 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:09,858 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.19209063
2015-10-18 18:03:10,171 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.19158794
2015-10-18 18:03:10,561 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.19211523
2015-10-18 18:03:10,733 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:10,733 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:10,733 INFO [IPC Server handler 11 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.19212553
2015-10-18 18:03:11,764 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:11,764 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:12,796 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:12,796 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:12,874 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.19209063
2015-10-18 18:03:13,577 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.19158794
2015-10-18 18:03:13,827 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:13,827 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:13,983 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.19211523
2015-10-18 18:03:14,155 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.19212553
2015-10-18 18:03:14,843 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:14,843 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:15,858 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:15,858 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:15,921 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.20639901
2015-10-18 18:03:16,905 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:16,905 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:17,296 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.19158794
2015-10-18 18:03:17,562 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.19211523
2015-10-18 18:03:17,874 INFO [IPC Server handler 16 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.19212553
2015-10-18 18:03:17,968 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:17,968 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:18,968 INFO [IPC Server handler 16 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.27765483
2015-10-18 18:03:18,999 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:18,999 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:20,077 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:20,077 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:20,890 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.19158794
2015-10-18 18:03:21,062 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.19211523
2015-10-18 18:03:21,327 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:21,327 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:21,609 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.19212553
2015-10-18 18:03:22,109 INFO [IPC Server handler 3 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.27765483
2015-10-18 18:03:22,437 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:22,437 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:23,484 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:23,484 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:24,390 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.19158794
2015-10-18 18:03:24,546 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:24,546 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:24,562 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.19211523
2015-10-18 18:03:25,281 INFO [IPC Server handler 6 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.27765483
2015-10-18 18:03:25,406 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.19212553
2015-10-18 18:03:25,953 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:25,953 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:26,984 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:26,984 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:27,875 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.19158794
2015-10-18 18:03:28,000 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.19211523
2015-10-18 18:03:28,109 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:28,109 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:28,297 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.358454
2015-10-18 18:03:28,937 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.19212553
2015-10-18 18:03:29,297 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:29,297 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:30,531 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:30,531 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:31,328 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.36323506
2015-10-18 18:03:31,391 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.19158794
2015-10-18 18:03:31,656 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:31,656 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:32,250 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.19211523
2015-10-18 18:03:32,641 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.19212553
2015-10-18 18:03:32,984 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:32,984 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:34,188 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:34,188 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:34,422 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.36323506
2015-10-18 18:03:34,844 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.19158794
2015-10-18 18:03:35,250 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:35,250 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:35,625 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.20757815
2015-10-18 18:03:36,063 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.19212553
2015-10-18 18:03:36,297 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:36,297 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:37,391 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:37,391 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:37,532 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.36323506
2015-10-18 18:03:38,360 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.19158794
2015-10-18 18:03:38,500 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:38,500 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:38,875 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.24035259
2015-10-18 18:03:39,219 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.22765201
2015-10-18 18:03:39,625 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:39,625 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:40,610 INFO [IPC Server handler 29 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.4486067
2015-10-18 18:03:40,688 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:40,688 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:41,766 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:41,766 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:41,907 INFO [IPC Server handler 23 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.19716828
2015-10-18 18:03:42,391 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.26542902
2015-10-18 18:03:42,719 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.26314905
2015-10-18 18:03:42,907 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:42,907 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:43,766 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.4486067
2015-10-18 18:03:44,048 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:44,048 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:45,063 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:45,063 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:45,345 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.23859076
2015-10-18 18:03:45,876 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.27776006
2015-10-18 18:03:46,157 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:46,157 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:46,282 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.27772525
2015-10-18 18:03:46,813 INFO [IPC Server handler 29 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.4486067
2015-10-18 18:03:47,173 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:47,173 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:48,204 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:48,204 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:49,189 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.27776006
2015-10-18 18:03:49,235 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:49,235 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:49,360 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.2754006
2015-10-18 18:03:49,735 INFO [IPC Server handler 29 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.27772525
2015-10-18 18:03:49,845 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.5091932
2015-10-18 18:03:50,267 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:50,267 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:51,298 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:51,298 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:52,298 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:52,298 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:52,611 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.27776006
2015-10-18 18:03:52,861 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.27696857
2015-10-18 18:03:52,876 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.5343203
2015-10-18 18:03:53,111 INFO [IPC Server handler 23 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.27772525
2015-10-18 18:03:53,329 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:53,329 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:54,361 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:54,361 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:55,408 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 2
2015-10-18 18:03:55,408 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445144423722_0020_01_000006 to attempt_1445144423722_0020_m_000004_0
2015-10-18 18:03:55,408 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445144423722_0020_01_000007 to attempt_1445144423722_0020_m_000005_0
2015-10-18 18:03:55,408 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:55,408 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:55,408 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:4 ScheduledReds:0 AssignedMaps:6 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:6 ContRel:0 HostLocal:3 RackLocal:3
2015-10-18 18:03:55,408 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:03:55,408 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000004_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2015-10-18 18:03:55,423 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:03:55,423 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000005_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2015-10-18 18:03:55,517 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000006 taskAttempt attempt_1445144423722_0020_m_000004_0
2015-10-18 18:03:55,517 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445144423722_0020_m_000004_0
2015-10-18 18:03:55,517 INFO [ContainerLauncher #4] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:7109
2015-10-18 18:03:55,595 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000007 taskAttempt attempt_1445144423722_0020_m_000005_0
2015-10-18 18:03:55,595 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445144423722_0020_m_000005_0
2015-10-18 18:03:55,595 INFO [ContainerLauncher #5] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:7109
2015-10-18 18:03:55,923 INFO [ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445144423722_0020_m_000004_0 : 13562
2015-10-18 18:03:55,939 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445144423722_0020_m_000004_0] using containerId: [container_1445144423722_0020_01_000006 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:7109]
2015-10-18 18:03:55,939 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000004_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2015-10-18 18:03:55,939 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445144423722_0020_m_000004
2015-10-18 18:03:55,939 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000004 Task Transitioned from SCHEDULED to RUNNING
2015-10-18 18:03:55,939 INFO [ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445144423722_0020_m_000005_0 : 13562
2015-10-18 18:03:55,939 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.5343203
2015-10-18 18:03:55,939 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445144423722_0020_m_000005_0] using containerId: [container_1445144423722_0020_01_000007 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:7109]
2015-10-18 18:03:55,939 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000005_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2015-10-18 18:03:55,939 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445144423722_0020_m_000005
2015-10-18 18:03:55,939 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000005 Task Transitioned from SCHEDULED to RUNNING
2015-10-18 18:03:56,064 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.27776006
2015-10-18 18:03:56,361 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.27696857
2015-10-18 18:03:56,455 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=4 release= 0 newContainers=1 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
2015-10-18 18:03:56,455 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
2015-10-18 18:03:56,455 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445144423722_0020_01_000008 to attempt_1445144423722_0020_m_000006_0
2015-10-18 18:03:56,455 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:56,455 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:56,455 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:3 ScheduledReds:0 AssignedMaps:7 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:7 ContRel:0 HostLocal:4 RackLocal:3
2015-10-18 18:03:56,455 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:03:56,455 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000006_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2015-10-18 18:03:56,501 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.27772525
2015-10-18 18:03:56,548 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000008 taskAttempt attempt_1445144423722_0020_m_000006_0
2015-10-18 18:03:56,548 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445144423722_0020_m_000006_0
2015-10-18 18:03:56,548 INFO [ContainerLauncher #6] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:7109
2015-10-18 18:03:56,798 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445144423722_0020_m_000006_0 : 13562
2015-10-18 18:03:56,798 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445144423722_0020_m_000006_0] using containerId: [container_1445144423722_0020_01_000008 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:7109]
2015-10-18 18:03:56,798 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000006_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2015-10-18 18:03:56,798 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445144423722_0020_m_000006
2015-10-18 18:03:56,798 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000006 Task Transitioned from SCHEDULED to RUNNING
2015-10-18 18:03:57,502 INFO [Socket Reader #1 for port 62270] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1445144423722_0020 (auth:SIMPLE)
2015-10-18 18:03:57,533 INFO [Socket Reader #1 for port 62270] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1445144423722_0020 (auth:SIMPLE)
2015-10-18 18:03:57,564 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445144423722_0020_m_000007 asked for a task
2015-10-18 18:03:57,564 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445144423722_0020_m_000006 asked for a task
2015-10-18 18:03:57,564 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445144423722_0020_m_000007 given task: attempt_1445144423722_0020_m_000005_0
2015-10-18 18:03:57,564 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445144423722_0020_m_000006 given task: attempt_1445144423722_0020_m_000004_0
2015-10-18 18:03:57,564 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
2015-10-18 18:03:57,564 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:57,564 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:58,533 INFO [Socket Reader #1 for port 62270] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1445144423722_0020 (auth:SIMPLE)
2015-10-18 18:03:58,611 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445144423722_0020_m_000008 asked for a task
2015-10-18 18:03:58,611 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445144423722_0020_m_000008 given task: attempt_1445144423722_0020_m_000006_0
2015-10-18 18:03:58,611 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:58,611 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:59,017 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.5343203
2015-10-18 18:03:59,502 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.27776006
2015-10-18 18:03:59,642 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:03:59,642 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:03:59,892 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.27772525
2015-10-18 18:04:00,049 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.27696857
2015-10-18 18:04:00,658 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:00,658 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:01,721 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:01,721 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:02,080 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.5806522
2015-10-18 18:04:02,752 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:02,752 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:03,143 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.27776006
2015-10-18 18:04:03,471 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.27772525
2015-10-18 18:04:03,549 INFO [IPC Server handler 19 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.27696857
2015-10-18 18:04:03,799 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:03,799 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:04,877 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.10685723
2015-10-18 18:04:04,877 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
2015-10-18 18:04:04,877 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445144423722_0020_01_000009 to attempt_1445144423722_0020_m_000007_0
2015-10-18 18:04:04,877 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:04,877 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:04,877 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:2 ScheduledReds:0 AssignedMaps:8 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:8 ContRel:0 HostLocal:5 RackLocal:3
2015-10-18 18:04:04,877 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:04:04,877 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000007_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2015-10-18 18:04:04,986 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.10680563
2015-10-18 18:04:04,986 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000009 taskAttempt attempt_1445144423722_0020_m_000007_0
2015-10-18 18:04:04,986 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445144423722_0020_m_000007_0
2015-10-18 18:04:04,986 INFO [ContainerLauncher #7] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:7109
2015-10-18 18:04:05,127 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.6199081
2015-10-18 18:04:05,127 INFO [ContainerLauncher #7] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445144423722_0020_m_000007_0 : 13562
2015-10-18 18:04:05,127 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445144423722_0020_m_000007_0] using containerId: [container_1445144423722_0020_01_000009 on NM: [MSRA-SA-41.fareast.corp.microsoft.com:7109]
2015-10-18 18:04:05,127 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000007_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2015-10-18 18:04:05,127 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445144423722_0020_m_000007
2015-10-18 18:04:05,127 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000007 Task Transitioned from SCHEDULED to RUNNING
2015-10-18 18:04:05,830 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.106964506
2015-10-18 18:04:05,893 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
2015-10-18 18:04:05,893 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:05,893 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:06,502 INFO [IPC Server handler 23 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.27776006
2015-10-18 18:04:06,861 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.27772525
2015-10-18 18:04:06,940 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:06,940 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:06,955 INFO [Socket Reader #1 for port 62270] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1445144423722_0020 (auth:SIMPLE)
2015-10-18 18:04:06,971 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445144423722_0020_m_000009 asked for a task
2015-10-18 18:04:06,971 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445144423722_0020_m_000009 given task: attempt_1445144423722_0020_m_000007_0
2015-10-18 18:04:06,971 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.27696857
2015-10-18 18:04:07,908 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.10685723
2015-10-18 18:04:07,955 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
2015-10-18 18:04:07,955 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445144423722_0020_01_000010 to attempt_1445144423722_0020_m_000008_0
2015-10-18 18:04:07,955 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:07,955 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:07,955 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:1 ScheduledReds:0 AssignedMaps:9 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:9 ContRel:0 HostLocal:6 RackLocal:3
2015-10-18 18:04:07,955 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:04:07,955 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000008_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2015-10-18 18:04:08,018 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.10680563
2015-10-18 18:04:08,065 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000010 taskAttempt attempt_1445144423722_0020_m_000008_0
2015-10-18 18:04:08,065 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445144423722_0020_m_000008_0
2015-10-18 18:04:08,065 INFO [ContainerLauncher #8] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:28345
2015-10-18 18:04:08,190 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.6199081
2015-10-18 18:04:08,205 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445144423722_0020_m_000008_0 : 13562
2015-10-18 18:04:08,205 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445144423722_0020_m_000008_0] using containerId: [container_1445144423722_0020_01_000010 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:28345]
2015-10-18 18:04:08,205 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000008_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2015-10-18 18:04:08,205 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445144423722_0020_m_000008
2015-10-18 18:04:08,205 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000008 Task Transitioned from SCHEDULED to RUNNING
2015-10-18 18:04:08,877 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.106964506
2015-10-18 18:04:08,987 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=4 release= 0 newContainers=1 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
2015-10-18 18:04:08,987 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
2015-10-18 18:04:08,987 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1445144423722_0020_01_000011 to attempt_1445144423722_0020_m_000009_0
2015-10-18 18:04:08,987 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:08,987 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:08,987 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:10 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:10 ContRel:0 HostLocal:7 RackLocal:3
2015-10-18 18:04:08,987 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:04:08,987 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000009_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2015-10-18 18:04:09,096 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1445144423722_0020_01_000011 taskAttempt attempt_1445144423722_0020_m_000009_0
2015-10-18 18:04:09,096 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1445144423722_0020_m_000009_0
2015-10-18 18:04:09,096 INFO [ContainerLauncher #9] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-39.fareast.corp.microsoft.com:28345
2015-10-18 18:04:09,268 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1445144423722_0020_m_000009_0 : 13562
2015-10-18 18:04:09,268 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1445144423722_0020_m_000009_0] using containerId: [container_1445144423722_0020_01_000011 on NM: [MSRA-SA-39.fareast.corp.microsoft.com:28345]
2015-10-18 18:04:09,268 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000009_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2015-10-18 18:04:09,268 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1445144423722_0020_m_000009
2015-10-18 18:04:09,268 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000009 Task Transitioned from SCHEDULED to RUNNING
2015-10-18 18:04:09,955 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.27776006
2015-10-18 18:04:10,002 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=4 release= 0 newContainers=1 finishedContainers=0 resourcelimit=<memory:0, vCores:-27> knownNMs=4
2015-10-18 18:04:10,002 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 1
2015-10-18 18:04:10,002 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Cannot assign container Container: [ContainerId: container_1445144423722_0020_01_000012, NodeId: MSRA-SA-39.fareast.corp.microsoft.com:28345, NodeHttpAddress: MSRA-SA-39.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 172.22.149.145:28345 }, ] for a map as either  container memory less than required <memory:1024, vCores:1> or no pending map tasks - maps.isEmpty=true
2015-10-18 18:04:10,002 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:10,002 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:10,002 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:10 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:11 ContRel:1 HostLocal:7 RackLocal:3
2015-10-18 18:04:10,315 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.27772525
2015-10-18 18:04:10,424 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.27696857
2015-10-18 18:04:10,940 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.10685723
2015-10-18 18:04:11,034 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=0 release= 1 newContainers=0 finishedContainers=1 resourcelimit=<memory:1024, vCores:-26> knownNMs=4
2015-10-18 18:04:11,034 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445144423722_0020_01_000012
2015-10-18 18:04:11,034 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Container complete event for unknown container id container_1445144423722_0020_01_000012
2015-10-18 18:04:11,034 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:1024, vCores:-26>
2015-10-18 18:04:11,034 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:11,049 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.10680563
2015-10-18 18:04:11,237 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.6199081
2015-10-18 18:04:11,612 INFO [Socket Reader #1 for port 62270] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1445144423722_0020 (auth:SIMPLE)
2015-10-18 18:04:11,643 INFO [IPC Server handler 23 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445144423722_0020_m_000010 asked for a task
2015-10-18 18:04:11,643 INFO [IPC Server handler 23 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445144423722_0020_m_000010 given task: attempt_1445144423722_0020_m_000008_0
2015-10-18 18:04:11,924 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.106964506
2015-10-18 18:04:12,065 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:12,065 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1
2015-10-18 18:04:12,112 INFO [Socket Reader #1 for port 62270] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1445144423722_0020 (auth:SIMPLE)
2015-10-18 18:04:12,143 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1445144423722_0020_m_000011 asked for a task
2015-10-18 18:04:12,143 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1445144423722_0020_m_000011 given task: attempt_1445144423722_0020_m_000009_0
2015-10-18 18:04:13,346 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.29115766
2015-10-18 18:04:13,596 INFO [IPC Server handler 23 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.6199081
2015-10-18 18:04:13,674 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.2898827
2015-10-18 18:04:13,971 INFO [IPC Server handler 11 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.27696857
2015-10-18 18:04:14,018 INFO [IPC Server handler 27 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.19247705
2015-10-18 18:04:14,159 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.18529637
2015-10-18 18:04:14,159 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.10681946
2015-10-18 18:04:14,315 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.667
2015-10-18 18:04:15,049 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.19266446
2015-10-18 18:04:16,440 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.31981927
2015-10-18 18:04:16,768 INFO [IPC Server handler 19 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.32958937
2015-10-18 18:04:17,128 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.19247705
2015-10-18 18:04:17,237 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.19242907
2015-10-18 18:04:17,237 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.10681946
2015-10-18 18:04:17,362 INFO [IPC Server handler 29 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.27696857
2015-10-18 18:04:17,425 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.667
2015-10-18 18:04:18,190 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.19266446
2015-10-18 18:04:18,987 INFO [IPC Server handler 3 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.106881365
2015-10-18 18:04:19,753 INFO [IPC Server handler 19 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.295472
2015-10-18 18:04:20,206 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.19247705
2015-10-18 18:04:20,206 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.3512319
2015-10-18 18:04:20,362 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.19242907
2015-10-18 18:04:20,362 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.10681946
2015-10-18 18:04:20,394 INFO [IPC Server handler 29 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.36317363
2015-10-18 18:04:20,550 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.667
2015-10-18 18:04:20,800 INFO [IPC Server handler 22 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.27696857
2015-10-18 18:04:21,237 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.19266446
2015-10-18 18:04:22,019 INFO [IPC Server handler 3 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.106881365
2015-10-18 18:04:22,784 INFO [IPC Server handler 19 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.295472
2015-10-18 18:04:23,269 INFO [IPC Server handler 15 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.19247705
2015-10-18 18:04:23,409 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.19242907
2015-10-18 18:04:23,409 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.1337379
2015-10-18 18:04:23,581 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.6854124
2015-10-18 18:04:23,706 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.36319977
2015-10-18 18:04:23,972 INFO [IPC Server handler 3 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.36317363
2015-10-18 18:04:24,238 INFO [IPC Server handler 6 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.29998285
2015-10-18 18:04:24,284 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.19266446
2015-10-18 18:04:25,034 INFO [IPC Server handler 6 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.106881365
2015-10-18 18:04:25,785 INFO [IPC Server handler 19 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.295472
2015-10-18 18:04:26,300 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.27813601
2015-10-18 18:04:26,488 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.25258622
2015-10-18 18:04:26,488 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.19255035
2015-10-18 18:04:26,660 INFO [IPC Server handler 29 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.7198902
2015-10-18 18:04:27,160 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.36319977
2015-10-18 18:04:27,269 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.36317363
2015-10-18 18:04:27,316 INFO [IPC Server handler 28 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.27613032
2015-10-18 18:04:27,706 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.33995733
2015-10-18 18:04:28,082 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.106881365
2015-10-18 18:04:28,800 INFO [IPC Server handler 19 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.295472
2015-10-18 18:04:29,332 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.27813601
2015-10-18 18:04:29,503 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.2781602
2015-10-18 18:04:29,503 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.19255035
2015-10-18 18:04:29,707 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.7607953
2015-10-18 18:04:30,379 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.2783809
2015-10-18 18:04:30,644 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.36319977
2015-10-18 18:04:30,816 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.36317363
2015-10-18 18:04:31,082 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.106881365
2015-10-18 18:04:31,300 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.3624012
2015-10-18 18:04:31,847 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.295472
2015-10-18 18:04:32,363 INFO [IPC Server handler 28 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.27813601
2015-10-18 18:04:32,566 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.2781602
2015-10-18 18:04:32,566 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.19255035
2015-10-18 18:04:32,738 INFO [IPC Server handler 29 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.80356
2015-10-18 18:04:33,410 INFO [IPC Server handler 15 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.2783809
2015-10-18 18:04:34,098 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.36319977
2015-10-18 18:04:34,098 INFO [IPC Server handler 23 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.18216328
2015-10-18 18:04:34,363 INFO [IPC Server handler 3 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.36317363
2015-10-18 18:04:34,832 INFO [IPC Server handler 29 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.3624012
2015-10-18 18:04:34,894 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.4956123
2015-10-18 18:04:35,410 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.32285523
2015-10-18 18:04:35,660 INFO [IPC Server handler 11 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.25511068
2015-10-18 18:04:35,660 INFO [IPC Server handler 11 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.2781602
2015-10-18 18:04:35,816 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.83331466
2015-10-18 18:04:36,441 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.29597813
2015-10-18 18:04:37,098 INFO [IPC Server handler 23 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.19258286
2015-10-18 18:04:37,754 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.36319977
2015-10-18 18:04:37,957 INFO [IPC Server handler 29 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.5323719
2015-10-18 18:04:37,957 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.36317363
2015-10-18 18:04:38,473 INFO [IPC Server handler 6 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.3624012
2015-10-18 18:04:38,473 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.36390656
2015-10-18 18:04:38,723 INFO [IPC Server handler 17 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.27825075
2015-10-18 18:04:38,723 INFO [IPC Server handler 21 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.36388028
2015-10-18 18:04:38,848 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.87672114
2015-10-18 18:04:39,457 INFO [IPC Server handler 3 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.36404583
2015-10-18 18:04:40,129 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.19258286
2015-10-18 18:04:40,957 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.5323719
2015-10-18 18:04:41,270 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.36319977
2015-10-18 18:04:41,489 INFO [IPC Server handler 22 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.36390656
2015-10-18 18:04:41,535 INFO [IPC Server handler 3 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.36317363
2015-10-18 18:04:41,739 INFO [IPC Server handler 17 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.36388028
2015-10-18 18:04:41,739 INFO [IPC Server handler 17 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.27825075
2015-10-18 18:04:41,879 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.9185183
2015-10-18 18:04:42,145 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.3624012
2015-10-18 18:04:42,536 INFO [IPC Server handler 22 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.36404583
2015-10-18 18:04:43,395 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.19258286
2015-10-18 18:04:44,036 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.5323719
2015-10-18 18:04:44,598 INFO [IPC Server handler 22 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.36390656
2015-10-18 18:04:44,864 INFO [IPC Server handler 17 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.36319977
2015-10-18 18:04:44,864 INFO [IPC Server handler 21 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.36388028
2015-10-18 18:04:44,879 INFO [IPC Server handler 7 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.27825075
2015-10-18 18:04:45,176 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.36317363
2015-10-18 18:04:45,176 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.9543898
2015-10-18 18:04:45,567 INFO [IPC Server handler 22 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.36404583
2015-10-18 18:04:45,754 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.3624012
2015-10-18 18:04:45,754 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.5323719
2015-10-18 18:04:46,661 INFO [IPC Server handler 22 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.27811313
2015-10-18 18:04:47,255 INFO [IPC Server handler 19 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.667
2015-10-18 18:04:47,942 INFO [IPC Server handler 21 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.43890014
2015-10-18 18:04:47,942 INFO [IPC Server handler 21 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.36388028
2015-10-18 18:04:47,942 INFO [IPC Server handler 21 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.35880664
2015-10-18 18:04:48,395 INFO [IPC Server handler 24 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 0.9854844
2015-10-18 18:04:48,864 INFO [IPC Server handler 3 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.4156165
2015-10-18 18:04:49,177 INFO [IPC Server handler 11 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.3624012
2015-10-18 18:04:49,895 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.27811313
2015-10-18 18:04:49,895 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000003_0 is : 1.0
2015-10-18 18:04:50,208 INFO [IPC Server handler 11 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.36319977
2015-10-18 18:04:50,208 INFO [IPC Server handler 26 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Done acknowledgement from attempt_1445144423722_0020_m_000003_0
2015-10-18 18:04:50,208 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000003_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
2015-10-18 18:04:50,208 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445144423722_0020_01_000005 taskAttempt attempt_1445144423722_0020_m_000003_0
2015-10-18 18:04:50,208 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445144423722_0020_m_000003_0
2015-10-18 18:04:50,208 INFO [ContainerLauncher #0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MSRA-SA-41.fareast.corp.microsoft.com:7109
2015-10-18 18:04:50,286 INFO [IPC Server handler 25 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.3673702
2015-10-18 18:04:50,286 INFO [IPC Server handler 16 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.667
2015-10-18 18:04:50,755 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000003_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
2015-10-18 18:04:50,755 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Task succeeded with attempt attempt_1445144423722_0020_m_000003_0
2015-10-18 18:04:50,755 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1445144423722_0020_m_000003 Task Transitioned from RUNNING to SUCCEEDED
2015-10-18 18:04:50,770 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Num completed Tasks: 1
2015-10-18 18:04:51,114 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:10 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:11 ContRel:1 HostLocal:7 RackLocal:3
2015-10-18 18:04:51,114 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:0, vCores:-27>
2015-10-18 18:04:51,114 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold reached. Scheduling reduces.
2015-10-18 18:04:51,114 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: All maps assigned. Ramping up all remaining reduces:1
2015-10-18 18:04:51,114 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:0 ScheduledReds:1 AssignedMaps:10 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:11 ContRel:1 HostLocal:7 RackLocal:3
2015-10-18 18:04:51,145 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.3638923
2015-10-18 18:04:51,145 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.44950968
2015-10-18 18:04:51,145 INFO [IPC Server handler 28 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.44968578
2015-10-18 18:04:51,755 INFO [DefaultSpeculator background processing] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445144423722_0020_m_000000
2015-10-18 18:04:51,755 INFO [DefaultSpeculator background processing] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: We launched 1 speculations.  Sleeping 15000 milliseconds.
2015-10-18 18:04:51,755 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Scheduling a redundant attempt for task task_1445144423722_0020_m_000000
2015-10-18 18:04:51,755 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:04:51,755 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:04:51,755 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000000_1 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:04:52,161 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:1 AssignedMaps:10 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:11 ContRel:1 HostLocal:7 RackLocal:3
2015-10-18 18:04:52,286 INFO [IPC Server handler 11 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.44980705
2015-10-18 18:04:52,552 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1445144423722_0020: ask=5 release= 0 newContainers=0 finishedContainers=1 resourcelimit=<memory:0, vCores:-27> knownNMs=4
2015-10-18 18:04:52,552 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1445144423722_0020_01_000005
2015-10-18 18:04:52,552 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:0 ScheduledMaps:1 ScheduledReds:1 AssignedMaps:9 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:11 ContRel:1 HostLocal:7 RackLocal:3
2015-10-18 18:04:52,552 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445144423722_0020_m_000003_0: Container killed by the ApplicationMaster.
2015-10-18 18:04:52,880 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.3624012
2015-10-18 18:04:53,286 INFO [IPC Server handler 25 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37225527
2015-10-18 18:04:53,364 INFO [IPC Server handler 16 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.3787692
2015-10-18 18:04:53,489 INFO [IPC Server handler 19 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.667
2015-10-18 18:04:54,067 INFO [IPC Server handler 22 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.27811313
2015-10-18 18:04:54,333 INFO [IPC Server handler 16 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.3638923
2015-10-18 18:04:54,693 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000005_0 is : 0.44950968
2015-10-18 18:04:54,708 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000004_0 is : 0.44968578
2015-10-18 18:04:55,630 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000006_0 is : 0.44980705
2015-10-18 18:04:56,318 INFO [IPC Server handler 16 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37322965
2015-10-18 18:04:56,396 INFO [IPC Server handler 19 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38007197
2015-10-18 18:04:56,568 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000000_0 is : 0.3624012
2015-10-18 18:04:57,396 INFO [IPC Server handler 19 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000009_0 is : 0.76133776
2015-10-18 18:04:57,427 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000008_0 is : 0.34610128
2015-10-18 18:04:57,443 INFO [IPC Server handler 14 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000007_0 is : 0.3707891
2015-10-18 18:04:59,771 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:04:59,787 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:05:02,802 INFO [IPC Server handler 10 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:05:02,818 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:05:27,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:27,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 30 seconds.  Will retry shortly ...
2015-10-18 18:05:28,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:28,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 31 seconds.  Will retry shortly ...
2015-10-18 18:05:29,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:29,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 32 seconds.  Will retry shortly ...
2015-10-18 18:05:30,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:30,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 33 seconds.  Will retry shortly ...
2015-10-18 18:05:31,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:31,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 34 seconds.  Will retry shortly ...
2015-10-18 18:05:32,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:32,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 35 seconds.  Will retry shortly ...
2015-10-18 18:05:33,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:33,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 36 seconds.  Will retry shortly ...
2015-10-18 18:05:34,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:34,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 37 seconds.  Will retry shortly ...
2015-10-18 18:05:35,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:35,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 38 seconds.  Will retry shortly ...
2015-10-18 18:05:36,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:36,570 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 39 seconds.  Will retry shortly ...
2015-10-18 18:05:37,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:37,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 40 seconds.  Will retry shortly ...
2015-10-18 18:05:38,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:38,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 41 seconds.  Will retry shortly ...
2015-10-18 18:05:39,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:39,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 42 seconds.  Will retry shortly ...
2015-10-18 18:05:40,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:40,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 43 seconds.  Will retry shortly ...
2015-10-18 18:05:41,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:41,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 44 seconds.  Will retry shortly ...
2015-10-18 18:05:42,633 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:42,633 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 45 seconds.  Will retry shortly ...
2015-10-18 18:05:43,633 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:43,633 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 46 seconds.  Will retry shortly ...
2015-10-18 18:05:44,633 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:44,633 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 47 seconds.  Will retry shortly ...
2015-10-18 18:05:45,633 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:45,633 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 48 seconds.  Will retry shortly ...
2015-10-18 18:05:46,633 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:46,633 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 49 seconds.  Will retry shortly ...
2015-10-18 18:05:47,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:47,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 50 seconds.  Will retry shortly ...
2015-10-18 18:05:48,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:48,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 51 seconds.  Will retry shortly ...
2015-10-18 18:05:49,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:49,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 52 seconds.  Will retry shortly ...
2015-10-18 18:05:50,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:50,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 53 seconds.  Will retry shortly ...
2015-10-18 18:05:51,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:51,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 54 seconds.  Will retry shortly ...
2015-10-18 18:05:52,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:52,680 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 55 seconds.  Will retry shortly ...
2015-10-18 18:05:53,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:53,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 56 seconds.  Will retry shortly ...
2015-10-18 18:05:54,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:54,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 57 seconds.  Will retry shortly ...
2015-10-18 18:05:55,696 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:55,696 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 58 seconds.  Will retry shortly ...
2015-10-18 18:05:56,696 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:56,696 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 59 seconds.  Will retry shortly ...
2015-10-18 18:05:57,009 WARN [ResponseProcessor for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731] org.apache.hadoop.hdfs.DFSClient: Slow ReadProcessor read fields took 65020ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [10.86.169.121:50010, 10.190.173.170:50010]
2015-10-18 18:05:57,009 WARN [ResponseProcessor for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731] org.apache.hadoop.hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731
2015-10-18 18:05:57,024 WARN [DataStreamer for file /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job_1445144423722_0020_1.jhist block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731] org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731 in pipeline 10.86.169.121:50010, 10.190.173.170:50010: bad datanode 10.190.173.170:50010
2015-10-18 18:05:57,024 WARN [DataStreamer for file /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job_1445144423722_0020_1.jhist block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:57,024 WARN [DataStreamer for file /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job_1445144423722_0020_1.jhist block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731] org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception
2015-10-18 18:05:57,718 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:57,719 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 60 seconds.  Will retry shortly ...
2015-10-18 18:05:58,722 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:58,722 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 61 seconds.  Will retry shortly ...
2015-10-18 18:05:59,725 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:05:59,725 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 62 seconds.  Will retry shortly ...
2015-10-18 18:06:00,731 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:00,731 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 63 seconds.  Will retry shortly ...
2015-10-18 18:06:01,747 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:01,747 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 64 seconds.  Will retry shortly ...
2015-10-18 18:06:01,840 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:02,794 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:02,794 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 65 seconds.  Will retry shortly ...
2015-10-18 18:06:02,841 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:03,028 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:06:03,794 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:03,794 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 66 seconds.  Will retry shortly ...
2015-10-18 18:06:03,856 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:03,856 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:04,809 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:04,809 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 67 seconds.  Will retry shortly ...
2015-10-18 18:06:04,872 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:05,825 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:05,825 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 68 seconds.  Will retry shortly ...
2015-10-18 18:06:05,888 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:05,934 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:06,169 INFO [IPC Server handler 2 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:06:06,294 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:06:06,841 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:06,841 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 69 seconds.  Will retry shortly ...
2015-10-18 18:06:06,935 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:07,888 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:07,888 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 70 seconds.  Will retry shortly ...
2015-10-18 18:06:07,950 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:07,950 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:08,903 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:08,903 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 71 seconds.  Will retry shortly ...
2015-10-18 18:06:08,950 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:09,325 INFO [IPC Server handler 18 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:06:09,528 INFO [IPC Server handler 22 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:06:09,919 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:09,919 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 72 seconds.  Will retry shortly ...
2015-10-18 18:06:09,966 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:09,966 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:10,919 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:10,919 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 73 seconds.  Will retry shortly ...
2015-10-18 18:06:10,982 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:11,935 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:11,935 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 74 seconds.  Will retry shortly ...
2015-10-18 18:06:11,997 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:11,997 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:12,466 INFO [IPC Server handler 9 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:06:12,654 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:06:12,951 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:12,951 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 75 seconds.  Will retry shortly ...
2015-10-18 18:06:13,013 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:13,966 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:13,966 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 76 seconds.  Will retry shortly ...
2015-10-18 18:06:14,013 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:14,013 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:14,966 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:14,966 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 77 seconds.  Will retry shortly ...
2015-10-18 18:06:15,013 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:15,623 INFO [IPC Server handler 20 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:06:15,826 INFO [IPC Server handler 21 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:06:15,966 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:15,966 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 78 seconds.  Will retry shortly ...
2015-10-18 18:06:16,029 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:16,029 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:16,982 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:16,982 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 79 seconds.  Will retry shortly ...
2015-10-18 18:06:17,029 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:18,029 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:18,029 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 80 seconds.  Will retry shortly ...
2015-10-18 18:06:18,045 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:18,045 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:18,763 INFO [IPC Server handler 12 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:06:18,920 INFO [IPC Server handler 16 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:06:19,029 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:19,029 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 81 seconds.  Will retry shortly ...
2015-10-18 18:06:19,060 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:20,045 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:20,045 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 82 seconds.  Will retry shortly ...
2015-10-18 18:06:20,076 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:20,076 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:21,045 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:21,045 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 83 seconds.  Will retry shortly ...
2015-10-18 18:06:21,076 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:21,904 INFO [IPC Server handler 26 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:06:22,045 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:22,045 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 84 seconds.  Will retry shortly ...
2015-10-18 18:06:22,060 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:06:22,092 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:22,092 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:23,045 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:23,045 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 85 seconds.  Will retry shortly ...
2015-10-18 18:06:23,092 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:24,045 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:24,045 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 86 seconds.  Will retry shortly ...
2015-10-18 18:06:24,092 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:24,092 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:25,061 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:25,061 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 87 seconds.  Will retry shortly ...
2015-10-18 18:06:25,092 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:06:25,107 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:25,232 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:06:25,998 INFO [IPC Server handler 5 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000002_0 is : 0.38137424
2015-10-18 18:06:26,029 FATAL [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1445144423722_0020_m_000002_0 - exited : java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
2015-10-18 18:06:26,029 INFO [IPC Server handler 13 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1445144423722_0020_m_000002_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
2015-10-18 18:06:26,029 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445144423722_0020_m_000002_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
2015-10-18 18:06:26,029 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000002_0 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP
2015-10-18 18:06:26,029 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445144423722_0020_01_000004 taskAttempt attempt_1445144423722_0020_m_000002_0
2015-10-18 18:06:26,029 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445144423722_0020_m_000002_0
2015-10-18 18:06:26,029 INFO [ContainerLauncher #1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:52368
2015-10-18 18:06:26,061 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:26,061 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 88 seconds.  Will retry shortly ...
2015-10-18 18:06:26,108 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:26,108 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:26,123 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000002_0 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP
2015-10-18 18:06:26,123 INFO [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT
2015-10-18 18:06:26,139 WARN [CommitterEvent Processor #1] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:26,139 WARN [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Task cleanup failed for attempt attempt_1445144423722_0020_m_000002_0
2015-10-18 18:06:26,139 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000002_0 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED
2015-10-18 18:06:26,139 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:06:26,139 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:06:26,139 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000002_1 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:06:26,139 ERROR [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent@7317849d
2015-10-18 18:06:26,139 ERROR [eventHandlingThread] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[eventHandlingThread,5,main] threw an Exception.
2015-10-18 18:06:26,139 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 1 failures on node MININT-FNANLI5.fareast.corp.microsoft.com
2015-10-18 18:06:26,139 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Added attempt_1445144423722_0020_m_000002_1 to list of failed maps
2015-10-18 18:06:27,061 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:27,061 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 89 seconds.  Will retry shortly ...
2015-10-18 18:06:27,108 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:2 ScheduledReds:1 AssignedMaps:9 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:11 ContRel:1 HostLocal:7 RackLocal:3
2015-10-18 18:06:27,108 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:28,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:28,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 90 seconds.  Will retry shortly ...
2015-10-18 18:06:28,108 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:28,108 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:28,123 INFO [IPC Server handler 8 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:06:28,170 INFO [IPC Server handler 0 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1445144423722_0020_m_000001_0 is : 0.37551183
2015-10-18 18:06:28,217 FATAL [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1445144423722_0020_m_000001_0 - exited : java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
2015-10-18 18:06:28,217 INFO [IPC Server handler 4 on 62270] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1445144423722_0020_m_000001_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
2015-10-18 18:06:28,217 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1445144423722_0020_m_000001_0: Error: java.net.NoRouteToHostException: No Route to Host from  MININT-FNANLI5/127.0.0.1 to msra-sa-41:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host: no further information; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
2015-10-18 18:06:28,217 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000001_0 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP
2015-10-18 18:06:28,217 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1445144423722_0020_01_000003 taskAttempt attempt_1445144423722_0020_m_000001_0
2015-10-18 18:06:28,217 INFO [ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1445144423722_0020_m_000001_0
2015-10-18 18:06:28,217 INFO [ContainerLauncher #2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : MININT-FNANLI5.fareast.corp.microsoft.com:52368
2015-10-18 18:06:28,233 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000001_0 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP
2015-10-18 18:06:28,233 INFO [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT
2015-10-18 18:06:28,233 WARN [CommitterEvent Processor #2] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:28,248 WARN [CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Task cleanup failed for attempt attempt_1445144423722_0020_m_000001_0
2015-10-18 18:06:28,248 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000001_0 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED
2015-10-18 18:06:28,248 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-39.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:06:28,248 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved MSRA-SA-41.fareast.corp.microsoft.com to /default-rack
2015-10-18 18:06:28,248 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1445144423722_0020_m_000001_1 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-10-18 18:06:28,248 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 2 failures on node MININT-FNANLI5.fareast.corp.microsoft.com
2015-10-18 18:06:28,248 INFO [Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Added attempt_1445144423722_0020_m_000001_1 to list of failed maps
2015-10-18 18:06:29,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:29,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 91 seconds.  Will retry shortly ...
2015-10-18 18:06:29,108 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:3 ScheduledReds:1 AssignedMaps:9 AssignedReds:0 CompletedMaps:1 CompletedReds:0 ContAlloc:11 ContRel:1 HostLocal:7 RackLocal:3
2015-10-18 18:06:29,108 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:30,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:30,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 92 seconds.  Will retry shortly ...
2015-10-18 18:06:30,108 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:30,108 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:31,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:31,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 93 seconds.  Will retry shortly ...
2015-10-18 18:06:31,108 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:32,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:32,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 94 seconds.  Will retry shortly ...
2015-10-18 18:06:32,108 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:32,108 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:33,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:33,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 95 seconds.  Will retry shortly ...
2015-10-18 18:06:33,108 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:34,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:34,092 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 96 seconds.  Will retry shortly ...
2015-10-18 18:06:34,108 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:34,108 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:35,093 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:35,093 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 97 seconds.  Will retry shortly ...
2015-10-18 18:06:35,108 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:36,093 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:36,093 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 98 seconds.  Will retry shortly ...
2015-10-18 18:06:36,108 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:36,108 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:37,108 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:37,108 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:37,108 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 99 seconds.  Will retry shortly ...
2015-10-18 18:06:38,108 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:38,108 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:38,140 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:38,140 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 100 seconds.  Will retry shortly ...
2015-10-18 18:06:39,108 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:39,140 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:39,140 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 101 seconds.  Will retry shortly ...
2015-10-18 18:06:40,108 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:40,108 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:40,140 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:40,140 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 102 seconds.  Will retry shortly ...
2015-10-18 18:06:41,109 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:41,140 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:41,140 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 103 seconds.  Will retry shortly ...
2015-10-18 18:06:42,109 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:42,109 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:42,140 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:42,140 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 104 seconds.  Will retry shortly ...
2015-10-18 18:06:43,124 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:43,156 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:43,156 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 105 seconds.  Will retry shortly ...
2015-10-18 18:06:44,124 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:44,124 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:44,156 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:44,156 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 106 seconds.  Will retry shortly ...
2015-10-18 18:06:45,124 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:45,156 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:45,156 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 107 seconds.  Will retry shortly ...
2015-10-18 18:06:46,124 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:46,124 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:46,156 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:46,156 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 108 seconds.  Will retry shortly ...
2015-10-18 18:06:47,125 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:47,156 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:47,156 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 109 seconds.  Will retry shortly ...
2015-10-18 18:06:48,140 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:48,140 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:48,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:48,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 110 seconds.  Will retry shortly ...
2015-10-18 18:06:49,140 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:49,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:49,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 111 seconds.  Will retry shortly ...
2015-10-18 18:06:50,140 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:50,140 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:50,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:50,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 112 seconds.  Will retry shortly ...
2015-10-18 18:06:51,140 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:51,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:51,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 113 seconds.  Will retry shortly ...
2015-10-18 18:06:52,140 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:52,140 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:52,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:52,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 114 seconds.  Will retry shortly ...
2015-10-18 18:06:53,140 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:53,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:53,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 115 seconds.  Will retry shortly ...
2015-10-18 18:06:54,141 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:54,141 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:54,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:54,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 116 seconds.  Will retry shortly ...
2015-10-18 18:06:55,141 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:55,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:55,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 117 seconds.  Will retry shortly ...
2015-10-18 18:06:56,156 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:56,156 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:56,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:56,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 118 seconds.  Will retry shortly ...
2015-10-18 18:06:57,156 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:57,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:57,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 119 seconds.  Will retry shortly ...
2015-10-18 18:06:58,156 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:06:58,156 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:06:58,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:58,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 120 seconds.  Will retry shortly ...
2015-10-18 18:06:59,188 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:06:59,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:06:59,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 121 seconds.  Will retry shortly ...
2015-10-18 18:07:00,188 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:00,188 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:00,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:00,203 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 122 seconds.  Will retry shortly ...
2015-10-18 18:07:01,188 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:01,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:01,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 123 seconds.  Will retry shortly ...
2015-10-18 18:07:02,188 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:02,188 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:02,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:02,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 124 seconds.  Will retry shortly ...
2015-10-18 18:07:03,188 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:03,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:03,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 125 seconds.  Will retry shortly ...
2015-10-18 18:07:04,188 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:04,188 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:04,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:04,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 126 seconds.  Will retry shortly ...
2015-10-18 18:07:05,188 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:05,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:05,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 127 seconds.  Will retry shortly ...
2015-10-18 18:07:06,188 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:06,188 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:06,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:06,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 128 seconds.  Will retry shortly ...
2015-10-18 18:07:07,188 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:07,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:07,219 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 129 seconds.  Will retry shortly ...
2015-10-18 18:07:08,188 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:08,188 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:08,220 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:08,220 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 130 seconds.  Will retry shortly ...
2015-10-18 18:07:09,188 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:09,266 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:09,266 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 131 seconds.  Will retry shortly ...
2015-10-18 18:07:10,188 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:10,188 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:10,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:10,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 132 seconds.  Will retry shortly ...
2015-10-18 18:07:11,188 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:11,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:11,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 133 seconds.  Will retry shortly ...
2015-10-18 18:07:12,189 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:12,189 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:12,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:12,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 134 seconds.  Will retry shortly ...
2015-10-18 18:07:13,189 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:13,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:13,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 135 seconds.  Will retry shortly ...
2015-10-18 18:07:14,189 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:14,189 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:14,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:14,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 136 seconds.  Will retry shortly ...
2015-10-18 18:07:15,189 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:15,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:15,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 137 seconds.  Will retry shortly ...
2015-10-18 18:07:16,189 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:16,189 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:16,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:16,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 138 seconds.  Will retry shortly ...
2015-10-18 18:07:17,189 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:17,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:17,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 139 seconds.  Will retry shortly ...
2015-10-18 18:07:18,189 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:18,189 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:18,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:18,267 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 140 seconds.  Will retry shortly ...
2015-10-18 18:07:19,189 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:19,298 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:19,298 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 141 seconds.  Will retry shortly ...
2015-10-18 18:07:20,189 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:20,189 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:20,298 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:20,298 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 142 seconds.  Will retry shortly ...
2015-10-18 18:07:21,189 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:21,298 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:21,298 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 143 seconds.  Will retry shortly ...
2015-10-18 18:07:22,189 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:22,189 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:22,314 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:22,314 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 144 seconds.  Will retry shortly ...
2015-10-18 18:07:23,189 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:23,314 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:23,314 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 145 seconds.  Will retry shortly ...
2015-10-18 18:07:24,189 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:24,189 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:24,314 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:24,314 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 146 seconds.  Will retry shortly ...
2015-10-18 18:07:25,189 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:25,314 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:25,314 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 147 seconds.  Will retry shortly ...
2015-10-18 18:07:26,189 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:26,189 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:26,330 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:26,330 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 148 seconds.  Will retry shortly ...
2015-10-18 18:07:27,189 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:27,330 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:27,330 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 149 seconds.  Will retry shortly ...
2015-10-18 18:07:28,189 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:28,189 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:28,330 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:28,330 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 150 seconds.  Will retry shortly ...
2015-10-18 18:07:29,190 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:29,361 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:29,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 151 seconds.  Will retry shortly ...
2015-10-18 18:07:30,190 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:30,190 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:30,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:30,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 152 seconds.  Will retry shortly ...
2015-10-18 18:07:31,190 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:31,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:31,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 153 seconds.  Will retry shortly ...
2015-10-18 18:07:32,190 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:32,190 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:32,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:32,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 154 seconds.  Will retry shortly ...
2015-10-18 18:07:33,205 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:33,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:33,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 155 seconds.  Will retry shortly ...
2015-10-18 18:07:34,205 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:34,205 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:34,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:34,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 156 seconds.  Will retry shortly ...
2015-10-18 18:07:35,221 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:35,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:35,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 157 seconds.  Will retry shortly ...
2015-10-18 18:07:36,221 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:36,221 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:36,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:36,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 158 seconds.  Will retry shortly ...
2015-10-18 18:07:37,221 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:37,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:37,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 159 seconds.  Will retry shortly ...
2015-10-18 18:07:38,221 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:38,221 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:38,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:38,393 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 160 seconds.  Will retry shortly ...
2015-10-18 18:07:39,221 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:39,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:39,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 161 seconds.  Will retry shortly ...
2015-10-18 18:07:40,221 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:40,221 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:40,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:40,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 162 seconds.  Will retry shortly ...
2015-10-18 18:07:41,222 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:41,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:41,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 163 seconds.  Will retry shortly ...
2015-10-18 18:07:42,222 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:42,222 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:42,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:42,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 164 seconds.  Will retry shortly ...
2015-10-18 18:07:43,222 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:43,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:43,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 165 seconds.  Will retry shortly ...
2015-10-18 18:07:44,222 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:44,222 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:44,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:44,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 166 seconds.  Will retry shortly ...
2015-10-18 18:07:45,222 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:45,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:45,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 167 seconds.  Will retry shortly ...
2015-10-18 18:07:46,222 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:46,222 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:46,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:46,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 168 seconds.  Will retry shortly ...
2015-10-18 18:07:47,222 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:47,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:47,425 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 169 seconds.  Will retry shortly ...
2015-10-18 18:07:48,238 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:48,238 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:48,441 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:48,441 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 171 seconds.  Will retry shortly ...
2015-10-18 18:07:49,238 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:49,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:49,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 172 seconds.  Will retry shortly ...
2015-10-18 18:07:50,238 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:50,238 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:50,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:50,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 173 seconds.  Will retry shortly ...
2015-10-18 18:07:51,238 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:51,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:51,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 174 seconds.  Will retry shortly ...
2015-10-18 18:07:52,238 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:52,238 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:52,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:52,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 175 seconds.  Will retry shortly ...
2015-10-18 18:07:53,238 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:53,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:53,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 176 seconds.  Will retry shortly ...
2015-10-18 18:07:54,238 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:54,238 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:54,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:54,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 177 seconds.  Will retry shortly ...
2015-10-18 18:07:55,238 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:55,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:55,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 178 seconds.  Will retry shortly ...
2015-10-18 18:07:56,238 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:56,238 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:56,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:56,472 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 179 seconds.  Will retry shortly ...
2015-10-18 18:07:57,238 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:57,473 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:57,473 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 180 seconds.  Will retry shortly ...
2015-10-18 18:07:58,238 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:07:58,238 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:07:58,473 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:58,473 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 181 seconds.  Will retry shortly ...
2015-10-18 18:07:59,238 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:07:59,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:07:59,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 182 seconds.  Will retry shortly ...
2015-10-18 18:08:00,238 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:00,238 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:00,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:00,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 183 seconds.  Will retry shortly ...
2015-10-18 18:08:01,238 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:01,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:01,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 184 seconds.  Will retry shortly ...
2015-10-18 18:08:02,238 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:02,238 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:02,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:02,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 185 seconds.  Will retry shortly ...
2015-10-18 18:08:03,238 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:03,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:03,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 186 seconds.  Will retry shortly ...
2015-10-18 18:08:04,239 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:04,239 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:04,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:04,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 187 seconds.  Will retry shortly ...
2015-10-18 18:08:05,239 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:05,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:05,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 188 seconds.  Will retry shortly ...
2015-10-18 18:08:06,239 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:06,239 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:06,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:06,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 189 seconds.  Will retry shortly ...
2015-10-18 18:08:07,239 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:07,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:07,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 190 seconds.  Will retry shortly ...
2015-10-18 18:08:08,239 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:08,239 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:08,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:08,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 191 seconds.  Will retry shortly ...
2015-10-18 18:08:09,239 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:09,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:09,504 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 192 seconds.  Will retry shortly ...
2015-10-18 18:08:10,239 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:10,239 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:10,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:10,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 193 seconds.  Will retry shortly ...
2015-10-18 18:08:11,239 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:11,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:11,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 194 seconds.  Will retry shortly ...
2015-10-18 18:08:12,239 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:12,239 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:12,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:12,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 195 seconds.  Will retry shortly ...
2015-10-18 18:08:13,239 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:13,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:13,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 196 seconds.  Will retry shortly ...
2015-10-18 18:08:14,239 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:14,239 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:14,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:14,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 197 seconds.  Will retry shortly ...
2015-10-18 18:08:15,239 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:15,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:15,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 198 seconds.  Will retry shortly ...
2015-10-18 18:08:16,239 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:16,239 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:16,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:16,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 199 seconds.  Will retry shortly ...
2015-10-18 18:08:17,239 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:17,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:17,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 200 seconds.  Will retry shortly ...
2015-10-18 18:08:18,239 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:18,239 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:18,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:18,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 201 seconds.  Will retry shortly ...
2015-10-18 18:08:19,255 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:19,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:19,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 202 seconds.  Will retry shortly ...
2015-10-18 18:08:20,255 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:20,255 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:20,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:20,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 203 seconds.  Will retry shortly ...
2015-10-18 18:08:21,286 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:21,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:21,536 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 204 seconds.  Will retry shortly ...
2015-10-18 18:08:22,287 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:22,287 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:22,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:22,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 205 seconds.  Will retry shortly ...
2015-10-18 18:08:23,287 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:23,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:23,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 206 seconds.  Will retry shortly ...
2015-10-18 18:08:24,302 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:24,302 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:24,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:24,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 207 seconds.  Will retry shortly ...
2015-10-18 18:08:25,302 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:25,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:25,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 208 seconds.  Will retry shortly ...
2015-10-18 18:08:26,302 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:26,302 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:26,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:26,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 209 seconds.  Will retry shortly ...
2015-10-18 18:08:27,302 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:27,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:27,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 210 seconds.  Will retry shortly ...
2015-10-18 18:08:28,303 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:28,303 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:28,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:28,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 211 seconds.  Will retry shortly ...
2015-10-18 18:08:29,303 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:29,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:29,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 212 seconds.  Will retry shortly ...
2015-10-18 18:08:30,303 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:30,303 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:30,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:30,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 213 seconds.  Will retry shortly ...
2015-10-18 18:08:31,334 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:31,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:31,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 214 seconds.  Will retry shortly ...
2015-10-18 18:08:32,334 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:32,334 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:32,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:32,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 215 seconds.  Will retry shortly ...
2015-10-18 18:08:33,334 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:33,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:33,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 216 seconds.  Will retry shortly ...
2015-10-18 18:08:34,334 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:34,334 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:34,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:34,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 217 seconds.  Will retry shortly ...
2015-10-18 18:08:35,334 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:35,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:35,537 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 218 seconds.  Will retry shortly ...
2015-10-18 18:08:36,334 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:36,334 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:36,553 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:36,553 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 219 seconds.  Will retry shortly ...
2015-10-18 18:08:37,334 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:37,553 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:37,553 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 220 seconds.  Will retry shortly ...
2015-10-18 18:08:38,334 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:38,334 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:38,553 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:38,553 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 221 seconds.  Will retry shortly ...
2015-10-18 18:08:39,334 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:39,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:39,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 222 seconds.  Will retry shortly ...
2015-10-18 18:08:40,334 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:40,334 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:40,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:40,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 223 seconds.  Will retry shortly ...
2015-10-18 18:08:41,366 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:41,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:41,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 224 seconds.  Will retry shortly ...
2015-10-18 18:08:42,366 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:42,366 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:42,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:42,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 225 seconds.  Will retry shortly ...
2015-10-18 18:08:43,366 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:43,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:43,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 226 seconds.  Will retry shortly ...
2015-10-18 18:08:44,366 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:44,366 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:44,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:44,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 227 seconds.  Will retry shortly ...
2015-10-18 18:08:45,366 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:45,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:45,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 228 seconds.  Will retry shortly ...
2015-10-18 18:08:46,366 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:46,366 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:46,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:46,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 229 seconds.  Will retry shortly ...
2015-10-18 18:08:47,366 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:47,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:47,569 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 230 seconds.  Will retry shortly ...
2015-10-18 18:08:48,382 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:48,382 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:48,585 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:48,585 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 231 seconds.  Will retry shortly ...
2015-10-18 18:08:49,382 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:49,585 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:49,585 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 232 seconds.  Will retry shortly ...
2015-10-18 18:08:50,398 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:50,398 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:50,585 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:50,585 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 233 seconds.  Will retry shortly ...
2015-10-18 18:08:51,429 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:51,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:51,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 234 seconds.  Will retry shortly ...
2015-10-18 18:08:52,429 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:52,429 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:52,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:52,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 235 seconds.  Will retry shortly ...
2015-10-18 18:08:53,429 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:53,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:53,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 236 seconds.  Will retry shortly ...
2015-10-18 18:08:54,429 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:54,429 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:54,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:54,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 237 seconds.  Will retry shortly ...
2015-10-18 18:08:55,429 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:55,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:55,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 238 seconds.  Will retry shortly ...
2015-10-18 18:08:56,429 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:56,429 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:56,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:56,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 239 seconds.  Will retry shortly ...
2015-10-18 18:08:57,429 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:57,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:57,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 240 seconds.  Will retry shortly ...
2015-10-18 18:08:58,429 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:08:58,429 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:08:58,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:58,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 241 seconds.  Will retry shortly ...
2015-10-18 18:08:59,429 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:08:59,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:08:59,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 242 seconds.  Will retry shortly ...
2015-10-18 18:09:00,429 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:00,429 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:00,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:00,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 243 seconds.  Will retry shortly ...
2015-10-18 18:09:01,461 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:01,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:01,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 244 seconds.  Will retry shortly ...
2015-10-18 18:09:02,461 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:02,461 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:02,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:02,601 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 245 seconds.  Will retry shortly ...
2015-10-18 18:09:03,461 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:03,602 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:03,602 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 246 seconds.  Will retry shortly ...
2015-10-18 18:09:04,461 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:04,461 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:04,602 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:04,602 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 247 seconds.  Will retry shortly ...
2015-10-18 18:09:05,461 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:05,602 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:05,602 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 248 seconds.  Will retry shortly ...
2015-10-18 18:09:06,461 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:06,461 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:06,602 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:06,602 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 249 seconds.  Will retry shortly ...
2015-10-18 18:09:07,461 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:07,602 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:07,602 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 250 seconds.  Will retry shortly ...
2015-10-18 18:09:08,461 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:08,461 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:08,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:08,617 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 251 seconds.  Will retry shortly ...
2015-10-18 18:09:09,461 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:09,618 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:09,618 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 252 seconds.  Will retry shortly ...
2015-10-18 18:09:10,461 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:10,461 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:10,618 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:10,618 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 253 seconds.  Will retry shortly ...
2015-10-18 18:09:11,461 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:11,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:11,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 254 seconds.  Will retry shortly ...
2015-10-18 18:09:12,461 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:12,461 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:12,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:12,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 255 seconds.  Will retry shortly ...
2015-10-18 18:09:13,461 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:13,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:13,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 256 seconds.  Will retry shortly ...
2015-10-18 18:09:14,462 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:14,462 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:14,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:14,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 257 seconds.  Will retry shortly ...
2015-10-18 18:09:15,462 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:15,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:15,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 258 seconds.  Will retry shortly ...
2015-10-18 18:09:16,462 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:16,462 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:16,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:16,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 259 seconds.  Will retry shortly ...
2015-10-18 18:09:17,462 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:17,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:17,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 260 seconds.  Will retry shortly ...
2015-10-18 18:09:18,462 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:18,462 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:18,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:18,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 261 seconds.  Will retry shortly ...
2015-10-18 18:09:19,462 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:19,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:19,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 262 seconds.  Will retry shortly ...
2015-10-18 18:09:20,462 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:20,462 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:20,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:20,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 263 seconds.  Will retry shortly ...
2015-10-18 18:09:21,462 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:21,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:21,649 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 264 seconds.  Will retry shortly ...
2015-10-18 18:09:22,462 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:22,462 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:22,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:22,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 265 seconds.  Will retry shortly ...
2015-10-18 18:09:23,462 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:23,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:23,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 266 seconds.  Will retry shortly ...
2015-10-18 18:09:24,462 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:24,462 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:24,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:24,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 267 seconds.  Will retry shortly ...
2015-10-18 18:09:25,462 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:25,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:25,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 268 seconds.  Will retry shortly ...
2015-10-18 18:09:26,462 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:26,462 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:26,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:26,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 269 seconds.  Will retry shortly ...
2015-10-18 18:09:27,462 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:27,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:27,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 270 seconds.  Will retry shortly ...
2015-10-18 18:09:28,462 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:28,462 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:28,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:28,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 271 seconds.  Will retry shortly ...
2015-10-18 18:09:29,462 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:29,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:29,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 272 seconds.  Will retry shortly ...
2015-10-18 18:09:30,463 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:30,463 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:30,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:30,681 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 273 seconds.  Will retry shortly ...
2015-10-18 18:09:31,463 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:31,697 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:31,697 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 274 seconds.  Will retry shortly ...
2015-10-18 18:09:32,463 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:32,463 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:32,728 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:32,728 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 275 seconds.  Will retry shortly ...
2015-10-18 18:09:33,463 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:33,728 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:33,728 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 276 seconds.  Will retry shortly ...
2015-10-18 18:09:34,463 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:34,463 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:34,728 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:34,728 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 277 seconds.  Will retry shortly ...
2015-10-18 18:09:35,463 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:35,728 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:35,728 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 278 seconds.  Will retry shortly ...
2015-10-18 18:09:36,463 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:36,463 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:36,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:36,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 279 seconds.  Will retry shortly ...
2015-10-18 18:09:37,463 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:37,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:37,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 280 seconds.  Will retry shortly ...
2015-10-18 18:09:38,463 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:38,463 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:38,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:38,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 281 seconds.  Will retry shortly ...
2015-10-18 18:09:39,479 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:39,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:39,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 282 seconds.  Will retry shortly ...
2015-10-18 18:09:40,479 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:40,479 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:40,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:40,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 283 seconds.  Will retry shortly ...
2015-10-18 18:09:41,479 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:41,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:41,729 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 284 seconds.  Will retry shortly ...
2015-10-18 18:09:42,479 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:42,479 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:42,760 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:42,760 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 285 seconds.  Will retry shortly ...
2015-10-18 18:09:43,479 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:43,760 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:43,760 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 286 seconds.  Will retry shortly ...
2015-10-18 18:09:44,479 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:44,479 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:44,760 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:44,760 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 287 seconds.  Will retry shortly ...
2015-10-18 18:09:45,495 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:45,760 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:45,760 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 288 seconds.  Will retry shortly ...
2015-10-18 18:09:46,495 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:46,495 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:46,760 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:46,760 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 289 seconds.  Will retry shortly ...
2015-10-18 18:09:47,510 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:47,776 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:47,776 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 290 seconds.  Will retry shortly ...
2015-10-18 18:09:48,510 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:48,510 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:48,792 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:48,792 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 291 seconds.  Will retry shortly ...
2015-10-18 18:09:49,511 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:49,792 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:49,792 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 292 seconds.  Will retry shortly ...
2015-10-18 18:09:50,511 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:50,511 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:50,792 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:50,792 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 293 seconds.  Will retry shortly ...
2015-10-18 18:09:51,511 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:51,792 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:51,792 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 294 seconds.  Will retry shortly ...
2015-10-18 18:09:52,511 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:52,511 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:52,823 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:52,823 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 295 seconds.  Will retry shortly ...
2015-10-18 18:09:53,511 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:53,823 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:53,823 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 296 seconds.  Will retry shortly ...
2015-10-18 18:09:54,511 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:54,511 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:54,823 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:54,823 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 297 seconds.  Will retry shortly ...
2015-10-18 18:09:55,511 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:55,823 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:55,823 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 298 seconds.  Will retry shortly ...
2015-10-18 18:09:56,511 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:56,511 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:56,823 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:56,823 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 299 seconds.  Will retry shortly ...
2015-10-18 18:09:57,511 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:57,824 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:57,824 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 300 seconds.  Will retry shortly ...
2015-10-18 18:09:58,511 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:09:58,511 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:09:58,824 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:58,824 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 301 seconds.  Will retry shortly ...
2015-10-18 18:09:59,511 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:09:59,824 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:09:59,824 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 302 seconds.  Will retry shortly ...
2015-10-18 18:10:00,511 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:00,511 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:00,824 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:00,824 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 303 seconds.  Will retry shortly ...
2015-10-18 18:10:01,511 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:01,824 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:01,824 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 304 seconds.  Will retry shortly ...
2015-10-18 18:10:02,511 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:02,511 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:02,855 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:02,855 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 305 seconds.  Will retry shortly ...
2015-10-18 18:10:03,511 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:03,855 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:03,855 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 306 seconds.  Will retry shortly ...
2015-10-18 18:10:04,511 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:04,511 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:04,855 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:04,980 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 307 seconds.  Will retry shortly ...
2015-10-18 18:10:05,512 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:05,980 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:05,980 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 308 seconds.  Will retry shortly ...
2015-10-18 18:10:06,512 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:06,512 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:06,980 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:06,980 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 309 seconds.  Will retry shortly ...
2015-10-18 18:10:07,512 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:07,980 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:07,980 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 310 seconds.  Will retry shortly ...
2015-10-18 18:10:08,527 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:08,527 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:08,980 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:08,980 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 311 seconds.  Will retry shortly ...
2015-10-18 18:10:09,527 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:09,981 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:09,981 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 312 seconds.  Will retry shortly ...
2015-10-18 18:10:10,527 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:10,527 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:10,981 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:10,981 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 313 seconds.  Will retry shortly ...
2015-10-18 18:10:11,528 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:11,981 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:11,981 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 314 seconds.  Will retry shortly ...
2015-10-18 18:10:12,528 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:12,528 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:13,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:13,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 315 seconds.  Will retry shortly ...
2015-10-18 18:10:13,528 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:14,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:14,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 316 seconds.  Will retry shortly ...
2015-10-18 18:10:14,528 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:14,528 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:15,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:15,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 317 seconds.  Will retry shortly ...
2015-10-18 18:10:15,528 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:16,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:16,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 318 seconds.  Will retry shortly ...
2015-10-18 18:10:16,528 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:16,528 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:17,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:17,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 319 seconds.  Will retry shortly ...
2015-10-18 18:10:17,528 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:18,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:18,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 320 seconds.  Will retry shortly ...
2015-10-18 18:10:18,528 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:18,528 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:19,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:19,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 321 seconds.  Will retry shortly ...
2015-10-18 18:10:19,528 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:20,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:20,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 322 seconds.  Will retry shortly ...
2015-10-18 18:10:20,528 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:20,528 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:21,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:21,012 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 323 seconds.  Will retry shortly ...
2015-10-18 18:10:21,528 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:22,013 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:22,013 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 324 seconds.  Will retry shortly ...
2015-10-18 18:10:22,528 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:22,528 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:23,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:23,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 325 seconds.  Will retry shortly ...
2015-10-18 18:10:23,528 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:24,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:24,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 326 seconds.  Will retry shortly ...
2015-10-18 18:10:24,528 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:24,528 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:25,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:25,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 327 seconds.  Will retry shortly ...
2015-10-18 18:10:25,528 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:26,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:26,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 328 seconds.  Will retry shortly ...
2015-10-18 18:10:26,528 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:26,528 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:27,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:27,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 329 seconds.  Will retry shortly ...
2015-10-18 18:10:27,544 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:28,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:28,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 330 seconds.  Will retry shortly ...
2015-10-18 18:10:28,544 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:28,544 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:29,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:29,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 331 seconds.  Will retry shortly ...
2015-10-18 18:10:29,544 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:30,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:30,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 332 seconds.  Will retry shortly ...
2015-10-18 18:10:30,544 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:30,544 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:31,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:31,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 333 seconds.  Will retry shortly ...
2015-10-18 18:10:31,544 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:32,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:32,044 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 334 seconds.  Will retry shortly ...
2015-10-18 18:10:32,544 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:32,544 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:33,091 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:33,091 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 335 seconds.  Will retry shortly ...
2015-10-18 18:10:33,544 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:34,091 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:34,091 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 336 seconds.  Will retry shortly ...
2015-10-18 18:10:34,545 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:34,545 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:35,091 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:35,091 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 337 seconds.  Will retry shortly ...
2015-10-18 18:10:35,545 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:36,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:36,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 338 seconds.  Will retry shortly ...
2015-10-18 18:10:36,545 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:36,545 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:37,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:37,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 339 seconds.  Will retry shortly ...
2015-10-18 18:10:37,545 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:38,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:38,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 340 seconds.  Will retry shortly ...
2015-10-18 18:10:38,545 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:38,545 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:39,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:39,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 341 seconds.  Will retry shortly ...
2015-10-18 18:10:39,545 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:40,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:40,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 342 seconds.  Will retry shortly ...
2015-10-18 18:10:40,545 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:40,545 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:41,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:41,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 343 seconds.  Will retry shortly ...
2015-10-18 18:10:41,545 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:42,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:42,107 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 344 seconds.  Will retry shortly ...
2015-10-18 18:10:42,545 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:42,545 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:43,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:43,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 345 seconds.  Will retry shortly ...
2015-10-18 18:10:43,545 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:44,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:44,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 346 seconds.  Will retry shortly ...
2015-10-18 18:10:44,545 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:44,545 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:45,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:45,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 347 seconds.  Will retry shortly ...
2015-10-18 18:10:45,545 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:46,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:46,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 348 seconds.  Will retry shortly ...
2015-10-18 18:10:46,545 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:46,545 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:47,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:47,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 349 seconds.  Will retry shortly ...
2015-10-18 18:10:47,545 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:48,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:48,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 350 seconds.  Will retry shortly ...
2015-10-18 18:10:48,545 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:48,545 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:49,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:49,139 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 351 seconds.  Will retry shortly ...
2015-10-18 18:10:49,545 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:50,155 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:50,155 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 352 seconds.  Will retry shortly ...
2015-10-18 18:10:50,545 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:50,545 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:51,155 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:51,155 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 353 seconds.  Will retry shortly ...
2015-10-18 18:10:51,546 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:52,155 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:52,155 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 354 seconds.  Will retry shortly ...
2015-10-18 18:10:52,546 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:52,546 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:53,202 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:53,202 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 355 seconds.  Will retry shortly ...
2015-10-18 18:10:53,546 WARN [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:8030 New: msra-sa-41:8030
2015-10-18 18:10:54,202 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
2015-10-18 18:10:54,202 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_NONMAPREDUCE_1537864556_1] for 356 seconds.  Will retry shortly ...
2015-10-18 18:10:54,546 INFO [RMCommunicator Allocator] org.apache.hadoop.ipc.Client: Retrying connect to server: msra-sa-41:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-18 18:10:54,546 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
2015-10-18 18:10:55,202 WARN [LeaseRenewer:msrabi@msra-sa-41:9000] org.apache.hadoop.ipc.Client: Address change detected. Old: msra-sa-41/10.190.173.170:9000 New: msra-sa-41:9000
- 1135643288 2005.12.26 R37-M1-NC-C:J02-U11 2005-12-26-16.28.08.716176 R37-M1-NC-C:J02-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1135651811 2005.12.26 R37-M1-NC-C:J02-U11 2005-12-26-18.50.11.312622 R37-M1-NC-C:J02-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1135653442 2005.12.26 R37-M1-NC-C:J02-U11 2005-12-26-19.17.22.803508 R37-M1-NC-C:J02-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1135661328 2005.12.26 R37-M1-NC-C:J02-U11 2005-12-26-21.28.48.643483 R37-M1-NC-C:J02-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1135665476 2005.12.26 R37-M1-NC-C:J02-U11 2005-12-26-22.37.56.529871 R37-M1-NC-C:J02-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1135669430 2005.12.26 R37-M1-NC-C:J02-U11 2005-12-26-23.43.50.733716 R37-M1-NC-C:J02-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1135669517 2005.12.26 R37-M1-NC-C:J02-U11 2005-12-26-23.45.17.825965 R37-M1-NC-C:J02-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1135675498 2005.12.27 R37-M1-NC-C:J02-U11 2005-12-27-01.24.58.711345 R37-M1-NC-C:J02-U11 RAS KERNEL INFO instruction cache parity error corrected
- 1136301189 2006.01.03 R07-M0-N0-I:J18-U11 2006-01-03-07.13.09.127918 R07-M0-N0-I:J18-U11 RAS KERNEL INFO ciod: generated 128 core files for program /g/g24/germann2/SPaSM_mini/MEAM/r13
081109 203615 148 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_38865049064139660 terminating
081109 203807 222 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-6952295868487656571 terminating
081109 204005 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_7128370237687728475 size 67108864
081109 204015 308 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8229193803249955061 terminating
081109 204106 329 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6670958622368987959 terminating
081109 204132 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.115:50010 is added to blk_3050920587428079149 size 67108864
081109 204324 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.80:50010 is added to blk_7888946331804732825 size 67108864
081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864
081109 204525 512 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_572492839287299681 terminating
081109 204655 556 INFO dfs.DataNode$PacketResponder: Received block blk_3587508140051953248 of size 67108864 from /10.251.42.84
081109 204722 567 INFO dfs.DataNode$PacketResponder: Received block blk_5402003568334525940 of size 67108864 from /10.251.214.112
081109 204815 653 INFO dfs.DataNode$DataXceiver: Receiving block blk_5792489080791696128 src: /10.251.30.6:33145 dest: /10.251.30.6:50010
081109 204842 663 INFO dfs.DataNode$DataXceiver: Receiving block blk_1724757848743533110 src: /10.251.111.130:49851 dest: /10.251.111.130:50010
081109 204908 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.110.8:50010 is added to blk_8015913224713045110 size 67108864
081109 204925 673 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5623176793330377570 src: /10.251.75.228:53725 dest: /10.251.75.228:50010
081109 205035 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811092030_0001_m_000590_0/part-00590. blk_-1727475099218615100
081109 205056 710 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_5017373558217225674 terminating
081109 205157 752 INFO dfs.DataNode$PacketResponder: Received block blk_9212264480425680329 of size 67108864 from /10.251.123.1
081109 205315 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811092030_0001_m_000742_0/part-00742. blk_-7878121102358435702
081109 205409 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.130:50010 is added to blk_4568434182693165548 size 67108864
081109 205412 832 INFO dfs.DataNode$PacketResponder: Received block blk_-5704899712662113150 of size 67108864 from /10.251.91.229
081109 205632 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.74.79:50010 is added to blk_-4794867979917102672 size 67108864
081109 205739 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.38.197:50010 is added to blk_8763662564934652249 size 67108864
081109 205742 1001 INFO dfs.DataNode$PacketResponder: Received block blk_-5861636720645142679 of size 67108864 from /10.251.70.211
081109 205746 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.74.134:50010 is added to blk_7453815855294711849 size 67108864
081109 205749 997 INFO dfs.DataNode$DataXceiver: Receiving block blk_-28342503914935090 src: /10.251.123.132:57542 dest: /10.251.123.132:50010
081109 205754 952 INFO dfs.DataNode$PacketResponder: Received block blk_8291449241650212794 of size 67108864 from /10.251.89.155
081109 205858 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811092030_0001_m_000487_0/part-00487. blk_-5319073033164653435
081109 205931 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-4980916519894289629
081109 210022 1110 INFO dfs.DataNode$PacketResponder: Received block blk_-5974833545991408899 of size 67108864 from /10.251.31.180
081109 210037 1084 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5009020203888190378 src: /10.251.199.19:52622 dest: /10.251.199.19:50010
081109 210248 1138 INFO dfs.DataNode$PacketResponder: Received block blk_6921674711959888070 of size 67108864 from /10.251.65.203
081109 210407 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.244:50010 is added to blk_5165786360127153975 size 67108864
081109 210458 1278 INFO dfs.DataNode$DataXceiver: Receiving block blk_2937758977269298350 src: /10.251.194.129:37476 dest: /10.251.194.129:50010
081109 210551 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.6.191:50010 is added to blk_673825774073966710 size 67108864
081109 210637 1283 INFO dfs.DataNode$PacketResponder: Received block blk_-7526945448667194862 of size 67108864 from /10.251.203.80
081109 210656 1334 INFO dfs.DataNode$PacketResponder: Received block blk_-2094397855762091248 of size 67108864 from /10.251.126.83
081109 210712 1333 INFO dfs.DataNode$PacketResponder: Received block blk_-8523968015014407246 of size 67108864 from /10.251.214.225
081109 210743 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.89.155:50010 is added to blk_8181993091797661153 size 67108864
081109 210801 1357 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6276023454199613372 src: /10.251.65.237:34085 dest: /10.251.65.237:50010
081109 210807 1408 INFO dfs.DataNode$PacketResponder: Received block blk_4755566011267050000 of size 67108864 from /10.251.75.79
081109 210812 1395 INFO dfs.DataNode$PacketResponder: Received block blk_-3909548841543565741 of size 3542967 from /10.251.195.33
081109 210921 1452 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6809181994368905854 src: /10.250.17.225:51754 dest: /10.250.17.225:50010
081109 210935 1458 INFO dfs.DataNode$PacketResponder: Received block blk_8829027411458566099 of size 67108864 from /10.251.38.214
081109 211029 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.50:50010 is added to blk_-29548654251973735 size 67108864
081109 211038 1490 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-5073870177832699716 terminating
081109 211216 1504 INFO dfs.DataNode$PacketResponder: Received block blk_-8241093737585222406 of size 67108864 from /10.250.5.161
081109 211301 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.228:50010 is added to blk_-2480595760294717232 size 67108864
081109 211353 1574 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_7791237942696729620 terminating
081109 211403 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.202.134:50010 is added to blk_2113880130496815041 size 3549917
081109 211453 1623 INFO dfs.DataNode$PacketResponder: Received block blk_1064470652608359218 of size 67108864 from /10.251.39.242
081109 211528 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.110.8:50010 is added to blk_-1661553043410372067 size 67108864
081109 211617 1635 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1527267659500322006 terminating
081109 211918 1777 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_4886940526690879848 terminating
081109 212029 1814 INFO dfs.DataNode$PacketResponder: Received block blk_-2452477352812192142 of size 67108864 from /10.250.7.244
081109 212219 1885 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-4229375751333894621 terminating
081109 212220 1946 INFO dfs.DataNode$DataXceiver: Receiving block blk_-774267833966018354 src: /10.251.38.53:51057 dest: /10.251.38.53:50010
081109 212245 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811092030_0001_m_001648_0/part-01648. blk_2513940824125131775
081109 212317 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811092030_0001_m_001866_0/part-01866. blk_-1282453782148343691
081109 212338 2007 INFO dfs.DataNode$PacketResponder: Received block blk_-518701095493827363 of size 67108864 from /10.251.214.67
081109 212403 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.50:50010 is added to blk_-2530087534157630851 size 67108864
081109 212440 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.198.196:50010 is added to blk_427714267500527780 size 67108864
081109 212703 2053 INFO dfs.DataNode$PacketResponder: Received block blk_-967145856473901804 of size 67108864 from /10.250.6.191
081109 212904 2174 INFO dfs.DataNode$PacketResponder: Received block blk_8408125361497769001 of size 67108864 from /10.251.70.211
081109 212931 2211 INFO dfs.DataNode$PacketResponder: Received block blk_-1614641487214609125 of size 67108864 from /10.251.193.175
081109 213009 2207 INFO dfs.DataNode$PacketResponder: Received block blk_7577595658377008671 of size 67108864 from /10.251.71.97
081109 213028 2206 INFO dfs.DataNode$PacketResponder: Received block blk_3777400576053320362 of size 67108864 from /10.251.31.5
081109 213217 2267 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-5852844080027817147 terminating
081109 213353 2313 INFO dfs.DataNode$PacketResponder: Received block blk_-2129171714384027465 of size 67108864 from /10.251.75.228
081109 213436 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-2827716238972737794
081109 213506 2421 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3509323198988774369 src: /10.250.6.214:52922 dest: /10.250.6.214:50010
081109 213510 2384 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_9093049293972551787 terminating
081109 213837 19 INFO dfs.FSDataset: Deleting block blk_1781953582842324563 file /mnt/hadoop/dfs/data/current/subdir5/blk_1781953582842324563
081109 213847 2552 INFO dfs.DataNode$DataXceiver: 10.251.194.213:50010 Served block blk_-7724713468912166542 to /10.251.203.80
081109 213907 2497 INFO dfs.DataNode$DataXceiver: 10.251.91.229:50010 Served block blk_-3358448553918665902 to /10.251.91.229
081109 213908 2549 INFO dfs.DataNode$DataXceiver: 10.251.39.192:50010 Served block blk_-5341992729755584578 to /10.251.39.192
081109 214009 2594 INFO dfs.DataNode$DataXceiver: 10.250.5.237:50010 Served block blk_3166960787499091856 to /10.251.43.147
081109 214043 2561 WARN dfs.DataNode$DataXceiver: 10.251.30.85:50010:Got exception while serving blk_-2918118818249673980 to /10.251.90.64:
081109 214402 2677 WARN dfs.DataNode$DataXceiver: 10.251.126.255:50010:Got exception while serving blk_8376667364205250596 to /10.251.91.159:
081109 214524 2633 INFO dfs.DataNode$DataXceiver: 10.251.71.68:50010 Served block blk_-2794533871450434534 to /10.251.199.150
081109 214529 2747 WARN dfs.DataNode$DataXceiver: 10.251.123.132:50010:Got exception while serving blk_3763728533434719668 to /10.251.38.214:
081109 214910 2848 WARN dfs.DataNode$DataXceiver: 10.250.13.188:50010:Got exception while serving blk_6241141267506413726 to /10.251.194.245:
081109 214919 2899 INFO dfs.DataNode$DataXceiver: 10.251.214.32:50010 Served block blk_-6520030462660619051 to /10.251.215.70
081109 215136 2868 WARN dfs.DataNode$DataXceiver: 10.251.199.19:50010:Got exception while serving blk_8466246428293623262 to /10.251.106.37:
081109 215259 2934 WARN dfs.DataNode$DataXceiver: 10.250.9.207:50010:Got exception while serving blk_-3140754468249228022 to /10.250.9.207:
081109 215702 3022 WARN dfs.DataNode$DataXceiver: 10.251.202.134:50010:Got exception while serving blk_3441699978641526775 to /10.251.126.5:
081109 215734 3055 INFO dfs.DataNode$DataXceiver: 10.250.6.214:50010 Served block blk_5739119717322549945 to /10.251.43.115
081109 220032 3137 WARN dfs.DataNode$DataXceiver: 10.250.14.196:50010:Got exception while serving blk_-305633040016166849 to /10.251.38.53:
081109 220403 3148 WARN dfs.DataNode$DataXceiver: 10.251.107.227:50010:Got exception while serving blk_-6290631608800952376 to /10.251.109.209:
081109 220528 3174 INFO dfs.DataNode$DataXceiver: 10.251.203.166:50010 Served block blk_8787656642683881295 to /10.251.107.98
081109 221105 3338 WARN dfs.DataNode$DataXceiver: 10.251.90.64:50010:Got exception while serving blk_-4841792440390267307 to /10.251.90.239:
081109 221151 3361 WARN dfs.DataNode$DataXceiver: 10.250.10.144:50010:Got exception while serving blk_5126469776250053435 to /10.250.11.100:
081109 222040 3463 WARN dfs.DataNode$DataXceiver: 10.251.71.146:50010:Got exception while serving blk_-2032740670708110312 to /10.251.197.161:
081109 222041 3390 WARN dfs.DataNode$DataXceiver: 10.251.67.113:50010:Got exception while serving blk_-62891505109755100 to /10.250.7.96:
081109 222342 3462 WARN dfs.DataNode$DataXceiver: 10.251.74.79:50010:Got exception while serving blk_2244903517044280975 to /10.251.74.134:
081109 222650 3459 WARN dfs.DataNode$DataXceiver: 10.251.214.112:50010:Got exception while serving blk_5905933788014151041 to /10.251.214.112:
081109 223211 3544 INFO dfs.DataNode$DataXceiver: 10.251.111.80:50010 Served block blk_6296828743242110158 to /10.251.42.246
081109 223910 3540 WARN dfs.DataNode$DataXceiver: 10.251.43.210:50010:Got exception while serving blk_2969087638814291714 to /10.251.199.86:
081109 224054 3560 WARN dfs.DataNode$DataXceiver: 10.251.126.255:50010:Got exception while serving blk_2879780987351022871 to /10.251.39.144:
081109 224234 3638 WARN dfs.DataNode$DataXceiver: 10.251.73.220:50010:Got exception while serving blk_4934527196392001803 to /10.251.203.246:
081109 224420 3666 WARN dfs.DataNode$DataXceiver: 10.251.73.188:50010:Got exception while serving blk_7517964792804498202 to /10.250.6.191:
081109 224741 3699 WARN dfs.DataNode$DataXceiver: 10.251.35.1:50010:Got exception while serving blk_7940316270494947483 to /10.251.122.38:
081109 230110 3647 WARN dfs.DataNode$DataXceiver: 10.251.90.134:50010:Got exception while serving blk_7154985168984871115 to /10.251.110.160:
081109 232200 3856 INFO dfs.DataNode$PacketResponder: Received block blk_-6867873931012347356 of size 67108864 from /10.251.39.64
081109 232324 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.19:50010 is added to blk_6093743385844975689 size 67108864
081109 232346 3820 INFO dfs.DataNode$DataXceiver: Receiving block blk_8692428775973608797 src: /10.250.10.213:56574 dest: /10.250.10.213:50010
081109 232639 3792 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-6837050820114491742 terminating
081109 232828 3875 INFO dfs.DataNode$PacketResponder: Received block blk_-5195432543794777448 of size 67108864 from /10.250.10.6
081109 232829 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.110.68:50010 is added to blk_1636660629634995787 size 67108864
081109 232833 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.75.79:50010 is added to blk_467589865104023338 size 67108864
081109 232923 3792 INFO dfs.DataNode$PacketResponder: Received block blk_7929144620252342335 of size 67108864 from /10.251.31.5
081109 232926 3783 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1798958443298220150 src: /10.251.111.209:57419 dest: /10.251.111.209:50010
081109 233108 3872 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-54240672253074715 terminating
081109 233356 3917 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7172347691639518373 src: /10.251.111.37:35327 dest: /10.251.111.37:50010
081109 233753 3950 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6412155847905320411 src: /10.251.30.85:58995 dest: /10.251.30.85:50010
081109 233835 4139 INFO dfs.DataNode$DataXceiver: Receiving block blk_699651392443620263 src: /10.251.70.5:34703 dest: /10.251.70.5:50010
081109 233836 4061 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8788032532930439012 src: /10.251.126.255:41104 dest: /10.251.126.255:50010
081109 233849 4087 INFO dfs.DataNode$PacketResponder: Received block blk_-3455521536483250153 of size 67108864 from /10.251.106.37
081109 234027 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.31.85:50010 is added to blk_-7017553867379051457 size 67108864
081109 234107 4261 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1259190292740306590 src: /10.251.65.203:36289 dest: /10.251.65.203:50010
081109 234110 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_200811092030_0002_r_000296_0/part-00296. blk_-6620182933895093708
081109 234111 4288 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_2028803850179311599 terminating
081109 234112 4136 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8562334328670278932 src: /10.251.194.147:46714 dest: /10.251.194.147:50010
081109 234325 4246 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-8108913101671416218 terminating
081109 234512 4223 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-7640601843607665080 terminating
081109 234516 4136 INFO dfs.DataNode$DataXceiver: Receiving block blk_7654726977769174867 src: /10.251.110.160:49990 dest: /10.251.110.160:50010
081109 234555 4345 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_8320088109701056055 terminating
081109 234605 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.197.226:50010 is added to blk_4585055992495043771 size 67108864
081109 234644 4177 INFO dfs.DataNode$PacketResponder: Received block blk_-4330512786228504138 of size 67108864 from /10.251.67.225
081109 234706 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.30.179:50010 is added to blk_4628142183191390143 size 67108864
081109 234719 4344 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6766351519628312408 terminating
081109 234728 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.5.237:50010 is added to blk_-476906696485288376 size 67108864
081109 234835 3972 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-7436749154361133879 terminating
081109 234835 4259 INFO dfs.DataNode$DataXceiver: Receiving block blk_7061131805920430446 src: /10.250.7.32:53397 dest: /10.250.7.32:50010
081109 234924 4275 INFO dfs.DataNode$PacketResponder: Received block blk_-684595390732854503 of size 67108864 from /10.250.6.191
081109 235030 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.244:50010 is added to blk_-6956067134432991406 size 67108864
081109 235138 3469 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_549647953114643908 terminating
081109 235140 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_200811092030_0002_r_000230_0/part-00230. blk_559204981722276126
081109 235305 4419 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-165500353496111884 terminating
081109 235311 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_200811092030_0002_r_000169_0/part-00169. blk_-7105305952901940477
081109 235322 3968 INFO dfs.DataNode$DataXceiver: Receiving block blk_-9103764593642564543 src: /10.251.43.115:46385 dest: /10.251.43.115:50010
081109 235337 4459 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_3160319368100972992 terminating
081109 235426 4327 INFO dfs.DataNode$PacketResponder: Received block blk_-562725280853087685 of size 67108864 from /10.251.91.84
081109 235609 4400 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_4568377692254637284 terminating
081109 235727 4429 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5217146995455608468 src: /10.251.214.67:38871 dest: /10.251.214.67:50010
081109 235813 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.199.225:50010 is added to blk_7481361125021302112 size 67108864
081109 235903 4454 INFO dfs.DataNode$PacketResponder: Received block blk_-71429470616906355 of size 67108864 from /10.250.19.16
081109 235904 4520 INFO dfs.DataNode$DataXceiver: Receiving block blk_-522892190802801712 src: /10.251.74.134:39584 dest: /10.251.74.134:50010
081109 235919 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.19:50010 is added to blk_-3249711809227781266 size 67108864
081109 235951 4525 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6329494504520216748 src: /10.251.25.237:39332 dest: /10.251.25.237:50010
081110 000117 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_200811092030_0002_r_000318_0/part-00318. blk_-207775976836691685
081110 000136 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.6.4:50010 is added to blk_5114010683183383297 size 67108864
081110 000136 4627 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-7656610147924319419 terminating
081110 000232 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_200811092030_0002_r_000318_0/part-00318. blk_2096692261399680562
081110 000238 4717 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-6500549130351529113 terminating
081110 000319 4568 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8255732027871411321 src: /10.251.39.179:33950 dest: /10.251.39.179:50010
081110 000507 4619 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-2896670847306153652 terminating
081110 000546 4624 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8856628810470361280 src: /10.250.5.237:41923 dest: /10.250.5.237:50010
081110 000616 4552 INFO dfs.DataNode$PacketResponder: Received block blk_3463540868626173125 of size 67108864 from /10.251.30.134
081110 000657 4822 INFO dfs.DataNode$PacketResponder: Received block blk_-5309211894896608429 of size 67108864 from /10.251.202.181
081110 000727 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.15.240:50010 is added to blk_-1055254430948037872 size 67108864
081110 000755 4717 INFO dfs.DataNode$DataXceiver: Receiving block blk_3962600675982666210 src: /10.251.123.195:37374 dest: /10.251.123.195:50010
081110 000820 4687 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-730776401372551814 terminating
081110 000841 4727 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-7553009628768897700 terminating
081110 001250 4631 INFO dfs.DataNode$PacketResponder: Received block blk_3068335169639013019 of size 67108864 from /10.251.71.68
081110 001307 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.90.239:50010 is added to blk_-9069971851590882719 size 67108864
081110 001337 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.67.4:50010 is added to blk_-8924134715029713489 size 67108864
081110 001350 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.146:50010 is added to blk_278357163850888 size 67108864
081110 001745 4905 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5135777574223073423 src: /10.251.65.237:48648 dest: /10.251.65.237:50010
081110 001818 4837 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1148756466197125339 terminating
081110 002041 4863 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_9074030793567748915 terminating
081110 002103 4928 INFO dfs.DataNode$PacketResponder: Received block blk_-4067446915270471579 of size 25933924 from /10.251.110.8
081110 002221 4855 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_3259872359515191972 terminating
081110 002223 4824 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_5963816136267066159 terminating
081110 002253 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_200811092030_0002_r_000138_0/part-00138. blk_-210021574616486609
081110 002337 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-1547954353065580372
081110 002914 4955 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8524274644018377752 src: /10.251.42.16:33011 dest: /10.251.42.16:50010
081110 003441 4986 INFO dfs.DataNode$DataXceiver: Receiving block blk_-2015110815660478655 src: /10.251.35.1:53344 dest: /10.251.35.1:50010
081110 003502 4937 INFO dfs.DataNode$PacketResponder: Received block blk_7484759945731484842 of size 67108864 from /10.251.203.149
081110 004247 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.242:50010 is added to blk_-4110733372292809607 size 67108864
081110 010354 5184 INFO dfs.DataNode$PacketResponder: Received block blk_-7152190882826520998 of size 67108864 from /10.250.14.143
081110 010438 4917 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6543889749770222531 src: /10.251.107.98:56224 dest: /10.251.107.98:50010
081110 010506 5128 INFO dfs.DataNode$PacketResponder: Received block blk_8772892713521115147 of size 67108864 from /10.251.194.245
081110 010525 5099 INFO dfs.DataNode$PacketResponder: Received block blk_-6979800421545566530 of size 67108864 from /10.250.10.176
081110 010543 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_000382_0/part-00382. blk_8935202950442998446
081110 010629 5132 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-4799092443170733505 terminating
081110 010759 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_000392_0/part-00392. blk_-3010126661650043258
081110 010801 5186 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-5215520749152394224 terminating
081110 010815 5133 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_397848436489409658 terminating
081110 010851 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.199.86:50010 is added to blk_-9181958042427679309 size 67108864
081110 010917 5109 INFO dfs.DataNode$PacketResponder: Received block blk_-5124326558394984439 of size 67108864 from /10.251.122.79
081110 010918 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.37:50010 is added to blk_-4741127506068309449 size 67108864
081110 010932 5174 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-4937215748597091532 terminating
081110 011001 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.25.237:50010 is added to blk_541463031152673662 size 67108864
081110 011100 5230 INFO dfs.DataNode$PacketResponder: Received block blk_-7514806214244798312 of size 67108864 from /10.250.15.101
081110 011133 5157 INFO dfs.DataNode$PacketResponder: Received block blk_-7007208336332784130 of size 67108864 from /10.251.27.63
081110 011237 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_6996194389878584395
081110 011335 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.146:50010 is added to blk_-6836642008179678048 size 67108864
081110 011352 5321 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_8060713381303305129 terminating
081110 011406 5296 INFO dfs.DataNode$DataXceiver: Receiving block blk_5760391051658436046 src: /10.251.42.84:59752 dest: /10.251.42.84:50010
081110 011431 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_000675_0/part-00675. blk_-7866582011201618766
081110 011448 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.18:50010 is added to blk_-7555801584297376435 size 67108864
081110 011505 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.26.177:50010 is added to blk_-5949883083428548093 size 67108864
081110 011521 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_000705_0/part-00705. blk_-9162090767925303921
081110 011538 5164 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-4196449318426994195 terminating
081110 011713 5420 INFO dfs.DataNode$DataXceiver: Receiving block blk_4238127626194855481 src: /10.250.11.100:59324 dest: /10.250.11.100:50010
081110 011731 5472 INFO dfs.DataNode$PacketResponder: Received block blk_-3805070405372551730 of size 67108864 from /10.251.109.236
081110 011846 5274 INFO dfs.DataNode$PacketResponder: Received block blk_-2013946318750925538 of size 67108864 from /10.251.126.22
081110 011925 5356 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_3278280638950561822 terminating
081110 012005 5368 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-26568864070436835 terminating
081110 012245 5396 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-9161369067334198861 terminating
081110 012351 5301 INFO dfs.DataNode$PacketResponder: Received block blk_-1094025072435547068 of size 67108864 from /10.251.66.3
081110 012357 5523 INFO dfs.DataNode$PacketResponder: Received block blk_6534693994677948144 of size 67108864 from /10.251.201.204
081110 012426 5538 INFO dfs.DataNode$DataXceiver: Receiving block blk_7672124247291403094 src: /10.251.66.3:55531 dest: /10.251.66.3:50010
081110 012522 5491 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-5569558931000121451 terminating
081110 012753 5586 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-4566099704755845087 terminating
081110 012835 5558 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-8093500164295459838 terminating
081110 012855 5509 INFO dfs.DataNode$PacketResponder: Received block blk_-3787594914477940893 of size 67108864 from /10.251.71.16
081110 012857 5605 INFO dfs.DataNode$PacketResponder: Received block blk_426381928692704514 of size 67108864 from /10.251.122.38
081110 013125 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001095_0/part-01095. blk_-7413910507260978729
081110 013220 5704 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_4774656339381154282 terminating
081110 013231 5659 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8227485321105019293 src: /10.251.195.33:56999 dest: /10.251.195.33:50010
081110 013306 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.75.79:50010 is added to blk_5210211633167259832 size 67108864
081110 013406 5493 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_2917692073041583988 terminating
081110 013412 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.199.159:50010 is added to blk_-8558845834445970595 size 67108864
081110 013502 5576 INFO dfs.DataNode$PacketResponder: Received block blk_3852281952649038166 of size 67108864 from /10.251.122.65
081110 013506 5649 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-5442040196315589272 terminating
081110 013537 5804 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6486714614940767660 src: /10.250.11.53:48454 dest: /10.250.11.53:50010
081110 013542 5646 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_755689327380778318 terminating
081110 013545 5696 INFO dfs.DataNode$PacketResponder: Received block blk_-8847552694913526632 of size 67108864 from /10.251.201.204
081110 013559 5614 INFO dfs.DataNode$PacketResponder: Received block blk_7736147489540456118 of size 67108864 from /10.251.74.227
081110 013715 5706 INFO dfs.DataNode$PacketResponder: Received block blk_-3677956642639780952 of size 67108864 from /10.251.123.20
081110 013721 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_3073404534069578592 size 67108864
081110 013855 5764 INFO dfs.DataNode$PacketResponder: Received block blk_1498192967843651378 of size 67108864 from /10.251.42.84
081110 013938 4501 INFO dfs.DataNode$DataXceiver: Receiving block blk_148513789886825227 src: /10.251.123.33:54308 dest: /10.251.123.33:50010
081110 014002 5798 INFO dfs.DataNode$DataXceiver: Receiving block blk_7229716389774599497 src: /10.251.201.204:38310 dest: /10.251.201.204:50010
081110 014023 5733 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_4741107979793372752 terminating
081110 014033 5854 INFO dfs.DataNode$DataXceiver: Receiving block blk_691517018986659607 src: /10.251.195.70:59379 dest: /10.251.195.70:50010
081110 014043 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.129:50010 is added to blk_6002109944224710483 size 67108864
081110 014050 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.122.65:50010 is added to blk_5095897972833086609 size 67108864
081110 014138 5832 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-3802183646922512795 terminating
081110 014145 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.201.204:50010 is added to blk_9072829105973334075 size 67108864
081110 014207 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001365_0/part-01365. blk_4841101867353115844
081110 014257 5803 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_1997668456720275587 terminating
081110 014314 5599 INFO dfs.DataNode$PacketResponder: Received block blk_3304535870056962597 of size 67108864 from /10.251.203.166
081110 014349 5817 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-4853183643928986291 terminating
081110 014421 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.196:50010 is added to blk_6828121630547887549 size 67108864
081110 014423 5775 INFO dfs.DataNode$PacketResponder: Received block blk_3084588477220583373 of size 67108864 from /10.251.125.174
081110 014428 5801 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-653434419514937486 terminating
081110 014519 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001506_0/part-01506. blk_1104376461837247304
081110 014520 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.130:50010 is added to blk_-5511630179413516751 size 67108864
081110 014536 6005 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_4404895858852856851 terminating
081110 014727 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001597_0/part-01597. blk_-591552904934794824
081110 014738 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001377_0/part-01377. blk_-8408087700903896282
081110 014808 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.122.38:50010 is added to blk_-5404881839098659470 size 67108864
081110 014847 5846 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_703046261120042821 terminating
081110 014914 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001728_0/part-01728. blk_-488298625688742454
081110 014953 5959 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_6465095458973828118 terminating
081110 015013 6133 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-9110604833387121363 terminating
081110 015018 5852 INFO dfs.DataNode$DataXceiver: Receiving block blk_5977008528949258706 src: /10.251.214.32:43860 dest: /10.251.214.32:50010
081110 015024 5938 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_3781645897486585075 terminating
081110 015031 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.194:50010 is added to blk_-1635752698830216319 size 67108864
081110 015334 5970 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_3070718541088657475 terminating
081110 015340 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001661_0/part-01661. blk_-1674094254141559552
081110 015423 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.197.161:50010 is added to blk_6080034782037237879 size 67108864
081110 015432 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001630_0/part-01630. blk_-8645365827804358878
081110 015600 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001817_0/part-01817. blk_3291173852929756766
081110 015607 6086 INFO dfs.DataNode$PacketResponder: Received block blk_892900079071425422 of size 67108864 from /10.251.73.188
081110 015626 5971 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_2243777557693112935 terminating
081110 015649 6096 INFO dfs.DataNode$PacketResponder: Received block blk_-8192956077351896648 of size 67108864 from /10.251.110.196
081110 015716 6066 INFO dfs.DataNode$PacketResponder: Received block blk_5250287988089075029 of size 67108864 from /10.251.66.63
081110 015719 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.30.6:50010 is added to blk_446513188203036714 size 67108864
081110 015740 6116 INFO dfs.DataNode$PacketResponder: Received block blk_-2020273862360001901 of size 67108864 from /10.250.7.244
081110 015750 5956 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_3338047823148530794 terminating
081110 015759 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001840_0/part-01840. blk_4210124392039605712
081110 015832 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.240:50010 is added to blk_-6330812010936261924 size 67108864
081110 015942 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_200811092030_0003_m_001976_0/part-01976. blk_-8380267327243110056
081110 015959 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.110.160:50010 is added to blk_-8077373411465716253 size 67108864
081110 020011 6037 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-9198375937828232046 terminating
081110 020019 5935 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_9107849594802857322 terminating
081110 020034 6204 INFO dfs.DataNode$DataXceiver: Receiving block blk_271142976916798281 src: /10.251.70.112:50176 dest: /10.251.70.112:50010
081110 020140 6146 INFO dfs.DataNode$PacketResponder: Received block blk_4029732570544915384 of size 67108864 from /10.251.35.1
081110 020249 6148 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_679168189072909208 terminating
081110 020302 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.192:50010 is added to blk_6285161494041403917 size 67108864
081110 020313 6034 INFO dfs.DataNode$PacketResponder: Received block blk_-5306443381473737599 of size 67108864 from /10.251.195.52
081110 020322 6131 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-2466006073687914843 terminating
081110 020325 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.210:50010 is added to blk_-6995454028444097802 size 67108864
081110 020357 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.6.4:50010 is added to blk_344934934043642744 size 67108864
081110 020458 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.230:50010 is added to blk_4984720043834039361 size 67108864
081110 020509 6043 INFO dfs.DataNode$DataXceiver: Receiving block blk_6398122733993275620 src: /10.251.111.80:53737 dest: /10.251.111.80:50010
081110 020632 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.192:50010 is added to blk_325981833999429148 size 67108864
081110 020724 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2568309208894455676 is added to invalidSet of 10.251.31.160:50010
081110 022226 6281 INFO dfs.DataNode$DataXceiver: 10.250.19.16:50010 Served block blk_1078000656626961731 to /10.250.19.16
081110 023456 6415 WARN dfs.DataNode$DataXceiver: 10.251.67.225:50010:Got exception while serving blk_-6900989714336081087 to /10.251.25.237:
081110 024834 6371 INFO dfs.DataNode$DataXceiver: 10.251.126.227:50010 Served block blk_-8306714721294235181 to /10.251.126.227
081110 030331 6561 WARN dfs.DataNode$DataXceiver: 10.251.42.191:50010:Got exception while serving blk_-8023826090828946372 to /10.251.214.130:
081110 030942 6646 WARN dfs.DataNode$DataXceiver: 10.251.31.5:50010:Got exception while serving blk_-1367876730256254709 to /10.251.67.225:
081110 031019 6604 INFO dfs.DataNode$DataXceiver: 10.251.126.83:50010 Served block blk_-3860894070657427592 to /10.251.126.83
081110 032126 6555 WARN dfs.DataNode$DataXceiver: 10.251.26.8:50010:Got exception while serving blk_-7983508786213002472 to /10.251.38.197:
081110 035357 6606 INFO dfs.DataNode$DataXceiver: 10.251.71.97:50010 Served block blk_5454332143498402824 to /10.250.15.67
081110 040800 6739 INFO dfs.DataNode$DataXceiver: 10.250.6.214:50010 Served block blk_-3384560576963801177 to /10.250.6.214
081110 042826 6827 INFO dfs.DataNode$DataXceiver: 10.251.67.4:50010 Served block blk_-2901225370888235702 to /10.251.91.84
081110 045413 6957 INFO dfs.DataNode$DataXceiver: 10.251.215.50:50010 Served block blk_3963698556744744747 to /10.251.107.196
081110 050300 6683 INFO dfs.DataNode$DataXceiver: 10.251.30.134:50010 Served block blk_2039230511363331616 to /10.251.65.203
081110 051323 6956 INFO dfs.DataNode$DataXceiver: 10.251.71.146:50010 Served block blk_8482111101480751895 to /10.251.71.146
081110 054642 7145 INFO dfs.DataNode$DataXceiver: 10.251.215.16:50010 Served block blk_-5919767990596301121 to /10.251.215.16
081110 060453 7193 INFO dfs.DataNode$DataXceiver: 10.251.199.225:50010 Served block blk_8457344665564381337 to /10.251.199.225
081110 060934 7211 INFO dfs.DataNode$DataXceiver: 10.251.66.102:50010 Served block blk_2986720270598512615 to /10.251.66.102
081110 065635 7324 INFO dfs.DataNode$DataXceiver: 10.251.90.64:50010 Served block blk_-5719934513583495857 to /10.251.199.245
081110 070326 7430 WARN dfs.DataNode$DataXceiver: 10.251.75.228:50010:Got exception while serving blk_-7680599654910200999 to /10.251.75.228:
081110 070334 7327 INFO dfs.DataNode$DataXceiver: 10.251.111.37:50010 Served block blk_-6050976999174805557 to /10.251.111.37
081110 070347 7574 INFO dfs.DataNode$DataXceiver: 10.250.10.6:50010 Served block blk_1598414622053793245 to /10.251.90.239
081110 070614 7513 INFO dfs.DataNode$DataXceiver: 10.250.19.16:50010 Served block blk_-7837339190764609698 to /10.251.197.161
081110 070921 7744 INFO dfs.DataNode$DataXceiver: 10.251.39.144:50010 Served block blk_-8187008844253719581 to /10.251.91.32
081110 071154 7627 INFO dfs.DataNode$DataXceiver: 10.251.214.112:50010 Served block blk_4081177399275502985 to /10.251.110.68
081110 071426 7742 INFO dfs.DataNode$DataXceiver: 10.251.42.191:50010 Served block blk_3515154079719300106 to /10.251.42.191
081110 071657 7700 WARN dfs.DataNode$DataXceiver: 10.251.38.214:50010:Got exception while serving blk_5547569777499890340 to /10.251.195.52:
081110 072121 7820 INFO dfs.DataNode$DataXceiver: 10.251.123.99:50010 Served block blk_5385121219895615240 to /10.251.71.146
081110 072124 7772 WARN dfs.DataNode$DataXceiver: 10.251.42.246:50010:Got exception while serving blk_-7658293778087733436 to /10.251.30.179:
081110 080546 7970 WARN dfs.DataNode$DataXceiver: 10.251.111.130:50010:Got exception while serving blk_3169060243663461885 to /10.251.214.32:
081110 080555 8227 INFO dfs.DataNode$DataXceiver: 10.250.11.194:50010 Served block blk_3087787567144441647 to /10.251.91.84
081110 080718 7850 INFO dfs.DataNode$DataXceiver: 10.250.10.100:50010 Served block blk_-3657665801189425193 to /10.250.10.100
081110 080724 8080 INFO dfs.DataNode$DataXceiver: 10.251.74.79:50010 Served block blk_-3457731723401426942 to /10.251.74.79
081110 080814 8123 INFO dfs.DataNode$DataXceiver: 10.251.42.84:50010 Served block blk_6105506155797750768 to /10.251.42.84
081110 080847 8088 WARN dfs.DataNode$DataXceiver: 10.251.26.8:50010:Got exception while serving blk_-8522942048313632858 to /10.251.31.85:
081110 080922 7633 INFO dfs.DataNode$DataXceiver: 10.251.203.246:50010 Served block blk_365496398062338141 to /10.251.203.246
081110 080949 7994 INFO dfs.DataNode$DataXceiver: 10.250.19.227:50010 Served block blk_3979872751691718643 to /10.250.19.227
081110 081044 8125 WARN dfs.DataNode$DataXceiver: 10.251.123.1:50010:Got exception while serving blk_-272707591443354058 to /10.251.198.33:
081110 081054 8108 WARN dfs.DataNode$DataXceiver: 10.251.90.239:50010:Got exception while serving blk_-8679916835272129336 to /10.250.15.198:
081110 081337 8145 WARN dfs.DataNode$DataXceiver: 10.251.66.102:50010:Got exception while serving blk_6106884317961925960 to /10.251.66.102:
081110 081515 8312 WARN dfs.DataNode$DataXceiver: 10.251.31.5:50010:Got exception while serving blk_6332892729727950039 to /10.251.123.1:
081110 081643 8095 INFO dfs.DataNode$DataXceiver: 10.251.195.52:50010 Served block blk_6655622109568310643 to /10.251.195.52
081110 081643 8247 WARN dfs.DataNode$DataXceiver: 10.250.17.225:50010:Got exception while serving blk_-5935642747315643391 to /10.251.199.150:
081110 081741 8169 WARN dfs.DataNode$DataXceiver: 10.251.215.70:50010:Got exception while serving blk_-20269367189114433 to /10.251.30.179:
081110 082013 8326 INFO dfs.DataNode$DataXceiver: 10.251.43.115:50010 Served block blk_-7364557883931785608 to /10.251.43.115
081110 082043 8305 INFO dfs.DataNode$DataXceiver: 10.251.215.16:50010 Served block blk_2322432806134104317 to /10.251.215.16
081110 082444 8172 INFO dfs.DataNode$DataXceiver: 10.251.109.209:50010 Served block blk_4848669047361069041 to /10.251.26.177
081110 082702 8223 WARN dfs.DataNode$DataXceiver: 10.250.15.198:50010:Got exception while serving blk_-1851265222873801714 to /10.251.195.52:
081110 082706 8552 WARN dfs.DataNode$DataXceiver: 10.251.198.33:50010:Got exception while serving blk_-8495670552887053546 to /10.250.10.223:
081110 082737 8341 WARN dfs.DataNode$DataXceiver: 10.250.19.227:50010:Got exception while serving blk_-7372087176866857012 to /10.251.110.68:
081110 082954 8543 WARN dfs.DataNode$DataXceiver: 10.251.70.112:50010:Got exception while serving blk_4357276972386184626 to /10.251.74.79:
081110 083045 8495 WARN dfs.DataNode$DataXceiver: 10.251.215.16:50010:Got exception while serving blk_-4590972095204776122 to /10.251.30.6:
081110 083121 8197 INFO dfs.DataNode$DataXceiver: 10.251.106.214:50010 Served block blk_-8277873627721528374 to /10.251.122.79
081110 083231 8530 INFO dfs.DataNode$DataXceiver: 10.250.10.176:50010 Served block blk_3797494971676497497 to /10.251.29.239
081110 083328 8416 INFO dfs.DataNode$DataXceiver: 10.251.126.22:50010 Served block blk_805587860540600864 to /10.251.126.22
081110 083453 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_3141363517520802396
081110 085042 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-1383276859207001328
081110 085933 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-4117999745005013424
081110 091216 7593 WARN dfs.DataNode$DataXceiver: 10.251.107.50:50010:Got exception while serving blk_4524198807982839635 to /10.251.122.79:
081110 091550 8683 WARN dfs.DataNode$DataXceiver: 10.251.111.209:50010:Got exception while serving blk_7505828172725463922 to /10.251.111.209:
081110 091657 8720 WARN dfs.DataNode$DataXceiver: 10.251.70.211:50010:Got exception while serving blk_424255210146453297 to /10.251.203.179:
081110 091800 8380 INFO dfs.DataNode$DataXceiver: 10.250.14.224:50010 Served block blk_666713934549639791 to /10.250.14.224
081110 092131 8815 INFO dfs.DataNode$DataXceiver: 10.251.30.179:50010 Served block blk_-2975629975082443857 to /10.251.30.179
081110 092151 8601 WARN dfs.DataNode$DataXceiver: 10.251.126.22:50010:Got exception while serving blk_1686195200514944346 to /10.250.6.223:
081110 092459 8650 INFO dfs.DataNode$DataXceiver: 10.250.9.207:50010 Served block blk_4355450627202483068 to /10.250.9.207
081110 092815 8872 INFO dfs.DataNode$DataXceiver: 10.250.6.191:50010 Served block blk_5952254363678329024 to /10.250.6.191
081110 093020 8827 WARN dfs.DataNode$DataXceiver: 10.251.70.211:50010:Got exception while serving blk_-667933171485085225 to /10.251.203.246:
081110 093643 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_9188832735514090334
081110 093831 8571 INFO dfs.DataNode$DataXceiver: 10.251.106.10:50010 Served block blk_2273334621242106674 to /10.251.106.10
081110 094019 8808 WARN dfs.DataNode$DataXceiver: 10.251.125.193:50010:Got exception while serving blk_3790492230047189408 to /10.251.199.159:
081110 094657 7835 INFO dfs.DataNode$DataXceiver: 10.251.107.50:50010 Served block blk_-2285729896739318683 to /10.251.70.5
081110 103026 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1233005817943453613 is added to invalidSet of 10.251.75.49:50010
081110 103026 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8736461628840265232 is added to invalidSet of 10.251.195.70:50010
081110 103027 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_166171721314010075 is added to invalidSet of 10.251.30.85:50010
081110 103027 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3362838757940877177 is added to invalidSet of 10.250.5.161:50010
081110 103027 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8026264173633579073 is added to invalidSet of 10.251.214.18:50010
081110 103027 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8350646254685996250 is added to invalidSet of 10.251.199.159:50010
081110 103028 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-4702684774678057084 is added to invalidSet of 10.250.13.240:50010
081110 103029 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_434034567989619521 is added to invalidSet of 10.251.91.15:50010
081110 103030 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_413552235254424009 is added to invalidSet of 10.251.35.1:50010
081110 103030 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7177118590277025973 is added to invalidSet of 10.251.121.224:50010
081110 103032 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2576408434603492552 is added to invalidSet of 10.251.106.37:50010
081110 103034 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_356592557993755459 is added to invalidSet of 10.251.109.236:50010
081110 103035 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_9111860234027679206 is added to invalidSet of 10.251.39.209:50010
081110 103036 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5178539130945718870 is added to invalidSet of 10.251.107.19:50010
081110 103037 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1331750856499198500 is added to invalidSet of 10.251.38.53:50010
081110 103038 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3792836284792472725 is added to invalidSet of 10.251.106.50:50010
081110 103038 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6031076349976322578 is added to invalidSet of 10.251.67.225:50010
081110 103040 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_4438674026376140804 is added to invalidSet of 10.251.106.50:50010
081110 103041 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1777689952069868672 is added to invalidSet of 10.251.31.85:50010
081110 103044 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3275788837420923000 is added to invalidSet of 10.251.109.236:50010
081110 103045 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6056336045486998577 is added to invalidSet of 10.251.126.255:50010
081110 103046 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5368528328022648701 is added to invalidSet of 10.251.202.181:50010
081110 103047 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-9056276727170871967 is added to invalidSet of 10.251.66.3:50010
081110 103049 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6289001686128460561 is added to invalidSet of 10.251.91.84:50010
081110 103049 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6855535886504612774 is added to invalidSet of 10.251.70.211:50010
081110 103050 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6477590833521072407 is added to invalidSet of 10.251.39.179:50010
081110 103050 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_865453227320668516 is added to invalidSet of 10.251.71.240:50010
081110 103054 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2149728827322918746 is added to invalidSet of 10.250.11.194:50010
081110 103055 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6324712740029479576 is added to invalidSet of 10.251.30.6:50010
081110 103056 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6763863888784019261 is added to invalidSet of 10.251.70.37:50010
081110 103057 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1543605971733759530 is added to invalidSet of 10.251.195.33:50010
081110 103059 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6114220623980201749 is added to invalidSet of 10.250.13.240:50010
081110 103059 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-945770324989341170 is added to invalidSet of 10.251.31.180:50010
081110 103107 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5145886774827574591 is added to invalidSet of 10.251.105.189:50010
081110 103109 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3598904881888399067 is added to invalidSet of 10.250.5.237:50010
081110 103111 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6255015379143356507 is added to invalidSet of 10.251.111.37:50010
081110 103111 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8247499802516177421 is added to invalidSet of 10.250.7.96:50010
081110 103111 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8403618945096094999 is added to invalidSet of 10.251.42.9:50010
081110 103112 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-2012430628930443984 is added to invalidSet of 10.250.11.100:50010
081110 103112 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7819984111120374407 is added to invalidSet of 10.251.30.6:50010
081110 103113 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_4499140370762613763 is added to invalidSet of 10.251.43.115:50010
081110 103114 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2304296078821316612 is added to invalidSet of 10.251.39.64:50010
081110 103114 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_358331073189608564 is added to invalidSet of 10.251.67.225:50010
081110 103114 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8404604116961181201 is added to invalidSet of 10.250.13.188:50010
081110 103117 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3582070652284403613 is added to invalidSet of 10.250.10.213:50010
081110 103123 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-4954795063440871977 is added to invalidSet of 10.251.203.80:50010
081110 103127 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6640779266188771131 is added to invalidSet of 10.251.126.83:50010
081110 103130 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3858015337960682724 is added to invalidSet of 10.251.214.130:50010
081110 103130 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_570916157110129687 is added to invalidSet of 10.251.214.18:50010
081110 103132 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7418499425236853516 is added to invalidSet of 10.251.203.4:50010
081110 103136 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7894824934989391122 is added to invalidSet of 10.251.38.197:50010
081110 103137 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5005618994659797504 is added to invalidSet of 10.251.125.193:50010
081110 103137 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7932704730736035192 is added to invalidSet of 10.251.199.245:50010
081110 103139 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-2834195813213005328 is added to invalidSet of 10.250.7.244:50010
081110 103141 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5972745462629982078 is added to invalidSet of 10.251.31.5:50010
081110 103142 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7111788899695387329 is added to invalidSet of 10.251.75.228:50010
081110 103146 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1110880362999960078 is added to invalidSet of 10.251.194.147:50010
081110 103146 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6489659315944341351 is added to invalidSet of 10.251.106.50:50010
081110 103146 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7324196727355442810 is added to invalidSet of 10.251.203.80:50010
081110 103147 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-2589662411410634043 is added to invalidSet of 10.251.42.16:50010
081110 103147 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8602496359012842630 is added to invalidSet of 10.250.7.32:50010
081110 103148 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-133919539232421451 is added to invalidSet of 10.250.10.176:50010
081110 103149 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6136511938931864993 is added to invalidSet of 10.250.14.38:50010
081110 103149 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8871064397020855320 is added to invalidSet of 10.251.42.84:50010
081110 103201 19 INFO dfs.FSDataset: Deleting block blk_8483848473254499625 file /mnt/hadoop/dfs/data/current/subdir51/blk_8483848473254499625
081110 103233 8946 INFO dfs.DataNode$PacketResponder: Received block blk_-9188933731705791676 of size 67108864 from /10.251.39.192
081110 103257 19 INFO dfs.FSDataset: Deleting block blk_-8898274302731129139 file /mnt/hadoop/dfs/data/current/subdir18/blk_-8898274302731129139
081110 103320 18 INFO dfs.FSDataset: Deleting block blk_-8014701913801168461 file /mnt/hadoop/dfs/data/current/subdir27/blk_-8014701913801168461
081110 103321 19 INFO dfs.FSDataset: Deleting block blk_-8775602795571523802 file /mnt/hadoop/dfs/data/current/subdir29/blk_-8775602795571523802
081110 103327 19 INFO dfs.FSDataset: Deleting block blk_-7928230000822317050 file /mnt/hadoop/dfs/data/current/subdir9/blk_-7928230000822317050
081110 103328 19 INFO dfs.FSDataset: Deleting block blk_8303413189200230139 file /mnt/hadoop/dfs/data/current/subdir32/blk_8303413189200230139
081110 103331 19 INFO dfs.FSDataset: Deleting block blk_-9038475355621289969 file /mnt/hadoop/dfs/data/current/subdir41/blk_-9038475355621289969
081110 103333 9016 INFO dfs.DataNode$PacketResponder: Received block blk_4263664068073345850 of size 67108864 from /10.251.214.67
081110 103335 19 INFO dfs.FSDataset: Deleting block blk_-8848810702648406400 file /mnt/hadoop/dfs/data/current/subdir11/blk_-8848810702648406400
081110 103340 9214 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6997219177553478996 terminating
081110 103343 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_000206_0/part-00206. blk_-2230892279625104430
081110 103347 19 INFO dfs.FSDataset: Deleting block blk_-7515058933811567980 file /mnt/hadoop/dfs/data/current/subdir1/blk_-7515058933811567980
081110 103348 19 INFO dfs.FSDataset: Deleting block blk_-8484827087450439270 file /mnt/hadoop/dfs/data/current/subdir25/blk_-8484827087450439270
081110 103349 19 INFO dfs.FSDataset: Deleting block blk_-8144387882075572886 file /mnt/hadoop/dfs/data/current/subdir42/blk_-8144387882075572886
081110 103354 19 INFO dfs.FSDataset: Deleting block blk_-8383503596684323017 file /mnt/hadoop/dfs/data/current/subdir26/blk_-8383503596684323017
081110 103400 19 INFO dfs.FSDataset: Deleting block blk_-8796256534683400159 file /mnt/hadoop/dfs/data/current/subdir18/blk_-8796256534683400159
081110 103403 19 INFO dfs.FSDataset: Deleting block blk_-8775602795571523802 file /mnt/hadoop/dfs/data/current/subdir41/blk_-8775602795571523802
081110 103412 7249 INFO dfs.DataNode$PacketResponder: Received block blk_4098532420907418423 of size 67108864 from /10.251.43.147
081110 103430 19 INFO dfs.FSDataset: Deleting block blk_-4368236874477798428 file /mnt/hadoop/dfs/data/current/subdir33/blk_-4368236874477798428
081110 103432 19 INFO dfs.FSDataset: Deleting block blk_-4223625526513535681 file /mnt/hadoop/dfs/data/current/subdir60/blk_-4223625526513535681
081110 103437 19 INFO dfs.FSDataset: Deleting block blk_-6400421658804613208 file /mnt/hadoop/dfs/data/current/subdir29/blk_-6400421658804613208
081110 103442 19 INFO dfs.FSDataset: Deleting block blk_-5880591395067598620 file /mnt/hadoop/dfs/data/current/subdir32/blk_-5880591395067598620
081110 103442 19 INFO dfs.FSDataset: Deleting block blk_-7096508472270520039 file /mnt/hadoop/dfs/data/current/subdir20/blk_-7096508472270520039
081110 103444 19 INFO dfs.FSDataset: Deleting block blk_-1939956833921604216 file /mnt/hadoop/dfs/data/current/subdir34/blk_-1939956833921604216
081110 103512 19 INFO dfs.FSDataset: Deleting block blk_-3860035787700398391 file /mnt/hadoop/dfs/data/current/subdir6/blk_-3860035787700398391
081110 103524 19 INFO dfs.FSDataset: Deleting block blk_-6008397446568765070 file /mnt/hadoop/dfs/data/current/blk_-6008397446568765070
081110 103527 19 INFO dfs.FSDataset: Deleting block blk_-2425070029518924403 file /mnt/hadoop/dfs/data/current/subdir39/blk_-2425070029518924403
081110 103533 18 INFO dfs.FSDataset: Deleting block blk_826351419727053015 file /mnt/hadoop/dfs/data/current/subdir6/blk_826351419727053015
081110 103533 19 INFO dfs.FSDataset: Deleting block blk_-3437475890798229344 file /mnt/hadoop/dfs/data/current/subdir57/blk_-3437475890798229344
081110 103534 19 INFO dfs.FSDataset: Deleting block blk_-1492402439215451727 file /mnt/hadoop/dfs/data/current/subdir25/blk_-1492402439215451727
081110 103538 18 INFO dfs.FSDataset: Deleting block blk_-4298255715894292387 file /mnt/hadoop/dfs/data/current/subdir38/blk_-4298255715894292387
081110 103547 19 INFO dfs.FSDataset: Deleting block blk_-4295805058711840933 file /mnt/hadoop/dfs/data/current/subdir0/blk_-4295805058711840933
081110 103548 19 INFO dfs.FSDataset: Deleting block blk_830855781964014378 file /mnt/hadoop/dfs/data/current/subdir48/blk_830855781964014378
081110 103551 19 INFO dfs.FSDataset: Deleting block blk_8291708741145026623 file /mnt/hadoop/dfs/data/current/subdir57/blk_8291708741145026623
081110 103552 19 INFO dfs.FSDataset: Deleting block blk_-921231092429668424 file /mnt/hadoop/dfs/data/current/subdir23/blk_-921231092429668424
081110 103553 19 INFO dfs.FSDataset: Deleting block blk_-1581605246928123722 file /mnt/hadoop/dfs/data/current/subdir13/blk_-1581605246928123722
081110 103554 18 INFO dfs.FSDataset: Deleting block blk_-2445347158368483245 file /mnt/hadoop/dfs/data/current/subdir30/blk_-2445347158368483245
081110 103554 19 INFO dfs.FSDataset: Deleting block blk_-1897524391075610396 file /mnt/hadoop/dfs/data/current/subdir47/blk_-1897524391075610396
081110 103557 19 INFO dfs.FSDataset: Deleting block blk_-8588908000680498 file /mnt/hadoop/dfs/data/current/subdir21/blk_-8588908000680498
081110 103558 19 INFO dfs.FSDataset: Deleting block blk_561417755647618727 file /mnt/hadoop/dfs/data/current/subdir4/blk_561417755647618727
081110 103600 19 INFO dfs.FSDataset: Deleting block blk_-6127741703501838658 file /mnt/hadoop/dfs/data/current/subdir53/blk_-6127741703501838658
081110 103603 18 INFO dfs.FSDataset: Deleting block blk_-3636681193077858741 file /mnt/hadoop/dfs/data/current/subdir37/blk_-3636681193077858741
081110 103615 18 INFO dfs.FSDataset: Deleting block blk_-643763844763678010 file /mnt/hadoop/dfs/data/current/subdir29/blk_-643763844763678010
081110 103615 19 INFO dfs.FSDataset: Deleting block blk_-2826763604408365998 file /mnt/hadoop/dfs/data/current/subdir40/blk_-2826763604408365998
081110 103618 19 INFO dfs.FSDataset: Deleting block blk_2278708746675870163 file /mnt/hadoop/dfs/data/current/subdir27/blk_2278708746675870163
081110 103621 19 INFO dfs.FSDataset: Deleting block blk_153508158653341141 file /mnt/hadoop/dfs/data/current/subdir50/blk_153508158653341141
081110 103621 19 INFO dfs.FSDataset: Deleting block blk_-1775264355226395799 file /mnt/hadoop/dfs/data/current/subdir26/blk_-1775264355226395799
081110 103626 18 INFO dfs.FSDataset: Deleting block blk_-1599621098605751235 file /mnt/hadoop/dfs/data/current/subdir16/blk_-1599621098605751235
081110 103631 19 INFO dfs.FSDataset: Deleting block blk_-1111344660779690782 file /mnt/hadoop/dfs/data/current/subdir40/blk_-1111344660779690782
081110 103631 19 INFO dfs.FSDataset: Deleting block blk_3585588704565901062 file /mnt/hadoop/dfs/data/current/subdir15/blk_3585588704565901062
081110 103634 19 INFO dfs.FSDataset: Deleting block blk_2756799797410851893 file /mnt/hadoop/dfs/data/current/subdir24/blk_2756799797410851893
081110 103641 19 INFO dfs.FSDataset: Deleting block blk_7323287581932922676 file /mnt/hadoop/dfs/data/current/subdir12/blk_7323287581932922676
081110 103655 9020 INFO dfs.DataNode$PacketResponder: Received block blk_-4935573595350235765 of size 67108864 from /10.251.123.195
081110 103657 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_000097_0/part-00097. blk_496376132244907301
081110 103659 19 INFO dfs.FSDataset: Deleting block blk_2731617036199392057 file /mnt/hadoop/dfs/data/current/subdir39/blk_2731617036199392057
081110 103704 19 INFO dfs.FSDataset: Deleting block blk_7190156588310412626 file /mnt/hadoop/dfs/data/current/subdir30/blk_7190156588310412626
081110 103717 19 INFO dfs.FSDataset: Deleting block blk_8189008752707332487 file /mnt/hadoop/dfs/data/current/subdir7/blk_8189008752707332487
081110 103719 19 INFO dfs.FSDataset: Deleting block blk_6828263829429857572 file /mnt/hadoop/dfs/data/current/subdir26/blk_6828263829429857572
081110 103729 19 INFO dfs.FSDataset: Deleting block blk_8492196963764530259 file /mnt/hadoop/dfs/data/current/subdir45/blk_8492196963764530259
081110 103731 19 INFO dfs.FSDataset: Deleting block blk_7942183449206967755 file /mnt/hadoop/dfs/data/current/subdir30/blk_7942183449206967755
081110 103734 19 INFO dfs.FSDataset: Deleting block blk_6223504502438507199 file /mnt/hadoop/dfs/data/current/subdir0/blk_6223504502438507199
081110 103735 19 INFO dfs.FSDataset: Deleting block blk_7519257846502835091 file /mnt/hadoop/dfs/data/current/subdir15/blk_7519257846502835091
081110 103739 19 INFO dfs.FSDataset: Deleting block blk_5272005015837990021 file /mnt/hadoop/dfs/data/current/subdir0/blk_5272005015837990021
081110 103743 19 INFO dfs.FSDataset: Deleting block blk_7090935285617451450 file /mnt/hadoop/dfs/data/current/subdir60/blk_7090935285617451450
081110 103749 19 INFO dfs.FSDataset: Deleting block blk_8145984793403459836 file /mnt/hadoop/dfs/data/current/blk_8145984793403459836
081110 103755 19 INFO dfs.FSDataset: Deleting block blk_7603278760520020831 file /mnt/hadoop/dfs/data/current/subdir59/blk_7603278760520020831
081110 103801 19 INFO dfs.FSDataset: Deleting block blk_9169945668827621796 file /mnt/hadoop/dfs/data/current/subdir50/blk_9169945668827621796
081110 103812 9115 INFO dfs.DataNode$PacketResponder: Received block blk_-5668584889663038916 of size 67108864 from /10.250.9.207
081110 103818 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.194.147:50010 is added to blk_5277338321835341602 size 67108864
081110 103819 19 INFO dfs.FSDataset: Deleting block blk_6973321719929905054 file /mnt/hadoop/dfs/data/current/subdir14/blk_6973321719929905054
081110 103821 19 INFO dfs.FSDataset: Deleting block blk_5948455948626091880 file /mnt/hadoop/dfs/data/current/subdir28/blk_5948455948626091880
081110 103827 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.75.49:50010 is added to blk_-3684613761619321001 size 67108864
081110 103840 19 INFO dfs.FSDataset: Deleting block blk_6497260538939100482 file /mnt/hadoop/dfs/data/current/subdir46/blk_6497260538939100482
081110 103840 9117 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6991853982611346454 terminating
081110 103850 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_000056_0/part-00056. blk_6140788650991100539
081110 103922 19 INFO dfs.FSDataset: Deleting block blk_7754477838724397551 file /mnt/hadoop/dfs/data/current/subdir46/blk_7754477838724397551
081110 104018 9085 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-3452734573703603740 terminating
081110 104047 8882 INFO dfs.DataNode$PacketResponder: Received block blk_-4034227974316905164 of size 67108864 from /10.250.15.198
081110 104224 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_000346_0/part-00346. blk_-2016403519138424998
081110 104231 8978 INFO dfs.DataNode$DataXceiver: Receiving block blk_2972699288862031202 src: /10.250.5.161:51077 dest: /10.250.5.161:50010
081110 104309 9091 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4956232013010963754 src: /10.251.126.22:58984 dest: /10.251.126.22:50010
081110 104321 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.15.198:50010 is added to blk_8894334107354324777 size 3549832
081110 104357 9263 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6035642070260562886 terminating
081110 104401 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.196:50010 is added to blk_4802366409699286298 size 3547304
081110 104404 9034 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1753161039594546561 src: /10.251.203.149:33685 dest: /10.251.203.149:50010
081110 104407 9110 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_859668537217019080 terminating
081110 104600 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_000121_0/part-00121. blk_-2073265102548439691
081110 104825 9232 INFO dfs.DataNode$PacketResponder: Received block blk_3083630442630693250 of size 67108864 from /10.251.215.70
081110 104834 9213 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-4624597753370705281 terminating
081110 104839 9163 INFO dfs.DataNode$PacketResponder: Received block blk_-5236578579938215340 of size 67108864 from /10.250.7.244
081110 104847 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.144:50010 is added to blk_-6592952246351162022 size 67108864
081110 104914 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.14.224:50010 is added to blk_-3329473910909732817 size 67108864
081110 105000 19 INFO dfs.FSDataset: Deleting block blk_-1347057913082589493 file /mnt/hadoop/dfs/data/current/subdir56/blk_-1347057913082589493
081110 105149 9274 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8198423074054354279 src: /10.251.203.149:46429 dest: /10.251.203.149:50010
081110 105203 9267 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_391813102554451063 terminating
081110 105219 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.67:50010 is added to blk_-6449891932967810179 size 67108864
081110 105252 9416 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8563999607069929505 src: /10.251.107.19:60076 dest: /10.251.107.19:50010
081110 105345 19 INFO dfs.FSDataset: Deleting block blk_-4304240611296196935 file /mnt/hadoop/dfs/data/current/subdir55/blk_-4304240611296196935
081110 105456 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.110.68:50010 is added to blk_-3663004104512304532 size 67108864
081110 105512 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_000749_0/part-00749. blk_-6909607053783233975
081110 105521 9428 INFO dfs.DataNode$PacketResponder: Received block blk_-2141627858612526069 of size 67108864 from /10.251.111.37
081110 105553 9531 INFO dfs.DataNode$DataXceiver: Receiving block blk_5272731391668336674 src: /10.250.6.191:46509 dest: /10.250.6.191:50010
081110 105607 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.179:50010 is added to blk_5491101910066444061 size 67108864
081110 105628 19 INFO dfs.FSDataset: Deleting block blk_5075966182561035053 file /mnt/hadoop/dfs/data/current/subdir53/blk_5075966182561035053
081110 105910 9510 INFO dfs.DataNode$PacketResponder: Received block blk_-5421365489363678155 of size 67108864 from /10.251.122.38
081110 105927 9498 INFO dfs.DataNode$PacketResponder: Received block blk_2294700909811950327 of size 67108864 from /10.251.90.64
081110 110021 9609 INFO dfs.DataNode$PacketResponder: Received block blk_7224257732880626802 of size 67108864 from /10.251.122.79
081110 110036 9429 INFO dfs.DataNode$PacketResponder: Received block blk_-8049908920306758342 of size 67108864 from /10.251.73.220
081110 110055 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.244:50010 is added to blk_-6582383997201802925 size 67108864
081110 110059 9428 INFO dfs.DataNode$PacketResponder: Received block blk_6746178639682808096 of size 67108864 from /10.251.125.174
081110 110101 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.38.214:50010 is added to blk_-5363213609331400263 size 67108864
081110 110155 9579 INFO dfs.DataNode$PacketResponder: Received block blk_-5586529360624346565 of size 67108864 from /10.251.43.115
081110 110208 9551 INFO dfs.DataNode$PacketResponder: Received block blk_2063252936853409868 of size 67108864 from /10.251.43.192
081110 110324 8497 INFO dfs.DataNode$DataXceiver: Receiving block blk_8752665860595246441 src: /10.251.70.5:44006 dest: /10.251.70.5:50010
081110 110353 9313 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_2974262690755997396 terminating
081110 110627 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.194.213:50010 is added to blk_-7030770677241078854 size 67108864
081110 110827 9664 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_3318864983801406136 terminating
081110 110921 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_001197_0/part-01197. blk_-5255262711039499896
081110 110933 9676 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3612946656596896413 src: /10.251.105.189:48488 dest: /10.251.105.189:50010
081110 110942 9749 INFO dfs.DataNode$PacketResponder: Received block blk_8751697793900992450 of size 67108864 from /10.251.71.16
081110 111026 9675 INFO dfs.DataNode$DataXceiver: Receiving block blk_5816171803790152026 src: /10.251.194.102:43940 dest: /10.251.194.102:50010
081110 111043 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.192:50010 is added to blk_-6068587951382721748 size 67108864
081110 111043 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_001353_0/part-01353. blk_5782729236964719738
081110 111229 9504 INFO dfs.DataNode$PacketResponder: Received block blk_-2856928563366064757 of size 67108864 from /10.251.42.9
081110 111248 9723 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8348114876238571434 terminating
081110 111308 9973 INFO dfs.DataNode$DataXceiver: Receiving block blk_1744485040428751334 src: /10.250.5.237:52234 dest: /10.250.5.237:50010
081110 111404 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_001495_0/part-01495. blk_-7592643790414500555
081110 111405 9848 INFO dfs.DataNode$PacketResponder: Received block blk_-5960135894031733009 of size 67108864 from /10.251.126.83
081110 111409 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.14.143:50010 is added to blk_8952177028039691517 size 67108864
081110 111415 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.13.240:50010 is added to blk_939695665085237271 size 67108864
081110 111547 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200811101024_0001_m_001587_0/part-01587. blk_6374836031306809839
081110 111638 10033 INFO dfs.DataNode$DataXceiver: Receiving block blk_1394107495884949139 src: /10.250.10.100:33314 dest: /10.250.10.100:50010
081110 111647 9920 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1751996688557225241 terminating
081110 111650 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.127.47:50010 is added to blk_-3990317238913332296 size 67108864
081110 111732 9557 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_7837529475466774167 terminating
081110 111748 9582 INFO dfs.DataNode$DataXceiver: Receiving block blk_1057763407402503745 src: /10.251.30.134:36059 dest: /10.251.30.134:50010
081110 111807 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.67.113:50010 is added to blk_8988793695103110546 size 67108864
081110 111826 9879 INFO dfs.DataNode$PacketResponder: Received block blk_-5340841684362207777 of size 67108864 from /10.251.198.33
081110 111909 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.123.99:50010 is added to blk_-2600305892036631113 size 67108864
081110 111958 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.19:50010 is added to blk_-2144263949276079479 size 3547172
081110 112055 10002 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6927870228542972281 src: /10.250.7.230:41059 dest: /10.250.7.230:50010
081110 112152 9621 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_6610244864386894422 terminating
081110 112155 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_774612454978154966
081110 112234 9888 INFO dfs.DataNode$DataXceiver: Receiving block blk_6672666770563118759 src: /10.251.71.68:44457 dest: /10.251.71.68:50010
081110 112310 9858 INFO dfs.DataNode$DataXceiver: Receiving block blk_4783621682384104986 src: /10.251.193.224:52556 dest: /10.251.193.224:50010
081110 112324 9997 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1387034299358896632 terminating
081110 112448 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.228:50010 is added to blk_-5012639079435860269 size 67108864
081110 112511 10021 INFO dfs.DataNode$PacketResponder: Received block blk_5600017220681756254 of size 67108864 from /10.251.214.130
081110 112516 10267 INFO dfs.DataNode$DataXceiver: Receiving block blk_8235751201501175836 src: /10.250.11.194:37449 dest: /10.250.11.194:50010
081110 112615 9713 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8838161455213213548 src: /10.251.199.150:58165 dest: /10.251.199.150:50010
081110 112629 10027 INFO dfs.DataNode$PacketResponder: Received block blk_-3741526967786688540 of size 67108864 from /10.251.74.79
081110 112653 10042 INFO dfs.DataNode$PacketResponder: Received block blk_120277101968534224 of size 67108864 from /10.251.122.65
081110 112716 9992 INFO dfs.DataNode$DataXceiver: Receiving block blk_7535677865140289610 src: /10.251.26.131:47660 dest: /10.251.26.131:50010
081110 112739 10109 INFO dfs.DataNode$PacketResponder: Received block blk_-5814328448485070742 of size 67108864 from /10.251.74.134
081110 112744 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.100:50010 is added to blk_1021828391502582165 size 67108864
081110 112807 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.38.53:50010 is added to blk_3202507081359904616 size 67108864
081110 113017 10093 INFO dfs.DataNode$DataXceiver: Receiving block blk_102441832197223997 src: /10.251.123.195:49611 dest: /10.251.123.195:50010
081110 113407 10381 INFO dfs.DataNode$PacketResponder: Received block blk_3242229894054064344 of size 3540106 from /10.250.15.240
081110 113636 19 INFO dfs.FSDataset: Deleting block blk_-7601381921195183756 file /mnt/hadoop/dfs/data/current/subdir48/blk_-7601381921195183756
081110 113919 10209 INFO dfs.DataNode$PacketResponder: Received block blk_2157471834570224881 of size 67108864 from /10.251.126.22
081110 113952 10299 INFO dfs.DataNode$PacketResponder: Received block blk_-7029628814943626474 of size 67108864 from /10.250.7.230
081110 113955 10279 INFO dfs.DataNode$DataXceiver: Receiving block blk_-931590117143327078 src: /10.251.42.191:44480 dest: /10.251.42.191:50010
081110 114005 10102 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_4967492069970766092 terminating
081110 114019 10028 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1485317050914225917 terminating
081110 114020 10232 INFO dfs.DataNode$DataXceiver: Receiving block blk_1727371778357941761 src: /10.251.126.255:42844 dest: /10.251.126.255:50010
081110 114119 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.26.177:50010 is added to blk_-2402036602753320612 size 67108864
081110 114144 10436 INFO dfs.DataNode$PacketResponder: Received block blk_-1849105860762445186 of size 67108864 from /10.251.91.229
081110 114231 10363 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7820996703068538967 src: /10.251.35.1:59569 dest: /10.251.35.1:50010
081110 114309 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt2/_temporary/_task_200811101024_0002_m_000299_0/part-00299. blk_4984150784048864430
081110 114315 10226 INFO dfs.DataNode$PacketResponder: Received block blk_1646909113279517647 of size 67108864 from /10.251.214.130
081110 114319 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.25.237:50010 is added to blk_-8145226289286457574 size 67108864
081110 114347 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.18:50010 is added to blk_5809900021002850724 size 67108864
081110 114354 10356 INFO dfs.DataNode$DataXceiver: Receiving block blk_2735034460607366827 src: /10.251.91.15:57370 dest: /10.251.91.15:50010
081110 114357 10454 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7486746679002157277 src: /10.251.42.207:57669 dest: /10.251.42.207:50010
081110 114449 10421 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8876879715755468215 src: /10.251.111.80:43834 dest: /10.251.111.80:50010
081110 114454 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt2/_temporary/_task_200811101024_0002_m_000054_0/part-00054. blk_-4139299269696044017
081110 114504 10561 INFO dfs.DataNode$DataXceiver: Receiving block blk_591323161994959677 src: /10.251.195.33:50123 dest: /10.251.195.33:50010
081110 114637 10468 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-5828360070550196756 terminating
081110 114717 10303 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-7683667795522321323 terminating
081110 114724 10433 INFO dfs.DataNode$DataXceiver: Receiving block blk_7733243482102436613 src: /10.250.6.223:34069 dest: /10.250.6.223:50010
081110 114738 10395 INFO dfs.DataNode$DataXceiver: Receiving block blk_2980964138205299397 src: /10.251.110.196:53450 dest: /10.251.110.196:50010
081110 115017 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.127.191:50010 is added to blk_1589878256898761558 size 67108864
081110 115202 10582 INFO dfs.DataNode$DataXceiver: Receiving block blk_-2746863406163228797 src: /10.251.39.160:35642 dest: /10.251.39.160:50010
081110 115204 10569 INFO dfs.DataNode$DataXceiver: Receiving block blk_3067558337579347170 src: /10.250.6.191:34137 dest: /10.250.6.191:50010
081110 115226 10579 INFO dfs.DataNode$DataXceiver: Receiving block blk_8992894896370738220 src: /10.251.43.210:54766 dest: /10.251.43.210:50010
081110 115408 10446 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7230947007726948080 src: /10.251.29.239:53564 dest: /10.251.29.239:50010
081110 115526 10492 INFO dfs.DataNode$DataXceiver: Receiving block blk_1869228153483717375 src: /10.251.203.4:56599 dest: /10.251.203.4:50010
081110 115614 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.65.203:50010 is added to blk_-3949633496933134307 size 67108864
081110 115700 10671 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-8745453424063341007 terminating
081110 115734 10668 INFO dfs.DataNode$PacketResponder: Received block blk_-2117066166574895060 of size 67108864 from /10.251.73.188
081110 115737 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.195.33:50010 is added to blk_512039711533506696 size 67108864
081110 115742 10672 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_6017354285686373326 terminating
081110 115804 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.126.83:50010 is added to blk_5176855630425795751 size 67108864
081110 115934 10597 INFO dfs.DataNode$PacketResponder: Received block blk_-4433556521525567493 of size 28492032 from /10.250.6.4
081110 120103 10778 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-5321676321043683563 terminating
081110 120139 10638 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-357695136519539363 terminating
081110 120233 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.37:50010 is added to blk_-7603740654202748654 size 67108864
081110 120246 10414 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_1279439901767890758 terminating
081110 120247 10427 INFO dfs.DataNode$DataXceiver: Receiving block blk_8942969309743886844 src: /10.251.110.68:58266 dest: /10.251.110.68:50010
081110 120247 10525 INFO dfs.DataNode$PacketResponder: Received block blk_469871968689793326 of size 67108864 from /10.251.30.179
081110 120410 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt2/_temporary/_task_200811101024_0002_m_001080_0/part-01080. blk_2847401988110989655
081110 120429 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.197.226:50010 is added to blk_4307207852226914003 size 67108864
081110 120516 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_3506875694435075421 size 67108864
081110 120524 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.202.134:50010 is added to blk_-4945182943709543868 size 67108864
081110 120621 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt2/_temporary/_task_200811101024_0002_m_000809_0/part-00809. blk_7550266985912909372
081110 120718 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.215.192:50010 is added to blk_-4619809198345103330 size 67108864
081110 120807 10546 INFO dfs.DataNode$PacketResponder: Received block blk_4614530673206341566 of size 67108864 from /10.251.91.229
081110 120857 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.201.204:50010 is added to blk_6125469222540396851 size 67108864
081110 120931 10872 INFO dfs.DataNode$DataXceiver: Receiving block blk_6868050965297613251 src: /10.251.201.204:47734 dest: /10.251.201.204:50010
081110 121045 10833 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7266711648018793699 src: /10.251.31.242:47086 dest: /10.251.31.242:50010
081110 121136 10865 INFO dfs.DataNode$DataXceiver: Receiving block blk_9146254623576405494 src: /10.250.7.146:60294 dest: /10.250.7.146:50010
081110 121153 10880 INFO dfs.DataNode$PacketResponder: Received block blk_-6966935672736779591 of size 67108864 from /10.251.90.64
081110 121156 11034 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7411687001739959508 src: /10.251.122.65:46080 dest: /10.251.122.65:50010
081110 121202 11005 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_6514181997629420906 terminating
081110 121248 10737 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4434479572312007079 src: /10.251.30.101:60539 dest: /10.251.30.101:50010
081110 121402 10935 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-3339018122410311668 terminating
081110 121647 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.91.32:50010 is added to blk_5272761429354463572 size 67108864
081110 121711 11025 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1680563440028826904 terminating
081110 121758 10889 INFO dfs.DataNode$PacketResponder: Received block blk_-5671206752084326639 of size 67108864 from /10.251.214.67
081110 121758 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_1649162819905816843
081110 121943 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt2/_temporary/_task_200811101024_0002_m_001335_0/part-01335. blk_-2541428592126088938
081110 121951 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.3:50010 is added to blk_6547729596040509998 size 67108864
081110 122121 10760 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3673133694834392576 src: /10.251.109.209:49088 dest: /10.251.109.209:50010
081110 122207 11264 INFO dfs.DataNode$PacketResponder: Received block blk_48742413802609430 of size 67108864 from /10.251.106.10
081110 122210 11081 INFO dfs.DataNode$DataXceiver: Receiving block blk_6085274723649229095 src: /10.251.127.191:51113 dest: /10.251.127.191:50010
081110 122309 11240 INFO dfs.DataNode$PacketResponder: Received block blk_-227411387418683230 of size 67108864 from /10.251.194.213
081110 122314 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.31.160:50010 is added to blk_3079488924320077240 size 67108864
081110 122407 11042 INFO dfs.DataNode$PacketResponder: Received block blk_692184983396207285 of size 67108864 from /10.250.6.214
081110 122444 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.102:50010 is added to blk_-5009274684359267777 size 67108864
081110 122607 11193 INFO dfs.DataNode$PacketResponder: Received block blk_2599050023684318911 of size 67108864 from /10.251.106.10
081110 122609 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.123.195:50010 is added to blk_3911263675181899748 size 67108864
081110 122630 10903 INFO dfs.DataNode$DataXceiver: Receiving block blk_5641239200978828454 src: /10.251.42.9:51808 dest: /10.251.42.9:50010
081110 122648 11281 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8852638972311349374 src: /10.251.107.227:46913 dest: /10.251.107.227:50010
081110 122702 11168 INFO dfs.DataNode$DataXceiver: Receiving block blk_-719498947487451844 src: /10.251.125.193:52079 dest: /10.251.125.193:50010
081110 122717 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.63:50010 is added to blk_-3170270010972115378 size 67108864
081110 122719 11316 INFO dfs.DataNode$DataXceiver: Receiving block blk_6988110707613820142 src: /10.251.123.132:47658 dest: /10.251.123.132:50010
081110 123008 11210 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1490623381339215624 terminating
081110 123012 10997 INFO dfs.DataNode$DataXceiver: Receiving block blk_-2192382893903170466 src: /10.251.201.204:36915 dest: /10.251.201.204:50010
081110 123040 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.149:50010 is added to blk_-6247364168500873179 size 67108864
081110 123114 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.125.237:50010 is added to blk_4234385041723406944 size 67108864
081110 123132 11170 INFO dfs.DataNode$PacketResponder: Received block blk_8968608251219902913 of size 67108864 from /10.251.195.52
081110 123237 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.100:50010 is added to blk_-8502460962248995911 size 28489733
081110 123252 11126 INFO dfs.DataNode$PacketResponder: Received block blk_-1201318815778694289 of size 67108864 from /10.251.194.147
081110 123252 11243 INFO dfs.DataNode$PacketResponder: Received block blk_-9089815063425241739 of size 67108864 from /10.251.126.255
081110 123309 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.32:50010 is added to blk_1389634598105954660 size 67108864
081110 123401 10856 INFO dfs.DataNode$PacketResponder: Received block blk_557803157214911261 of size 67108864 from /10.251.198.196
081110 123453 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.42.9:50010 is added to blk_-1435656632270396446 size 28490269
081110 123517 11398 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_7345322407792942877 terminating
081110 123541 11406 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-8939321760994370713 terminating
081110 123543 11010 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_3958940914408609637 terminating
081110 123726 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt2/_temporary/_task_200811101024_0002_m_001968_1/part-01968. blk_5106303344783117537
081110 123810 11328 INFO dfs.DataNode$DataXceiver: Receiving block blk_6628512193350164674 src: /10.251.73.220:33460 dest: /10.251.73.220:50010
081110 123827 11309 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-5587827566367944828 terminating
081110 124158 11529 INFO dfs.DataNode$DataXceiver: 10.250.11.53:50010 Served block blk_-743349875664670637 to /10.250.11.53
081110 124159 11369 WARN dfs.DataNode$DataXceiver: 10.251.70.5:50010:Got exception while serving blk_4581070264530964999 to /10.250.13.188:
081110 124817 11533 WARN dfs.DataNode$DataXceiver: 10.251.74.79:50010:Got exception while serving blk_2139707931694085102 to /10.251.111.228:
081110 125138 11558 WARN dfs.DataNode$DataXceiver: 10.251.42.246:50010:Got exception while serving blk_-3056652732896088330 to /10.251.27.63:
081110 125914 11536 WARN dfs.DataNode$DataXceiver: 10.250.13.188:50010:Got exception while serving blk_73389299186801686 to /10.251.123.33:
081110 125920 11517 WARN dfs.DataNode$DataXceiver: 10.251.42.16:50010:Got exception while serving blk_4963214289850966664 to /10.251.202.181:
081110 130315 11823 WARN dfs.DataNode$DataXceiver: 10.251.71.240:50010:Got exception while serving blk_-8082428389627750477 to /10.251.71.97:
081110 130558 11608 INFO dfs.DataNode$DataXceiver: 10.251.39.192:50010 Served block blk_6338761369201338121 to /10.251.90.81
081110 131041 11754 WARN dfs.DataNode$DataXceiver: 10.251.122.38:50010:Got exception while serving blk_7489155602821840025 to /10.251.26.8:
081110 131128 11976 INFO dfs.DataNode$DataXceiver: 10.251.67.225:50010 Served block blk_2038610753592374847 to /10.250.18.114
081110 131233 11867 WARN dfs.DataNode$DataXceiver: 10.251.197.226:50010:Got exception while serving blk_2101007907134483259 to /10.251.31.242:
081110 131806 11757 WARN dfs.DataNode$DataXceiver: 10.251.26.177:50010:Got exception while serving blk_3848747149919646864 to /10.251.125.174:
081110 131950 11797 WARN dfs.DataNode$DataXceiver: 10.251.30.85:50010:Got exception while serving blk_8059204889272489644 to /10.250.7.96:
081110 132523 11832 WARN dfs.DataNode$DataXceiver: 10.251.39.179:50010:Got exception while serving blk_1683075246463470765 to /10.250.14.38:
081110 133337 11768 INFO dfs.DataNode$DataXceiver: 10.250.10.100:50010 Served block blk_9216955386716663841 to /10.251.90.64
081110 133657 11701 WARN dfs.DataNode$DataXceiver: 10.251.123.132:50010:Got exception while serving blk_4048438071314937575 to /10.251.194.213:
081110 133823 11970 INFO dfs.DataNode$DataXceiver: 10.251.203.149:50010 Served block blk_-1432303681669932458 to /10.250.10.176
081110 133946 11947 WARN dfs.DataNode$DataXceiver: 10.251.126.255:50010:Got exception while serving blk_1910039736720341651 to /10.251.31.85:
081110 135429 12030 WARN dfs.DataNode$DataXceiver: 10.251.70.37:50010:Got exception while serving blk_1646534811870220828 to /10.250.11.100:
081110 135645 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-4984504651603069357
081110 135653 12034 WARN dfs.DataNode$DataXceiver: 10.251.214.130:50010:Got exception while serving blk_424461424221613461 to /10.251.71.193:
081110 135803 12201 INFO dfs.DataNode$DataXceiver: 10.251.214.18:50010 Served block blk_7297060562345904886 to /10.251.29.239
081110 135806 12060 WARN dfs.DataNode$DataXceiver: 10.251.215.70:50010:Got exception while serving blk_-7391656937286046074 to /10.251.107.196:
081110 142841 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand2/_temporary/_task_200811101024_0003_r_000258_0/part-00258. blk_387577766066135394
081110 143222 12360 INFO dfs.DataNode$DataXceiver: Receiving block blk_6107935679779418195 src: /10.251.43.115:55925 dest: /10.251.43.115:50010
081110 143232 12469 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_3584162266158512993 terminating
081110 143318 12458 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-2591043665372016488 terminating
081110 143336 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand2/_temporary/_task_200811101024_0003_r_000257_0/part-00257. blk_-1732662141253491971
081110 143353 12141 INFO dfs.DataNode$DataXceiver: Receiving block blk_4959462704623252283 src: /10.251.125.174:44166 dest: /10.251.125.174:50010
081110 143430 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.96:50010 is added to blk_4553790855855864497 size 67108864
081110 143551 12299 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3261177124784025140 src: /10.251.29.239:43937 dest: /10.251.29.239:50010
081110 143554 12294 INFO dfs.DataNode$PacketResponder: Received block blk_7501235595045510958 of size 67108864 from /10.251.214.67
081110 143613 12543 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3486808304671603275 src: /10.251.30.85:37518 dest: /10.251.30.85:50010
081110 143651 12577 INFO dfs.DataNode$DataXceiver: Receiving block blk_6563909003697001371 src: /10.251.39.144:48872 dest: /10.251.39.144:50010
081110 143725 12558 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_4068667188725811395 terminating
081110 143736 12448 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7355153279766988632 src: /10.251.194.245:35876 dest: /10.251.194.245:50010
081110 143920 12287 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4854562124894030819 src: /10.250.10.100:40486 dest: /10.250.10.100:50010
081110 143945 12653 INFO dfs.DataNode$PacketResponder: Received block blk_1828089683746536118 of size 67108864 from /10.250.6.223
081110 144004 12354 INFO dfs.DataNode$DataXceiver: Receiving block blk_1345503778930512538 src: /10.251.75.163:40307 dest: /10.251.75.163:50010
081110 144024 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.195.70:50010 is added to blk_4318179566727871532 size 67108864
081110 144035 12577 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_727000711161641724 terminating
081110 144100 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.123.33:50010 is added to blk_-2916658835497879256 size 67108864
081110 144201 12384 INFO dfs.DataNode$PacketResponder: Received block blk_7227035060836018780 of size 67108864 from /10.250.5.237
081110 144220 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk_4069436988023676023 size 67108864
081110 144309 12287 INFO dfs.DataNode$PacketResponder: Received block blk_-3981735565440284420 of size 67108864 from /10.251.127.243
081110 144404 12709 INFO dfs.DataNode$DataXceiver: Receiving block blk_728165942214842306 src: /10.251.30.179:57828 dest: /10.251.30.179:50010
081110 144509 12519 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3157541550945505354 src: /10.251.125.193:45917 dest: /10.251.125.193:50010
081110 144546 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.27.63:50010 is added to blk_7838843222458775811 size 67108864
081110 144547 12473 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_3088175971503531298 terminating
081110 144643 12257 INFO dfs.DataNode$PacketResponder: Received block blk_-3620767662957963389 of size 67108864 from /10.251.203.179
081110 144735 12785 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5078186985701364847 src: /10.251.37.240:35757 dest: /10.251.37.240:50010
081110 144750 12364 INFO dfs.DataNode$DataXceiver: Receiving block blk_7193551490488433552 src: /10.251.74.227:44191 dest: /10.251.74.227:50010
081110 144848 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.102:50010 is added to blk_-8022986908643041177 size 67108864
081110 144947 12810 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5003122231679852524 src: /10.250.7.230:51033 dest: /10.250.7.230:50010
081110 145021 12676 INFO dfs.DataNode$PacketResponder: Received block blk_-3276232266876914214 of size 67108864 from /10.250.7.146
081110 145037 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand2/_temporary/_task_200811101024_0003_r_000210_0/part-00210. blk_-7667417209393167541
081110 145114 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.31.242:50010 is added to blk_-5400783031085008882 size 67108864
081110 145120 12820 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1197396592032418307 terminating
081110 145203 12416 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_7533949673910842243 terminating
081110 145213 12875 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8842097432176169457 terminating
081110 145452 12780 INFO dfs.DataNode$PacketResponder: Received block blk_560063894682806537 of size 67108864 from /10.251.194.129
081110 145506 12603 INFO dfs.DataNode$PacketResponder: Received block blk_2652444451422301232 of size 67108864 from /10.251.214.225
081110 145606 12992 INFO dfs.DataNode$PacketResponder: Received block blk_7016884347337429710 of size 67108864 from /10.251.194.129
081110 145606 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.26.131:50010 is added to blk_-4174618838156947380 size 67108864
081110 145703 12721 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-3402248757106237109 terminating
081110 145746 12982 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8915123233040672605 src: /10.250.10.176:58294 dest: /10.250.10.176:50010
081110 145749 12962 INFO dfs.DataNode$PacketResponder: Received block blk_-5633587983998853189 of size 67108864 from /10.251.73.220
081110 145818 12949 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_2570966823909513791 terminating
081110 145931 12812 INFO dfs.DataNode$PacketResponder: Received block blk_-6168484338203583997 of size 67108864 from /10.251.39.192
081110 145944 12814 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3172594752427649592 src: /10.251.198.196:57807 dest: /10.251.198.196:50010
081110 150009 12830 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8285441926344578159 terminating
081110 150125 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.193:50010 is added to blk_7415823309220171667 size 67108864
081110 150205 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand2/_temporary/_task_200811101024_0003_r_000116_0/part-00116. blk_6749232449432458524
081110 150238 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.38.197:50010 is added to blk_-5427918313707752877 size 67108864
081110 150240 12700 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4673509946769671609 src: /10.251.30.85:34022 dest: /10.251.30.85:50010
081110 150254 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-7803343374223161433
081110 150316 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.105.189:50010 is added to blk_-3275237990609680332 size 67108864
081110 150436 13158 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8584158329012779023 src: /10.250.19.16:50122 dest: /10.250.19.16:50010
081110 150455 13155 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4468166796563085237 src: /10.251.215.50:53705 dest: /10.251.215.50:50010
081110 150531 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand2/_temporary/_task_200811101024_0003_r_000315_0/part-00315. blk_-463998587096565542
081110 150600 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_578261619176233743 size 67108864
081110 150618 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.188:50010 is added to blk_-3897674589140998025 size 67108864
081110 150751 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.17.225:50010 is added to blk_-3573208405519699182 size 67108864
081110 150809 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.67:50010 is added to blk_4154740411570608045 size 67108864
081110 150933 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.179:50010 is added to blk_8188837227311063557 size 67108864
081110 151115 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.42.191:50010 is added to blk_-4208396523159735354 size 67108864
081110 151202 13109 INFO dfs.DataNode$DataXceiver: Receiving block blk_7572868180557515526 src: /10.251.214.130:38983 dest: /10.251.214.130:50010
081110 151229 13144 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1733803097614960767 terminating
081110 151307 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand2/_temporary/_task_200811101024_0003_r_000152_0/part-00152. blk_-7603795308074215276
081110 151427 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/sortrand2/_temporary/_task_200811101024_0003_r_000344_0/part-00344. blk_-2004529166303428909
081110 151503 13083 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5817942497672492100 src: /10.251.107.19:35639 dest: /10.251.107.19:50010
081110 151558 13270 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-7512746652453970810 terminating
081110 151600 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.75.143:50010 is added to blk_2618566506268332756 size 67108864
081110 151717 12949 INFO dfs.DataNode$PacketResponder: Received block blk_-265588615173040885 of size 67108864 from /10.251.215.70
081110 151902 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.13.188:50010 is added to blk_5971534893446541111 size 67108864
081110 151940 12996 INFO dfs.DataNode$DataXceiver: Receiving block blk_4742178675580222210 src: /10.251.42.9:60232 dest: /10.251.42.9:50010
081110 152137 13387 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-900229825899812461 terminating
081110 152147 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.70.37:50010 is added to blk_300967862449910205 size 67108864
081110 152215 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.19.227:50010 is added to blk_2059389709317340334 size 67108864
081110 152240 13092 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-6865167803683277211 terminating
081110 152613 13407 INFO dfs.DataNode$DataXceiver: Receiving block blk_1321799290531408498 src: /10.251.126.255:43447 dest: /10.251.126.255:50010
081110 153301 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-8507209254958698084
081110 153917 13338 INFO dfs.DataNode$PacketResponder: Received block blk_-7716943410942322104 of size 67108864 from /10.251.126.5
081110 155142 13655 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6884056342993438262 src: /10.251.193.175:46638 dest: /10.251.193.175:50010
081110 160100 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.70.5:50010 is added to blk_-4092973201423074374 size 67108864
081110 163007 13812 INFO dfs.DataNode$DataXceiver: 10.251.67.225:50010 Served block blk_5240632065631390212 to /10.251.67.225
081110 163625 13543 INFO dfs.DataNode$DataXceiver: 10.251.106.50:50010 Served block blk_-8907436477337775860 to /10.251.106.50
081110 163945 13711 WARN dfs.DataNode$DataXceiver: 10.251.30.101:50010:Got exception while serving blk_-5340885607150360719 to /10.251.30.101:
081110 170322 13709 WARN dfs.DataNode$DataXceiver: 10.251.195.33:50010:Got exception while serving blk_6702054389824523327 to /10.251.195.33:
081110 173556 13687 INFO dfs.DataNode$DataXceiver: 10.251.194.147:50010 Served block blk_7703498406755483188 to /10.251.111.209
081110 173657 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_2904134648927436267
081110 175354 14021 WARN dfs.DataNode$DataXceiver: 10.250.11.53:50010:Got exception while serving blk_-7125370327234065733 to /10.250.15.240:
081110 175426 13827 INFO dfs.DataNode$DataXceiver: 10.251.202.209:50010 Served block blk_-5086369325828333421 to /10.251.194.213
081110 175818 13937 WARN dfs.DataNode$DataXceiver: 10.250.10.223:50010:Got exception while serving blk_8478683141242372735 to /10.251.70.5:
081110 175826 14005 INFO dfs.DataNode$DataXceiver: 10.251.215.50:50010 Served block blk_668295516093167045 to /10.251.30.6
081110 175934 12298 WARN dfs.DataNode$DataXceiver: 10.251.107.50:50010:Got exception while serving blk_-4737948505610037427 to /10.251.75.79:
081110 183200 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_1584196996810164915
081110 190333 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_1150231966878829887
081110 191403 14120 WARN dfs.DataNode$DataXceiver: 10.250.14.196:50010:Got exception while serving blk_2967299761311489673 to /10.251.89.155:
081110 192015 14269 WARN dfs.DataNode$DataXceiver: 10.250.10.223:50010:Got exception while serving blk_9095955277706686312 to /10.250.5.161:
081110 193334 14294 WARN dfs.DataNode$DataXceiver: 10.251.39.179:50010:Got exception while serving blk_-8558281124275910849 to /10.251.30.134:
081110 193551 14191 INFO dfs.DataNode$DataXceiver: 10.251.71.97:50010 Served block blk_6552418180158772377 to /10.251.39.242
081110 194159 14349 WARN dfs.DataNode$DataXceiver: 10.251.26.8:50010:Got exception while serving blk_-3572295706132449090 to /10.251.111.130:
081110 194544 14270 INFO dfs.DataNode$DataXceiver: 10.251.202.209:50010 Served block blk_-1442905060580014339 to /10.251.202.209
081110 194735 14044 WARN dfs.DataNode$DataXceiver: 10.251.127.243:50010:Got exception while serving blk_7760079751081658559 to /10.251.215.70:
081110 195500 14389 INFO dfs.DataNode$DataXceiver: 10.251.39.179:50010 Served block blk_798664047382151445 to /10.251.39.179
081110 200311 14622 INFO dfs.DataNode$DataXceiver: 10.250.11.194:50010 Served block blk_8591104451552720112 to /10.250.11.194
081110 210112 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-531469051872229488 is added to invalidSet of 10.251.71.146:50010
081110 210115 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-4657423175636611807 is added to invalidSet of 10.251.193.175:50010
081110 210115 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6117917602803602155 is added to invalidSet of 10.251.67.211:50010
081110 210115 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7584542530081657301 is added to invalidSet of 10.250.10.223:50010
081110 210118 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_4640277741339926850 is added to invalidSet of 10.251.26.131:50010
081110 210118 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7285565997486715015 is added to invalidSet of 10.250.6.191:50010
081110 210123 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3652427357251545055 is added to invalidSet of 10.250.10.223:50010
081110 210123 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5163387277374142808 is added to invalidSet of 10.251.71.193:50010
081110 210124 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1342669964196815853 is added to invalidSet of 10.250.17.225:50010
081110 210125 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-424018194735922479 is added to invalidSet of 10.251.42.84:50010
081110 210126 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3754029959138975487 is added to invalidSet of 10.251.214.112:50010
081110 210126 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-4149539113118855204 is added to invalidSet of 10.251.71.16:50010
081110 210126 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5868517220927475394 is added to invalidSet of 10.251.67.113:50010
081110 210127 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6093858391888371854 is added to invalidSet of 10.251.42.207:50010
081110 210128 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7048740572660501729 is added to invalidSet of 10.251.107.98:50010
081110 210129 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-2025578204394955498 is added to invalidSet of 10.251.91.159:50010
081110 210129 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-795209964982424254 is added to invalidSet of 10.251.107.227:50010
081110 210131 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7606019272539977822 is added to invalidSet of 10.251.67.211:50010
081110 210132 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1960893136630391966 is added to invalidSet of 10.251.39.144:50010
081110 210142 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8632488927959646482 is added to invalidSet of 10.251.39.64:50010
081110 210144 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1302893891174665780 is added to invalidSet of 10.251.214.225:50010
081110 210145 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-490230821467544285 is added to invalidSet of 10.250.10.6:50010
081110 210146 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_4713968111267226537 is added to invalidSet of 10.251.70.5:50010
081110 210146 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7473411975130090672 is added to invalidSet of 10.251.75.143:50010
081110 210147 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_753559000954836892 is added to invalidSet of 10.251.30.101:50010
081110 210147 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_9085898147979939648 is added to invalidSet of 10.251.67.225:50010
081110 210148 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5403209762224435166 is added to invalidSet of 10.251.125.193:50010
081110 210149 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6932221506331060207 is added to invalidSet of 10.251.74.79:50010
081110 210149 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7693277678281547790 is added to invalidSet of 10.251.30.6:50010
081110 210151 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3792448673967861371 is added to invalidSet of 10.251.89.155:50010
081110 210151 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5496448884081025648 is added to invalidSet of 10.251.123.195:50010
081110 210153 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1102735311308675356 is added to invalidSet of 10.251.71.97:50010
081110 210153 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7094265091490664593 is added to invalidSet of 10.251.71.68:50010
081110 210157 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1271403908410292619 is added to invalidSet of 10.251.43.147:50010
081110 210201 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-2439225174716664723 is added to invalidSet of 10.251.42.207:50010
081110 210201 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-9220604860626391374 is added to invalidSet of 10.251.30.179:50010
081110 210210 14441 INFO dfs.DataNode$DataXceiver: 10.250.15.101:50010 Served block blk_-2557432794238860908 to /10.251.74.134
081110 210243 14482 INFO dfs.DataNode$PacketResponder: Received block blk_-8214089520956286311 of size 67108864 from /10.251.26.131
081110 210252 18 INFO dfs.FSDataset: Deleting block blk_-8949548167466276130 file /mnt/hadoop/dfs/data/current/subdir2/blk_-8949548167466276130
081110 210255 14644 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4831379247767940447 src: /10.251.65.237:42885 dest: /10.251.65.237:50010
081110 210259 19 INFO dfs.FSDataset: Deleting block blk_-7548149518969960333 file /mnt/hadoop/dfs/data/current/subdir8/blk_-7548149518969960333
081110 210308 19 INFO dfs.FSDataset: Deleting block blk_-7353572928777348242 file /mnt/hadoop/dfs/data/current/subdir22/blk_-7353572928777348242
081110 210330 19 INFO dfs.FSDataset: Deleting block blk_7206839389762980382 file /mnt/hadoop/dfs/data/current/subdir61/blk_7206839389762980382
081110 210339 19 INFO dfs.FSDataset: Deleting block blk_-9171803179611497035 file /mnt/hadoop/dfs/data/current/subdir35/blk_-9171803179611497035
081110 210354 19 INFO dfs.FSDataset: Deleting block blk_-4598246018559857954 file /mnt/hadoop/dfs/data/current/subdir34/blk_-4598246018559857954
081110 210354 19 INFO dfs.FSDataset: Deleting block blk_-6901909114834172466 file /mnt/hadoop/dfs/data/current/subdir57/blk_-6901909114834172466
081110 210401 19 INFO dfs.FSDataset: Deleting block blk_-6075803209168884005 file /mnt/hadoop/dfs/data/current/subdir31/blk_-6075803209168884005
081110 210401 19 INFO dfs.FSDataset: Deleting block blk_-8617088607862430908 file /mnt/hadoop/dfs/data/current/subdir53/blk_-8617088607862430908
081110 210403 14498 INFO dfs.DataNode$DataXceiver: Receiving block blk_7347539858784653625 src: /10.250.11.194:46600 dest: /10.250.11.194:50010
081110 210404 19 INFO dfs.FSDataset: Deleting block blk_7855353385462032387 file /mnt/hadoop/dfs/data/current/subdir41/blk_7855353385462032387
081110 210413 19 INFO dfs.FSDataset: Deleting block blk_3733339790842827121 file /mnt/hadoop/dfs/data/current/subdir42/blk_3733339790842827121
081110 210417 19 INFO dfs.FSDataset: Deleting block blk_-9017308542351369260 file /mnt/hadoop/dfs/data/current/subdir14/blk_-9017308542351369260
081110 210422 11315 INFO dfs.DataNode$PacketResponder: Received block blk_-3734839878416656496 of size 67108864 from /10.251.66.102
081110 210423 19 INFO dfs.FSDataset: Deleting block blk_-8360774680830158225 file /mnt/hadoop/dfs/data/current/subdir31/blk_-8360774680830158225
081110 210425 19 INFO dfs.FSDataset: Deleting block blk_-6942784423611603919 file /mnt/hadoop/dfs/data/current/subdir38/blk_-6942784423611603919
081110 210427 19 INFO dfs.FSDataset: Deleting block blk_-5603192027616633151 file /mnt/hadoop/dfs/data/current/subdir34/blk_-5603192027616633151
081110 210442 19 INFO dfs.FSDataset: Deleting block blk_-7854501109698465657 file /mnt/hadoop/dfs/data/current/subdir34/blk_-7854501109698465657
081110 210449 19 INFO dfs.FSDataset: Deleting block blk_-4551689611865410817 file /mnt/hadoop/dfs/data/current/blk_-4551689611865410817
081110 210450 19 INFO dfs.FSDataset: Deleting block blk_-1750007847201154657 file /mnt/hadoop/dfs/data/current/subdir1/blk_-1750007847201154657
081110 210502 19 INFO dfs.FSDataset: Deleting block blk_-3564493590065542578 file /mnt/hadoop/dfs/data/current/subdir55/blk_-3564493590065542578
081110 210509 18 INFO dfs.FSDataset: Deleting block blk_-2929627680703662712 file /mnt/hadoop/dfs/data/current/subdir32/blk_-2929627680703662712
081110 210510 19 INFO dfs.FSDataset: Deleting block blk_2708206493147801743 file /mnt/hadoop/dfs/data/current/subdir15/blk_2708206493147801743
081110 210511 19 INFO dfs.FSDataset: Deleting block blk_-4110699195744660699 file /mnt/hadoop/dfs/data/current/subdir34/blk_-4110699195744660699
081110 210512 19 INFO dfs.FSDataset: Deleting block blk_7721951162137927252 file /mnt/hadoop/dfs/data/current/subdir52/blk_7721951162137927252
081110 210515 19 INFO dfs.FSDataset: Deleting block blk_-1535908830476495455 file /mnt/hadoop/dfs/data/current/subdir10/blk_-1535908830476495455
081110 210516 19 INFO dfs.FSDataset: Deleting block blk_1184972950286241114 file /mnt/hadoop/dfs/data/current/subdir39/blk_1184972950286241114
081110 210517 19 INFO dfs.FSDataset: Deleting block blk_-3396836569886642130 file /mnt/hadoop/dfs/data/current/subdir32/blk_-3396836569886642130
081110 210532 19 INFO dfs.FSDataset: Deleting block blk_22457656256024207 file /mnt/hadoop/dfs/data/current/subdir33/blk_22457656256024207
081110 210534 19 INFO dfs.FSDataset: Deleting block blk_5905689738251144455 file /mnt/hadoop/dfs/data/current/blk_5905689738251144455
081110 210537 19 INFO dfs.FSDataset: Deleting block blk_-2210703755506918304 file /mnt/hadoop/dfs/data/current/subdir26/blk_-2210703755506918304
081110 210540 18 INFO dfs.FSDataset: Deleting block blk_773599351835388127 file /mnt/hadoop/dfs/data/current/subdir60/blk_773599351835388127
081110 210540 19 INFO dfs.FSDataset: Deleting block blk_3921292580505776296 file /mnt/hadoop/dfs/data/current/subdir57/blk_3921292580505776296
081110 210542 19 INFO dfs.FSDataset: Deleting block blk_4162832865986721185 file /mnt/hadoop/dfs/data/current/subdir4/blk_4162832865986721185
081110 210545 19 INFO dfs.FSDataset: Deleting block blk_2156262111024914941 file /mnt/hadoop/dfs/data/current/subdir14/blk_2156262111024914941
081110 210548 19 INFO dfs.FSDataset: Deleting block blk_4658551441818960395 file /mnt/hadoop/dfs/data/current/subdir19/blk_4658551441818960395
081110 210549 19 INFO dfs.FSDataset: Deleting block blk_3076992299328673559 file /mnt/hadoop/dfs/data/current/subdir54/blk_3076992299328673559
081110 210549 19 INFO dfs.FSDataset: Deleting block blk_6085442148007558065 file /mnt/hadoop/dfs/data/current/subdir63/blk_6085442148007558065
081110 210555 19 INFO dfs.FSDataset: Deleting block blk_1746384029810476219 file /mnt/hadoop/dfs/data/current/subdir18/blk_1746384029810476219
081110 210555 19 INFO dfs.FSDataset: Deleting block blk_4939101994162863442 file /mnt/hadoop/dfs/data/current/subdir24/blk_4939101994162863442
081110 210558 19 INFO dfs.FSDataset: Deleting block blk_1144571179430079218 file /mnt/hadoop/dfs/data/current/subdir33/blk_1144571179430079218
081110 210558 19 INFO dfs.FSDataset: Deleting block blk_8693774866554287286 file /mnt/hadoop/dfs/data/current/subdir55/blk_8693774866554287286
081110 210600 18 INFO dfs.FSDataset: Deleting block blk_6695517786834913217 file /mnt/hadoop/dfs/data/current/subdir15/blk_6695517786834913217
081110 210601 19 INFO dfs.FSDataset: Deleting block blk_70980361971054326 file /mnt/hadoop/dfs/data/current/subdir27/blk_70980361971054326
081110 210608 19 INFO dfs.FSDataset: Deleting block blk_6499414220139660338 file /mnt/hadoop/dfs/data/current/subdir59/blk_6499414220139660338
081110 210620 19 INFO dfs.FSDataset: Deleting block blk_8598135237831029828 file /mnt/hadoop/dfs/data/current/subdir59/blk_8598135237831029828
081110 210640 19 INFO dfs.FSDataset: Deleting block blk_7108974241330205218 file /mnt/hadoop/dfs/data/current/subdir22/blk_7108974241330205218
081110 210704 19 INFO dfs.FSDataset: Deleting block blk_8929806114713265094 file /mnt/hadoop/dfs/data/current/subdir1/blk_8929806114713265094
081110 210710 19 INFO dfs.FSDataset: Deleting block blk_6445339027653789144 file /mnt/hadoop/dfs/data/current/subdir16/blk_6445339027653789144
081110 210721 14857 INFO dfs.DataNode$DataXceiver: Receiving block blk_3235988328669699941 src: /10.251.199.159:43276 dest: /10.251.199.159:50010
081110 210737 19 INFO dfs.FSDataset: Deleting block blk_8699831307500970779 file /mnt/hadoop/dfs/data/current/subdir54/blk_8699831307500970779
081110 210753 19 INFO dfs.FSDataset: Deleting block blk_3221204570666147236 file /mnt/hadoop/dfs/data/current/subdir34/blk_3221204570666147236
081110 210832 14473 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5808893254440551816 src: /10.250.11.194:53256 dest: /10.250.11.194:50010
081110 210837 14418 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_3940282600163903101 terminating
081110 211021 14407 INFO dfs.DataNode$DataXceiver: Receiving block blk_4233013542759682998 src: /10.251.109.209:48041 dest: /10.251.109.209:50010
081110 211030 19 INFO dfs.FSDataset: Deleting block blk_7360639743953947587 file /mnt/hadoop/dfs/data/current/subdir35/blk_7360639743953947587
081110 211108 14882 INFO dfs.DataNode$PacketResponder: Received block blk_4182287230523664560 of size 67108864 from /10.251.111.130
081110 211109 14620 INFO dfs.DataNode$DataXceiver: Receiving block blk_995094303184954394 src: /10.251.91.84:35590 dest: /10.251.91.84:50010
081110 211122 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.30.6:50010 is added to blk_3585424189730250601 size 67108864
081110 211131 14600 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-5680441982827916621 terminating
081110 211323 14609 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2089955821703690032 terminating
081110 211355 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.10:50010 is added to blk_1151012386083696649 size 67108864
081110 211358 14754 INFO dfs.DataNode$DataXceiver: Receiving block blk_4149909442914612270 src: /10.251.195.33:55490 dest: /10.251.195.33:50010
081110 211410 14947 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2282833575688432184 terminating
081110 211504 14451 INFO dfs.DataNode$PacketResponder: Received block blk_3960527526103274525 of size 67108864 from /10.251.30.134
081110 211523 14688 INFO dfs.DataNode$DataXceiver: Receiving block blk_-852118157909078164 src: /10.251.30.101:57254 dest: /10.251.30.101:50010
081110 211541 18 INFO dfs.DataNode: 10.250.15.198:50010 Starting thread to transfer block blk_4292382298896622412 to 10.250.15.240:50010
081110 211737 14817 INFO dfs.DataNode$PacketResponder: Received block blk_-185994527242886002 of size 3552954 from /10.251.39.160
081110 211845 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.188:50010 is added to blk_8761786041973172187 size 67108864
081110 211926 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.70.211:50010 is added to blk_-5471660287004025096 size 67108864
081110 212039 14833 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_3755116621101133849 terminating
081110 212051 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.144:50010 is added to blk_-5542336029922582303 size 67108864
081110 212111 14543 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_165707910410871749 terminating
081110 212139 14759 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2957756905007569214 terminating
081110 212221 15037 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_4884219613602801814 terminating
081110 212234 14745 INFO dfs.DataNode$PacketResponder: Received block blk_6698851413708497072 of size 67108864 from /10.251.74.134
081110 212249 14727 INFO dfs.DataNode$PacketResponder: Received block blk_6730979070070986233 of size 67108864 from /10.251.90.239
081110 212258 19 INFO dfs.FSDataset: Deleting block blk_-468830939205980722 file /mnt/hadoop/dfs/data/current/subdir10/blk_-468830939205980722
081110 212312 15013 INFO dfs.DataNode$PacketResponder: Received block blk_-7056746602297676104 of size 67108864 from /10.251.39.209
081110 212329 14784 INFO dfs.DataNode$DataXceiver: Receiving block blk_8473858691873523269 src: /10.250.11.53:60609 dest: /10.250.11.53:50010
081110 212501 15023 INFO dfs.DataNode$PacketResponder: Received block blk_6686361711974377273 of size 3554889 from /10.251.214.130
081110 212509 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.123.33:50010 is added to blk_-6442394521537090621 size 67108864
081110 212510 19 INFO dfs.FSNamesystem: BLOCK* ask 10.250.18.114:50010 to delete  blk_-5140072410813878235
081110 212556 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.91.84:50010 is added to blk_6039898256291822367 size 67108864
081110 212644 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand3/_temporary/_task_200811101024_0005_m_000720_0/part-00720. blk_-7988596544606686086
081110 212831 14962 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_5729560286355779655 terminating
081110 212851 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.126.5:50010 is added to blk_-989798817617753264 size 67108864
081110 212942 15258 INFO dfs.DataNode$PacketResponder: Received block blk_-2242088550278361724 of size 67108864 from /10.251.123.20
081110 213021 15065 INFO dfs.DataNode$PacketResponder: Received block blk_5602895463700536678 of size 67108864 from /10.251.30.179
081110 213026 15051 INFO dfs.DataNode$DataXceiver: Receiving block blk_7580930377559819562 src: /10.251.110.68:54793 dest: /10.251.110.68:50010
081110 213221 15038 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_429279891070363457 terminating
081110 213334 15049 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-9132218772786220159 terminating
081110 213351 15202 INFO dfs.DataNode$DataXceiver: Receiving block blk_-2901266958756042716 src: /10.251.110.160:35787 dest: /10.251.110.160:50010
081110 213423 14805 INFO dfs.DataNode$PacketResponder: Received block blk_6323743466920239525 of size 67108864 from /10.250.15.67
081110 213434 15194 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-7948158002985968470 terminating
081110 213448 15242 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8974021306351083168 src: /10.251.91.15:40699 dest: /10.251.91.15:50010
081110 213534 15015 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-4917866339076366459 terminating
081110 213723 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.112:50010 is added to blk_8063831442998721547 size 67108864
081110 213816 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand3/_temporary/_task_200811101024_0005_m_001091_0/part-01091. blk_-4468645577339843974
081110 213828 15327 INFO dfs.DataNode$DataXceiver: Receiving block blk_9121519145462575118 src: /10.250.10.223:58281 dest: /10.250.10.223:50010
081110 213833 15315 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6773301007994789531 terminating
081110 213854 15334 INFO dfs.DataNode$DataXceiver: Receiving block blk_-878226404201600469 src: /10.250.7.96:40849 dest: /10.250.7.96:50010
081110 213959 15103 INFO dfs.DataNode$PacketResponder: Received block blk_-6031720401337601269 of size 67108864 from /10.250.10.100
081110 214044 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.198.33:50010 is added to blk_-5159573936338473350 size 67108864
081110 214101 15116 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-703436021914343546 terminating
081110 214117 15141 INFO dfs.DataNode$PacketResponder: Received block blk_-8641647081502666269 of size 67108864 from /10.250.10.100
081110 214226 15186 INFO dfs.DataNode$DataXceiver: Receiving block blk_3542047035353248258 src: /10.250.6.214:59051 dest: /10.250.6.214:50010
081110 214321 15339 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-736188013503336717 terminating
081110 214404 15111 INFO dfs.DataNode$PacketResponder: Received block blk_1188202922232921994 of size 67108864 from /10.250.10.100
081110 214525 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.195.33:50010 is added to blk_-9049344623659872649 size 67108864
081110 214634 15356 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_4575129504644372394 terminating
081110 214823 15099 INFO dfs.DataNode$DataXceiver: Receiving block blk_8574493302165689212 src: /10.251.194.245:34656 dest: /10.251.194.245:50010
081110 214907 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.19:50010 is added to blk_5574898547439146021 size 67108864
081110 215003 15501 INFO dfs.DataNode$PacketResponder: Received block blk_2474460483915499078 of size 67108864 from /10.251.195.52
081110 215057 15637 INFO dfs.DataNode$PacketResponder: Received block blk_7851643274522055587 of size 67108864 from /10.251.214.112
081110 215111 15220 INFO dfs.DataNode$PacketResponder: Received block blk_8097409435735325492 of size 67108864 from /10.251.111.209
081110 215117 15436 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-966013175018359901 terminating
081110 215253 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand3/_temporary/_task_200811101024_0005_m_001655_0/part-01655. blk_-6802153775367591521
081110 215310 15040 INFO dfs.DataNode$PacketResponder: Received block blk_-3300851862683571086 of size 67108864 from /10.251.203.166
081110 215310 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.105.189:50010 is added to blk_-2035996502664501008 size 67108864
081110 215357 15296 INFO dfs.DataNode$DataXceiver: Receiving block blk_8222237992861447018 src: /10.251.203.4:60086 dest: /10.251.203.4:50010
081110 215546 15564 INFO dfs.DataNode$DataXceiver: Receiving block blk_6556854018013706836 src: /10.251.202.134:48721 dest: /10.251.202.134:50010
081110 215546 15644 INFO dfs.DataNode$PacketResponder: Received block blk_8302956190102508375 of size 67108864 from /10.251.109.236
081110 215654 15437 INFO dfs.DataNode$DataXceiver: Receiving block blk_-2583370070230800221 src: /10.251.31.160:58714 dest: /10.251.31.160:50010
081110 215722 15450 INFO dfs.DataNode$DataXceiver: Receiving block blk_392347292517057138 src: /10.251.31.160:58721 dest: /10.251.31.160:50010
081110 215726 15711 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_1639683981444528748 terminating
081110 215726 15721 INFO dfs.DataNode$DataXceiver: Receiving block blk_5525607749827896334 src: /10.251.66.102:33326 dest: /10.251.66.102:50010
081110 215806 15819 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_6809155287484296510 terminating
081110 215813 15758 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-5908283405801966540 terminating
081110 215817 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand3/_temporary/_task_200811101024_0005_m_001824_0/part-01824. blk_-4654885003331291932
081110 220009 15450 INFO dfs.DataNode$PacketResponder: Received block blk_-8923698979247229952 of size 67108864 from /10.251.90.239
081110 220042 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.91.84:50010 is added to blk_-2321016155994168744 size 67108864
081110 220148 15613 INFO dfs.DataNode$PacketResponder: Received block blk_-2151458111813215098 of size 67108864 from /10.250.9.207
081110 220238 15679 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1733154814375651491 terminating
081110 220312 15681 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3187293586306086277 src: /10.251.35.1:50933 dest: /10.251.35.1:50010
081110 220314 15849 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7914996925059840931 src: /10.251.91.15:59714 dest: /10.251.91.15:50010
081110 220350 15848 INFO dfs.DataNode$PacketResponder: Received block blk_5379939266348893280 of size 67108864 from /10.251.66.63
081110 220626 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8981795097778421111 is added to invalidSet of 10.251.194.129:50010
081110 220630 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1979447750586388860 is added to invalidSet of 10.251.215.192:50010
081110 220636 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3332848207645320273 is added to invalidSet of 10.251.90.64:50010
081110 220636 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6881216037013929188 is added to invalidSet of 10.251.91.32:50010
081110 220637 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7541621664406141215 is added to invalidSet of 10.251.214.32:50010
081110 220645 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1680175975862749862 is added to invalidSet of 10.250.14.38:50010
081110 220646 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5176179471361959897 is added to invalidSet of 10.250.9.207:50010
081110 220648 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-4385347171136037814 is added to invalidSet of 10.251.67.113:50010
081110 220648 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8384329882986724706 is added to invalidSet of 10.251.123.195:50010
081110 220650 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6392434729242722288 is added to invalidSet of 10.251.30.134:50010
081110 220651 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1714886759174770356 is added to invalidSet of 10.251.31.5:50010
081110 220651 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7400944190976416273 is added to invalidSet of 10.251.127.243:50010
081110 220651 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8442157310069672329 is added to invalidSet of 10.251.39.64:50010
081110 220653 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5624558200776510955 is added to invalidSet of 10.251.107.242:50010
081110 220655 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1481009974400305784 is added to invalidSet of 10.251.39.242:50010
081110 220655 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_355286618997526877 is added to invalidSet of 10.250.7.32:50010
081110 220655 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7369651436133452001 is added to invalidSet of 10.251.127.191:50010
081110 220656 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8353423262983821010 is added to invalidSet of 10.251.39.209:50010
081110 220658 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7017399031777870797 is added to invalidSet of 10.250.5.161:50010
081110 220708 19 INFO dfs.FSDataset: Deleting block blk_-6891122177443047531 file /mnt/hadoop/dfs/data/current/subdir36/blk_-6891122177443047531
081110 220711 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt3/_temporary/_task_200811101024_0007_m_000190_0/part-00190. blk_641646158310569454
081110 220749 19 INFO dfs.FSDataset: Deleting block blk_-9171930401624803515 file /mnt/hadoop/dfs/data/current/subdir56/blk_-9171930401624803515
081110 220801 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.42.84:50010 is added to blk_5938666289587069744 size 67108864
081110 220804 19 INFO dfs.FSDataset: Deleting block blk_-8294986551316947211 file /mnt/hadoop/dfs/data/current/subdir34/blk_-8294986551316947211
081110 220811 19 INFO dfs.FSDataset: Deleting block blk_-5718504625725408070 file /mnt/hadoop/dfs/data/current/blk_-5718504625725408070
081110 220830 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt3/_temporary/_task_200811101024_0007_m_000374_0/part-00374. blk_-1941312615450572331
081110 220853 19 INFO dfs.FSDataset: Deleting block blk_2353918919834315845 file /mnt/hadoop/dfs/data/current/subdir35/blk_2353918919834315845
081110 220907 19 INFO dfs.FSDataset: Deleting block blk_391507259220628397 file /mnt/hadoop/dfs/data/current/blk_391507259220628397
081110 220909 19 INFO dfs.FSDataset: Deleting block blk_-1263913006422225885 file /mnt/hadoop/dfs/data/current/subdir2/blk_-1263913006422225885
081110 220911 19 INFO dfs.FSDataset: Deleting block blk_6542592144596063478 file /mnt/hadoop/dfs/data/current/subdir35/blk_6542592144596063478
081110 220916 15860 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4473277914543734578 src: /10.251.215.16:34762 dest: /10.251.215.16:50010
081110 220921 19 INFO dfs.FSDataset: Deleting block blk_1689772850125323349 file /mnt/hadoop/dfs/data/current/subdir27/blk_1689772850125323349
081110 220923 19 INFO dfs.FSDataset: Deleting block blk_8615522912610884162 file /mnt/hadoop/dfs/data/current/subdir35/blk_8615522912610884162
081110 220925 19 INFO dfs.FSDataset: Deleting block blk_107727140948970996 file /mnt/hadoop/dfs/data/current/subdir33/blk_107727140948970996
081110 220936 19 INFO dfs.FSDataset: Deleting block blk_-1598760065436384831 file /mnt/hadoop/dfs/data/current/subdir53/blk_-1598760065436384831
081110 220955 19 INFO dfs.FSDataset: Deleting block blk_8950932638808086257 file /mnt/hadoop/dfs/data/current/subdir18/blk_8950932638808086257
081110 221033 15695 INFO dfs.DataNode$PacketResponder: Received block blk_-8835758154471700679 of size 67108864 from /10.251.73.188
081110 221038 16067 INFO dfs.DataNode$DataXceiver: Receiving block blk_323887415551221572 src: /10.251.39.144:46278 dest: /10.251.39.144:50010
081110 221125 15347 INFO dfs.DataNode$PacketResponder: Received block blk_231502148921342925 of size 67108864 from /10.251.215.192
081110 221134 16153 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_1349228381785843634 terminating
081110 221212 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt3/_temporary/_task_200811101024_0007_m_000186_0/part-00186. blk_-6754647973871368247
081110 221253 15107 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7539658633533020967 src: /10.250.5.161:52151 dest: /10.250.5.161:50010
081110 221343 15521 INFO dfs.DataNode$PacketResponder: Received block blk_6406847509870023726 of size 67108864 from /10.251.106.50
081110 221447 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.115:50010 is added to blk_-7893636450787030202 size 67108864
081110 221653 15684 INFO dfs.DataNode$PacketResponder: Received block blk_-5777981614298940026 of size 67108864 from /10.251.214.67
081110 221803 15957 INFO dfs.DataNode$PacketResponder: Received block blk_4231049049582051919 of size 28496610 from /10.250.19.227
081110 221932 19 INFO dfs.FSNamesystem: BLOCK* ask 10.251.122.79:50010 to delete  blk_8048594464172649365
081110 221943 15858 INFO dfs.DataNode$DataXceiver: Receiving block blk_4414644179961808738 src: /10.251.107.98:46161 dest: /10.251.107.98:50010
081110 222039 16041 INFO dfs.DataNode$DataXceiver: Receiving block blk_7661189768086976193 src: /10.251.195.52:56915 dest: /10.251.195.52:50010
081110 222052 16100 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-7170175035335391571 terminating
081110 222130 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.15.198:50010 is added to blk_4992622607400168655 size 67108864
081110 222221 15824 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_8971893898579544235 terminating
081110 222321 16072 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7418372263556457716 src: /10.251.89.155:53560 dest: /10.251.89.155:50010
081110 222413 16030 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3417101330634563680 src: /10.251.110.68:44200 dest: /10.251.110.68:50010
081110 222455 16220 INFO dfs.DataNode$PacketResponder: Received block blk_4294115716822011564 of size 67108864 from /10.251.43.210
081110 222459 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.160:50010 is added to blk_1865953191447352631 size 67108864
081110 222511 16129 INFO dfs.DataNode$DataXceiver: Receiving block blk_2147212041377545584 src: /10.251.75.49:34485 dest: /10.251.75.49:50010
081110 222512 16222 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_708101772844351082 terminating
081110 222529 15925 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-5610308397633191834 terminating
081110 222621 16005 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-7159695919045792245 terminating
081110 222712 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.68:50010 is added to blk_8850044049297409379 size 67108864
081110 222946 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt3/_temporary/_task_200811101024_0007_m_000585_0/part-00585. blk_8561332122820208519
081110 222948 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.67.113:50010 is added to blk_-8393060055740236791 size 67108864
081110 222951 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.122.38:50010 is added to blk_-5861348317118763621 size 28500222
081110 223000 16174 INFO dfs.DataNode$PacketResponder: Received block blk_5174461779237739025 of size 67108864 from /10.250.14.143
081110 223002 15950 INFO dfs.DataNode$DataXceiver: Receiving block blk_1463530401988623658 src: /10.251.111.37:41293 dest: /10.251.111.37:50010
081110 223032 16147 INFO dfs.DataNode$PacketResponder: Received block blk_-8377763431920892140 of size 67108864 from /10.250.6.223
081110 223108 16252 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1728983474612515818 src: /10.251.70.211:39459 dest: /10.251.70.211:50010
081110 223140 15976 INFO dfs.DataNode$DataXceiver: Receiving block blk_8007544297333060851 src: /10.251.90.134:47340 dest: /10.251.90.134:50010
081110 223206 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.230:50010 is added to blk_6238543795209416553 size 67108864
081110 223311 16266 INFO dfs.DataNode$DataXceiver: Receiving block blk_4599950495841528213 src: /10.251.194.102:59433 dest: /10.251.194.102:50010
081110 223410 16237 INFO dfs.DataNode$PacketResponder: Received block blk_-8524952715666214329 of size 67108864 from /10.251.90.64
081110 223615 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.127.243:50010 is added to blk_-5882295985950637419 size 67108864
081110 223623 16188 INFO dfs.DataNode$PacketResponder: Received block blk_-1485109068671160157 of size 67108864 from /10.251.71.68
081110 223802 15593 INFO dfs.DataNode$PacketResponder: Received block blk_3586596428873150919 of size 67108864 from /10.251.26.131
081110 223833 16297 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7815259387557421424 src: /10.250.13.240:50649 dest: /10.250.13.240:50010
081110 223849 16493 INFO dfs.DataNode$DataXceiver: Receiving block blk_3358274959811806117 src: /10.251.106.214:51334 dest: /10.251.106.214:50010
081110 223854 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.90.239:50010 is added to blk_-5927343391895571936 size 67108864
081110 223905 16320 INFO dfs.DataNode$DataXceiver: Receiving block blk_8907528844412790999 src: /10.251.126.227:42572 dest: /10.251.126.227:50010
081110 224059 16244 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_5262186175243998904 terminating
081110 224154 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.215.192:50010 is added to blk_-1704027189958470814 size 67108864
081110 224209 16540 INFO dfs.DataNode$DataXceiver: Receiving block blk_-337665304078248571 src: /10.251.106.37:55462 dest: /10.251.106.37:50010
081110 224212 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.50:50010 is added to blk_-2023442822091562967 size 67108864
081110 224218 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.198.196:50010 is added to blk_1779145401405339993 size 28499417
081110 224338 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.199.159:50010 is added to blk_7705518309270121951 size 67108864
081110 224437 16512 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-9120005470237070778 terminating
081110 224504 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.32:50010 is added to blk_3849239820458663566 size 67108864
081110 224632 16607 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1657856234642335063 src: /10.251.38.197:44516 dest: /10.251.38.197:50010
081110 224636 16576 INFO dfs.DataNode$PacketResponder: Received block blk_6490003020931145707 of size 67108864 from /10.251.42.246
081110 224637 16371 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_6048652012880073760 terminating
081110 224652 16437 INFO dfs.DataNode$PacketResponder: Received block blk_5648034804629064273 of size 67108864 from /10.251.91.159
081110 224732 16377 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_5912213716795619450 terminating
081110 224746 16694 INFO dfs.DataNode$PacketResponder: Received block blk_-5534560875087182998 of size 67108864 from /10.250.7.230
081110 224755 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt3/_temporary/_task_200811101024_0007_m_001560_0/part-01560. blk_3376391218693716925
081110 224817 16597 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_2691304991047255190 terminating
081110 224932 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt3/_temporary/_task_200811101024_0007_m_001230_0/part-01230. blk_8708862520108736953
081110 224938 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.75.79:50010 is added to blk_-5718873743060788176 size 67108864
081110 224958 16603 INFO dfs.DataNode$DataXceiver: Receiving block blk_-2989288139685694818 src: /10.250.19.227:60160 dest: /10.250.19.227:50010
081110 225056 16728 INFO dfs.DataNode$DataXceiver: Receiving block blk_6245594177708848366 src: /10.250.15.198:46793 dest: /10.250.15.198:50010
081110 225124 16591 INFO dfs.DataNode$PacketResponder: Received block blk_4433676399725693167 of size 67108864 from /10.251.37.240
081110 225213 16632 INFO dfs.DataNode$PacketResponder: Received block blk_-5644397018622018109 of size 67108864 from /10.251.109.236
081110 225305 16687 INFO dfs.DataNode$PacketResponder: Received block blk_-6986311820863339652 of size 67108864 from /10.251.71.240
081110 225351 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.64:50010 is added to blk_207875475387433291 size 67108864
081110 225400 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.199.225:50010 is added to blk_-7115635482536854135 size 67108864
081110 225423 16747 INFO dfs.DataNode$PacketResponder: Received block blk_-1956140031907143783 of size 67108864 from /10.251.70.112
081110 225642 16655 INFO dfs.DataNode$PacketResponder: Received block blk_7534378757788640871 of size 67108864 from /10.251.214.130
081110 225740 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt3/_temporary/_task_200811101024_0007_m_001662_0/part-01662. blk_8196657927516481490
081110 225813 16867 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5029807829679779067 src: /10.251.122.79:49600 dest: /10.251.122.79:50010
081110 225905 16532 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3657751236932275338 src: /10.251.123.99:50481 dest: /10.251.123.99:50010
081110 225947 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.74.227:50010 is added to blk_1594026090263820445 size 67108864
081110 230105 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_3894335463008345041
081110 230202 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.193.224:50010 is added to blk_-6299730951392868297 size 67108864
081110 230314 16898 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_157913239647511296 terminating
081110 230315 16811 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-6454539193224916125 terminating
081110 230338 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.194.213:50010 is added to blk_-3575568207121859434 size 67108864
081110 230457 16876 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-4325209272072286951 terminating
081110 230508 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.91.159:50010 is added to blk_-5739126205963920408 size 67108864
081110 230524 16637 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-9056865861421808370 terminating
081110 230536 16598 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-6934248664959947757 terminating
081110 230545 16742 INFO dfs.DataNode$PacketResponder: Received block blk_-2082401934988378573 of size 67108864 from /10.251.214.112
081110 230627 16366 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_7627900514276120558 terminating
081110 230628 16721 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1250775350296476256 terminating
081110 230633 16678 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_2389216938154332862 terminating
081110 230859 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.38.53:50010 is added to blk_7430820184211246929 size 67108864
081110 231236 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-608202790716579401 is added to invalidSet of 10.251.198.33:50010
081110 231241 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1335063409877429524 is added to invalidSet of 10.251.107.196:50010
081110 231556 16543 INFO dfs.DataNode$DataXceiver: 10.251.202.181:50010 Served block blk_-5236655516146555545 to /10.251.31.5
081110 231904 17028 WARN dfs.DataNode$DataXceiver: 10.251.75.143:50010:Got exception while serving blk_2262752117989012641 to /10.250.13.188:
081110 231911 16933 WARN dfs.DataNode$DataXceiver: 10.251.126.227:50010:Got exception while serving blk_-4181768899028058192 to /10.251.26.177:
081110 232739 16715 INFO dfs.DataNode$DataXceiver: 10.251.123.99:50010 Served block blk_-7990763409688650846 to /10.251.197.161
081110 233940 17094 INFO dfs.DataNode$DataXceiver: 10.250.17.225:50010 Served block blk_-2656323385657954732 to /10.250.17.225
081110 233954 17191 WARN dfs.DataNode$DataXceiver: 10.250.7.230:50010:Got exception while serving blk_-7029628814943626474 to /10.251.38.197:
081110 235445 16879 INFO dfs.DataNode$DataXceiver: 10.251.127.243:50010 Served block blk_8711213005689112399 to /10.251.74.134
Dec 10 06:55:46 LabSZ sshd[24200]: reverse mapping checking getaddrinfo for ns.marryaldkfaczcz.com [173.234.31.186] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 06:55:46 LabSZ sshd[24200]: Invalid user webmaster from 173.234.31.186
Dec 10 06:55:46 LabSZ sshd[24200]: input_userauth_request: invalid user webmaster [preauth]
Dec 10 06:55:46 LabSZ sshd[24200]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 06:55:46 LabSZ sshd[24200]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=173.234.31.186 
Dec 10 06:55:48 LabSZ sshd[24200]: Failed password for invalid user webmaster from 173.234.31.186 port 38926 ssh2
Dec 10 06:55:48 LabSZ sshd[24200]: Connection closed by 173.234.31.186 [preauth]
Dec 10 07:02:47 LabSZ sshd[24203]: Connection closed by 212.47.254.145 [preauth]
Dec 10 07:07:38 LabSZ sshd[24206]: Invalid user test9 from 52.80.34.196
Dec 10 07:07:38 LabSZ sshd[24206]: input_userauth_request: invalid user test9 [preauth]
Dec 10 07:07:38 LabSZ sshd[24206]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 07:07:38 LabSZ sshd[24206]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=ec2-52-80-34-196.cn-north-1.compute.amazonaws.com.cn 
Dec 10 07:07:45 LabSZ sshd[24206]: Failed password for invalid user test9 from 52.80.34.196 port 36060 ssh2
Dec 10 07:07:45 LabSZ sshd[24206]: Received disconnect from 52.80.34.196: 11: Bye Bye [preauth]
Dec 10 07:08:28 LabSZ sshd[24208]: reverse mapping checking getaddrinfo for ns.marryaldkfaczcz.com [173.234.31.186] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 07:08:28 LabSZ sshd[24208]: Invalid user webmaster from 173.234.31.186
Dec 10 07:08:28 LabSZ sshd[24208]: input_userauth_request: invalid user webmaster [preauth]
Dec 10 07:08:28 LabSZ sshd[24208]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 07:08:28 LabSZ sshd[24208]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=173.234.31.186 
Dec 10 07:08:30 LabSZ sshd[24208]: Failed password for invalid user webmaster from 173.234.31.186 port 39257 ssh2
Dec 10 07:08:30 LabSZ sshd[24208]: Connection closed by 173.234.31.186 [preauth]
Dec 10 07:11:42 LabSZ sshd[24224]: Invalid user chen from 202.100.179.208
Dec 10 07:11:42 LabSZ sshd[24224]: input_userauth_request: invalid user chen [preauth]
Dec 10 07:11:42 LabSZ sshd[24224]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 07:11:42 LabSZ sshd[24224]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=202.100.179.208 
Dec 10 07:11:44 LabSZ sshd[24224]: Failed password for invalid user chen from 202.100.179.208 port 32484 ssh2
Dec 10 07:11:44 LabSZ sshd[24224]: Received disconnect from 202.100.179.208: 11: Bye Bye [preauth]
Dec 10 07:13:31 LabSZ sshd[24227]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.36.59.76.dynamic-dsl-ip.omantel.net.om  user=root
Dec 10 07:13:43 LabSZ sshd[24227]: Failed password for root from 5.36.59.76 port 42393 ssh2
Dec 10 07:13:56 LabSZ sshd[24227]: message repeated 5 times: [ Failed password for root from 5.36.59.76 port 42393 ssh2]
Dec 10 07:13:56 LabSZ sshd[24227]: Disconnecting: Too many authentication failures for root [preauth]
Dec 10 07:13:56 LabSZ sshd[24227]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.36.59.76.dynamic-dsl-ip.omantel.net.om  user=root
Dec 10 07:13:56 LabSZ sshd[24227]: PAM service(sshd) ignoring max retries; 6 > 3
Dec 10 07:27:50 LabSZ sshd[24235]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:27:52 LabSZ sshd[24235]: Failed password for root from 112.95.230.3 port 45378 ssh2
Dec 10 07:27:52 LabSZ sshd[24235]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:27:53 LabSZ sshd[24237]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:27:55 LabSZ sshd[24237]: Failed password for root from 112.95.230.3 port 47068 ssh2
Dec 10 07:27:55 LabSZ sshd[24237]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:27:55 LabSZ sshd[24239]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:27:58 LabSZ sshd[24239]: Failed password for root from 112.95.230.3 port 49188 ssh2
Dec 10 07:27:58 LabSZ sshd[24239]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:27:58 LabSZ sshd[24241]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:00 LabSZ sshd[24241]: Failed password for root from 112.95.230.3 port 50999 ssh2
Dec 10 07:28:00 LabSZ sshd[24241]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:01 LabSZ sshd[24243]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:03 LabSZ sshd[24243]: Failed password for root from 112.95.230.3 port 52660 ssh2
Dec 10 07:28:03 LabSZ sshd[24243]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:03 LabSZ sshd[24245]: Invalid user pgadmin from 112.95.230.3
Dec 10 07:28:03 LabSZ sshd[24245]: input_userauth_request: invalid user pgadmin [preauth]
Dec 10 07:28:03 LabSZ sshd[24245]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 07:28:03 LabSZ sshd[24245]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3 
Dec 10 07:28:05 LabSZ sshd[24245]: Failed password for invalid user pgadmin from 112.95.230.3 port 54087 ssh2
Dec 10 07:28:05 LabSZ sshd[24245]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:06 LabSZ sshd[24247]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:08 LabSZ sshd[24247]: Failed password for root from 112.95.230.3 port 55618 ssh2
Dec 10 07:28:08 LabSZ sshd[24247]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:08 LabSZ sshd[24249]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:10 LabSZ sshd[24249]: Failed password for root from 112.95.230.3 port 57138 ssh2
Dec 10 07:28:10 LabSZ sshd[24249]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:10 LabSZ sshd[24251]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:12 LabSZ sshd[24251]: Failed password for root from 112.95.230.3 port 58304 ssh2
Dec 10 07:28:12 LabSZ sshd[24251]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:12 LabSZ sshd[24253]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:14 LabSZ sshd[24253]: Failed password for root from 112.95.230.3 port 59849 ssh2
Dec 10 07:28:14 LabSZ sshd[24253]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:14 LabSZ sshd[24255]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:16 LabSZ sshd[24255]: Failed password for root from 112.95.230.3 port 32977 ssh2
Dec 10 07:28:16 LabSZ sshd[24255]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:16 LabSZ sshd[24257]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:18 LabSZ sshd[24257]: Failed password for root from 112.95.230.3 port 35113 ssh2
Dec 10 07:28:18 LabSZ sshd[24257]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:19 LabSZ sshd[24259]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:21 LabSZ sshd[24259]: Failed password for root from 112.95.230.3 port 37035 ssh2
Dec 10 07:28:21 LabSZ sshd[24259]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:21 LabSZ sshd[24261]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:23 LabSZ sshd[24261]: Failed password for root from 112.95.230.3 port 39041 ssh2
Dec 10 07:28:23 LabSZ sshd[24261]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:23 LabSZ sshd[24263]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:25 LabSZ sshd[24263]: Failed password for root from 112.95.230.3 port 40388 ssh2
Dec 10 07:28:25 LabSZ sshd[24263]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:25 LabSZ sshd[24265]: Invalid user utsims from 112.95.230.3
Dec 10 07:28:25 LabSZ sshd[24265]: input_userauth_request: invalid user utsims [preauth]
Dec 10 07:28:25 LabSZ sshd[24265]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 07:28:25 LabSZ sshd[24265]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3 
Dec 10 07:28:28 LabSZ sshd[24265]: Failed password for invalid user utsims from 112.95.230.3 port 41506 ssh2
Dec 10 07:28:28 LabSZ sshd[24265]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:28 LabSZ sshd[24267]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:30 LabSZ sshd[24267]: Failed password for root from 112.95.230.3 port 42881 ssh2
Dec 10 07:28:30 LabSZ sshd[24267]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:31 LabSZ sshd[24269]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:33 LabSZ sshd[24269]: Failed password for root from 112.95.230.3 port 43981 ssh2
Dec 10 07:28:33 LabSZ sshd[24269]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:33 LabSZ sshd[24271]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:35 LabSZ sshd[24271]: Failed password for root from 112.95.230.3 port 44900 ssh2
Dec 10 07:28:35 LabSZ sshd[24271]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:35 LabSZ sshd[24273]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:37 LabSZ sshd[24273]: Failed password for root from 112.95.230.3 port 45699 ssh2
Dec 10 07:28:37 LabSZ sshd[24273]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:37 LabSZ sshd[24275]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:39 LabSZ sshd[24275]: Failed password for root from 112.95.230.3 port 46577 ssh2
Dec 10 07:28:39 LabSZ sshd[24275]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:40 LabSZ sshd[24277]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:42 LabSZ sshd[24277]: Failed password for root from 112.95.230.3 port 47836 ssh2
Dec 10 07:28:42 LabSZ sshd[24277]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:42 LabSZ sshd[24279]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:44 LabSZ sshd[24279]: Failed password for root from 112.95.230.3 port 49133 ssh2
Dec 10 07:28:44 LabSZ sshd[24279]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:45 LabSZ sshd[24281]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:46 LabSZ sshd[24281]: Failed password for root from 112.95.230.3 port 50655 ssh2
Dec 10 07:28:46 LabSZ sshd[24281]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:47 LabSZ sshd[24283]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:49 LabSZ sshd[24283]: Failed password for root from 112.95.230.3 port 51982 ssh2
Dec 10 07:28:49 LabSZ sshd[24283]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:28:49 LabSZ sshd[24285]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=112.95.230.3  user=root
Dec 10 07:28:51 LabSZ sshd[24285]: Failed password for root from 112.95.230.3 port 53584 ssh2
Dec 10 07:28:51 LabSZ sshd[24285]: Received disconnect from 112.95.230.3: 11: Bye Bye [preauth]
Dec 10 07:32:24 LabSZ sshd[24287]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=123.235.32.19  user=root
Dec 10 07:32:27 LabSZ sshd[24287]: Failed password for root from 123.235.32.19 port 40652 ssh2
Dec 10 07:32:27 LabSZ sshd[24287]: Received disconnect from 123.235.32.19: 11: Bye Bye [preauth]
Dec 10 07:32:27 LabSZ sshd[24289]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=123.235.32.19  user=root
Dec 10 07:32:29 LabSZ sshd[24289]: Failed password for root from 123.235.32.19 port 49720 ssh2
Dec 10 07:32:29 LabSZ sshd[24289]: Received disconnect from 123.235.32.19: 11: Bye Bye [preauth]
Dec 10 07:33:58 LabSZ sshd[24291]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=123.235.32.19  user=root
Dec 10 07:34:00 LabSZ sshd[24291]: Failed password for root from 123.235.32.19 port 45568 ssh2
Dec 10 07:34:00 LabSZ sshd[24291]: Received disconnect from 123.235.32.19: 11: Bye Bye [preauth]
Dec 10 07:34:02 LabSZ sshd[24293]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=123.235.32.19  user=root
Dec 10 07:34:04 LabSZ sshd[24293]: Failed password for root from 123.235.32.19 port 48588 ssh2
Dec 10 07:34:06 LabSZ sshd[24293]: Received disconnect from 123.235.32.19: 11: Bye Bye [preauth]
Dec 10 07:34:07 LabSZ sshd[24295]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=123.235.32.19  user=root
Dec 10 07:34:10 LabSZ sshd[24295]: Failed password for root from 123.235.32.19 port 50950 ssh2
Dec 10 07:34:10 LabSZ sshd[24295]: Received disconnect from 123.235.32.19: 11: Bye Bye [preauth]
Dec 10 07:34:13 LabSZ sshd[24297]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=123.235.32.19  user=root
Dec 10 07:34:15 LabSZ sshd[24297]: Failed password for root from 123.235.32.19 port 54024 ssh2
Dec 10 07:34:15 LabSZ sshd[24297]: Received disconnect from 123.235.32.19: 11: Bye Bye [preauth]
Dec 10 07:34:21 LabSZ sshd[24299]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=123.235.32.19  user=root
Dec 10 07:34:23 LabSZ sshd[24299]: Failed password for root from 123.235.32.19 port 57100 ssh2
Dec 10 07:34:24 LabSZ sshd[24299]: Received disconnect from 123.235.32.19: 11: Bye Bye [preauth]
Dec 10 07:34:33 LabSZ sshd[24301]: Did not receive identification string from 123.235.32.19
Dec 10 07:35:15 LabSZ sshd[24303]: Did not receive identification string from 177.79.82.136
Dec 10 07:42:49 LabSZ sshd[24318]: Invalid user inspur from 183.136.162.51
Dec 10 07:42:49 LabSZ sshd[24318]: input_userauth_request: invalid user inspur [preauth]
Dec 10 07:42:49 LabSZ sshd[24318]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 07:42:49 LabSZ sshd[24318]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.136.162.51 
Dec 10 07:42:51 LabSZ sshd[24318]: Failed password for invalid user inspur from 183.136.162.51 port 55204 ssh2
Dec 10 07:42:51 LabSZ sshd[24318]: Received disconnect from 183.136.162.51: 11: Bye Bye [preauth]
Dec 10 07:48:00 LabSZ sshd[24321]: reverse mapping checking getaddrinfo for 191-210-223-172.user.vivozap.com.br [191.210.223.172] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 07:48:00 LabSZ sshd[24321]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=191.210.223.172  user=root
Dec 10 07:48:03 LabSZ sshd[24321]: Failed password for root from 191.210.223.172 port 31473 ssh2
Dec 10 07:48:03 LabSZ sshd[24321]: Connection closed by 191.210.223.172 [preauth]
Dec 10 07:51:09 LabSZ sshd[24323]: Did not receive identification string from 195.154.37.122
Dec 10 07:51:12 LabSZ sshd[24324]: reverse mapping checking getaddrinfo for 195-154-37-122.rev.poneytelecom.eu [195.154.37.122] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 07:51:12 LabSZ sshd[24324]: Invalid user support from 195.154.37.122
Dec 10 07:51:12 LabSZ sshd[24324]: input_userauth_request: invalid user support [preauth]
Dec 10 07:51:12 LabSZ sshd[24324]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 07:51:12 LabSZ sshd[24324]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=195.154.37.122 
Dec 10 07:51:15 LabSZ sshd[24324]: Failed password for invalid user support from 195.154.37.122 port 56539 ssh2
Dec 10 07:51:15 LabSZ sshd[24324]: error: Received disconnect from 195.154.37.122: 3: com.jcraft.jsch.JSchException: Auth fail [preauth]
Dec 10 07:51:17 LabSZ sshd[24326]: reverse mapping checking getaddrinfo for 195-154-37-122.rev.poneytelecom.eu [195.154.37.122] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 07:51:18 LabSZ sshd[24326]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=195.154.37.122  user=uucp
Dec 10 07:51:20 LabSZ sshd[24326]: Failed password for uucp from 195.154.37.122 port 59266 ssh2
Dec 10 07:51:20 LabSZ sshd[24326]: error: Received disconnect from 195.154.37.122: 3: com.jcraft.jsch.JSchException: Auth fail [preauth]
Dec 10 07:53:26 LabSZ sshd[24329]: Connection closed by 194.190.163.22 [preauth]
Dec 10 07:55:55 LabSZ sshd[24331]: Invalid user test from 52.80.34.196
Dec 10 07:55:55 LabSZ sshd[24331]: input_userauth_request: invalid user test [preauth]
Dec 10 07:55:55 LabSZ sshd[24331]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 07:55:55 LabSZ sshd[24331]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=ec2-52-80-34-196.cn-north-1.compute.amazonaws.com.cn 
Dec 10 07:56:02 LabSZ sshd[24331]: Failed password for invalid user test from 52.80.34.196 port 36060 ssh2
Dec 10 07:56:02 LabSZ sshd[24331]: Received disconnect from 52.80.34.196: 11: Bye Bye [preauth]
Dec 10 07:56:13 LabSZ sshd[24333]: Did not receive identification string from 103.207.39.165
Dec 10 07:56:14 LabSZ sshd[24334]: Invalid user support from 103.207.39.165
Dec 10 07:56:14 LabSZ sshd[24334]: input_userauth_request: invalid user support [preauth]
Dec 10 07:56:14 LabSZ sshd[24334]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 07:56:14 LabSZ sshd[24334]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.207.39.165 
Dec 10 07:56:15 LabSZ sshd[24334]: Failed password for invalid user support from 103.207.39.165 port 58158 ssh2
Dec 10 07:56:15 LabSZ sshd[24334]: Received disconnect from 103.207.39.165: 11: Closed due to user request. [preauth]
Dec 10 08:07:00 LabSZ sshd[24336]: Connection closed by 194.190.163.22 [preauth]
Dec 10 08:08:41 LabSZ sshd[24338]: Invalid user inspur from 175.102.13.6
Dec 10 08:08:41 LabSZ sshd[24338]: input_userauth_request: invalid user inspur [preauth]
Dec 10 08:08:41 LabSZ sshd[24338]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:08:41 LabSZ sshd[24338]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=175.102.13.6 
Dec 10 08:08:43 LabSZ sshd[24338]: Failed password for invalid user inspur from 175.102.13.6 port 47130 ssh2
Dec 10 08:08:43 LabSZ sshd[24338]: Received disconnect from 175.102.13.6: 11: Bye Bye [preauth]
Dec 10 08:20:23 LabSZ sshd[24358]: Connection closed by 194.190.163.22 [preauth]
Dec 10 08:24:32 LabSZ sshd[24361]: Invalid user  0101 from 5.188.10.180
Dec 10 08:24:32 LabSZ sshd[24361]: input_userauth_request: invalid user  0101 [preauth]
Dec 10 08:24:32 LabSZ sshd[24361]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:24:32 LabSZ sshd[24361]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:24:35 LabSZ sshd[24361]: Failed password for invalid user  0101 from 5.188.10.180 port 36279 ssh2
Dec 10 08:24:36 LabSZ sshd[24361]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:24:40 LabSZ sshd[24363]: Invalid user 0 from 5.188.10.180
Dec 10 08:24:40 LabSZ sshd[24363]: input_userauth_request: invalid user 0 [preauth]
Dec 10 08:24:40 LabSZ sshd[24363]: Failed none for invalid user 0 from 5.188.10.180 port 49811 ssh2
Dec 10 08:24:43 LabSZ sshd[24363]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:24:43 LabSZ sshd[24363]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:24:45 LabSZ sshd[24363]: Failed password for invalid user 0 from 5.188.10.180 port 49811 ssh2
Dec 10 08:24:46 LabSZ sshd[24363]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:24:50 LabSZ sshd[24365]: Invalid user 1234 from 5.188.10.180
Dec 10 08:24:50 LabSZ sshd[24365]: input_userauth_request: invalid user 1234 [preauth]
Dec 10 08:24:50 LabSZ sshd[24365]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:24:50 LabSZ sshd[24365]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:24:52 LabSZ sshd[24365]: Failed password for invalid user 1234 from 5.188.10.180 port 45541 ssh2
Dec 10 08:24:54 LabSZ sshd[24365]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:24:58 LabSZ sshd[24367]: Invalid user admin from 5.188.10.180
Dec 10 08:24:58 LabSZ sshd[24367]: input_userauth_request: invalid user admin [preauth]
Dec 10 08:24:58 LabSZ sshd[24367]: Failed none for invalid user admin from 5.188.10.180 port 52631 ssh2
Dec 10 08:24:59 LabSZ sshd[24367]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:25:06 LabSZ sshd[24369]: Invalid user admin from 5.188.10.180
Dec 10 08:25:06 LabSZ sshd[24369]: input_userauth_request: invalid user admin [preauth]
Dec 10 08:25:06 LabSZ sshd[24369]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:06 LabSZ sshd[24369]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:25:08 LabSZ sshd[24369]: Failed password for invalid user admin from 5.188.10.180 port 60682 ssh2
Dec 10 08:25:10 LabSZ sshd[24369]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:11 LabSZ sshd[24369]: Failed password for invalid user admin from 5.188.10.180 port 60682 ssh2
Dec 10 08:25:13 LabSZ sshd[24369]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:15 LabSZ sshd[24369]: Failed password for invalid user admin from 5.188.10.180 port 60682 ssh2
Dec 10 08:25:16 LabSZ sshd[24369]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:18 LabSZ sshd[24369]: Failed password for invalid user admin from 5.188.10.180 port 60682 ssh2
Dec 10 08:25:19 LabSZ sshd[24369]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:21 LabSZ sshd[24369]: Failed password for invalid user admin from 5.188.10.180 port 60682 ssh2
Dec 10 08:25:22 LabSZ sshd[24369]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:25:22 LabSZ sshd[24369]: PAM 4 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:25:22 LabSZ sshd[24369]: PAM service(sshd) ignoring max retries; 5 > 3
Dec 10 08:25:27 LabSZ sshd[24371]: Invalid user admin from 5.188.10.180
Dec 10 08:25:27 LabSZ sshd[24371]: input_userauth_request: invalid user admin [preauth]
Dec 10 08:25:27 LabSZ sshd[24371]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:27 LabSZ sshd[24371]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:25:28 LabSZ sshd[24371]: Failed password for invalid user admin from 5.188.10.180 port 59647 ssh2
Dec 10 08:25:30 LabSZ sshd[24371]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:32 LabSZ sshd[24371]: Failed password for invalid user admin from 5.188.10.180 port 59647 ssh2
Dec 10 08:25:33 LabSZ sshd[24371]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:35 LabSZ sshd[24371]: Failed password for invalid user admin from 5.188.10.180 port 59647 ssh2
Dec 10 08:25:36 LabSZ sshd[24371]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:38 LabSZ sshd[24371]: Failed password for invalid user admin from 5.188.10.180 port 59647 ssh2
Dec 10 08:25:39 LabSZ sshd[24371]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:41 LabSZ sshd[24371]: Failed password for invalid user admin from 5.188.10.180 port 59647 ssh2
Dec 10 08:25:41 LabSZ sshd[24371]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:25:41 LabSZ sshd[24371]: PAM 4 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:25:41 LabSZ sshd[24371]: PAM service(sshd) ignoring max retries; 5 > 3
Dec 10 08:25:48 LabSZ sshd[24373]: Invalid user admin from 5.188.10.180
Dec 10 08:25:48 LabSZ sshd[24373]: input_userauth_request: invalid user admin [preauth]
Dec 10 08:25:48 LabSZ sshd[24373]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:48 LabSZ sshd[24373]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:25:50 LabSZ sshd[24373]: Failed password for invalid user admin from 5.188.10.180 port 56345 ssh2
Dec 10 08:25:51 LabSZ sshd[24373]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:25:58 LabSZ sshd[24375]: Invalid user default from 5.188.10.180
Dec 10 08:25:58 LabSZ sshd[24375]: input_userauth_request: invalid user default [preauth]
Dec 10 08:25:58 LabSZ sshd[24375]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:25:58 LabSZ sshd[24375]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:26:00 LabSZ sshd[24375]: Failed password for invalid user default from 5.188.10.180 port 41538 ssh2
Dec 10 08:26:01 LabSZ sshd[24375]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:26:03 LabSZ sshd[24375]: Failed password for invalid user default from 5.188.10.180 port 41538 ssh2
Dec 10 08:26:04 LabSZ sshd[24375]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:26:04 LabSZ sshd[24375]: PAM 1 more authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:26:09 LabSZ sshd[24377]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180  user=ftp
Dec 10 08:26:12 LabSZ sshd[24377]: Failed password for ftp from 5.188.10.180 port 54715 ssh2
Dec 10 08:26:14 LabSZ sshd[24377]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:26:22 LabSZ sshd[24379]: Invalid user guest from 5.188.10.180
Dec 10 08:26:22 LabSZ sshd[24379]: input_userauth_request: invalid user guest [preauth]
Dec 10 08:26:22 LabSZ sshd[24379]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:26:22 LabSZ sshd[24379]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=5.188.10.180 
Dec 10 08:26:24 LabSZ sshd[24379]: Failed password for invalid user guest from 5.188.10.180 port 47337 ssh2
Dec 10 08:26:25 LabSZ sshd[24379]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:26:32 LabSZ sshd[24381]: Connection closed by 5.188.10.180 [preauth]
Dec 10 08:26:40 LabSZ sshd[24383]: Did not receive identification string from 5.188.10.180
Dec 10 08:33:23 LabSZ sshd[24384]: Did not receive identification string from 103.207.39.212
Dec 10 08:33:24 LabSZ sshd[24385]: Invalid user support from 103.207.39.212
Dec 10 08:33:24 LabSZ sshd[24385]: input_userauth_request: invalid user support [preauth]
Dec 10 08:33:24 LabSZ sshd[24385]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:33:24 LabSZ sshd[24385]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.207.39.212 
Dec 10 08:33:26 LabSZ sshd[24385]: Failed password for invalid user support from 103.207.39.212 port 52644 ssh2
Dec 10 08:33:26 LabSZ sshd[24385]: Received disconnect from 103.207.39.212: 11: Closed due to user request. [preauth]
Dec 10 08:33:27 LabSZ sshd[24387]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.207.39.212  user=uucp
Dec 10 08:33:29 LabSZ sshd[24387]: Failed password for uucp from 103.207.39.212 port 51528 ssh2
Dec 10 08:33:29 LabSZ sshd[24387]: Received disconnect from 103.207.39.212: 11: Closed due to user request. [preauth]
Dec 10 08:33:29 LabSZ sshd[24389]: Invalid user admin from 103.207.39.212
Dec 10 08:33:29 LabSZ sshd[24389]: input_userauth_request: invalid user admin [preauth]
Dec 10 08:33:29 LabSZ sshd[24389]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:33:29 LabSZ sshd[24389]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.207.39.212 
Dec 10 08:33:31 LabSZ sshd[24389]: Failed password for invalid user admin from 103.207.39.212 port 58447 ssh2
Dec 10 08:33:31 LabSZ sshd[24389]: Received disconnect from 103.207.39.212: 11: Closed due to user request. [preauth]
Dec 10 08:33:40 LabSZ sshd[24391]: Connection closed by 194.190.163.22 [preauth]
Dec 10 08:39:47 LabSZ sshd[24408]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=106.5.5.195  user=root
Dec 10 08:39:49 LabSZ sshd[24408]: Failed password for root from 106.5.5.195 port 50719 ssh2
Dec 10 08:39:59 LabSZ sshd[24408]: message repeated 5 times: [ Failed password for root from 106.5.5.195 port 50719 ssh2]
Dec 10 08:39:59 LabSZ sshd[24408]: Disconnecting: Too many authentication failures for root [preauth]
Dec 10 08:39:59 LabSZ sshd[24408]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=106.5.5.195  user=root
Dec 10 08:39:59 LabSZ sshd[24408]: PAM service(sshd) ignoring max retries; 6 > 3
Dec 10 08:44:20 LabSZ sshd[24410]: Invalid user matlab from 52.80.34.196
Dec 10 08:44:20 LabSZ sshd[24410]: input_userauth_request: invalid user matlab [preauth]
Dec 10 08:44:20 LabSZ sshd[24410]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 08:44:20 LabSZ sshd[24410]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=ec2-52-80-34-196.cn-north-1.compute.amazonaws.com.cn 
Dec 10 08:44:27 LabSZ sshd[24410]: Failed password for invalid user matlab from 52.80.34.196 port 46199 ssh2
Dec 10 08:44:27 LabSZ sshd[24410]: Received disconnect from 52.80.34.196: 11: Bye Bye [preauth]
Dec 10 09:04:46 LabSZ sshd[24414]: Did not receive identification string from 188.132.244.89
Dec 10 09:07:23 LabSZ sshd[24415]: Invalid user 0 from 185.190.58.151
Dec 10 09:07:23 LabSZ sshd[24415]: input_userauth_request: invalid user 0 [preauth]
Dec 10 09:07:23 LabSZ sshd[24415]: Failed none for invalid user 0 from 185.190.58.151 port 55495 ssh2
Dec 10 09:07:24 LabSZ sshd[24415]: Connection closed by 185.190.58.151 [preauth]
Dec 10 09:07:56 LabSZ sshd[24417]: Invalid user 123 from 185.190.58.151
Dec 10 09:07:56 LabSZ sshd[24417]: input_userauth_request: invalid user 123 [preauth]
Dec 10 09:07:56 LabSZ sshd[24417]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:07:56 LabSZ sshd[24417]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=185.190.58.151 
Dec 10 09:07:58 LabSZ sshd[24417]: Failed password for invalid user 123 from 185.190.58.151 port 48700 ssh2
Dec 10 09:08:03 LabSZ sshd[24417]: Connection closed by 185.190.58.151 [preauth]
Dec 10 09:08:38 LabSZ sshd[24419]: Invalid user admin from 185.190.58.151
Dec 10 09:08:38 LabSZ sshd[24419]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:08:38 LabSZ sshd[24419]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:08:38 LabSZ sshd[24419]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=185.190.58.151 
Dec 10 09:08:40 LabSZ sshd[24419]: Failed password for invalid user admin from 185.190.58.151 port 49673 ssh2
Dec 10 09:08:44 LabSZ sshd[24419]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:08:47 LabSZ sshd[24419]: Failed password for invalid user admin from 185.190.58.151 port 49673 ssh2
Dec 10 09:08:52 LabSZ sshd[24419]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:08:54 LabSZ sshd[24419]: Failed password for invalid user admin from 185.190.58.151 port 49673 ssh2
Dec 10 09:08:59 LabSZ sshd[24419]: Connection closed by 185.190.58.151 [preauth]
Dec 10 09:08:59 LabSZ sshd[24419]: PAM 2 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=185.190.58.151 
Dec 10 09:09:39 LabSZ sshd[24421]: Invalid user admin from 185.190.58.151
Dec 10 09:09:39 LabSZ sshd[24421]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:09:39 LabSZ sshd[24421]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:09:39 LabSZ sshd[24421]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=185.190.58.151 
Dec 10 09:09:42 LabSZ sshd[24421]: Failed password for invalid user admin from 185.190.58.151 port 41650 ssh2
Dec 10 09:09:54 LabSZ sshd[24421]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:09:56 LabSZ sshd[24421]: Failed password for invalid user admin from 185.190.58.151 port 41650 ssh2
Dec 10 09:10:03 LabSZ sshd[24421]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:10:06 LabSZ sshd[24421]: Failed password for invalid user admin from 185.190.58.151 port 41650 ssh2
Dec 10 09:10:09 LabSZ sshd[24421]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:10:11 LabSZ sshd[24421]: Failed password for invalid user admin from 185.190.58.151 port 41650 ssh2
Dec 10 09:10:17 LabSZ sshd[24421]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:10:19 LabSZ sshd[24421]: Failed password for invalid user admin from 185.190.58.151 port 41650 ssh2
Dec 10 09:10:32 LabSZ sshd[24421]: Connection closed by 185.190.58.151 [preauth]
Dec 10 09:10:32 LabSZ sshd[24421]: PAM 4 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=185.190.58.151 
Dec 10 09:10:32 LabSZ sshd[24421]: PAM service(sshd) ignoring max retries; 5 > 3
Dec 10 09:11:00 LabSZ sshd[24437]: Invalid user admin from 185.190.58.151
Dec 10 09:11:00 LabSZ sshd[24437]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:11:00 LabSZ sshd[24437]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:00 LabSZ sshd[24437]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=185.190.58.151 
Dec 10 09:11:03 LabSZ sshd[24437]: Failed password for invalid user admin from 185.190.58.151 port 44155 ssh2
Dec 10 09:11:09 LabSZ sshd[24437]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:11 LabSZ sshd[24437]: Failed password for invalid user admin from 185.190.58.151 port 44155 ssh2
Dec 10 09:11:16 LabSZ sshd[24437]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:18 LabSZ sshd[24437]: Failed password for invalid user admin from 185.190.58.151 port 44155 ssh2
Dec 10 09:11:20 LabSZ sshd[24439]: Invalid user admin from 103.99.0.122
Dec 10 09:11:20 LabSZ sshd[24439]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:11:20 LabSZ sshd[24439]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:20 LabSZ sshd[24439]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:11:21 LabSZ sshd[24439]: Failed password for invalid user admin from 103.99.0.122 port 55177 ssh2
Dec 10 09:11:22 LabSZ sshd[24439]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:23 LabSZ sshd[24441]: Invalid user support from 103.99.0.122
Dec 10 09:11:23 LabSZ sshd[24441]: input_userauth_request: invalid user support [preauth]
Dec 10 09:11:23 LabSZ sshd[24441]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:23 LabSZ sshd[24441]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:11:24 LabSZ sshd[24437]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:25 LabSZ sshd[24441]: Failed password for invalid user support from 103.99.0.122 port 57317 ssh2
Dec 10 09:11:26 LabSZ sshd[24441]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:26 LabSZ sshd[24443]: Invalid user user from 103.99.0.122
Dec 10 09:11:26 LabSZ sshd[24443]: input_userauth_request: invalid user user [preauth]
Dec 10 09:11:26 LabSZ sshd[24443]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:26 LabSZ sshd[24443]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:11:26 LabSZ sshd[24437]: Failed password for invalid user admin from 185.190.58.151 port 44155 ssh2
Dec 10 09:11:28 LabSZ sshd[24443]: Failed password for invalid user user from 103.99.0.122 port 62581 ssh2
Dec 10 09:11:28 LabSZ sshd[24443]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:29 LabSZ sshd[24445]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=root
Dec 10 09:11:31 LabSZ sshd[24445]: Failed password for root from 103.99.0.122 port 49486 ssh2
Dec 10 09:11:31 LabSZ sshd[24445]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:32 LabSZ sshd[24447]: Invalid user 1234 from 103.99.0.122
Dec 10 09:11:32 LabSZ sshd[24447]: input_userauth_request: invalid user 1234 [preauth]
Dec 10 09:11:32 LabSZ sshd[24447]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:32 LabSZ sshd[24447]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:11:33 LabSZ sshd[24437]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:34 LabSZ sshd[24447]: Failed password for invalid user 1234 from 103.99.0.122 port 53950 ssh2
Dec 10 09:11:34 LabSZ sshd[24447]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:34 LabSZ sshd[24437]: Failed password for invalid user admin from 185.190.58.151 port 44155 ssh2
Dec 10 09:11:35 LabSZ sshd[24449]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=root
Dec 10 09:11:37 LabSZ sshd[24449]: Failed password for root from 103.99.0.122 port 58123 ssh2
Dec 10 09:11:38 LabSZ sshd[24449]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:39 LabSZ sshd[24451]: Invalid user anonymous from 103.99.0.122
Dec 10 09:11:39 LabSZ sshd[24451]: input_userauth_request: invalid user anonymous [preauth]
Dec 10 09:11:39 LabSZ sshd[24451]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:39 LabSZ sshd[24451]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:11:40 LabSZ sshd[24451]: Failed password for invalid user anonymous from 103.99.0.122 port 54051 ssh2
Dec 10 09:11:41 LabSZ sshd[24451]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:41 LabSZ sshd[24453]: Invalid user admin from 103.99.0.122
Dec 10 09:11:41 LabSZ sshd[24453]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:11:41 LabSZ sshd[24453]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:41 LabSZ sshd[24453]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:11:41 LabSZ sshd[24437]: Connection closed by 185.190.58.151 [preauth]
Dec 10 09:11:41 LabSZ sshd[24437]: PAM 4 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=185.190.58.151 
Dec 10 09:11:41 LabSZ sshd[24437]: PAM service(sshd) ignoring max retries; 5 > 3
Dec 10 09:11:44 LabSZ sshd[24453]: Failed password for invalid user admin from 103.99.0.122 port 57750 ssh2
Dec 10 09:11:44 LabSZ sshd[24453]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:45 LabSZ sshd[24456]: Invalid user ubnt from 103.99.0.122
Dec 10 09:11:45 LabSZ sshd[24456]: input_userauth_request: invalid user ubnt [preauth]
Dec 10 09:11:45 LabSZ sshd[24456]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:45 LabSZ sshd[24456]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:11:47 LabSZ sshd[24456]: Failed password for invalid user ubnt from 103.99.0.122 port 60608 ssh2
Dec 10 09:11:47 LabSZ sshd[24456]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:48 LabSZ sshd[24458]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=uucp
Dec 10 09:11:50 LabSZ sshd[24458]: Failed password for uucp from 103.99.0.122 port 64009 ssh2
Dec 10 09:11:50 LabSZ sshd[24458]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:51 LabSZ sshd[24460]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=sshd
Dec 10 09:11:52 LabSZ sshd[24460]: Failed password for sshd from 103.99.0.122 port 51359 ssh2
Dec 10 09:11:52 LabSZ sshd[24460]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:53 LabSZ sshd[24462]: Invalid user admin from 103.99.0.122
Dec 10 09:11:53 LabSZ sshd[24462]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:11:53 LabSZ sshd[24462]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:53 LabSZ sshd[24462]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:11:55 LabSZ sshd[24462]: Failed password for invalid user admin from 103.99.0.122 port 54739 ssh2
Dec 10 09:11:56 LabSZ sshd[24462]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:56 LabSZ sshd[24464]: Invalid user cisco from 103.99.0.122
Dec 10 09:11:56 LabSZ sshd[24464]: input_userauth_request: invalid user cisco [preauth]
Dec 10 09:11:56 LabSZ sshd[24464]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:56 LabSZ sshd[24464]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:11:57 LabSZ sshd[24464]: Failed password for invalid user cisco from 103.99.0.122 port 58309 ssh2
Dec 10 09:11:58 LabSZ sshd[24464]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:11:58 LabSZ sshd[24467]: Invalid user test from 103.99.0.122
Dec 10 09:11:58 LabSZ sshd[24467]: input_userauth_request: invalid user test [preauth]
Dec 10 09:11:58 LabSZ sshd[24467]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:11:58 LabSZ sshd[24467]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:00 LabSZ sshd[24467]: Failed password for invalid user test from 103.99.0.122 port 60250 ssh2
Dec 10 09:12:00 LabSZ sshd[24467]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:01 LabSZ sshd[24469]: Invalid user guest from 103.99.0.122
Dec 10 09:12:01 LabSZ sshd[24469]: input_userauth_request: invalid user guest [preauth]
Dec 10 09:12:01 LabSZ sshd[24469]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:01 LabSZ sshd[24469]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:03 LabSZ sshd[24469]: Failed password for invalid user guest from 103.99.0.122 port 63270 ssh2
Dec 10 09:12:03 LabSZ sshd[24469]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:04 LabSZ sshd[24471]: Invalid user user from 103.99.0.122
Dec 10 09:12:04 LabSZ sshd[24471]: input_userauth_request: invalid user user [preauth]
Dec 10 09:12:04 LabSZ sshd[24471]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:04 LabSZ sshd[24471]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:06 LabSZ sshd[24471]: Failed password for invalid user user from 103.99.0.122 port 49813 ssh2
Dec 10 09:12:06 LabSZ sshd[24471]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:06 LabSZ sshd[24473]: Invalid user operator from 103.99.0.122
Dec 10 09:12:06 LabSZ sshd[24473]: input_userauth_request: invalid user operator [preauth]
Dec 10 09:12:06 LabSZ sshd[24473]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:06 LabSZ sshd[24473]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:08 LabSZ sshd[24455]: Invalid user admin from 185.190.58.151
Dec 10 09:12:08 LabSZ sshd[24455]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:12:08 LabSZ sshd[24455]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:08 LabSZ sshd[24455]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=185.190.58.151 
Dec 10 09:12:08 LabSZ sshd[24473]: Failed password for invalid user operator from 103.99.0.122 port 53492 ssh2
Dec 10 09:12:09 LabSZ sshd[24473]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:10 LabSZ sshd[24455]: Failed password for invalid user admin from 185.190.58.151 port 49948 ssh2
Dec 10 09:12:10 LabSZ sshd[24475]: Invalid user admin from 103.99.0.122
Dec 10 09:12:10 LabSZ sshd[24475]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:12:10 LabSZ sshd[24475]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:10 LabSZ sshd[24475]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:12 LabSZ sshd[24475]: Failed password for invalid user admin from 103.99.0.122 port 56901 ssh2
Dec 10 09:12:12 LabSZ sshd[24475]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:12 LabSZ sshd[24477]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=root
Dec 10 09:12:15 LabSZ sshd[24477]: Failed password for root from 103.99.0.122 port 59841 ssh2
Dec 10 09:12:15 LabSZ sshd[24477]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:16 LabSZ sshd[24479]: Invalid user admin from 103.99.0.122
Dec 10 09:12:16 LabSZ sshd[24479]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:12:16 LabSZ sshd[24479]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:16 LabSZ sshd[24479]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:18 LabSZ sshd[24479]: Failed password for invalid user admin from 103.99.0.122 port 63168 ssh2
Dec 10 09:12:18 LabSZ sshd[24479]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:19 LabSZ sshd[24455]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:20 LabSZ sshd[24481]: Invalid user admin from 103.99.0.122
Dec 10 09:12:20 LabSZ sshd[24481]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:12:20 LabSZ sshd[24481]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:20 LabSZ sshd[24481]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:21 LabSZ sshd[24455]: Failed password for invalid user admin from 185.190.58.151 port 49948 ssh2
Dec 10 09:12:21 LabSZ sshd[24481]: Failed password for invalid user admin from 103.99.0.122 port 50011 ssh2
Dec 10 09:12:21 LabSZ sshd[24481]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:22 LabSZ sshd[24483]: Invalid user admin from 103.99.0.122
Dec 10 09:12:22 LabSZ sshd[24483]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:12:22 LabSZ sshd[24483]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:22 LabSZ sshd[24483]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:24 LabSZ sshd[24483]: Failed password for invalid user admin from 103.99.0.122 port 53531 ssh2
Dec 10 09:12:24 LabSZ sshd[24483]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:24 LabSZ sshd[24485]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=ftp
Dec 10 09:12:26 LabSZ sshd[24485]: Failed password for ftp from 103.99.0.122 port 56079 ssh2
Dec 10 09:12:27 LabSZ sshd[24455]: Connection closed by 185.190.58.151 [preauth]
Dec 10 09:12:27 LabSZ sshd[24455]: PAM 1 more authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=185.190.58.151 
Dec 10 09:12:27 LabSZ sshd[24485]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:28 LabSZ sshd[24488]: Invalid user monitor from 103.99.0.122
Dec 10 09:12:28 LabSZ sshd[24488]: input_userauth_request: invalid user monitor [preauth]
Dec 10 09:12:28 LabSZ sshd[24488]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:28 LabSZ sshd[24488]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:30 LabSZ sshd[24488]: Failed password for invalid user monitor from 103.99.0.122 port 59812 ssh2
Dec 10 09:12:30 LabSZ sshd[24488]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:30 LabSZ sshd[24490]: Invalid user ftpuser from 103.99.0.122
Dec 10 09:12:30 LabSZ sshd[24490]: input_userauth_request: invalid user ftpuser [preauth]
Dec 10 09:12:30 LabSZ sshd[24490]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:30 LabSZ sshd[24490]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:32 LabSZ sshd[24490]: Failed password for invalid user ftpuser from 103.99.0.122 port 62891 ssh2
Dec 10 09:12:32 LabSZ sshd[24490]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:33 LabSZ sshd[24492]: Invalid user pi from 103.99.0.122
Dec 10 09:12:33 LabSZ sshd[24492]: input_userauth_request: invalid user pi [preauth]
Dec 10 09:12:33 LabSZ sshd[24492]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:33 LabSZ sshd[24492]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:35 LabSZ sshd[24492]: Failed password for invalid user pi from 103.99.0.122 port 49289 ssh2
Dec 10 09:12:35 LabSZ sshd[24492]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:35 LabSZ sshd[24494]: Invalid user PlcmSpIp from 103.99.0.122
Dec 10 09:12:35 LabSZ sshd[24494]: input_userauth_request: invalid user PlcmSpIp [preauth]
Dec 10 09:12:35 LabSZ sshd[24494]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:35 LabSZ sshd[24494]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:37 LabSZ sshd[24494]: Failed password for invalid user PlcmSpIp from 103.99.0.122 port 51966 ssh2
Dec 10 09:12:37 LabSZ sshd[24494]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:38 LabSZ sshd[24497]: Invalid user Management from 103.99.0.122
Dec 10 09:12:38 LabSZ sshd[24497]: input_userauth_request: invalid user Management [preauth]
Dec 10 09:12:38 LabSZ sshd[24497]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:38 LabSZ sshd[24497]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:40 LabSZ sshd[24497]: Failed password for invalid user Management from 103.99.0.122 port 55028 ssh2
Dec 10 09:12:40 LabSZ sshd[24497]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:40 LabSZ sshd[24499]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=root
Dec 10 09:12:42 LabSZ sshd[24499]: Failed password for root from 103.99.0.122 port 57956 ssh2
Dec 10 09:12:42 LabSZ sshd[24499]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:43 LabSZ sshd[24501]: Invalid user ftpuser from 103.99.0.122
Dec 10 09:12:43 LabSZ sshd[24501]: input_userauth_request: invalid user ftpuser [preauth]
Dec 10 09:12:43 LabSZ sshd[24501]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:43 LabSZ sshd[24501]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 09:12:44 LabSZ sshd[24501]: Failed password for invalid user ftpuser from 103.99.0.122 port 60836 ssh2
Dec 10 09:12:44 LabSZ sshd[24501]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 09:12:46 LabSZ sshd[24503]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:12:46 LabSZ sshd[24503]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:12:48 LabSZ sshd[24503]: Failed password for root from 187.141.143.180 port 33314 ssh2
Dec 10 09:12:48 LabSZ sshd[24503]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:12:51 LabSZ sshd[24505]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:12:51 LabSZ sshd[24505]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:12:53 LabSZ sshd[24505]: Failed password for root from 187.141.143.180 port 34508 ssh2
Dec 10 09:12:54 LabSZ sshd[24505]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:12:57 LabSZ sshd[24487]: Invalid user api from 185.190.58.151
Dec 10 09:12:57 LabSZ sshd[24487]: input_userauth_request: invalid user api [preauth]
Dec 10 09:12:57 LabSZ sshd[24487]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:12:57 LabSZ sshd[24487]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=185.190.58.151 
Dec 10 09:12:57 LabSZ sshd[24507]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:12:57 LabSZ sshd[24507]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:12:59 LabSZ sshd[24487]: Failed password for invalid user api from 185.190.58.151 port 36894 ssh2
Dec 10 09:12:59 LabSZ sshd[24507]: Failed password for root from 187.141.143.180 port 35685 ssh2
Dec 10 09:12:59 LabSZ sshd[24507]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:03 LabSZ sshd[24509]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:03 LabSZ sshd[24509]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:13:03 LabSZ sshd[24487]: Connection closed by 185.190.58.151 [preauth]
Dec 10 09:13:05 LabSZ sshd[24509]: Failed password for root from 187.141.143.180 port 36902 ssh2
Dec 10 09:13:05 LabSZ sshd[24509]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:08 LabSZ sshd[24512]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:08 LabSZ sshd[24512]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:13:10 LabSZ sshd[24512]: Failed password for root from 187.141.143.180 port 38180 ssh2
Dec 10 09:13:10 LabSZ sshd[24512]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:13 LabSZ sshd[24514]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:13 LabSZ sshd[24514]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:13:15 LabSZ sshd[24514]: Failed password for root from 187.141.143.180 port 39319 ssh2
Dec 10 09:13:15 LabSZ sshd[24514]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:19 LabSZ sshd[24516]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:19 LabSZ sshd[24516]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:13:21 LabSZ sshd[24516]: Failed password for root from 187.141.143.180 port 40414 ssh2
Dec 10 09:13:21 LabSZ sshd[24516]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:22 LabSZ sshd[24511]: Did not receive identification string from 185.190.58.151
Dec 10 09:13:25 LabSZ sshd[24518]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:25 LabSZ sshd[24518]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:13:26 LabSZ sshd[24518]: Failed password for root from 187.141.143.180 port 41834 ssh2
Dec 10 09:13:27 LabSZ sshd[24518]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:30 LabSZ sshd[24520]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:30 LabSZ sshd[24520]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:13:32 LabSZ sshd[24520]: Failed password for root from 187.141.143.180 port 43092 ssh2
Dec 10 09:13:33 LabSZ sshd[24520]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:36 LabSZ sshd[24522]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:36 LabSZ sshd[24522]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:13:38 LabSZ sshd[24522]: Failed password for root from 187.141.143.180 port 44328 ssh2
Dec 10 09:13:39 LabSZ sshd[24522]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:42 LabSZ sshd[24525]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:42 LabSZ sshd[24525]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:13:44 LabSZ sshd[24525]: Failed password for root from 187.141.143.180 port 45696 ssh2
Dec 10 09:13:45 LabSZ sshd[24525]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:48 LabSZ sshd[24527]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:48 LabSZ sshd[24527]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:13:50 LabSZ sshd[24527]: Failed password for root from 187.141.143.180 port 47004 ssh2
Dec 10 09:13:50 LabSZ sshd[24527]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:53 LabSZ sshd[24529]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:53 LabSZ sshd[24529]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:13:56 LabSZ sshd[24529]: Failed password for root from 187.141.143.180 port 48339 ssh2
Dec 10 09:13:56 LabSZ sshd[24529]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:13:59 LabSZ sshd[24531]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:13:59 LabSZ sshd[24531]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:01 LabSZ sshd[24531]: Failed password for root from 187.141.143.180 port 49674 ssh2
Dec 10 09:14:01 LabSZ sshd[24531]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:04 LabSZ sshd[24533]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:04 LabSZ sshd[24533]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:06 LabSZ sshd[24533]: Failed password for root from 187.141.143.180 port 50880 ssh2
Dec 10 09:14:07 LabSZ sshd[24533]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:09 LabSZ sshd[24535]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:09 LabSZ sshd[24535]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:11 LabSZ sshd[24535]: Failed password for root from 187.141.143.180 port 52176 ssh2
Dec 10 09:14:12 LabSZ sshd[24535]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:14 LabSZ sshd[24537]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:14 LabSZ sshd[24537]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:16 LabSZ sshd[24537]: Failed password for root from 187.141.143.180 port 53403 ssh2
Dec 10 09:14:16 LabSZ sshd[24537]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:19 LabSZ sshd[24539]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:19 LabSZ sshd[24539]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:21 LabSZ sshd[24539]: Failed password for root from 187.141.143.180 port 54560 ssh2
Dec 10 09:14:22 LabSZ sshd[24539]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:25 LabSZ sshd[24541]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:25 LabSZ sshd[24541]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:26 LabSZ sshd[24541]: Failed password for root from 187.141.143.180 port 55849 ssh2
Dec 10 09:14:27 LabSZ sshd[24541]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:30 LabSZ sshd[24543]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:30 LabSZ sshd[24543]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:32 LabSZ sshd[24543]: Failed password for root from 187.141.143.180 port 57037 ssh2
Dec 10 09:14:32 LabSZ sshd[24543]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:35 LabSZ sshd[24545]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:35 LabSZ sshd[24545]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:38 LabSZ sshd[24545]: Failed password for root from 187.141.143.180 port 58386 ssh2
Dec 10 09:14:38 LabSZ sshd[24545]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:41 LabSZ sshd[24547]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:41 LabSZ sshd[24547]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:43 LabSZ sshd[24547]: Failed password for root from 187.141.143.180 port 59705 ssh2
Dec 10 09:14:43 LabSZ sshd[24547]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:46 LabSZ sshd[24549]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:46 LabSZ sshd[24549]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:49 LabSZ sshd[24549]: Failed password for root from 187.141.143.180 port 60924 ssh2
Dec 10 09:14:49 LabSZ sshd[24549]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:52 LabSZ sshd[24551]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:52 LabSZ sshd[24551]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:54 LabSZ sshd[24551]: Failed password for root from 187.141.143.180 port 34001 ssh2
Dec 10 09:14:54 LabSZ sshd[24551]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:14:57 LabSZ sshd[24553]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:14:57 LabSZ sshd[24553]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:14:59 LabSZ sshd[24553]: Failed password for root from 187.141.143.180 port 35172 ssh2
Dec 10 09:14:59 LabSZ sshd[24553]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:02 LabSZ sshd[24555]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:02 LabSZ sshd[24555]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:04 LabSZ sshd[24555]: Failed password for root from 187.141.143.180 port 36419 ssh2
Dec 10 09:15:04 LabSZ sshd[24555]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:07 LabSZ sshd[24557]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:07 LabSZ sshd[24557]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:09 LabSZ sshd[24557]: Failed password for root from 187.141.143.180 port 37678 ssh2
Dec 10 09:15:09 LabSZ sshd[24557]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:12 LabSZ sshd[24559]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:12 LabSZ sshd[24559]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:14 LabSZ sshd[24559]: Failed password for root from 187.141.143.180 port 38937 ssh2
Dec 10 09:15:15 LabSZ sshd[24559]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:18 LabSZ sshd[24561]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:18 LabSZ sshd[24561]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:20 LabSZ sshd[24561]: Failed password for root from 187.141.143.180 port 40297 ssh2
Dec 10 09:15:20 LabSZ sshd[24561]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:23 LabSZ sshd[24563]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:23 LabSZ sshd[24563]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:25 LabSZ sshd[24563]: Failed password for root from 187.141.143.180 port 41667 ssh2
Dec 10 09:15:26 LabSZ sshd[24563]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:29 LabSZ sshd[24565]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:29 LabSZ sshd[24565]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:31 LabSZ sshd[24565]: Failed password for root from 187.141.143.180 port 42938 ssh2
Dec 10 09:15:31 LabSZ sshd[24565]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:34 LabSZ sshd[24567]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:34 LabSZ sshd[24567]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:36 LabSZ sshd[24567]: Failed password for root from 187.141.143.180 port 44414 ssh2
Dec 10 09:15:37 LabSZ sshd[24567]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:39 LabSZ sshd[24569]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:39 LabSZ sshd[24569]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:41 LabSZ sshd[24569]: Failed password for root from 187.141.143.180 port 45661 ssh2
Dec 10 09:15:41 LabSZ sshd[24569]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:44 LabSZ sshd[24571]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:44 LabSZ sshd[24571]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:47 LabSZ sshd[24571]: Failed password for root from 187.141.143.180 port 46878 ssh2
Dec 10 09:15:47 LabSZ sshd[24571]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:50 LabSZ sshd[24573]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:50 LabSZ sshd[24573]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:52 LabSZ sshd[24573]: Failed password for root from 187.141.143.180 port 48241 ssh2
Dec 10 09:15:52 LabSZ sshd[24573]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:15:55 LabSZ sshd[24575]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:15:55 LabSZ sshd[24575]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:15:57 LabSZ sshd[24575]: Failed password for root from 187.141.143.180 port 49494 ssh2
Dec 10 09:15:57 LabSZ sshd[24575]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:00 LabSZ sshd[24577]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:00 LabSZ sshd[24577]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:16:03 LabSZ sshd[24577]: Failed password for root from 187.141.143.180 port 50811 ssh2
Dec 10 09:16:03 LabSZ sshd[24577]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:06 LabSZ sshd[24579]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:06 LabSZ sshd[24579]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:16:08 LabSZ sshd[24579]: Failed password for root from 187.141.143.180 port 52212 ssh2
Dec 10 09:16:08 LabSZ sshd[24579]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:11 LabSZ sshd[24581]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:11 LabSZ sshd[24581]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:16:13 LabSZ sshd[24581]: Failed password for root from 187.141.143.180 port 53589 ssh2
Dec 10 09:16:14 LabSZ sshd[24581]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:17 LabSZ sshd[24583]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:17 LabSZ sshd[24583]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:16:19 LabSZ sshd[24583]: Failed password for root from 187.141.143.180 port 54980 ssh2
Dec 10 09:16:19 LabSZ sshd[24583]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:22 LabSZ sshd[24585]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:22 LabSZ sshd[24585]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:16:24 LabSZ sshd[24585]: Failed password for root from 187.141.143.180 port 56377 ssh2
Dec 10 09:16:24 LabSZ sshd[24585]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:27 LabSZ sshd[24587]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:27 LabSZ sshd[24587]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:16:29 LabSZ sshd[24587]: Failed password for root from 187.141.143.180 port 57704 ssh2
Dec 10 09:16:30 LabSZ sshd[24587]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:33 LabSZ sshd[24589]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:33 LabSZ sshd[24589]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:16:35 LabSZ sshd[24589]: Failed password for root from 187.141.143.180 port 59080 ssh2
Dec 10 09:16:35 LabSZ sshd[24589]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:38 LabSZ sshd[24591]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:38 LabSZ sshd[24591]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:16:40 LabSZ sshd[24591]: Failed password for root from 187.141.143.180 port 60433 ssh2
Dec 10 09:16:40 LabSZ sshd[24591]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:43 LabSZ sshd[24593]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:43 LabSZ sshd[24593]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:16:45 LabSZ sshd[24593]: Failed password for root from 187.141.143.180 port 33456 ssh2
Dec 10 09:16:45 LabSZ sshd[24593]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:48 LabSZ sshd[24595]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:48 LabSZ sshd[24595]: Invalid user eoor from 187.141.143.180
Dec 10 09:16:48 LabSZ sshd[24595]: input_userauth_request: invalid user eoor [preauth]
Dec 10 09:16:48 LabSZ sshd[24595]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:16:48 LabSZ sshd[24595]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:16:50 LabSZ sshd[24595]: Failed password for invalid user eoor from 187.141.143.180 port 45825 ssh2
Dec 10 09:16:50 LabSZ sshd[24595]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:53 LabSZ sshd[24597]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:53 LabSZ sshd[24597]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=root
Dec 10 09:16:55 LabSZ sshd[24597]: Failed password for root from 187.141.143.180 port 46973 ssh2
Dec 10 09:16:56 LabSZ sshd[24597]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:16:59 LabSZ sshd[24599]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:16:59 LabSZ sshd[24599]: Invalid user butter from 187.141.143.180
Dec 10 09:16:59 LabSZ sshd[24599]: input_userauth_request: invalid user butter [preauth]
Dec 10 09:16:59 LabSZ sshd[24599]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:16:59 LabSZ sshd[24599]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:00 LabSZ sshd[24599]: Failed password for invalid user butter from 187.141.143.180 port 48369 ssh2
Dec 10 09:17:01 LabSZ sshd[24599]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:05 LabSZ sshd[24604]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:05 LabSZ sshd[24604]: Invalid user redhat from 187.141.143.180
Dec 10 09:17:05 LabSZ sshd[24604]: input_userauth_request: invalid user redhat [preauth]
Dec 10 09:17:05 LabSZ sshd[24604]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:17:05 LabSZ sshd[24604]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:07 LabSZ sshd[24604]: Failed password for invalid user redhat from 187.141.143.180 port 49479 ssh2
Dec 10 09:17:08 LabSZ sshd[24604]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:11 LabSZ sshd[24606]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:11 LabSZ sshd[24606]: Invalid user oracle from 187.141.143.180
Dec 10 09:17:11 LabSZ sshd[24606]: input_userauth_request: invalid user oracle [preauth]
Dec 10 09:17:11 LabSZ sshd[24606]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:17:11 LabSZ sshd[24606]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:12 LabSZ sshd[24606]: Failed password for invalid user oracle from 187.141.143.180 port 51169 ssh2
Dec 10 09:17:13 LabSZ sshd[24606]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:15 LabSZ sshd[24608]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:15 LabSZ sshd[24608]: Invalid user oracle from 187.141.143.180
Dec 10 09:17:15 LabSZ sshd[24608]: input_userauth_request: invalid user oracle [preauth]
Dec 10 09:17:15 LabSZ sshd[24608]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:17:15 LabSZ sshd[24608]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:18 LabSZ sshd[24608]: Failed password for invalid user oracle from 187.141.143.180 port 52276 ssh2
Dec 10 09:17:18 LabSZ sshd[24608]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:21 LabSZ sshd[24610]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:21 LabSZ sshd[24610]: Invalid user oracle from 187.141.143.180
Dec 10 09:17:21 LabSZ sshd[24610]: input_userauth_request: invalid user oracle [preauth]
Dec 10 09:17:21 LabSZ sshd[24610]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:17:21 LabSZ sshd[24610]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:23 LabSZ sshd[24610]: Failed password for invalid user oracle from 187.141.143.180 port 53550 ssh2
Dec 10 09:17:23 LabSZ sshd[24610]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:26 LabSZ sshd[24612]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:26 LabSZ sshd[24612]: Invalid user postgres from 187.141.143.180
Dec 10 09:17:26 LabSZ sshd[24612]: input_userauth_request: invalid user postgres [preauth]
Dec 10 09:17:26 LabSZ sshd[24612]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:17:26 LabSZ sshd[24612]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:28 LabSZ sshd[24612]: Failed password for invalid user postgres from 187.141.143.180 port 54596 ssh2
Dec 10 09:17:28 LabSZ sshd[24612]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:31 LabSZ sshd[24614]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:31 LabSZ sshd[24614]: Invalid user nagios from 187.141.143.180
Dec 10 09:17:31 LabSZ sshd[24614]: input_userauth_request: invalid user nagios [preauth]
Dec 10 09:17:31 LabSZ sshd[24614]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:17:31 LabSZ sshd[24614]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:33 LabSZ sshd[24614]: Failed password for invalid user nagios from 187.141.143.180 port 55761 ssh2
Dec 10 09:17:33 LabSZ sshd[24614]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:36 LabSZ sshd[24616]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:36 LabSZ sshd[24616]: Invalid user www from 187.141.143.180
Dec 10 09:17:36 LabSZ sshd[24616]: input_userauth_request: invalid user www [preauth]
Dec 10 09:17:36 LabSZ sshd[24616]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:17:36 LabSZ sshd[24616]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:38 LabSZ sshd[24616]: Failed password for invalid user www from 187.141.143.180 port 56816 ssh2
Dec 10 09:17:38 LabSZ sshd[24616]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:41 LabSZ sshd[24618]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:41 LabSZ sshd[24618]: Invalid user abc from 187.141.143.180
Dec 10 09:17:41 LabSZ sshd[24618]: input_userauth_request: invalid user abc [preauth]
Dec 10 09:17:41 LabSZ sshd[24618]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:17:41 LabSZ sshd[24618]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:43 LabSZ sshd[24618]: Failed password for invalid user abc from 187.141.143.180 port 58106 ssh2
Dec 10 09:17:43 LabSZ sshd[24618]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:46 LabSZ sshd[24620]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:46 LabSZ sshd[24620]: Invalid user ted from 187.141.143.180
Dec 10 09:17:46 LabSZ sshd[24620]: input_userauth_request: invalid user ted [preauth]
Dec 10 09:17:46 LabSZ sshd[24620]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:17:46 LabSZ sshd[24620]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:48 LabSZ sshd[24620]: Failed password for invalid user ted from 187.141.143.180 port 59333 ssh2
Dec 10 09:17:49 LabSZ sshd[24620]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:52 LabSZ sshd[24622]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:52 LabSZ sshd[24622]: Invalid user vnc from 187.141.143.180
Dec 10 09:17:52 LabSZ sshd[24622]: input_userauth_request: invalid user vnc [preauth]
Dec 10 09:17:52 LabSZ sshd[24622]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:17:52 LabSZ sshd[24622]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:17:54 LabSZ sshd[24622]: Failed password for invalid user vnc from 187.141.143.180 port 60547 ssh2
Dec 10 09:17:55 LabSZ sshd[24622]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:17:58 LabSZ sshd[24624]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:17:58 LabSZ sshd[24624]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=git
Dec 10 09:18:00 LabSZ sshd[24624]: Failed password for git from 187.141.143.180 port 33532 ssh2
Dec 10 09:18:01 LabSZ sshd[24624]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:18:05 LabSZ sshd[24626]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:18:05 LabSZ sshd[24626]: Invalid user ghost from 187.141.143.180
Dec 10 09:18:05 LabSZ sshd[24626]: input_userauth_request: invalid user ghost [preauth]
Dec 10 09:18:05 LabSZ sshd[24626]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:05 LabSZ sshd[24626]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:18:06 LabSZ sshd[24626]: Failed password for invalid user ghost from 187.141.143.180 port 34759 ssh2
Dec 10 09:18:07 LabSZ sshd[24626]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:18:11 LabSZ sshd[24628]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:18:11 LabSZ sshd[24628]: Invalid user ubuntu from 187.141.143.180
Dec 10 09:18:11 LabSZ sshd[24628]: input_userauth_request: invalid user ubuntu [preauth]
Dec 10 09:18:11 LabSZ sshd[24628]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:11 LabSZ sshd[24628]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:18:12 LabSZ sshd[24628]: Failed password for invalid user ubuntu from 187.141.143.180 port 35697 ssh2
Dec 10 09:18:13 LabSZ sshd[24628]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:18:17 LabSZ sshd[24630]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:18:17 LabSZ sshd[24630]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=ftp
Dec 10 09:18:18 LabSZ sshd[24630]: Failed password for ftp from 187.141.143.180 port 36704 ssh2
Dec 10 09:18:19 LabSZ sshd[24630]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:18:22 LabSZ sshd[24632]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:18:22 LabSZ sshd[24632]: Invalid user test from 187.141.143.180
Dec 10 09:18:22 LabSZ sshd[24632]: input_userauth_request: invalid user test [preauth]
Dec 10 09:18:22 LabSZ sshd[24632]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:22 LabSZ sshd[24632]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:18:24 LabSZ sshd[24632]: Failed password for invalid user test from 187.141.143.180 port 37598 ssh2
Dec 10 09:18:24 LabSZ sshd[24632]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:18:27 LabSZ sshd[24636]: Did not receive identification string from 103.207.39.16
Dec 10 09:18:27 LabSZ sshd[24637]: Invalid user support from 103.207.39.16
Dec 10 09:18:27 LabSZ sshd[24637]: input_userauth_request: invalid user support [preauth]
Dec 10 09:18:28 LabSZ sshd[24634]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:18:28 LabSZ sshd[24634]: Invalid user deploy from 187.141.143.180
Dec 10 09:18:28 LabSZ sshd[24634]: input_userauth_request: invalid user deploy [preauth]
Dec 10 09:18:28 LabSZ sshd[24634]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:28 LabSZ sshd[24634]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:18:28 LabSZ sshd[24637]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:28 LabSZ sshd[24637]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.207.39.16 
Dec 10 09:18:30 LabSZ sshd[24634]: Failed password for invalid user deploy from 187.141.143.180 port 38606 ssh2
Dec 10 09:18:30 LabSZ sshd[24637]: Failed password for invalid user support from 103.207.39.16 port 33310 ssh2
Dec 10 09:18:30 LabSZ sshd[24634]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:18:30 LabSZ sshd[24637]: Received disconnect from 103.207.39.16: 11: Closed due to user request. [preauth]
Dec 10 09:18:31 LabSZ sshd[24639]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.207.39.16  user=uucp
Dec 10 09:18:33 LabSZ sshd[24639]: Failed password for uucp from 103.207.39.16 port 42435 ssh2
Dec 10 09:18:33 LabSZ sshd[24639]: Received disconnect from 103.207.39.16: 11: Closed due to user request. [preauth]
Dec 10 09:18:33 LabSZ sshd[24643]: Invalid user admin from 103.207.39.16
Dec 10 09:18:33 LabSZ sshd[24643]: input_userauth_request: invalid user admin [preauth]
Dec 10 09:18:33 LabSZ sshd[24643]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:33 LabSZ sshd[24643]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.207.39.16 
Dec 10 09:18:33 LabSZ sshd[24641]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:18:33 LabSZ sshd[24641]: Invalid user deploy from 187.141.143.180
Dec 10 09:18:33 LabSZ sshd[24641]: input_userauth_request: invalid user deploy [preauth]
Dec 10 09:18:33 LabSZ sshd[24641]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:33 LabSZ sshd[24641]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:18:35 LabSZ sshd[24643]: Failed password for invalid user admin from 103.207.39.16 port 46723 ssh2
Dec 10 09:18:35 LabSZ sshd[24643]: Received disconnect from 103.207.39.16: 11: Closed due to user request. [preauth]
Dec 10 09:18:35 LabSZ sshd[24641]: Failed password for invalid user deploy from 187.141.143.180 port 39710 ssh2
Dec 10 09:18:36 LabSZ sshd[24641]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:18:40 LabSZ sshd[24645]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:18:40 LabSZ sshd[24645]: Invalid user oralce from 187.141.143.180
Dec 10 09:18:40 LabSZ sshd[24645]: input_userauth_request: invalid user oralce [preauth]
Dec 10 09:18:40 LabSZ sshd[24645]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:40 LabSZ sshd[24645]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:18:42 LabSZ sshd[24645]: Failed password for invalid user oralce from 187.141.143.180 port 40988 ssh2
Dec 10 09:18:42 LabSZ sshd[24645]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:18:46 LabSZ sshd[24647]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:18:46 LabSZ sshd[24647]: Invalid user oracle from 187.141.143.180
Dec 10 09:18:46 LabSZ sshd[24647]: input_userauth_request: invalid user oracle [preauth]
Dec 10 09:18:46 LabSZ sshd[24647]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:46 LabSZ sshd[24647]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:18:48 LabSZ sshd[24647]: Failed password for invalid user oracle from 187.141.143.180 port 42342 ssh2
Dec 10 09:18:48 LabSZ sshd[24647]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:18:52 LabSZ sshd[24649]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:18:52 LabSZ sshd[24649]: Invalid user nagios1 from 187.141.143.180
Dec 10 09:18:52 LabSZ sshd[24649]: input_userauth_request: invalid user nagios1 [preauth]
Dec 10 09:18:52 LabSZ sshd[24649]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:52 LabSZ sshd[24649]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:18:54 LabSZ sshd[24649]: Failed password for invalid user nagios1 from 187.141.143.180 port 43647 ssh2
Dec 10 09:18:54 LabSZ sshd[24649]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:18:58 LabSZ sshd[24651]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:18:58 LabSZ sshd[24651]: Invalid user postgres1 from 187.141.143.180
Dec 10 09:18:58 LabSZ sshd[24651]: input_userauth_request: invalid user postgres1 [preauth]
Dec 10 09:18:58 LabSZ sshd[24651]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:18:58 LabSZ sshd[24651]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:19:00 LabSZ sshd[24651]: Failed password for invalid user postgres1 from 187.141.143.180 port 45073 ssh2
Dec 10 09:19:01 LabSZ sshd[24651]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:19:04 LabSZ sshd[24653]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:19:04 LabSZ sshd[24653]: Invalid user test1 from 187.141.143.180
Dec 10 09:19:04 LabSZ sshd[24653]: input_userauth_request: invalid user test1 [preauth]
Dec 10 09:19:04 LabSZ sshd[24653]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:19:04 LabSZ sshd[24653]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:19:06 LabSZ sshd[24653]: Failed password for invalid user test1 from 187.141.143.180 port 46519 ssh2
Dec 10 09:19:06 LabSZ sshd[24653]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:19:09 LabSZ sshd[24655]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:19:09 LabSZ sshd[24655]: Invalid user test2 from 187.141.143.180
Dec 10 09:19:09 LabSZ sshd[24655]: input_userauth_request: invalid user test2 [preauth]
Dec 10 09:19:09 LabSZ sshd[24655]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:19:09 LabSZ sshd[24655]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:19:11 LabSZ sshd[24655]: Failed password for invalid user test2 from 187.141.143.180 port 48023 ssh2
Dec 10 09:19:11 LabSZ sshd[24655]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:19:15 LabSZ sshd[24657]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:19:15 LabSZ sshd[24657]: Invalid user bssh from 187.141.143.180
Dec 10 09:19:15 LabSZ sshd[24657]: input_userauth_request: invalid user bssh [preauth]
Dec 10 09:19:15 LabSZ sshd[24657]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:19:15 LabSZ sshd[24657]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:19:17 LabSZ sshd[24657]: Failed password for invalid user bssh from 187.141.143.180 port 49412 ssh2
Dec 10 09:19:17 LabSZ sshd[24657]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:19:20 LabSZ sshd[24659]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:19:20 LabSZ sshd[24659]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=mysql
Dec 10 09:19:22 LabSZ sshd[24659]: Failed password for mysql from 187.141.143.180 port 51060 ssh2
Dec 10 09:19:23 LabSZ sshd[24659]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:19:26 LabSZ sshd[24661]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:19:26 LabSZ sshd[24661]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=mysql
Dec 10 09:19:28 LabSZ sshd[24661]: Failed password for mysql from 187.141.143.180 port 52586 ssh2
Dec 10 09:19:28 LabSZ sshd[24661]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:19:32 LabSZ sshd[24663]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:19:32 LabSZ sshd[24663]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180  user=git
Dec 10 09:19:34 LabSZ sshd[24663]: Failed password for git from 187.141.143.180 port 53992 ssh2
Dec 10 09:19:34 LabSZ sshd[24663]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:19:37 LabSZ sshd[24665]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:19:37 LabSZ sshd[24665]: Invalid user magnos from 187.141.143.180
Dec 10 09:19:37 LabSZ sshd[24665]: input_userauth_request: invalid user magnos [preauth]
Dec 10 09:19:37 LabSZ sshd[24665]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:19:37 LabSZ sshd[24665]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:19:39 LabSZ sshd[24665]: Failed password for invalid user magnos from 187.141.143.180 port 55517 ssh2
Dec 10 09:19:40 LabSZ sshd[24665]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:19:42 LabSZ sshd[24667]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:19:42 LabSZ sshd[24667]: Invalid user magnos from 187.141.143.180
Dec 10 09:19:42 LabSZ sshd[24667]: input_userauth_request: invalid user magnos [preauth]
Dec 10 09:19:42 LabSZ sshd[24667]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:19:42 LabSZ sshd[24667]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:19:45 LabSZ sshd[24667]: Failed password for invalid user magnos from 187.141.143.180 port 57031 ssh2
Dec 10 09:19:46 LabSZ sshd[24667]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:19:49 LabSZ sshd[24669]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:19:49 LabSZ sshd[24669]: Invalid user ingrid from 187.141.143.180
Dec 10 09:19:49 LabSZ sshd[24669]: input_userauth_request: invalid user ingrid [preauth]
Dec 10 09:19:49 LabSZ sshd[24669]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:19:49 LabSZ sshd[24669]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:19:51 LabSZ sshd[24669]: Failed password for invalid user ingrid from 187.141.143.180 port 58682 ssh2
Dec 10 09:19:51 LabSZ sshd[24669]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:19:54 LabSZ sshd[24671]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:19:54 LabSZ sshd[24671]: Invalid user jay from 187.141.143.180
Dec 10 09:19:54 LabSZ sshd[24671]: input_userauth_request: invalid user jay [preauth]
Dec 10 09:19:54 LabSZ sshd[24671]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:19:54 LabSZ sshd[24671]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:19:57 LabSZ sshd[24671]: Failed password for invalid user jay from 187.141.143.180 port 60259 ssh2
Dec 10 09:19:57 LabSZ sshd[24671]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:20:00 LabSZ sshd[24673]: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 09:20:00 LabSZ sshd[24673]: Invalid user cyrus from 187.141.143.180
Dec 10 09:20:00 LabSZ sshd[24673]: input_userauth_request: invalid user cyrus [preauth]
Dec 10 09:20:00 LabSZ sshd[24673]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:20:00 LabSZ sshd[24673]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=187.141.143.180 
Dec 10 09:20:02 LabSZ sshd[24673]: Failed password for invalid user cyrus from 187.141.143.180 port 33574 ssh2
Dec 10 09:20:03 LabSZ sshd[24673]: Received disconnect from 187.141.143.180: 11: Bye Bye [preauth]
Dec 10 09:31:22 LabSZ sshd[24676]: Invalid user FILTER from 104.192.3.34
Dec 10 09:31:22 LabSZ sshd[24676]: input_userauth_request: invalid user FILTER [preauth]
Dec 10 09:31:22 LabSZ sshd[24676]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:31:22 LabSZ sshd[24676]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=104.192.3.34 
Dec 10 09:31:24 LabSZ sshd[24676]: Failed password for invalid user FILTER from 104.192.3.34 port 33738 ssh2
Dec 10 09:31:24 LabSZ sshd[24676]: Received disconnect from 104.192.3.34: 11: Bye Bye [preauth]
Dec 10 09:31:32 LabSZ sshd[24678]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=104.192.3.34  user=root
Dec 10 09:31:34 LabSZ sshd[24678]: Failed password for root from 104.192.3.34 port 56524 ssh2
Dec 10 09:31:34 LabSZ sshd[24678]: Connection closed by 104.192.3.34 [preauth]
Dec 10 09:32:20 LabSZ sshd[24680]: Accepted password for fztu from 119.137.62.142 port 49116 ssh2
Dec 10 09:32:20 LabSZ sshd[24680]: pam_unix(sshd:session): session opened for user fztu by (uid=0)
Dec 10 09:32:35 LabSZ sshd[24787]: Invalid user matlab from 52.80.34.196
Dec 10 09:32:35 LabSZ sshd[24787]: input_userauth_request: invalid user matlab [preauth]
Dec 10 09:32:35 LabSZ sshd[24787]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 09:32:35 LabSZ sshd[24787]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=ec2-52-80-34-196.cn-north-1.compute.amazonaws.com.cn 
Dec 10 09:32:42 LabSZ sshd[24787]: Failed password for invalid user matlab from 52.80.34.196 port 36060 ssh2
Dec 10 09:32:42 LabSZ sshd[24787]: Received disconnect from 52.80.34.196: 11: Bye Bye [preauth]
Dec 10 09:45:06 LabSZ sshd[24761]: Received disconnect from 119.137.62.142: 11: disconnected by user
Dec 10 09:45:06 LabSZ sshd[24680]: pam_unix(sshd:session): session closed for user fztu
Dec 10 09:48:23 LabSZ sshd[24806]: Invalid user 0 from 181.214.87.4
Dec 10 09:48:23 LabSZ sshd[24806]: input_userauth_request: invalid user 0 [preauth]
Dec 10 09:48:23 LabSZ sshd[24806]: Failed none for invalid user 0 from 181.214.87.4 port 51889 ssh2
Dec 10 09:48:24 LabSZ sshd[24806]: Connection closed by 181.214.87.4 [preauth]
Dec 10 09:48:32 LabSZ sshd[24808]: Did not receive identification string from 181.214.87.4
Dec 10 10:04:52 LabSZ sshd[24809]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=60.2.12.12  user=root
Dec 10 10:04:54 LabSZ sshd[24809]: Failed password for root from 60.2.12.12 port 63646 ssh2
Dec 10 10:04:54 LabSZ sshd[24809]: Received disconnect from 60.2.12.12: 11: Bye Bye [preauth]
Dec 10 10:04:54 LabSZ sshd[24811]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=60.2.12.12  user=root
Dec 10 10:04:56 LabSZ sshd[24811]: Failed password for root from 60.2.12.12 port 65244 ssh2
Dec 10 10:04:56 LabSZ sshd[24811]: Received disconnect from 60.2.12.12: 11: Bye Bye [preauth]
Dec 10 10:05:00 LabSZ sshd[24813]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=60.2.12.12  user=root
Dec 10 10:05:03 LabSZ sshd[24813]: Failed password for root from 60.2.12.12 port 10217 ssh2
Dec 10 10:05:03 LabSZ sshd[24813]: Received disconnect from 60.2.12.12: 11: Bye Bye [preauth]
Dec 10 10:05:08 LabSZ sshd[24815]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=60.2.12.12  user=root
Dec 10 10:05:10 LabSZ sshd[24815]: Failed password for root from 60.2.12.12 port 15145 ssh2
Dec 10 10:05:10 LabSZ sshd[24815]: Received disconnect from 60.2.12.12: 11: Bye Bye [preauth]
Dec 10 10:05:19 LabSZ sshd[24817]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=60.2.12.12  user=root
Dec 10 10:05:22 LabSZ sshd[24817]: Failed password for root from 60.2.12.12 port 20658 ssh2
Dec 10 10:05:22 LabSZ sshd[24817]: Connection closed by 60.2.12.12 [preauth]
Dec 10 10:13:59 LabSZ sshd[24833]: Invalid user admin from 119.4.203.64
Dec 10 10:13:59 LabSZ sshd[24833]: input_userauth_request: invalid user admin [preauth]
Dec 10 10:13:59 LabSZ sshd[24833]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:13:59 LabSZ sshd[24833]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=119.4.203.64 
Dec 10 10:14:01 LabSZ sshd[24833]: Failed password for invalid user admin from 119.4.203.64 port 2191 ssh2
Dec 10 10:14:01 LabSZ sshd[24833]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:14:04 LabSZ sshd[24833]: Failed password for invalid user admin from 119.4.203.64 port 2191 ssh2
Dec 10 10:14:04 LabSZ sshd[24833]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:14:06 LabSZ sshd[24833]: Failed password for invalid user admin from 119.4.203.64 port 2191 ssh2
Dec 10 10:14:06 LabSZ sshd[24833]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:14:08 LabSZ sshd[24833]: Failed password for invalid user admin from 119.4.203.64 port 2191 ssh2
Dec 10 10:14:08 LabSZ sshd[24833]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:14:10 LabSZ sshd[24833]: Failed password for invalid user admin from 119.4.203.64 port 2191 ssh2
Dec 10 10:14:10 LabSZ sshd[24833]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:14:13 LabSZ sshd[24833]: Failed password for invalid user admin from 119.4.203.64 port 2191 ssh2
Dec 10 10:14:13 LabSZ sshd[24833]: Disconnecting: Too many authentication failures for admin [preauth]
Dec 10 10:14:13 LabSZ sshd[24833]: PAM 5 more authentication failures; logname= uid=0 euid=0 tty=ssh ruser= rhost=119.4.203.64 
Dec 10 10:14:13 LabSZ sshd[24833]: PAM service(sshd) ignoring max retries; 6 > 3
Dec 10 10:19:59 LabSZ sshd[24839]: Connection closed by 1.237.174.253 [preauth]
Dec 10 10:21:01 LabSZ sshd[24841]: Invalid user matlab from 52.80.34.196
Dec 10 10:21:01 LabSZ sshd[24841]: input_userauth_request: invalid user matlab [preauth]
Dec 10 10:21:01 LabSZ sshd[24841]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:21:01 LabSZ sshd[24841]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=ec2-52-80-34-196.cn-north-1.compute.amazonaws.com.cn 
Dec 10 10:21:09 LabSZ sshd[24841]: Failed password for invalid user matlab from 52.80.34.196 port 36060 ssh2
Dec 10 10:21:09 LabSZ sshd[24841]: Received disconnect from 52.80.34.196: 11: Bye Bye [preauth]
Dec 10 10:32:27 LabSZ sshd[24844]: Invalid user inspur from 183.136.162.51
Dec 10 10:32:27 LabSZ sshd[24844]: input_userauth_request: invalid user inspur [preauth]
Dec 10 10:32:27 LabSZ sshd[24844]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:32:27 LabSZ sshd[24844]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.136.162.51 
Dec 10 10:32:30 LabSZ sshd[24844]: Failed password for invalid user inspur from 183.136.162.51 port 26396 ssh2
Dec 10 10:32:30 LabSZ sshd[24844]: Received disconnect from 183.136.162.51: 11: Bye Bye [preauth]
Dec 10 10:33:55 LabSZ sshd[24846]: Connection closed by 1.237.174.253 [preauth]
Dec 10 10:47:18 LabSZ sshd[24862]: Connection closed by 88.147.143.242 [preauth]
Dec 10 10:50:37 LabSZ sshd[24865]: Connection closed by 1.237.174.253 [preauth]
Dec 10 10:54:27 LabSZ sshd[24868]: Invalid user zhangyan from 183.62.140.253
Dec 10 10:54:27 LabSZ sshd[24868]: input_userauth_request: invalid user zhangyan [preauth]
Dec 10 10:54:27 LabSZ sshd[24868]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:54:27 LabSZ sshd[24868]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253 
Dec 10 10:54:29 LabSZ sshd[24868]: Failed password for invalid user zhangyan from 183.62.140.253 port 33521 ssh2
Dec 10 10:54:29 LabSZ sshd[24868]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:29 LabSZ sshd[24870]: Invalid user dff from 183.62.140.253
Dec 10 10:54:29 LabSZ sshd[24870]: input_userauth_request: invalid user dff [preauth]
Dec 10 10:54:29 LabSZ sshd[24870]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:54:29 LabSZ sshd[24870]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253 
Dec 10 10:54:31 LabSZ sshd[24870]: Failed password for invalid user dff from 183.62.140.253 port 33902 ssh2
Dec 10 10:54:31 LabSZ sshd[24870]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:31 LabSZ sshd[24872]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:33 LabSZ sshd[24872]: Failed password for root from 183.62.140.253 port 34263 ssh2
Dec 10 10:54:33 LabSZ sshd[24872]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:33 LabSZ sshd[24874]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:35 LabSZ sshd[24874]: Failed password for root from 183.62.140.253 port 34712 ssh2
Dec 10 10:54:35 LabSZ sshd[24874]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:35 LabSZ sshd[24877]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:37 LabSZ sshd[24877]: Failed password for root from 183.62.140.253 port 35013 ssh2
Dec 10 10:54:37 LabSZ sshd[24877]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:37 LabSZ sshd[24879]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:39 LabSZ sshd[24879]: Failed password for root from 183.62.140.253 port 35457 ssh2
Dec 10 10:54:39 LabSZ sshd[24879]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:39 LabSZ sshd[24882]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:41 LabSZ sshd[24882]: Failed password for root from 183.62.140.253 port 35825 ssh2
Dec 10 10:54:41 LabSZ sshd[24882]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:41 LabSZ sshd[24884]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:43 LabSZ sshd[24884]: Failed password for root from 183.62.140.253 port 36196 ssh2
Dec 10 10:54:43 LabSZ sshd[24884]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:43 LabSZ sshd[24886]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:45 LabSZ sshd[24886]: Failed password for root from 183.62.140.253 port 36525 ssh2
Dec 10 10:54:45 LabSZ sshd[24886]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:45 LabSZ sshd[24888]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:47 LabSZ sshd[24888]: Failed password for root from 183.62.140.253 port 36961 ssh2
Dec 10 10:54:47 LabSZ sshd[24888]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:47 LabSZ sshd[24890]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:49 LabSZ sshd[24890]: Failed password for root from 183.62.140.253 port 37270 ssh2
Dec 10 10:54:49 LabSZ sshd[24890]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:49 LabSZ sshd[24893]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:50 LabSZ sshd[24893]: Failed password for root from 183.62.140.253 port 37652 ssh2
Dec 10 10:54:50 LabSZ sshd[24893]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:51 LabSZ sshd[24896]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:52 LabSZ sshd[24896]: Failed password for root from 183.62.140.253 port 37999 ssh2
Dec 10 10:54:52 LabSZ sshd[24896]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:52 LabSZ sshd[24898]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:54 LabSZ sshd[24898]: Failed password for root from 183.62.140.253 port 38375 ssh2
Dec 10 10:54:54 LabSZ sshd[24898]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:54 LabSZ sshd[24900]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:56 LabSZ sshd[24900]: Failed password for root from 183.62.140.253 port 38647 ssh2
Dec 10 10:54:56 LabSZ sshd[24900]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:56 LabSZ sshd[24903]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:54:58 LabSZ sshd[24903]: Failed password for root from 183.62.140.253 port 39004 ssh2
Dec 10 10:54:58 LabSZ sshd[24903]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:54:58 LabSZ sshd[24905]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:00 LabSZ sshd[24905]: Failed password for root from 183.62.140.253 port 39410 ssh2
Dec 10 10:55:00 LabSZ sshd[24905]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:00 LabSZ sshd[24907]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:02 LabSZ sshd[24907]: Failed password for root from 183.62.140.253 port 39827 ssh2
Dec 10 10:55:02 LabSZ sshd[24907]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:02 LabSZ sshd[24909]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:05 LabSZ sshd[24909]: Failed password for root from 183.62.140.253 port 40213 ssh2
Dec 10 10:55:05 LabSZ sshd[24909]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:05 LabSZ sshd[24912]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:07 LabSZ sshd[24912]: Failed password for root from 183.62.140.253 port 40631 ssh2
Dec 10 10:55:07 LabSZ sshd[24912]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:07 LabSZ sshd[24917]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:07 LabSZ sshd[24914]: Invalid user cheng from 202.100.179.208
Dec 10 10:55:07 LabSZ sshd[24914]: input_userauth_request: invalid user cheng [preauth]
Dec 10 10:55:07 LabSZ sshd[24914]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:55:07 LabSZ sshd[24914]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=202.100.179.208 
Dec 10 10:55:09 LabSZ sshd[24917]: Failed password for root from 183.62.140.253 port 41083 ssh2
Dec 10 10:55:09 LabSZ sshd[24917]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:09 LabSZ sshd[24919]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:10 LabSZ sshd[24914]: Failed password for invalid user cheng from 202.100.179.208 port 32891 ssh2
Dec 10 10:55:10 LabSZ sshd[24914]: Received disconnect from 202.100.179.208: 11: Bye Bye [preauth]
Dec 10 10:55:11 LabSZ sshd[24919]: Failed password for root from 183.62.140.253 port 41526 ssh2
Dec 10 10:55:11 LabSZ sshd[24919]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:11 LabSZ sshd[24921]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:13 LabSZ sshd[24921]: Failed password for root from 183.62.140.253 port 41908 ssh2
Dec 10 10:55:13 LabSZ sshd[24921]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:13 LabSZ sshd[24923]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:15 LabSZ sshd[24923]: Failed password for root from 183.62.140.253 port 42246 ssh2
Dec 10 10:55:15 LabSZ sshd[24923]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:15 LabSZ sshd[24925]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:17 LabSZ sshd[24925]: Failed password for root from 183.62.140.253 port 42606 ssh2
Dec 10 10:55:17 LabSZ sshd[24925]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:17 LabSZ sshd[24927]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:19 LabSZ sshd[24927]: Failed password for root from 183.62.140.253 port 42931 ssh2
Dec 10 10:55:19 LabSZ sshd[24927]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:19 LabSZ sshd[24929]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:22 LabSZ sshd[24929]: Failed password for root from 183.62.140.253 port 43391 ssh2
Dec 10 10:55:22 LabSZ sshd[24929]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:22 LabSZ sshd[24931]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:23 LabSZ sshd[24931]: Failed password for root from 183.62.140.253 port 43821 ssh2
Dec 10 10:55:23 LabSZ sshd[24931]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:24 LabSZ sshd[24933]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:26 LabSZ sshd[24933]: Failed password for root from 183.62.140.253 port 44163 ssh2
Dec 10 10:55:26 LabSZ sshd[24933]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:26 LabSZ sshd[24935]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:28 LabSZ sshd[24935]: Failed password for root from 183.62.140.253 port 44590 ssh2
Dec 10 10:55:28 LabSZ sshd[24935]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:29 LabSZ sshd[24937]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:31 LabSZ sshd[24937]: Failed password for root from 183.62.140.253 port 45079 ssh2
Dec 10 10:55:31 LabSZ sshd[24937]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:31 LabSZ sshd[24939]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:33 LabSZ sshd[24939]: Failed password for root from 183.62.140.253 port 45512 ssh2
Dec 10 10:55:33 LabSZ sshd[24939]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:33 LabSZ sshd[24942]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:35 LabSZ sshd[24942]: Failed password for root from 183.62.140.253 port 45902 ssh2
Dec 10 10:55:35 LabSZ sshd[24942]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:35 LabSZ sshd[24944]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:37 LabSZ sshd[24944]: Failed password for root from 183.62.140.253 port 46273 ssh2
Dec 10 10:55:37 LabSZ sshd[24944]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:37 LabSZ sshd[24947]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:39 LabSZ sshd[24947]: Failed password for root from 183.62.140.253 port 46626 ssh2
Dec 10 10:55:39 LabSZ sshd[24947]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:39 LabSZ sshd[24949]: Invalid user oracle from 183.62.140.253
Dec 10 10:55:39 LabSZ sshd[24949]: input_userauth_request: invalid user oracle [preauth]
Dec 10 10:55:39 LabSZ sshd[24949]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:55:39 LabSZ sshd[24949]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253 
Dec 10 10:55:41 LabSZ sshd[24949]: Failed password for invalid user oracle from 183.62.140.253 port 47098 ssh2
Dec 10 10:55:41 LabSZ sshd[24949]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:41 LabSZ sshd[24951]: Invalid user test from 183.62.140.253
Dec 10 10:55:41 LabSZ sshd[24951]: input_userauth_request: invalid user test [preauth]
Dec 10 10:55:41 LabSZ sshd[24951]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:55:41 LabSZ sshd[24951]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253 
Dec 10 10:55:43 LabSZ sshd[24951]: Failed password for invalid user test from 183.62.140.253 port 47409 ssh2
Dec 10 10:55:43 LabSZ sshd[24951]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:43 LabSZ sshd[24953]: Invalid user oracle from 183.62.140.253
Dec 10 10:55:43 LabSZ sshd[24953]: input_userauth_request: invalid user oracle [preauth]
Dec 10 10:55:43 LabSZ sshd[24953]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:55:43 LabSZ sshd[24953]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253 
Dec 10 10:55:45 LabSZ sshd[24953]: Failed password for invalid user oracle from 183.62.140.253 port 47782 ssh2
Dec 10 10:55:45 LabSZ sshd[24953]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:45 LabSZ sshd[24955]: Invalid user ubuntu from 183.62.140.253
Dec 10 10:55:45 LabSZ sshd[24955]: input_userauth_request: invalid user ubuntu [preauth]
Dec 10 10:55:45 LabSZ sshd[24955]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:55:45 LabSZ sshd[24955]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253 
Dec 10 10:55:47 LabSZ sshd[24955]: Failed password for invalid user ubuntu from 183.62.140.253 port 48168 ssh2
Dec 10 10:55:47 LabSZ sshd[24955]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:47 LabSZ sshd[24957]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=git
Dec 10 10:55:49 LabSZ sshd[24957]: Failed password for git from 183.62.140.253 port 48527 ssh2
Dec 10 10:55:49 LabSZ sshd[24957]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:49 LabSZ sshd[24959]: Invalid user boot from 183.62.140.253
Dec 10 10:55:49 LabSZ sshd[24959]: input_userauth_request: invalid user boot [preauth]
Dec 10 10:55:49 LabSZ sshd[24959]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:55:49 LabSZ sshd[24959]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253 
Dec 10 10:55:51 LabSZ sshd[24959]: Failed password for invalid user boot from 183.62.140.253 port 48976 ssh2
Dec 10 10:55:51 LabSZ sshd[24959]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:52 LabSZ sshd[24962]: Invalid user 123456 from 183.62.140.253
Dec 10 10:55:52 LabSZ sshd[24962]: input_userauth_request: invalid user 123456 [preauth]
Dec 10 10:55:52 LabSZ sshd[24962]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:55:52 LabSZ sshd[24962]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253 
Dec 10 10:55:54 LabSZ sshd[24962]: Failed password for invalid user 123456 from 183.62.140.253 port 49425 ssh2
Dec 10 10:55:54 LabSZ sshd[24962]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:54 LabSZ sshd[24964]: Invalid user 123 from 183.62.140.253
Dec 10 10:55:54 LabSZ sshd[24964]: input_userauth_request: invalid user 123 [preauth]
Dec 10 10:55:54 LabSZ sshd[24964]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 10:55:54 LabSZ sshd[24964]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253 
Dec 10 10:55:56 LabSZ sshd[24964]: Failed password for invalid user 123 from 183.62.140.253 port 49870 ssh2
Dec 10 10:55:56 LabSZ sshd[24964]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:56 LabSZ sshd[24966]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:55:58 LabSZ sshd[24966]: Failed password for root from 183.62.140.253 port 50263 ssh2
Dec 10 10:55:58 LabSZ sshd[24966]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:55:58 LabSZ sshd[24968]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:00 LabSZ sshd[24968]: Failed password for root from 183.62.140.253 port 50726 ssh2
Dec 10 10:56:00 LabSZ sshd[24968]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:00 LabSZ sshd[24970]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:02 LabSZ sshd[24970]: Failed password for root from 183.62.140.253 port 51066 ssh2
Dec 10 10:56:02 LabSZ sshd[24970]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:02 LabSZ sshd[24972]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:04 LabSZ sshd[24972]: Failed password for root from 183.62.140.253 port 51502 ssh2
Dec 10 10:56:04 LabSZ sshd[24972]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:05 LabSZ sshd[24975]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:06 LabSZ sshd[24975]: Failed password for root from 183.62.140.253 port 51925 ssh2
Dec 10 10:56:06 LabSZ sshd[24975]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:07 LabSZ sshd[24977]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:08 LabSZ sshd[24977]: Failed password for root from 183.62.140.253 port 52269 ssh2
Dec 10 10:56:08 LabSZ sshd[24977]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:09 LabSZ sshd[24979]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:10 LabSZ sshd[24979]: Failed password for root from 183.62.140.253 port 52663 ssh2
Dec 10 10:56:10 LabSZ sshd[24979]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:10 LabSZ sshd[24981]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:12 LabSZ sshd[24981]: Failed password for root from 183.62.140.253 port 52962 ssh2
Dec 10 10:56:12 LabSZ sshd[24981]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:12 LabSZ sshd[24983]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:14 LabSZ sshd[24983]: Failed password for root from 183.62.140.253 port 53309 ssh2
Dec 10 10:56:14 LabSZ sshd[24983]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:14 LabSZ sshd[24985]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:16 LabSZ sshd[24985]: Failed password for root from 183.62.140.253 port 53697 ssh2
Dec 10 10:56:16 LabSZ sshd[24985]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:16 LabSZ sshd[24987]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:18 LabSZ sshd[24987]: Failed password for root from 183.62.140.253 port 54137 ssh2
Dec 10 10:56:18 LabSZ sshd[24987]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:18 LabSZ sshd[24989]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:20 LabSZ sshd[24989]: Failed password for root from 183.62.140.253 port 54466 ssh2
Dec 10 10:56:20 LabSZ sshd[24989]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:21 LabSZ sshd[24992]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:22 LabSZ sshd[24992]: Failed password for root from 183.62.140.253 port 54954 ssh2
Dec 10 10:56:22 LabSZ sshd[24992]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:22 LabSZ sshd[24994]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:24 LabSZ sshd[24994]: Failed password for root from 183.62.140.253 port 55249 ssh2
Dec 10 10:56:24 LabSZ sshd[24994]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:24 LabSZ sshd[24997]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:27 LabSZ sshd[24997]: Failed password for root from 183.62.140.253 port 55676 ssh2
Dec 10 10:56:27 LabSZ sshd[24997]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:27 LabSZ sshd[24999]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:29 LabSZ sshd[24999]: Failed password for root from 183.62.140.253 port 56091 ssh2
Dec 10 10:56:29 LabSZ sshd[24999]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:29 LabSZ sshd[25002]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:30 LabSZ sshd[25002]: Failed password for root from 183.62.140.253 port 56499 ssh2
Dec 10 10:56:31 LabSZ sshd[25002]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:31 LabSZ sshd[25004]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:33 LabSZ sshd[25004]: Failed password for root from 183.62.140.253 port 56850 ssh2
Dec 10 10:56:33 LabSZ sshd[25004]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:33 LabSZ sshd[25006]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:35 LabSZ sshd[25006]: Failed password for root from 183.62.140.253 port 57292 ssh2
Dec 10 10:56:35 LabSZ sshd[25006]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:35 LabSZ sshd[25008]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:37 LabSZ sshd[25008]: Failed password for root from 183.62.140.253 port 57660 ssh2
Dec 10 10:56:37 LabSZ sshd[25008]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:37 LabSZ sshd[25010]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:39 LabSZ sshd[25010]: Failed password for root from 183.62.140.253 port 58028 ssh2
Dec 10 10:56:39 LabSZ sshd[25010]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:40 LabSZ sshd[25012]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:41 LabSZ sshd[25012]: Failed password for root from 183.62.140.253 port 58556 ssh2
Dec 10 10:56:41 LabSZ sshd[25012]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:41 LabSZ sshd[25014]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:43 LabSZ sshd[25014]: Failed password for root from 183.62.140.253 port 58889 ssh2
Dec 10 10:56:43 LabSZ sshd[25014]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:44 LabSZ sshd[25017]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:46 LabSZ sshd[25017]: Failed password for root from 183.62.140.253 port 59321 ssh2
Dec 10 10:56:46 LabSZ sshd[25017]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:46 LabSZ sshd[25019]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:48 LabSZ sshd[25019]: Failed password for root from 183.62.140.253 port 59788 ssh2
Dec 10 10:56:48 LabSZ sshd[25019]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:48 LabSZ sshd[25022]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:50 LabSZ sshd[25022]: Failed password for root from 183.62.140.253 port 60209 ssh2
Dec 10 10:56:50 LabSZ sshd[25022]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:50 LabSZ sshd[25024]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:53 LabSZ sshd[25024]: Failed password for root from 183.62.140.253 port 60656 ssh2
Dec 10 10:56:53 LabSZ sshd[25024]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:53 LabSZ sshd[25026]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:55 LabSZ sshd[25026]: Failed password for root from 183.62.140.253 port 32879 ssh2
Dec 10 10:56:55 LabSZ sshd[25026]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:55 LabSZ sshd[25028]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:56:58 LabSZ sshd[25028]: Failed password for root from 183.62.140.253 port 33304 ssh2
Dec 10 10:56:58 LabSZ sshd[25028]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:56:58 LabSZ sshd[25030]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:00 LabSZ sshd[25030]: Failed password for root from 183.62.140.253 port 33781 ssh2
Dec 10 10:57:00 LabSZ sshd[25030]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:00 LabSZ sshd[25033]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:02 LabSZ sshd[25033]: Failed password for root from 183.62.140.253 port 34226 ssh2
Dec 10 10:57:02 LabSZ sshd[25033]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:03 LabSZ sshd[25035]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:04 LabSZ sshd[25035]: Failed password for root from 183.62.140.253 port 34711 ssh2
Dec 10 10:57:04 LabSZ sshd[25035]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:04 LabSZ sshd[25037]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:06 LabSZ sshd[25037]: Failed password for root from 183.62.140.253 port 35021 ssh2
Dec 10 10:57:06 LabSZ sshd[25037]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:06 LabSZ sshd[25039]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:08 LabSZ sshd[25039]: Failed password for root from 183.62.140.253 port 35353 ssh2
Dec 10 10:57:08 LabSZ sshd[25039]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:08 LabSZ sshd[25041]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:10 LabSZ sshd[25041]: Failed password for root from 183.62.140.253 port 35810 ssh2
Dec 10 10:57:10 LabSZ sshd[25041]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:11 LabSZ sshd[25043]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:13 LabSZ sshd[25043]: Failed password for root from 183.62.140.253 port 36265 ssh2
Dec 10 10:57:13 LabSZ sshd[25043]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:13 LabSZ sshd[25045]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:15 LabSZ sshd[25045]: Failed password for root from 183.62.140.253 port 36644 ssh2
Dec 10 10:57:15 LabSZ sshd[25045]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:15 LabSZ sshd[25047]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:16 LabSZ sshd[25047]: Failed password for root from 183.62.140.253 port 37080 ssh2
Dec 10 10:57:16 LabSZ sshd[25047]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:17 LabSZ sshd[25049]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:19 LabSZ sshd[25049]: Failed password for root from 183.62.140.253 port 37388 ssh2
Dec 10 10:57:19 LabSZ sshd[25049]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:19 LabSZ sshd[25052]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:22 LabSZ sshd[25052]: Failed password for root from 183.62.140.253 port 37894 ssh2
Dec 10 10:57:22 LabSZ sshd[25052]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:22 LabSZ sshd[25054]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:24 LabSZ sshd[25054]: Failed password for root from 183.62.140.253 port 38431 ssh2
Dec 10 10:57:24 LabSZ sshd[25054]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:24 LabSZ sshd[25056]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:26 LabSZ sshd[25056]: Failed password for root from 183.62.140.253 port 38767 ssh2
Dec 10 10:57:26 LabSZ sshd[25056]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:26 LabSZ sshd[25058]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:29 LabSZ sshd[25058]: Failed password for root from 183.62.140.253 port 39237 ssh2
Dec 10 10:57:29 LabSZ sshd[25058]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:29 LabSZ sshd[25060]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:31 LabSZ sshd[25060]: Failed password for root from 183.62.140.253 port 39715 ssh2
Dec 10 10:57:31 LabSZ sshd[25060]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:31 LabSZ sshd[25063]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:34 LabSZ sshd[25063]: Failed password for root from 183.62.140.253 port 40121 ssh2
Dec 10 10:57:34 LabSZ sshd[25063]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:34 LabSZ sshd[25065]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:36 LabSZ sshd[25065]: Failed password for root from 183.62.140.253 port 40676 ssh2
Dec 10 10:57:36 LabSZ sshd[25065]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:36 LabSZ sshd[25068]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:38 LabSZ sshd[25068]: Failed password for root from 183.62.140.253 port 40993 ssh2
Dec 10 10:57:38 LabSZ sshd[25068]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:38 LabSZ sshd[25071]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:40 LabSZ sshd[25071]: Failed password for root from 183.62.140.253 port 41439 ssh2
Dec 10 10:57:40 LabSZ sshd[25071]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:40 LabSZ sshd[25073]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:43 LabSZ sshd[25073]: Failed password for root from 183.62.140.253 port 41872 ssh2
Dec 10 10:57:43 LabSZ sshd[25073]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:43 LabSZ sshd[25075]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:45 LabSZ sshd[25075]: Failed password for root from 183.62.140.253 port 42308 ssh2
Dec 10 10:57:45 LabSZ sshd[25075]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:45 LabSZ sshd[25077]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:47 LabSZ sshd[25077]: Failed password for root from 183.62.140.253 port 42836 ssh2
Dec 10 10:57:47 LabSZ sshd[25077]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:47 LabSZ sshd[25079]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:50 LabSZ sshd[25079]: Failed password for root from 183.62.140.253 port 43213 ssh2
Dec 10 10:57:50 LabSZ sshd[25079]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:50 LabSZ sshd[25082]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:51 LabSZ sshd[25082]: Failed password for root from 183.62.140.253 port 43666 ssh2
Dec 10 10:57:51 LabSZ sshd[25082]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:52 LabSZ sshd[25084]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:54 LabSZ sshd[25084]: Failed password for root from 183.62.140.253 port 43984 ssh2
Dec 10 10:57:54 LabSZ sshd[25084]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:54 LabSZ sshd[25087]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:56 LabSZ sshd[25087]: Failed password for root from 183.62.140.253 port 44456 ssh2
Dec 10 10:57:56 LabSZ sshd[25087]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:57 LabSZ sshd[25090]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:57:58 LabSZ sshd[25090]: Failed password for root from 183.62.140.253 port 44921 ssh2
Dec 10 10:57:58 LabSZ sshd[25090]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:57:58 LabSZ sshd[25092]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:00 LabSZ sshd[25092]: Failed password for root from 183.62.140.253 port 45256 ssh2
Dec 10 10:58:00 LabSZ sshd[25092]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:00 LabSZ sshd[25094]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:02 LabSZ sshd[25094]: Failed password for root from 183.62.140.253 port 45620 ssh2
Dec 10 10:58:02 LabSZ sshd[25094]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:02 LabSZ sshd[25096]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:04 LabSZ sshd[25096]: Failed password for root from 183.62.140.253 port 46013 ssh2
Dec 10 10:58:04 LabSZ sshd[25096]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:05 LabSZ sshd[25098]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:07 LabSZ sshd[25098]: Failed password for root from 183.62.140.253 port 46485 ssh2
Dec 10 10:58:07 LabSZ sshd[25098]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:07 LabSZ sshd[25100]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:09 LabSZ sshd[25100]: Failed password for root from 183.62.140.253 port 46880 ssh2
Dec 10 10:58:09 LabSZ sshd[25100]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:09 LabSZ sshd[25103]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:11 LabSZ sshd[25103]: Failed password for root from 183.62.140.253 port 47331 ssh2
Dec 10 10:58:11 LabSZ sshd[25103]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:11 LabSZ sshd[25105]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:13 LabSZ sshd[25105]: Failed password for root from 183.62.140.253 port 47653 ssh2
Dec 10 10:58:13 LabSZ sshd[25105]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:13 LabSZ sshd[25107]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:15 LabSZ sshd[25107]: Failed password for root from 183.62.140.253 port 48088 ssh2
Dec 10 10:58:15 LabSZ sshd[25107]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:15 LabSZ sshd[25110]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:17 LabSZ sshd[25110]: Failed password for root from 183.62.140.253 port 48505 ssh2
Dec 10 10:58:17 LabSZ sshd[25110]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:17 LabSZ sshd[25113]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:19 LabSZ sshd[25113]: Failed password for root from 183.62.140.253 port 48911 ssh2
Dec 10 10:58:19 LabSZ sshd[25113]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:20 LabSZ sshd[25115]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:22 LabSZ sshd[25115]: Failed password for root from 183.62.140.253 port 49345 ssh2
Dec 10 10:58:22 LabSZ sshd[25115]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:22 LabSZ sshd[25118]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:24 LabSZ sshd[25118]: Failed password for root from 183.62.140.253 port 49810 ssh2
Dec 10 10:58:24 LabSZ sshd[25118]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:24 LabSZ sshd[25121]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:26 LabSZ sshd[25121]: Failed password for root from 183.62.140.253 port 50215 ssh2
Dec 10 10:58:26 LabSZ sshd[25121]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:26 LabSZ sshd[25123]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:29 LabSZ sshd[25123]: Failed password for root from 183.62.140.253 port 50545 ssh2
Dec 10 10:58:29 LabSZ sshd[25123]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:29 LabSZ sshd[25125]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:31 LabSZ sshd[25125]: Failed password for root from 183.62.140.253 port 51094 ssh2
Dec 10 10:58:31 LabSZ sshd[25125]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:31 LabSZ sshd[25127]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:33 LabSZ sshd[25127]: Failed password for root from 183.62.140.253 port 51538 ssh2
Dec 10 10:58:33 LabSZ sshd[25127]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:33 LabSZ sshd[25129]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:35 LabSZ sshd[25129]: Failed password for root from 183.62.140.253 port 51965 ssh2
Dec 10 10:58:35 LabSZ sshd[25129]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:35 LabSZ sshd[25131]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:37 LabSZ sshd[25131]: Failed password for root from 183.62.140.253 port 52316 ssh2
Dec 10 10:58:37 LabSZ sshd[25131]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:37 LabSZ sshd[25134]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:39 LabSZ sshd[25134]: Failed password for root from 183.62.140.253 port 52590 ssh2
Dec 10 10:58:39 LabSZ sshd[25134]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:39 LabSZ sshd[25136]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:41 LabSZ sshd[25136]: Failed password for root from 183.62.140.253 port 53021 ssh2
Dec 10 10:58:41 LabSZ sshd[25136]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:41 LabSZ sshd[25138]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:43 LabSZ sshd[25138]: Failed password for root from 183.62.140.253 port 53440 ssh2
Dec 10 10:58:43 LabSZ sshd[25138]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:43 LabSZ sshd[25140]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:46 LabSZ sshd[25140]: Failed password for root from 183.62.140.253 port 53891 ssh2
Dec 10 10:58:46 LabSZ sshd[25140]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:46 LabSZ sshd[25143]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:47 LabSZ sshd[25143]: Failed password for root from 183.62.140.253 port 54339 ssh2
Dec 10 10:58:47 LabSZ sshd[25143]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:48 LabSZ sshd[25145]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:49 LabSZ sshd[25145]: Failed password for root from 183.62.140.253 port 54699 ssh2
Dec 10 10:58:49 LabSZ sshd[25145]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:50 LabSZ sshd[25147]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:52 LabSZ sshd[25147]: Failed password for root from 183.62.140.253 port 55069 ssh2
Dec 10 10:58:52 LabSZ sshd[25147]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:52 LabSZ sshd[25149]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:54 LabSZ sshd[25149]: Failed password for root from 183.62.140.253 port 55577 ssh2
Dec 10 10:58:54 LabSZ sshd[25149]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:54 LabSZ sshd[25151]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:56 LabSZ sshd[25151]: Failed password for root from 183.62.140.253 port 55956 ssh2
Dec 10 10:58:56 LabSZ sshd[25151]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:56 LabSZ sshd[25153]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:58:59 LabSZ sshd[25153]: Failed password for root from 183.62.140.253 port 56319 ssh2
Dec 10 10:58:59 LabSZ sshd[25153]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:58:59 LabSZ sshd[25155]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:00 LabSZ sshd[25155]: Failed password for root from 183.62.140.253 port 56847 ssh2
Dec 10 10:59:00 LabSZ sshd[25155]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:00 LabSZ sshd[25158]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:02 LabSZ sshd[25158]: Failed password for root from 183.62.140.253 port 57092 ssh2
Dec 10 10:59:02 LabSZ sshd[25158]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:02 LabSZ sshd[25160]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:05 LabSZ sshd[25160]: Failed password for root from 183.62.140.253 port 57485 ssh2
Dec 10 10:59:05 LabSZ sshd[25160]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:05 LabSZ sshd[25163]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:07 LabSZ sshd[25163]: Failed password for root from 183.62.140.253 port 57998 ssh2
Dec 10 10:59:07 LabSZ sshd[25163]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:07 LabSZ sshd[25165]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:10 LabSZ sshd[25165]: Failed password for root from 183.62.140.253 port 58430 ssh2
Dec 10 10:59:10 LabSZ sshd[25165]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:10 LabSZ sshd[25167]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:11 LabSZ sshd[25167]: Failed password for root from 183.62.140.253 port 58882 ssh2
Dec 10 10:59:11 LabSZ sshd[25167]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:11 LabSZ sshd[25169]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:12 LabSZ sshd[25169]: Failed password for root from 183.62.140.253 port 59160 ssh2
Dec 10 10:59:12 LabSZ sshd[25169]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:13 LabSZ sshd[25171]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:15 LabSZ sshd[25171]: Failed password for root from 183.62.140.253 port 59470 ssh2
Dec 10 10:59:15 LabSZ sshd[25171]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:15 LabSZ sshd[25174]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:17 LabSZ sshd[25174]: Failed password for root from 183.62.140.253 port 59905 ssh2
Dec 10 10:59:17 LabSZ sshd[25174]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:17 LabSZ sshd[25176]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:20 LabSZ sshd[25176]: Failed password for root from 183.62.140.253 port 60413 ssh2
Dec 10 10:59:20 LabSZ sshd[25176]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:20 LabSZ sshd[25178]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:22 LabSZ sshd[25178]: Failed password for root from 183.62.140.253 port 60834 ssh2
Dec 10 10:59:22 LabSZ sshd[25178]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:22 LabSZ sshd[25180]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:23 LabSZ sshd[25180]: Failed password for root from 183.62.140.253 port 32995 ssh2
Dec 10 10:59:23 LabSZ sshd[25180]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:23 LabSZ sshd[25182]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:25 LabSZ sshd[25182]: Failed password for root from 183.62.140.253 port 33258 ssh2
Dec 10 10:59:25 LabSZ sshd[25182]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:25 LabSZ sshd[25184]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:27 LabSZ sshd[25184]: Failed password for root from 183.62.140.253 port 33635 ssh2
Dec 10 10:59:27 LabSZ sshd[25184]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:28 LabSZ sshd[25186]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:30 LabSZ sshd[25186]: Failed password for root from 183.62.140.253 port 34091 ssh2
Dec 10 10:59:30 LabSZ sshd[25186]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:30 LabSZ sshd[25188]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:31 LabSZ sshd[25188]: Failed password for root from 183.62.140.253 port 34545 ssh2
Dec 10 10:59:31 LabSZ sshd[25188]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:32 LabSZ sshd[25190]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:34 LabSZ sshd[25190]: Failed password for root from 183.62.140.253 port 34849 ssh2
Dec 10 10:59:34 LabSZ sshd[25190]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:34 LabSZ sshd[25193]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:35 LabSZ sshd[25193]: Failed password for root from 183.62.140.253 port 35260 ssh2
Dec 10 10:59:35 LabSZ sshd[25193]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:35 LabSZ sshd[25195]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:37 LabSZ sshd[25195]: Failed password for root from 183.62.140.253 port 35525 ssh2
Dec 10 10:59:37 LabSZ sshd[25195]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:37 LabSZ sshd[25197]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:39 LabSZ sshd[25197]: Failed password for root from 183.62.140.253 port 35893 ssh2
Dec 10 10:59:39 LabSZ sshd[25197]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:39 LabSZ sshd[25200]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:41 LabSZ sshd[25200]: Failed password for root from 183.62.140.253 port 36335 ssh2
Dec 10 10:59:41 LabSZ sshd[25200]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:41 LabSZ sshd[25203]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:43 LabSZ sshd[25203]: Failed password for root from 183.62.140.253 port 36747 ssh2
Dec 10 10:59:43 LabSZ sshd[25203]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:43 LabSZ sshd[25205]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:45 LabSZ sshd[25205]: Failed password for root from 183.62.140.253 port 37033 ssh2
Dec 10 10:59:45 LabSZ sshd[25205]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:45 LabSZ sshd[25207]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:47 LabSZ sshd[25207]: Failed password for root from 183.62.140.253 port 37478 ssh2
Dec 10 10:59:47 LabSZ sshd[25207]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:47 LabSZ sshd[25209]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:49 LabSZ sshd[25209]: Failed password for root from 183.62.140.253 port 37854 ssh2
Dec 10 10:59:49 LabSZ sshd[25209]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:49 LabSZ sshd[25211]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:51 LabSZ sshd[25211]: Failed password for root from 183.62.140.253 port 38241 ssh2
Dec 10 10:59:51 LabSZ sshd[25211]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:51 LabSZ sshd[25214]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:53 LabSZ sshd[25214]: Failed password for root from 183.62.140.253 port 38693 ssh2
Dec 10 10:59:53 LabSZ sshd[25214]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:53 LabSZ sshd[25216]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:54 LabSZ sshd[25216]: Failed password for root from 183.62.140.253 port 39008 ssh2
Dec 10 10:59:54 LabSZ sshd[25216]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:55 LabSZ sshd[25218]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:57 LabSZ sshd[25218]: Failed password for root from 183.62.140.253 port 39309 ssh2
Dec 10 10:59:57 LabSZ sshd[25218]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:57 LabSZ sshd[25220]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 10:59:59 LabSZ sshd[25220]: Failed password for root from 183.62.140.253 port 39714 ssh2
Dec 10 10:59:59 LabSZ sshd[25220]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 10:59:59 LabSZ sshd[25222]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:00 LabSZ sshd[25222]: Failed password for root from 183.62.140.253 port 40110 ssh2
Dec 10 11:00:00 LabSZ sshd[25222]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:00 LabSZ sshd[25224]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:03 LabSZ sshd[25224]: Failed password for root from 183.62.140.253 port 40454 ssh2
Dec 10 11:00:03 LabSZ sshd[25224]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:03 LabSZ sshd[25227]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:04 LabSZ sshd[25227]: Failed password for root from 183.62.140.253 port 40862 ssh2
Dec 10 11:00:04 LabSZ sshd[25227]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:04 LabSZ sshd[25229]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:06 LabSZ sshd[25229]: Failed password for root from 183.62.140.253 port 41156 ssh2
Dec 10 11:00:06 LabSZ sshd[25229]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:06 LabSZ sshd[25231]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:08 LabSZ sshd[25231]: Failed password for root from 183.62.140.253 port 41452 ssh2
Dec 10 11:00:08 LabSZ sshd[25231]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:08 LabSZ sshd[25233]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:10 LabSZ sshd[25233]: Failed password for root from 183.62.140.253 port 41860 ssh2
Dec 10 11:00:10 LabSZ sshd[25233]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:10 LabSZ sshd[25235]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:11 LabSZ sshd[25235]: Failed password for root from 183.62.140.253 port 42239 ssh2
Dec 10 11:00:11 LabSZ sshd[25235]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:12 LabSZ sshd[25237]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:14 LabSZ sshd[25237]: Failed password for root from 183.62.140.253 port 42564 ssh2
Dec 10 11:00:14 LabSZ sshd[25237]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:14 LabSZ sshd[25240]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:16 LabSZ sshd[25240]: Failed password for root from 183.62.140.253 port 42948 ssh2
Dec 10 11:00:16 LabSZ sshd[25240]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:16 LabSZ sshd[25242]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:18 LabSZ sshd[25242]: Failed password for root from 183.62.140.253 port 43414 ssh2
Dec 10 11:00:18 LabSZ sshd[25242]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:18 LabSZ sshd[25244]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:20 LabSZ sshd[25244]: Failed password for root from 183.62.140.253 port 43714 ssh2
Dec 10 11:00:20 LabSZ sshd[25244]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:20 LabSZ sshd[25246]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:22 LabSZ sshd[25246]: Failed password for root from 183.62.140.253 port 44125 ssh2
Dec 10 11:00:22 LabSZ sshd[25246]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:22 LabSZ sshd[25248]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:24 LabSZ sshd[25248]: Failed password for root from 183.62.140.253 port 44532 ssh2
Dec 10 11:00:24 LabSZ sshd[25248]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:24 LabSZ sshd[25250]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:26 LabSZ sshd[25250]: Failed password for root from 183.62.140.253 port 44861 ssh2
Dec 10 11:00:26 LabSZ sshd[25250]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:26 LabSZ sshd[25252]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:28 LabSZ sshd[25252]: Failed password for root from 183.62.140.253 port 45203 ssh2
Dec 10 11:00:28 LabSZ sshd[25252]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:28 LabSZ sshd[25255]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:30 LabSZ sshd[25255]: Failed password for root from 183.62.140.253 port 45599 ssh2
Dec 10 11:00:30 LabSZ sshd[25255]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:30 LabSZ sshd[25257]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:32 LabSZ sshd[25257]: Failed password for root from 183.62.140.253 port 46008 ssh2
Dec 10 11:00:32 LabSZ sshd[25257]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:32 LabSZ sshd[25259]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:34 LabSZ sshd[25259]: Failed password for root from 183.62.140.253 port 46401 ssh2
Dec 10 11:00:34 LabSZ sshd[25259]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:34 LabSZ sshd[25261]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:36 LabSZ sshd[25261]: Failed password for root from 183.62.140.253 port 46822 ssh2
Dec 10 11:00:36 LabSZ sshd[25261]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:36 LabSZ sshd[25263]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:38 LabSZ sshd[25263]: Failed password for root from 183.62.140.253 port 47147 ssh2
Dec 10 11:00:38 LabSZ sshd[25263]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:38 LabSZ sshd[25265]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:40 LabSZ sshd[25265]: Failed password for root from 183.62.140.253 port 47547 ssh2
Dec 10 11:00:40 LabSZ sshd[25265]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:40 LabSZ sshd[25268]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:42 LabSZ sshd[25268]: Failed password for root from 183.62.140.253 port 47936 ssh2
Dec 10 11:00:42 LabSZ sshd[25268]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:42 LabSZ sshd[25270]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:44 LabSZ sshd[25270]: Failed password for root from 183.62.140.253 port 48337 ssh2
Dec 10 11:00:44 LabSZ sshd[25270]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:44 LabSZ sshd[25272]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:46 LabSZ sshd[25272]: Failed password for root from 183.62.140.253 port 48663 ssh2
Dec 10 11:00:46 LabSZ sshd[25272]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:46 LabSZ sshd[25274]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:48 LabSZ sshd[25274]: Failed password for root from 183.62.140.253 port 49096 ssh2
Dec 10 11:00:48 LabSZ sshd[25274]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:48 LabSZ sshd[25276]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:50 LabSZ sshd[25276]: Failed password for root from 183.62.140.253 port 49468 ssh2
Dec 10 11:00:50 LabSZ sshd[25276]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:50 LabSZ sshd[25278]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:52 LabSZ sshd[25278]: Failed password for root from 183.62.140.253 port 49883 ssh2
Dec 10 11:00:52 LabSZ sshd[25278]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:52 LabSZ sshd[25281]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:54 LabSZ sshd[25281]: Failed password for root from 183.62.140.253 port 50261 ssh2
Dec 10 11:00:54 LabSZ sshd[25281]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:54 LabSZ sshd[25286]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:56 LabSZ sshd[25286]: Failed password for root from 183.62.140.253 port 50655 ssh2
Dec 10 11:00:56 LabSZ sshd[25286]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:56 LabSZ sshd[25289]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:57 LabSZ sshd[25283]: Invalid user sandeep from 88.147.143.242
Dec 10 11:00:57 LabSZ sshd[25283]: input_userauth_request: invalid user sandeep [preauth]
Dec 10 11:00:57 LabSZ sshd[25283]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:00:57 LabSZ sshd[25283]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=88.147.143.242 
Dec 10 11:00:58 LabSZ sshd[25289]: Failed password for root from 183.62.140.253 port 50953 ssh2
Dec 10 11:00:58 LabSZ sshd[25289]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:00:58 LabSZ sshd[25291]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:00:59 LabSZ sshd[25283]: Failed password for invalid user sandeep from 88.147.143.242 port 49316 ssh2
Dec 10 11:00:59 LabSZ sshd[25283]: Connection closed by 88.147.143.242 [preauth]
Dec 10 11:01:01 LabSZ sshd[25291]: Failed password for root from 183.62.140.253 port 51395 ssh2
Dec 10 11:01:01 LabSZ sshd[25291]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:01 LabSZ sshd[25294]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:02 LabSZ sshd[25294]: Failed password for root from 183.62.140.253 port 51830 ssh2
Dec 10 11:01:02 LabSZ sshd[25294]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:03 LabSZ sshd[25296]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:04 LabSZ sshd[25296]: Failed password for root from 183.62.140.253 port 52182 ssh2
Dec 10 11:01:04 LabSZ sshd[25296]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:04 LabSZ sshd[25298]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:06 LabSZ sshd[25298]: Failed password for root from 183.62.140.253 port 52545 ssh2
Dec 10 11:01:06 LabSZ sshd[25298]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:06 LabSZ sshd[25300]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:08 LabSZ sshd[25300]: Failed password for root from 183.62.140.253 port 52850 ssh2
Dec 10 11:01:08 LabSZ sshd[25300]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:08 LabSZ sshd[25302]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:10 LabSZ sshd[25302]: Failed password for root from 183.62.140.253 port 53160 ssh2
Dec 10 11:01:10 LabSZ sshd[25302]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:10 LabSZ sshd[25304]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:13 LabSZ sshd[25304]: Failed password for root from 183.62.140.253 port 53630 ssh2
Dec 10 11:01:13 LabSZ sshd[25304]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:13 LabSZ sshd[25306]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:14 LabSZ sshd[25306]: Failed password for root from 183.62.140.253 port 54049 ssh2
Dec 10 11:01:14 LabSZ sshd[25306]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:15 LabSZ sshd[25309]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:16 LabSZ sshd[25309]: Failed password for root from 183.62.140.253 port 54395 ssh2
Dec 10 11:01:16 LabSZ sshd[25309]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:16 LabSZ sshd[25311]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:18 LabSZ sshd[25311]: Failed password for root from 183.62.140.253 port 54754 ssh2
Dec 10 11:01:18 LabSZ sshd[25311]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:18 LabSZ sshd[25313]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:20 LabSZ sshd[25313]: Failed password for root from 183.62.140.253 port 55072 ssh2
Dec 10 11:01:20 LabSZ sshd[25313]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:20 LabSZ sshd[25315]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:22 LabSZ sshd[25315]: Failed password for root from 183.62.140.253 port 55368 ssh2
Dec 10 11:01:22 LabSZ sshd[25315]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:22 LabSZ sshd[25317]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:24 LabSZ sshd[25317]: Failed password for root from 183.62.140.253 port 55784 ssh2
Dec 10 11:01:24 LabSZ sshd[25317]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:24 LabSZ sshd[25320]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:26 LabSZ sshd[25320]: Failed password for root from 183.62.140.253 port 56179 ssh2
Dec 10 11:01:26 LabSZ sshd[25320]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:26 LabSZ sshd[25322]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:29 LabSZ sshd[25322]: Failed password for root from 183.62.140.253 port 56571 ssh2
Dec 10 11:01:29 LabSZ sshd[25322]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:29 LabSZ sshd[25324]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:30 LabSZ sshd[25324]: Failed password for root from 183.62.140.253 port 57038 ssh2
Dec 10 11:01:30 LabSZ sshd[25324]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:30 LabSZ sshd[25326]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:32 LabSZ sshd[25326]: Failed password for root from 183.62.140.253 port 57303 ssh2
Dec 10 11:01:32 LabSZ sshd[25326]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:32 LabSZ sshd[25328]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:35 LabSZ sshd[25328]: Failed password for root from 183.62.140.253 port 57707 ssh2
Dec 10 11:01:35 LabSZ sshd[25328]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:35 LabSZ sshd[25330]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:37 LabSZ sshd[25330]: Failed password for root from 183.62.140.253 port 58118 ssh2
Dec 10 11:01:37 LabSZ sshd[25330]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:37 LabSZ sshd[25332]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:38 LabSZ sshd[25332]: Failed password for root from 183.62.140.253 port 58491 ssh2
Dec 10 11:01:38 LabSZ sshd[25332]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:39 LabSZ sshd[25334]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:40 LabSZ sshd[25334]: Failed password for root from 183.62.140.253 port 58824 ssh2
Dec 10 11:01:40 LabSZ sshd[25334]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:40 LabSZ sshd[25336]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:42 LabSZ sshd[25336]: Failed password for root from 183.62.140.253 port 59113 ssh2
Dec 10 11:01:42 LabSZ sshd[25336]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:42 LabSZ sshd[25338]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:44 LabSZ sshd[25338]: Failed password for root from 183.62.140.253 port 59422 ssh2
Dec 10 11:01:44 LabSZ sshd[25338]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:44 LabSZ sshd[25340]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:46 LabSZ sshd[25340]: Failed password for root from 183.62.140.253 port 59850 ssh2
Dec 10 11:01:46 LabSZ sshd[25340]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:46 LabSZ sshd[25343]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:48 LabSZ sshd[25343]: Failed password for root from 183.62.140.253 port 60231 ssh2
Dec 10 11:01:48 LabSZ sshd[25343]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:48 LabSZ sshd[25346]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:50 LabSZ sshd[25346]: Failed password for root from 183.62.140.253 port 60637 ssh2
Dec 10 11:01:50 LabSZ sshd[25346]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:50 LabSZ sshd[25348]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:52 LabSZ sshd[25348]: Failed password for root from 183.62.140.253 port 60948 ssh2
Dec 10 11:01:52 LabSZ sshd[25348]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:52 LabSZ sshd[25350]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:55 LabSZ sshd[25350]: Failed password for root from 183.62.140.253 port 33150 ssh2
Dec 10 11:01:55 LabSZ sshd[25350]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:55 LabSZ sshd[25352]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:57 LabSZ sshd[25352]: Failed password for root from 183.62.140.253 port 33581 ssh2
Dec 10 11:01:57 LabSZ sshd[25352]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:57 LabSZ sshd[25355]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:01:59 LabSZ sshd[25355]: Failed password for root from 183.62.140.253 port 33946 ssh2
Dec 10 11:01:59 LabSZ sshd[25355]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:01:59 LabSZ sshd[25357]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:01 LabSZ sshd[25357]: Failed password for root from 183.62.140.253 port 34346 ssh2
Dec 10 11:02:01 LabSZ sshd[25357]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:01 LabSZ sshd[25360]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:03 LabSZ sshd[25360]: Failed password for root from 183.62.140.253 port 34695 ssh2
Dec 10 11:02:03 LabSZ sshd[25360]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:03 LabSZ sshd[25362]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:05 LabSZ sshd[25362]: Failed password for root from 183.62.140.253 port 35113 ssh2
Dec 10 11:02:05 LabSZ sshd[25362]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:05 LabSZ sshd[25364]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:07 LabSZ sshd[25364]: Failed password for root from 183.62.140.253 port 35404 ssh2
Dec 10 11:02:07 LabSZ sshd[25364]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:07 LabSZ sshd[25366]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:09 LabSZ sshd[25366]: Failed password for root from 183.62.140.253 port 35876 ssh2
Dec 10 11:02:09 LabSZ sshd[25366]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:09 LabSZ sshd[25368]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:11 LabSZ sshd[25368]: Failed password for root from 183.62.140.253 port 36235 ssh2
Dec 10 11:02:11 LabSZ sshd[25368]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:11 LabSZ sshd[25370]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:13 LabSZ sshd[25370]: Failed password for root from 183.62.140.253 port 36634 ssh2
Dec 10 11:02:13 LabSZ sshd[25370]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:13 LabSZ sshd[25372]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:15 LabSZ sshd[25372]: Failed password for root from 183.62.140.253 port 37047 ssh2
Dec 10 11:02:15 LabSZ sshd[25372]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:15 LabSZ sshd[25374]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:17 LabSZ sshd[25374]: Failed password for root from 183.62.140.253 port 37437 ssh2
Dec 10 11:02:17 LabSZ sshd[25374]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:17 LabSZ sshd[25376]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:19 LabSZ sshd[25376]: Failed password for root from 183.62.140.253 port 37760 ssh2
Dec 10 11:02:19 LabSZ sshd[25376]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:19 LabSZ sshd[25378]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:21 LabSZ sshd[25378]: Failed password for root from 183.62.140.253 port 38133 ssh2
Dec 10 11:02:21 LabSZ sshd[25378]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:21 LabSZ sshd[25380]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:23 LabSZ sshd[25380]: Failed password for root from 183.62.140.253 port 38485 ssh2
Dec 10 11:02:23 LabSZ sshd[25380]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:23 LabSZ sshd[25382]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:25 LabSZ sshd[25382]: Failed password for root from 183.62.140.253 port 38850 ssh2
Dec 10 11:02:25 LabSZ sshd[25382]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:25 LabSZ sshd[25384]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:28 LabSZ sshd[25384]: Failed password for root from 183.62.140.253 port 39278 ssh2
Dec 10 11:02:28 LabSZ sshd[25384]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:28 LabSZ sshd[25386]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:30 LabSZ sshd[25386]: Failed password for root from 183.62.140.253 port 39710 ssh2
Dec 10 11:02:30 LabSZ sshd[25386]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:30 LabSZ sshd[25388]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:32 LabSZ sshd[25388]: Failed password for root from 183.62.140.253 port 40139 ssh2
Dec 10 11:02:32 LabSZ sshd[25388]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:32 LabSZ sshd[25390]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:34 LabSZ sshd[25390]: Failed password for root from 183.62.140.253 port 40476 ssh2
Dec 10 11:02:34 LabSZ sshd[25390]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:35 LabSZ sshd[25392]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:37 LabSZ sshd[25392]: Failed password for root from 183.62.140.253 port 40931 ssh2
Dec 10 11:02:37 LabSZ sshd[25392]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:37 LabSZ sshd[25394]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:39 LabSZ sshd[25394]: Failed password for root from 183.62.140.253 port 41350 ssh2
Dec 10 11:02:39 LabSZ sshd[25394]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:39 LabSZ sshd[25397]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:42 LabSZ sshd[25397]: Failed password for root from 183.62.140.253 port 41795 ssh2
Dec 10 11:02:42 LabSZ sshd[25397]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:42 LabSZ sshd[25399]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:44 LabSZ sshd[25399]: Failed password for root from 183.62.140.253 port 42294 ssh2
Dec 10 11:02:44 LabSZ sshd[25399]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:44 LabSZ sshd[25401]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:46 LabSZ sshd[25401]: Failed password for root from 183.62.140.253 port 42636 ssh2
Dec 10 11:02:46 LabSZ sshd[25401]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:46 LabSZ sshd[25404]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:48 LabSZ sshd[25404]: Failed password for root from 183.62.140.253 port 43083 ssh2
Dec 10 11:02:48 LabSZ sshd[25404]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:48 LabSZ sshd[25407]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:50 LabSZ sshd[25407]: Failed password for root from 183.62.140.253 port 43476 ssh2
Dec 10 11:02:50 LabSZ sshd[25407]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:51 LabSZ sshd[25409]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:53 LabSZ sshd[25409]: Failed password for root from 183.62.140.253 port 43897 ssh2
Dec 10 11:02:53 LabSZ sshd[25409]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:53 LabSZ sshd[25411]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:55 LabSZ sshd[25411]: Failed password for root from 183.62.140.253 port 44412 ssh2
Dec 10 11:02:55 LabSZ sshd[25411]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:55 LabSZ sshd[25413]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:02:57 LabSZ sshd[25413]: Failed password for root from 183.62.140.253 port 44785 ssh2
Dec 10 11:02:57 LabSZ sshd[25413]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:02:58 LabSZ sshd[25415]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:00 LabSZ sshd[25415]: Failed password for root from 183.62.140.253 port 45179 ssh2
Dec 10 11:03:00 LabSZ sshd[25415]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:00 LabSZ sshd[25417]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:02 LabSZ sshd[25417]: Failed password for root from 183.62.140.253 port 45648 ssh2
Dec 10 11:03:02 LabSZ sshd[25417]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:02 LabSZ sshd[25419]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:05 LabSZ sshd[25419]: Failed password for root from 183.62.140.253 port 46059 ssh2
Dec 10 11:03:05 LabSZ sshd[25419]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:05 LabSZ sshd[25422]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:07 LabSZ sshd[25422]: Failed password for root from 183.62.140.253 port 46515 ssh2
Dec 10 11:03:07 LabSZ sshd[25422]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:07 LabSZ sshd[25424]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:09 LabSZ sshd[25424]: Failed password for root from 183.62.140.253 port 46953 ssh2
Dec 10 11:03:09 LabSZ sshd[25424]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:09 LabSZ sshd[25426]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:12 LabSZ sshd[25426]: Failed password for root from 183.62.140.253 port 47366 ssh2
Dec 10 11:03:12 LabSZ sshd[25426]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:12 LabSZ sshd[25428]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:14 LabSZ sshd[25428]: Failed password for root from 183.62.140.253 port 47852 ssh2
Dec 10 11:03:14 LabSZ sshd[25428]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:14 LabSZ sshd[25430]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:17 LabSZ sshd[25430]: Failed password for root from 183.62.140.253 port 48252 ssh2
Dec 10 11:03:17 LabSZ sshd[25430]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:17 LabSZ sshd[25432]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:19 LabSZ sshd[25432]: Failed password for root from 183.62.140.253 port 48708 ssh2
Dec 10 11:03:19 LabSZ sshd[25432]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:19 LabSZ sshd[25434]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:21 LabSZ sshd[25434]: Failed password for root from 183.62.140.253 port 49161 ssh2
Dec 10 11:03:21 LabSZ sshd[25434]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:21 LabSZ sshd[25436]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:24 LabSZ sshd[25436]: Failed password for root from 183.62.140.253 port 49567 ssh2
Dec 10 11:03:24 LabSZ sshd[25436]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:24 LabSZ sshd[25438]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:27 LabSZ sshd[25438]: Failed password for root from 183.62.140.253 port 50101 ssh2
Dec 10 11:03:27 LabSZ sshd[25438]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:27 LabSZ sshd[25440]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:29 LabSZ sshd[25440]: Failed password for root from 183.62.140.253 port 50562 ssh2
Dec 10 11:03:29 LabSZ sshd[25440]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:29 LabSZ sshd[25442]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:31 LabSZ sshd[25442]: Failed password for root from 183.62.140.253 port 51018 ssh2
Dec 10 11:03:31 LabSZ sshd[25442]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:31 LabSZ sshd[25444]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:33 LabSZ sshd[25444]: Failed password for root from 183.62.140.253 port 51443 ssh2
Dec 10 11:03:33 LabSZ sshd[25444]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:34 LabSZ sshd[25446]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:36 LabSZ sshd[25446]: Failed password for root from 183.62.140.253 port 51867 ssh2
Dec 10 11:03:36 LabSZ sshd[25446]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:36 LabSZ sshd[25450]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:37 LabSZ sshd[25448]: Invalid user admin from 103.99.0.122
Dec 10 11:03:37 LabSZ sshd[25448]: input_userauth_request: invalid user admin [preauth]
Dec 10 11:03:37 LabSZ sshd[25448]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:03:37 LabSZ sshd[25448]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:03:38 LabSZ sshd[25450]: Failed password for root from 183.62.140.253 port 52289 ssh2
Dec 10 11:03:38 LabSZ sshd[25450]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:38 LabSZ sshd[25453]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:39 LabSZ sshd[25448]: Failed password for invalid user admin from 103.99.0.122 port 60150 ssh2
Dec 10 11:03:40 LabSZ sshd[25448]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:03:41 LabSZ sshd[25453]: Failed password for root from 183.62.140.253 port 52762 ssh2
Dec 10 11:03:41 LabSZ sshd[25453]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:41 LabSZ sshd[25455]: Invalid user support from 103.99.0.122
Dec 10 11:03:41 LabSZ sshd[25455]: input_userauth_request: invalid user support [preauth]
Dec 10 11:03:41 LabSZ sshd[25455]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:03:41 LabSZ sshd[25455]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:03:43 LabSZ sshd[25455]: Failed password for invalid user support from 103.99.0.122 port 60735 ssh2
Dec 10 11:03:44 LabSZ sshd[25455]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:03:45 LabSZ sshd[25459]: Invalid user user from 103.99.0.122
Dec 10 11:03:45 LabSZ sshd[25459]: input_userauth_request: invalid user user [preauth]
Dec 10 11:03:45 LabSZ sshd[25459]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:03:45 LabSZ sshd[25459]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:03:48 LabSZ sshd[25459]: Failed password for invalid user user from 103.99.0.122 port 61269 ssh2
Dec 10 11:03:48 LabSZ sshd[25459]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:03:50 LabSZ sshd[25461]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=root
Dec 10 11:03:51 LabSZ sshd[25457]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:51 LabSZ sshd[25463]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:52 LabSZ sshd[25461]: Failed password for root from 103.99.0.122 port 61906 ssh2
Dec 10 11:03:52 LabSZ sshd[25461]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:03:53 LabSZ sshd[25457]: Failed password for root from 183.62.140.253 port 53245 ssh2
Dec 10 11:03:53 LabSZ sshd[25457]: fatal: Write failed: Connection reset by peer [preauth]
Dec 10 11:03:53 LabSZ sshd[25463]: Failed password for root from 183.62.140.253 port 55138 ssh2
Dec 10 11:03:53 LabSZ sshd[25463]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:53 LabSZ sshd[25468]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:54 LabSZ sshd[25465]: Invalid user 1234 from 103.99.0.122
Dec 10 11:03:54 LabSZ sshd[25465]: input_userauth_request: invalid user 1234 [preauth]
Dec 10 11:03:54 LabSZ sshd[25465]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:03:54 LabSZ sshd[25465]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:03:56 LabSZ sshd[25468]: Failed password for root from 183.62.140.253 port 55557 ssh2
Dec 10 11:03:56 LabSZ sshd[25468]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:56 LabSZ sshd[25470]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:56 LabSZ sshd[25465]: Failed password for invalid user 1234 from 103.99.0.122 port 62429 ssh2
Dec 10 11:03:57 LabSZ sshd[25465]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:03:58 LabSZ sshd[25470]: Failed password for root from 183.62.140.253 port 55969 ssh2
Dec 10 11:03:58 LabSZ sshd[25470]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:03:58 LabSZ sshd[25474]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:03:58 LabSZ sshd[25472]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=root
Dec 10 11:04:00 LabSZ sshd[25474]: Failed password for root from 183.62.140.253 port 56423 ssh2
Dec 10 11:04:00 LabSZ sshd[25474]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:00 LabSZ sshd[25476]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:00 LabSZ sshd[25472]: Failed password for root from 103.99.0.122 port 63012 ssh2
Dec 10 11:04:00 LabSZ sshd[25472]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:04:02 LabSZ sshd[25478]: Invalid user anonymous from 103.99.0.122
Dec 10 11:04:02 LabSZ sshd[25478]: input_userauth_request: invalid user anonymous [preauth]
Dec 10 11:04:02 LabSZ sshd[25478]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:04:02 LabSZ sshd[25478]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:04:02 LabSZ sshd[25476]: Failed password for root from 183.62.140.253 port 56779 ssh2
Dec 10 11:04:02 LabSZ sshd[25476]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:02 LabSZ sshd[25480]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:04 LabSZ sshd[25478]: Failed password for invalid user anonymous from 103.99.0.122 port 63514 ssh2
Dec 10 11:04:04 LabSZ sshd[25478]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:04:04 LabSZ sshd[25480]: Failed password for root from 183.62.140.253 port 57223 ssh2
Dec 10 11:04:04 LabSZ sshd[25480]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:05 LabSZ sshd[25482]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:06 LabSZ sshd[25482]: Failed password for root from 183.62.140.253 port 57631 ssh2
Dec 10 11:04:06 LabSZ sshd[25482]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:06 LabSZ sshd[25487]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:07 LabSZ sshd[25484]: Invalid user admin from 103.99.0.122
Dec 10 11:04:07 LabSZ sshd[25484]: input_userauth_request: invalid user admin [preauth]
Dec 10 11:04:07 LabSZ sshd[25484]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:04:07 LabSZ sshd[25484]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:04:08 LabSZ sshd[25487]: Failed password for root from 183.62.140.253 port 57916 ssh2
Dec 10 11:04:08 LabSZ sshd[25487]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:08 LabSZ sshd[25490]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:10 LabSZ sshd[25484]: Failed password for invalid user admin from 103.99.0.122 port 64031 ssh2
Dec 10 11:04:10 LabSZ sshd[25484]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:04:11 LabSZ sshd[25490]: Failed password for root from 183.62.140.253 port 58365 ssh2
Dec 10 11:04:11 LabSZ sshd[25490]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:11 LabSZ sshd[25494]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:12 LabSZ sshd[25492]: Invalid user ubnt from 103.99.0.122
Dec 10 11:04:12 LabSZ sshd[25492]: input_userauth_request: invalid user ubnt [preauth]
Dec 10 11:04:12 LabSZ sshd[25492]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:04:12 LabSZ sshd[25492]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:04:13 LabSZ sshd[25494]: Failed password for root from 183.62.140.253 port 58869 ssh2
Dec 10 11:04:13 LabSZ sshd[25494]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:13 LabSZ sshd[25497]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:14 LabSZ sshd[25492]: Failed password for invalid user ubnt from 103.99.0.122 port 64908 ssh2
Dec 10 11:04:15 LabSZ sshd[25492]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:04:16 LabSZ sshd[25497]: Failed password for root from 183.62.140.253 port 59303 ssh2
Dec 10 11:04:16 LabSZ sshd[25497]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:16 LabSZ sshd[25501]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:16 LabSZ sshd[25499]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=uucp
Dec 10 11:04:17 LabSZ sshd[25501]: Failed password for root from 183.62.140.253 port 59702 ssh2
Dec 10 11:04:17 LabSZ sshd[25501]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:18 LabSZ sshd[25503]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:18 LabSZ sshd[25499]: Failed password for uucp from 103.99.0.122 port 65454 ssh2
Dec 10 11:04:18 LabSZ sshd[25499]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:04:20 LabSZ sshd[25503]: Failed password for root from 183.62.140.253 port 60061 ssh2
Dec 10 11:04:20 LabSZ sshd[25503]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:20 LabSZ sshd[25508]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:21 LabSZ sshd[25505]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122  user=sshd
Dec 10 11:04:23 LabSZ sshd[25508]: Failed password for root from 183.62.140.253 port 60554 ssh2
Dec 10 11:04:23 LabSZ sshd[25508]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:23 LabSZ sshd[25511]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:23 LabSZ sshd[25505]: Failed password for sshd from 103.99.0.122 port 49598 ssh2
Dec 10 11:04:23 LabSZ sshd[25505]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:04:25 LabSZ sshd[25511]: Failed password for root from 183.62.140.253 port 32826 ssh2
Dec 10 11:04:25 LabSZ sshd[25511]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:25 LabSZ sshd[25516]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:25 LabSZ sshd[25513]: Invalid user admin from 103.99.0.122
Dec 10 11:04:25 LabSZ sshd[25513]: input_userauth_request: invalid user admin [preauth]
Dec 10 11:04:25 LabSZ sshd[25513]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:04:25 LabSZ sshd[25513]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:04:27 LabSZ sshd[25516]: Failed password for root from 183.62.140.253 port 33233 ssh2
Dec 10 11:04:27 LabSZ sshd[25516]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:27 LabSZ sshd[25513]: Failed password for invalid user admin from 103.99.0.122 port 50289 ssh2
Dec 10 11:04:27 LabSZ sshd[25519]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:28 LabSZ sshd[25513]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:04:30 LabSZ sshd[25519]: Failed password for root from 183.62.140.253 port 33665 ssh2
Dec 10 11:04:30 LabSZ sshd[25519]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:30 LabSZ sshd[25523]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:30 LabSZ sshd[25521]: Invalid user cisco from 103.99.0.122
Dec 10 11:04:30 LabSZ sshd[25521]: input_userauth_request: invalid user cisco [preauth]
Dec 10 11:04:30 LabSZ sshd[25521]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:04:30 LabSZ sshd[25521]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:04:32 LabSZ sshd[25523]: Failed password for root from 183.62.140.253 port 34100 ssh2
Dec 10 11:04:32 LabSZ sshd[25523]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:32 LabSZ sshd[25521]: Failed password for invalid user cisco from 103.99.0.122 port 50890 ssh2
Dec 10 11:04:32 LabSZ sshd[25525]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:33 LabSZ sshd[25521]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:04:34 LabSZ sshd[25527]: Invalid user test from 103.99.0.122
Dec 10 11:04:34 LabSZ sshd[25527]: input_userauth_request: invalid user test [preauth]
Dec 10 11:04:34 LabSZ sshd[25527]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:04:34 LabSZ sshd[25527]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:04:35 LabSZ sshd[25525]: Failed password for root from 183.62.140.253 port 34642 ssh2
Dec 10 11:04:35 LabSZ sshd[25525]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:35 LabSZ sshd[25530]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:36 LabSZ sshd[25527]: Failed password for invalid user test from 103.99.0.122 port 51592 ssh2
Dec 10 11:04:37 LabSZ sshd[25527]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:04:37 LabSZ sshd[25530]: Failed password for root from 183.62.140.253 port 35101 ssh2
Dec 10 11:04:37 LabSZ sshd[25530]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:37 LabSZ sshd[25532]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:38 LabSZ sshd[25534]: Invalid user guest from 103.99.0.122
Dec 10 11:04:38 LabSZ sshd[25534]: input_userauth_request: invalid user guest [preauth]
Dec 10 11:04:38 LabSZ sshd[25534]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:04:38 LabSZ sshd[25534]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:04:40 LabSZ sshd[25532]: Failed password for root from 183.62.140.253 port 35545 ssh2
Dec 10 11:04:40 LabSZ sshd[25532]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:40 LabSZ sshd[25534]: Failed password for invalid user guest from 103.99.0.122 port 52172 ssh2
Dec 10 11:04:40 LabSZ sshd[25537]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:41 LabSZ sshd[25534]: error: Received disconnect from 103.99.0.122: 14: No more user authentication methods available. [preauth]
Dec 10 11:04:41 LabSZ sshd[25537]: Failed password for root from 183.62.140.253 port 36027 ssh2
Dec 10 11:04:41 LabSZ sshd[25537]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:41 LabSZ sshd[25541]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:42 LabSZ sshd[25539]: Invalid user user from 103.99.0.122
Dec 10 11:04:42 LabSZ sshd[25539]: input_userauth_request: invalid user user [preauth]
Dec 10 11:04:42 LabSZ sshd[25539]: pam_unix(sshd:auth): check pass; user unknown
Dec 10 11:04:42 LabSZ sshd[25539]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=103.99.0.122 
Dec 10 11:04:43 LabSZ sshd[25541]: Failed password for root from 183.62.140.253 port 36300 ssh2
Dec 10 11:04:43 LabSZ sshd[25541]: Received disconnect from 183.62.140.253: 11: Bye Bye [preauth]
Dec 10 11:04:43 LabSZ sshd[25544]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=183.62.140.253  user=root
Dec 10 11:04:45 LabSZ sshd[25539]: Failed password for invalid user user from 103.99.0.122 port 52683 ssh2
081111 000037 17163 INFO dfs.DataNode$DataXceiver: 10.251.195.70:50010 Served block blk_-3696162841836791939 to /10.251.195.70
081111 000626 17362 INFO dfs.DataNode$DataXceiver: 10.251.91.84:50010 Served block blk_-8915491531889006304 to /10.251.91.84
081111 000932 16928 INFO dfs.DataNode$DataXceiver: 10.251.90.64:50010 Served block blk_-657087263710195616 to /10.250.14.196
081111 002741 17343 INFO dfs.DataNode$DataXceiver: 10.250.10.213:50010 Served block blk_-1125621902344947014 to /10.250.10.213
081111 004102 17259 WARN dfs.DataNode$DataXceiver: 10.251.214.175:50010:Got exception while serving blk_481857539063371482 to /10.251.105.189:
081111 004657 17337 INFO dfs.DataNode$DataXceiver: 10.250.10.6:50010 Served block blk_1200136320542454615 to /10.250.10.6
081111 011254 17716 WARN dfs.DataNode$DataXceiver: 10.251.39.144:50010:Got exception while serving blk_-8083036675630459841 to /10.251.39.209:
081111 012254 17517 WARN dfs.DataNode$DataXceiver: 10.250.7.32:50010:Got exception while serving blk_-1508527605812345693 to /10.251.74.192:
081111 013241 17501 INFO dfs.DataNode$DataXceiver: 10.251.31.180:50010 Served block blk_3905759687686730625 to /10.251.31.180
081111 013322 17524 INFO dfs.DataNode$DataXceiver: 10.251.121.224:50010 Served block blk_342378162324355732 to /10.251.121.224
081111 013400 17543 INFO dfs.DataNode$DataXceiver: 10.250.14.38:50010 Served block blk_-8674089929114017279 to /10.250.14.38
081111 014431 17416 WARN dfs.DataNode$DataXceiver: 10.251.107.98:50010:Got exception while serving blk_-3140031507252212554 to /10.250.7.244:
081111 020752 17598 INFO dfs.DataNode$DataXceiver: 10.251.125.237:50010 Served block blk_-309134958179110212 to /10.251.125.237
081111 022143 17663 INFO dfs.DataNode$DataXceiver: 10.250.13.240:50010 Served block blk_-3134225108208373949 to /10.250.13.240
081111 023010 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8887373200836701910 is added to invalidSet of 10.251.193.224:50010
081111 023011 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1162676743264290624 is added to invalidSet of 10.251.194.213:50010
081111 023011 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5513640508500780385 is added to invalidSet of 10.251.203.179:50010
081111 023011 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7767902948513849386 is added to invalidSet of 10.251.71.146:50010
081111 023013 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5382946262718129284 is added to invalidSet of 10.250.5.161:50010
081111 023013 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5668137923110942035 is added to invalidSet of 10.251.91.229:50010
081111 023013 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7197514030988119866 is added to invalidSet of 10.251.75.49:50010
081111 023015 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5362598472846558076 is added to invalidSet of 10.250.14.143:50010
081111 023017 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-4269939382058952254 is added to invalidSet of 10.251.203.129:50010
081111 023017 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6978845808078082463 is added to invalidSet of 10.251.109.209:50010
081111 023022 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1071115021179995772 is added to invalidSet of 10.250.11.53:50010
081111 023022 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7336636250384047238 is added to invalidSet of 10.251.26.177:50010
081111 023022 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8493856762887493521 is added to invalidSet of 10.251.26.131:50010
081111 023024 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1043487525777829194 is added to invalidSet of 10.251.110.8:50010
081111 023024 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3225035678192156905 is added to invalidSet of 10.251.38.214:50010
081111 023025 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5572208681898881669 is added to invalidSet of 10.251.38.197:50010
081111 023027 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-2225353020301959077 is added to invalidSet of 10.251.125.237:50010
081111 023027 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3946183108430554053 is added to invalidSet of 10.250.11.100:50010
081111 023033 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3681974396824196300 is added to invalidSet of 10.250.17.177:50010
081111 023037 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2736251498135196075 is added to invalidSet of 10.251.90.64:50010
081111 023039 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1275198635723397883 is added to invalidSet of 10.251.202.134:50010
081111 023039 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3428998332517532825 is added to invalidSet of 10.251.43.192:50010
081111 023139 19 INFO dfs.FSDataset: Deleting block blk_-7106479503467535906 file /mnt/hadoop/dfs/data/current/subdir32/blk_-7106479503467535906
081111 023149 19 INFO dfs.FSDataset: Deleting block blk_-7411858598798393933 file /mnt/hadoop/dfs/data/current/subdir24/blk_-7411858598798393933
081111 023156 19 INFO dfs.FSDataset: Deleting block blk_-5429479049793046826 file /mnt/hadoop/dfs/data/current/subdir34/blk_-5429479049793046826
081111 023158 19 INFO dfs.FSDataset: Deleting block blk_-6431101765137189231 file /mnt/hadoop/dfs/data/current/subdir33/blk_-6431101765137189231
081111 023216 19 INFO dfs.FSDataset: Deleting block blk_-2923662094689783995 file /mnt/hadoop/dfs/data/current/subdir38/blk_-2923662094689783995
081111 023222 17683 INFO dfs.DataNode$DataXceiver: 10.250.14.143:50010 Served block blk_-664656559337730574 to /10.250.7.96
081111 023222 17788 INFO dfs.DataNode$PacketResponder: Received block blk_3224393364314749254 of size 67108864 from /10.251.37.240
081111 023229 19 INFO dfs.FSDataset: Deleting block blk_1483582953997932733 file /mnt/hadoop/dfs/data/current/subdir7/blk_1483582953997932733
081111 023230 19 INFO dfs.FSDataset: Deleting block blk_-3607708283707030582 file /mnt/hadoop/dfs/data/current/subdir31/blk_-3607708283707030582
081111 023234 19 INFO dfs.FSDataset: Deleting block blk_4365203784873840210 file /mnt/hadoop/dfs/data/current/subdir1/blk_4365203784873840210
081111 023243 19 INFO dfs.FSDataset: Deleting block blk_1920931690498309324 file /mnt/hadoop/dfs/data/current/subdir42/blk_1920931690498309324
081111 023243 19 INFO dfs.FSDataset: Deleting block blk_2731746367139956284 file /mnt/hadoop/dfs/data/current/subdir15/blk_2731746367139956284
081111 023247 19 INFO dfs.FSDataset: Deleting block blk_3312198496468502316 file /mnt/hadoop/dfs/data/current/subdir36/blk_3312198496468502316
081111 023247 19 INFO dfs.FSDataset: Deleting block blk_-626413149556394155 file /mnt/hadoop/dfs/data/current/subdir26/blk_-626413149556394155
081111 023248 18 INFO dfs.FSDataset: Deleting block blk_1350741653957819140 file /mnt/hadoop/dfs/data/current/subdir32/blk_1350741653957819140
081111 023248 19 INFO dfs.FSDataset: Deleting block blk_6675564098604634452 file /mnt/hadoop/dfs/data/current/subdir1/blk_6675564098604634452
081111 023249 19 INFO dfs.FSDataset: Deleting block blk_1692958087244489888 file /mnt/hadoop/dfs/data/current/subdir62/blk_1692958087244489888
081111 023250 18 INFO dfs.FSDataset: Deleting block blk_5528471097481810388 file /mnt/hadoop/dfs/data/current/subdir5/blk_5528471097481810388
081111 023250 19 INFO dfs.FSDataset: Deleting block blk_797263375273454863 file /mnt/hadoop/dfs/data/current/subdir16/blk_797263375273454863
081111 023255 19 INFO dfs.FSDataset: Deleting block blk_5209577451013188921 file /mnt/hadoop/dfs/data/current/subdir24/blk_5209577451013188921
081111 023304 18 INFO dfs.FSDataset: Deleting block blk_4713539908695785630 file /mnt/hadoop/dfs/data/current/subdir39/blk_4713539908695785630
081111 023310 19 INFO dfs.FSDataset: Deleting block blk_6148201552996212693 file /mnt/hadoop/dfs/data/current/subdir15/blk_6148201552996212693
081111 023328 19 INFO dfs.FSDataset: Deleting block blk_9014154925388243050 file /mnt/hadoop/dfs/data/current/subdir28/blk_9014154925388243050
081111 023330 19 INFO dfs.FSDataset: Deleting block blk_8761138012128091849 file /mnt/hadoop/dfs/data/current/subdir33/blk_8761138012128091849
081111 023749 17857 INFO dfs.DataNode$PacketResponder: Received block blk_-4071286774767564790 of size 67108864 from /10.251.197.226
081111 023914 17773 INFO dfs.DataNode$DataXceiver: 10.251.122.65:50010 Served block blk_2866275036574950116 to /10.250.11.53
081111 024105 17911 INFO dfs.DataNode$PacketResponder: Received block blk_-6811519872198278365 of size 67108864 from /10.250.6.4
081111 024133 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.199.19:50010 is added to blk_6748710057489679257 size 67108864
081111 024247 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.209:50010 is added to blk_1833093108290599780 size 67108864
081111 024304 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.146:50010 is added to blk_9163807899833139121 size 67108864
081111 024309 17772 INFO dfs.DataNode$DataXceiver: Receiving block blk_3941798034503185737 src: /10.250.11.85:42358 dest: /10.250.11.85:50010
081111 024325 17904 INFO dfs.DataNode$PacketResponder: Received block blk_6894918622505820383 of size 67108864 from /10.250.14.143
081111 024443 17867 INFO dfs.DataNode$DataXceiver: Receiving block blk_6491424745418112176 src: /10.251.214.130:57008 dest: /10.251.214.130:50010
081111 024443 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_200811101024_0009_m_000115_0/part-00115. blk_-17685631368401548
081111 024724 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_200811101024_0009_m_000176_0/part-00176. blk_8924207394547472950
081111 024742 17976 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4526931594457272384 src: /10.251.43.147:40348 dest: /10.251.43.147:50010
081111 024831 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.31.242:50010 is added to blk_5390763359901143305 size 67108864
081111 024901 18086 INFO dfs.DataNode$DataXceiver: Receiving block blk_-11867600358649678 src: /10.251.75.79:46421 dest: /10.251.75.79:50010
081111 024901 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.194:50010 is added to blk_2430417102497887445 size 67108864
081111 024901 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.18:50010 is added to blk_-747046374856916711 size 67108864
081111 025101 18187 INFO dfs.DataNode$PacketResponder: Received block blk_5062112794659861146 of size 67108864 from /10.251.91.84
081111 025208 18218 INFO dfs.DataNode$DataXceiver: Receiving block blk_-116589515245909549 src: /10.251.203.179:33198 dest: /10.251.203.179:50010
081111 025335 17997 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_4628975089896927394 terminating
081111 025336 18090 INFO dfs.DataNode$DataXceiver: Receiving block blk_-764747190320632607 src: /10.251.127.191:49914 dest: /10.251.127.191:50010
081111 025346 18124 INFO dfs.DataNode$PacketResponder: Received block blk_-8015797075596093783 of size 67108864 from /10.250.11.194
081111 025424 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.110.160:50010 is added to blk_-1675768179383804109 size 67108864
081111 025607 18122 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_1155029843511109085 terminating
081111 025619 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.67.113:50010 is added to blk_9060964400089016309 size 67108864
081111 025634 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.67:50010 is added to blk_-1417908110808566576 size 67108864
081111 025653 18457 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-7483426835701512490 terminating
081111 025732 18087 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3752135158991310104 src: /10.251.70.5:44375 dest: /10.251.70.5:50010
081111 025811 18303 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8649503431827393598 src: /10.251.199.19:48778 dest: /10.251.199.19:50010
081111 025903 18116 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6883339133893273874 terminating
081111 025915 18170 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_3695169334833427187 terminating
081111 025945 18127 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_6055109613223910610 terminating
081111 030015 18180 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1868325377308234190 src: /10.251.42.207:50666 dest: /10.251.42.207:50010
081111 030017 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.26.177:50010 is added to blk_-6338549285508312302 size 3530010
081111 030051 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.30.179:50010 is added to blk_-900758580041645081 size 3543900
081111 030153 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.3:50010 is added to blk_1756478622414930264 size 67108864
081111 030225 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.127.47:50010 is added to blk_-6555060221133060529 size 67108864
081111 030233 18516 INFO dfs.DataNode$DataXceiver: Receiving block blk_9076677650086747187 src: /10.251.35.1:52094 dest: /10.251.35.1:50010
081111 030236 18183 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-4165562468976768862 terminating
081111 030240 18460 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-3295982945091621137 terminating
081111 030300 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.15.240:50010 is added to blk_-7637577152926063076 size 67108864
081111 030318 18190 INFO dfs.DataNode$DataXceiver: Receiving block blk_3181161977343808286 src: /10.251.43.21:40127 dest: /10.251.43.21:50010
081111 030414 18143 INFO dfs.DataNode$DataXceiver: Receiving block blk_6807535563873595117 src: /10.250.17.177:48934 dest: /10.250.17.177:50010
081111 030416 18548 INFO dfs.DataNode$DataXceiver: Receiving block blk_7513976340045981966 src: /10.251.42.207:36468 dest: /10.251.42.207:50010
081111 030418 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_200811101024_0009_m_001061_0/part-01061. blk_-749176535135404637
081111 030501 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.199.245:50010 is added to blk_-7909610753867924116 size 67108864
081111 030726 18409 INFO dfs.DataNode$PacketResponder: Received block blk_-7247291679155656679 of size 67108864 from /10.250.10.176
081111 030736 18126 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-6369730481066968769 terminating
081111 030745 18290 INFO dfs.DataNode$DataXceiver: Receiving block blk_7780461223860192433 src: /10.251.127.191:58148 dest: /10.251.127.191:50010
081111 030942 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_200811101024_0009_m_001281_0/part-01281. blk_2084123600915946046
081111 030946 18336 INFO dfs.DataNode$DataXceiver: Receiving block blk_92273823183917102 src: /10.250.15.67:53441 dest: /10.250.15.67:50010
081111 030950 18479 INFO dfs.DataNode$PacketResponder: Received block blk_-1141332313413855099 of size 67108864 from /10.251.89.155
081111 030958 18454 INFO dfs.DataNode$DataXceiver: Receiving block blk_3016571528120363023 src: /10.251.194.102:60235 dest: /10.251.194.102:50010
081111 031106 18346 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7678242108759603056 src: /10.251.125.193:33553 dest: /10.251.125.193:50010
081111 031143 18339 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7526023914950151999 src: /10.250.5.161:48457 dest: /10.250.5.161:50010
081111 031257 18567 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6380328072112550668 src: /10.250.6.191:37701 dest: /10.250.6.191:50010
081111 031258 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.100:50010 is added to blk_3222673463853349950 size 67108864
081111 031307 18432 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-3183355402229300197 terminating
081111 031334 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.26.8:50010 is added to blk_-5963475042722370536 size 67108864
081111 031436 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_200811101024_0009_m_001398_0/part-01398. blk_-7527506469734664572
081111 031541 18484 INFO dfs.DataNode$PacketResponder: Received block blk_9072486569292195232 of size 67108864 from /10.251.71.68
081111 031621 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.199.159:50010 is added to blk_-4455205822455130625 size 67108864
081111 031650 18225 INFO dfs.DataNode$PacketResponder: Received block blk_3777399812922072863 of size 67108864 from /10.251.199.225
081111 031730 18310 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-4633546693082073737 terminating
081111 031735 18435 INFO dfs.DataNode$PacketResponder: Received block blk_6799391641508508235 of size 67108864 from /10.251.107.98
081111 031811 18619 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_1002945451269828722 terminating
081111 031900 18547 INFO dfs.DataNode$PacketResponder: Received block blk_566413670355737377 of size 67108864 from /10.251.122.79
081111 031938 16978 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_2819581679409796253 terminating
081111 031951 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.127.243:50010 is added to blk_1512923509505250283 size 67108864
081111 032053 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_200811101024_0009_m_001611_0/part-01611. blk_285336326661214154
081111 032121 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.90.134:50010 is added to blk_5955377624764578810 size 67108864
081111 032357 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_200811101024_0009_m_001574_0/part-01574. blk_-7458279618877678162
081111 032416 18576 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_5018765878120211503 terminating
081111 032531 18607 INFO dfs.DataNode$DataXceiver: Receiving block blk_313888672758374538 src: /10.251.39.179:40031 dest: /10.251.39.179:50010
081111 032544 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.109.209:50010 is added to blk_3764801892187716497 size 67108864
081111 032612 18586 INFO dfs.DataNode$PacketResponder: Received block blk_-3491385689517593364 of size 67108864 from /10.251.193.224
081111 032655 18710 INFO dfs.DataNode$PacketResponder: Received block blk_-2243784245784480521 of size 67108864 from /10.250.6.191
081111 032813 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.6.223:50010 is added to blk_-4052810234606327693 size 67108864
081111 032825 18758 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6695620358347030339 terminating
081111 032828 18581 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-5102453487463005287 terminating
081111 032900 18814 INFO dfs.DataNode$DataXceiver: Receiving block blk_7924393760966842810 src: /10.251.202.209:59148 dest: /10.251.202.209:50010
081111 032917 18710 INFO dfs.DataNode$PacketResponder: Received block blk_5915244038969911075 of size 67108864 from /10.250.11.53
081111 033110 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.144:50010 is added to blk_3243316191092266549 size 67108864
081111 033125 18859 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_8229768485681546149 terminating
081111 033211 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.21:50010 is added to blk_3206064173813164493 size 67108864
081111 033426 18894 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1957299792594627113 terminating
081111 033444 18940 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1690784932539647416 src: /10.251.31.180:50205 dest: /10.251.31.180:50010
081111 033505 18847 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_6745520736924802462 terminating
081111 033819 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1525964299678787959 is added to invalidSet of 10.251.37.240:50010
081111 033823 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7045475543362904558 is added to invalidSet of 10.251.106.214:50010
081111 033824 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6802124772693111025 is added to invalidSet of 10.251.66.63:50010
081111 033829 18 INFO dfs.FSDataset: Deleting block blk_-5205950257901523262 file /mnt/hadoop/dfs/data/current/subdir40/blk_-5205950257901523262
081111 033925 18906 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1649239999940418046 terminating
081111 033927 19172 INFO dfs.DataNode$PacketResponder: Received block blk_6880728743505526406 of size 67108864 from /10.251.197.226
081111 033935 18945 INFO dfs.DataNode$DataXceiver: Receiving block blk_5256716488526744434 src: /10.251.73.220:42064 dest: /10.251.73.220:50010
081111 033953 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.26.177:50010 is added to blk_6928449948805597176 size 67108864
081111 034105 18847 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_2529569087635823495 terminating
081111 034252 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt4/_temporary/_task_200811101024_0010_m_000222_0/part-00222. blk_3764836840384130740
081111 034323 19025 INFO dfs.DataNode$DataXceiver: Receiving block blk_3020717811556957482 src: /10.251.90.81:48077 dest: /10.251.90.81:50010
081111 034351 18690 INFO dfs.DataNode$PacketResponder: Received block blk_-577659324269246113 of size 67108864 from /10.251.203.179
081111 034456 18909 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_6137960563260046065 terminating
081111 034600 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.246:50010 is added to blk_-6909400912794057067 size 67108864
081111 034712 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.192:50010 is added to blk_8249475180346179102 size 67108864
081111 034726 19054 INFO dfs.DataNode$PacketResponder: Received block blk_-5045144756510626150 of size 67108864 from /10.251.39.160
081111 034742 19142 INFO dfs.DataNode$PacketResponder: Received block blk_752365829198406123 of size 67108864 from /10.250.6.214
081111 034937 19166 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_5663417083925711869 terminating
081111 034945 19257 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-2014243777892518956 terminating
081111 035010 19149 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8123824094195108685 src: /10.251.194.213:54308 dest: /10.251.194.213:50010
081111 035113 19229 INFO dfs.DataNode$DataXceiver: Receiving block blk_2080318574920472813 src: /10.251.39.64:40530 dest: /10.251.39.64:50010
081111 035115 18830 INFO dfs.DataNode$PacketResponder: Received block blk_-290874442865281380 of size 28497700 from /10.251.203.179
081111 035152 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_5953577136151423920 size 67108864
081111 035247 19144 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-2780312306380730204 terminating
081111 035417 19387 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-633826175832239328 terminating
081111 035532 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.15.101:50010 is added to blk_-6540455811923725840 size 67108864
081111 035620 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.123.1:50010 is added to blk_6308888821787547357 size 67108864
081111 035834 19205 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_4638603954320058978 terminating
081111 035913 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.91.229:50010 is added to blk_-4651707530079874243 size 67108864
081111 035915 19117 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_5115892145206906855 terminating
081111 035948 19511 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-2421414377433740760 terminating
081111 035957 19217 INFO dfs.DataNode$PacketResponder: Received block blk_-420564586555502239 of size 67108864 from /10.251.90.64
081111 040107 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt4/_temporary/_task_200811101024_0010_m_000881_0/part-00881. blk_-3091906986951252347
081111 040246 19412 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-4068751061953213467 terminating
081111 040251 19350 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-3025440686907267472 terminating
081111 040402 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.215.70:50010 is added to blk_-420818767915591010 size 67108864
081111 040405 19541 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8422946850755720617 terminating
081111 040425 19408 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5646228903161999640 src: /10.251.71.97:60870 dest: /10.251.71.97:50010
081111 040521 19253 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-7024895786822269814 terminating
081111 040537 19518 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-4430291708660291878 terminating
081111 040543 19431 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6250368566161066085 src: /10.251.31.180:42992 dest: /10.251.31.180:50010
081111 040544 19362 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6213380972217661202 src: /10.251.203.149:40478 dest: /10.251.203.149:50010
081111 040604 19555 INFO dfs.DataNode$PacketResponder: Received block blk_3813891701916249413 of size 67108864 from /10.251.121.224
081111 040626 19526 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_903961454948542202 terminating
081111 040653 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.42.16:50010 is added to blk_3954386352306480748 size 67108864
081111 040658 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.105.189:50010 is added to blk_4812792845864526586 size 67108864
081111 040728 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.70.211:50010 is added to blk_5570475465835003801 size 67108864
081111 040751 19585 INFO dfs.DataNode$PacketResponder: Received block blk_-9190864897806028230 of size 67108864 from /10.251.214.130
081111 040754 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.31.242:50010 is added to blk_-4342304152206614489 size 67108864
081111 040857 19185 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-190773840747023361 terminating
081111 041004 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.240:50010 is added to blk_-5580048401594934039 size 67108864
081111 041039 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.242:50010 is added to blk_6640583412927022083 size 67108864
081111 041111 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.65.203:50010 is added to blk_-1332315107905285608 size 67108864
081111 041218 19235 INFO dfs.DataNode$PacketResponder: Received block blk_3550669278239526374 of size 67108864 from /10.251.123.1
081111 041219 19820 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_4873550540813847659 terminating
081111 041252 19248 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1133573997287815203 src: /10.251.123.195:59430 dest: /10.251.123.195:50010
081111 041349 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.74.134:50010 is added to blk_2740792916077430444 size 28482383
081111 041425 19630 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2851869865185325732 terminating
081111 041443 19498 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1842096107468319932 terminating
081111 041509 20066 INFO dfs.DataNode$PacketResponder: Received block blk_-8965508153794244104 of size 28494398 from /10.251.70.112
081111 041532 19592 INFO dfs.DataNode$DataXceiver: Receiving block blk_1074754942323612586 src: /10.251.202.181:45279 dest: /10.251.202.181:50010
081111 041534 19842 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7727864185516010230 src: /10.251.43.192:36039 dest: /10.251.43.192:50010
081111 041701 19403 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-4779205255695723216 terminating
081111 041734 19799 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2261089810991980611 terminating
081111 041811 19543 INFO dfs.DataNode$DataXceiver: Receiving block blk_5335498635665743905 src: /10.251.73.220:50095 dest: /10.251.73.220:50010
081111 041926 19700 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5144503768267321004 src: /10.251.107.19:36862 dest: /10.251.107.19:50010
081111 042009 19800 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_1927336911952959118 terminating
081111 042024 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.166:50010 is added to blk_-2332150303475653460 size 67108864
081111 042042 19757 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_2111886617384066018 terminating
081111 042100 19804 INFO dfs.DataNode$DataXceiver: Receiving block blk_-2506231977021079951 src: /10.251.31.160:57634 dest: /10.251.31.160:50010
081111 042129 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.199.86:50010 is added to blk_4330492872922862223 size 67108864
081111 042353 19811 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_6918427074195439966 terminating
081111 042416 19265 INFO dfs.DataNode$DataXceiver: Receiving block blk_4284688617983009262 src: /10.251.91.32:57548 dest: /10.251.91.32:50010
081111 042427 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.31.5:50010 is added to blk_5813361416484537967 size 67108864
081111 042456 19952 INFO dfs.DataNode$DataXceiver: Receiving block blk_-2313277713021896570 src: /10.251.123.195:58823 dest: /10.251.123.195:50010
081111 042457 19726 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-7778922135767840250 terminating
081111 042625 19958 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7350482598011027441 src: /10.250.7.96:41476 dest: /10.250.7.96:50010
081111 042735 19901 INFO dfs.DataNode$PacketResponder: Received block blk_-3607860197409267245 of size 28503188 from /10.251.31.180
081111 042824 18264 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_4178945095089206682 terminating
081111 043015 19875 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-3363550789841756902 terminating
081111 043042 20058 INFO dfs.DataNode$DataXceiver: Receiving block blk_345577139488306634 src: /10.251.37.240:38873 dest: /10.251.37.240:50010
081111 043106 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.193:50010 is added to blk_4577632468651859430 size 67108864
081111 043132 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt4/_temporary/_task_200811101024_0010_m_001716_0/part-01716. blk_-2051219911597308805
081111 043140 19599 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-2680500627064966252 terminating
081111 043222 20077 INFO dfs.DataNode$DataXceiver: Receiving block blk_-9092685890953655332 src: /10.250.15.240:43274 dest: /10.250.15.240:50010
081111 043341 19898 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4473020641607592551 src: /10.251.42.191:48114 dest: /10.251.42.191:50010
081111 043358 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.37:50010 is added to blk_-524128780228307052 size 67108864
081111 043515 20251 INFO dfs.DataNode$PacketResponder: Received block blk_4443043493758564822 of size 67108864 from /10.251.106.37
081111 043630 20253 INFO dfs.DataNode$PacketResponder: Received block blk_5118390778403008591 of size 67108864 from /10.251.30.101
081111 043746 20226 INFO dfs.DataNode$DataXceiver: Receiving block blk_2737505352081488746 src: /10.251.194.245:52516 dest: /10.251.194.245:50010
081111 043825 20130 INFO dfs.DataNode$PacketResponder: Received block blk_-9013487557842539661 of size 28485342 from /10.251.215.192
081111 043903 20003 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-8638442557570632582 terminating
081111 044041 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.202.209:50010 is added to blk_111759050707136656 size 67108864
081111 044304 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2259206517962925438 is added to invalidSet of 10.251.215.16:50010
081111 044304 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3854777877236596649 is added to invalidSet of 10.251.39.160:50010
081111 044305 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-4265150363959543078 is added to invalidSet of 10.251.39.160:50010
081111 044309 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3729730843466061623 is added to invalidSet of 10.251.27.63:50010
081111 044310 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3876184938151091897 is added to invalidSet of 10.251.30.85:50010
081111 044311 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1227720343537039835 is added to invalidSet of 10.251.202.134:50010
081111 044314 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6921441628286995043 is added to invalidSet of 10.251.126.83:50010
081111 044315 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3539852484771802299 is added to invalidSet of 10.250.19.227:50010
081111 044316 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2341651676737321162 is added to invalidSet of 10.251.203.129:50010
081111 044318 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-765820154946060546 is added to invalidSet of 10.251.65.203:50010
081111 044319 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-2492400942012223395 is added to invalidSet of 10.251.202.209:50010
081111 044320 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8351015090014570293 is added to invalidSet of 10.251.199.86:50010
081111 044328 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-328970571549408286 is added to invalidSet of 10.251.214.32:50010
081111 044328 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8130925109202184099 is added to invalidSet of 10.251.125.193:50010
081111 044331 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3242342105286997235 is added to invalidSet of 10.251.71.97:50010
081111 044332 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2468407162668753567 is added to invalidSet of 10.251.106.37:50010
081111 044335 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_7841365850685201915 is added to invalidSet of 10.251.110.68:50010
081111 044337 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-6160667975382232149
081111 044338 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2426255659473300323 is added to invalidSet of 10.251.106.37:50010
081111 044338 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5203504995927133832 is added to invalidSet of 10.250.13.240:50010
081111 044340 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_815809455641029563 is added to invalidSet of 10.251.111.37:50010
081111 044343 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_6413710007211667486 is added to invalidSet of 10.251.30.179:50010
081111 044345 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-2907776069874917892 is added to invalidSet of 10.251.215.16:50010
081111 044347 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5802915611610989626 is added to invalidSet of 10.251.110.196:50010
081111 044349 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7668075074035173978 is added to invalidSet of 10.251.31.5:50010
081111 044351 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1573544581830458089 is added to invalidSet of 10.251.122.65:50010
081111 044353 19 INFO dfs.FSDataset: Deleting block blk_-1180596628202464893 file /mnt/hadoop/dfs/data/current/subdir52/blk_-1180596628202464893
081111 044427 20289 INFO dfs.DataNode$DataXceiver: Receiving block blk_1407935810673971357 src: /10.251.123.99:54184 dest: /10.251.123.99:50010
081111 044522 18 INFO dfs.FSDataset: Deleting block blk_-8167717961644709843 file /mnt/hadoop/dfs/data/current/blk_-8167717961644709843
081111 044543 19 INFO dfs.FSDataset: Deleting block blk_-7464161062429228045 file /mnt/hadoop/dfs/data/current/subdir19/blk_-7464161062429228045
081111 044546 19 INFO dfs.FSDataset: Deleting block blk_-5934436507161953485 file /mnt/hadoop/dfs/data/current/subdir33/blk_-5934436507161953485
081111 044546 20347 INFO dfs.DataNode$PacketResponder: Received block blk_-3922743468150895472 of size 67108864 from /10.251.39.179
081111 044550 19 INFO dfs.FSDataset: Deleting block blk_-7832937837605180330 file /mnt/hadoop/dfs/data/current/subdir49/blk_-7832937837605180330
081111 044554 19 INFO dfs.FSDataset: Deleting block blk_-8547071368131084410 file /mnt/hadoop/dfs/data/current/subdir38/blk_-8547071368131084410
081111 044556 19 INFO dfs.FSDataset: Deleting block blk_-5429042599096020592 file /mnt/hadoop/dfs/data/current/subdir34/blk_-5429042599096020592
081111 044612 19 INFO dfs.FSDataset: Deleting block blk_-6350492516478706425 file /mnt/hadoop/dfs/data/current/subdir36/blk_-6350492516478706425
081111 044624 19 INFO dfs.FSDataset: Deleting block blk_-6053005391233809507 file /mnt/hadoop/dfs/data/current/subdir27/blk_-6053005391233809507
081111 044624 20167 INFO dfs.DataNode$PacketResponder: Received block blk_-1522998827901549925 of size 67108864 from /10.251.127.47
081111 044630 19 INFO dfs.FSDataset: Deleting block blk_-4311652947826112510 file /mnt/hadoop/dfs/data/current/subdir57/blk_-4311652947826112510
081111 044635 19 INFO dfs.FSDataset: Deleting block blk_-1754204887541031211 file /mnt/hadoop/dfs/data/current/subdir58/blk_-1754204887541031211
081111 044639 19 INFO dfs.FSDataset: Deleting block blk_-4165176068096983220 file /mnt/hadoop/dfs/data/current/subdir34/blk_-4165176068096983220
081111 044648 19 INFO dfs.FSDataset: Deleting block blk_4863571548336254624 file /mnt/hadoop/dfs/data/current/subdir5/blk_4863571548336254624
081111 044650 19 INFO dfs.FSDataset: Deleting block blk_-4844505495441326897 file /mnt/hadoop/dfs/data/current/subdir36/blk_-4844505495441326897
081111 044652 19 INFO dfs.FSDataset: Deleting block blk_-7513928662244669476 file /mnt/hadoop/dfs/data/current/subdir11/blk_-7513928662244669476
081111 044654 19 INFO dfs.FSDataset: Deleting block blk_-5014786445735476734 file /mnt/hadoop/dfs/data/current/subdir9/blk_-5014786445735476734
081111 044700 19 INFO dfs.FSDataset: Deleting block blk_-1046472716157313227 file /mnt/hadoop/dfs/data/current/subdir62/blk_-1046472716157313227
081111 044700 19 INFO dfs.FSDataset: Deleting block blk_-1281276222788579863 file /mnt/hadoop/dfs/data/current/subdir30/blk_-1281276222788579863
081111 044704 19 INFO dfs.FSDataset: Deleting block blk_449049393237281194 file /mnt/hadoop/dfs/data/current/subdir18/blk_449049393237281194
081111 044716 19 INFO dfs.FSDataset: Deleting block blk_352185875883785141 file /mnt/hadoop/dfs/data/current/subdir44/blk_352185875883785141
081111 044720 19 INFO dfs.FSDataset: Deleting block blk_-2890295200276774269 file /mnt/hadoop/dfs/data/current/subdir29/blk_-2890295200276774269
081111 044728 18 INFO dfs.FSDataset: Deleting block blk_1379455396847512711 file /mnt/hadoop/dfs/data/current/blk_1379455396847512711
081111 044740 19 INFO dfs.FSDataset: Deleting block blk_2180875151184358991 file /mnt/hadoop/dfs/data/current/subdir31/blk_2180875151184358991
081111 044749 19766 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8879979113392880065 src: /10.251.202.181:33345 dest: /10.251.202.181:50010
081111 044752 19 INFO dfs.FSDataset: Deleting block blk_1553688227908800900 file /mnt/hadoop/dfs/data/current/subdir15/blk_1553688227908800900
081111 044757 19 INFO dfs.FSDataset: Deleting block blk_6138894560185644209 file /mnt/hadoop/dfs/data/current/subdir42/blk_6138894560185644209
081111 044759 19 INFO dfs.FSDataset: Deleting block blk_-1368324087589035770 file /mnt/hadoop/dfs/data/current/blk_-1368324087589035770
081111 044759 19 INFO dfs.FSDataset: Deleting block blk_3274968259628929519 file /mnt/hadoop/dfs/data/current/subdir19/blk_3274968259628929519
081111 044806 19 INFO dfs.FSDataset: Deleting block blk_5736622945587812309 file /mnt/hadoop/dfs/data/current/subdir19/blk_5736622945587812309
081111 044812 19 INFO dfs.FSDataset: Deleting block blk_9142114171015520823 file /mnt/hadoop/dfs/data/current/subdir48/blk_9142114171015520823
081111 044822 19 INFO dfs.FSDataset: Deleting block blk_8556592730481313831 file /mnt/hadoop/dfs/data/current/blk_8556592730481313831
081111 044834 19 INFO dfs.FSDataset: Deleting block blk_6147898566868510104 file /mnt/hadoop/dfs/data/current/subdir36/blk_6147898566868510104
081111 044847 20391 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4334868513923498900 src: /10.251.91.15:35440 dest: /10.251.91.15:50010
081111 044855 19 INFO dfs.FSDataset: Deleting block blk_9081834782322008396 file /mnt/hadoop/dfs/data/current/subdir40/blk_9081834782322008396
081111 044913 18 INFO dfs.FSDataset: Deleting block blk_8962411947457130719 file /mnt/hadoop/dfs/data/current/subdir16/blk_8962411947457130719
081111 045125 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.75.143:50010 is added to blk_2312679955495839894 size 67108864
081111 045125 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.75.143:50010 is added to blk_7434237665308221519 size 67108864
081111 045222 20321 INFO dfs.DataNode$DataXceiver: Receiving block blk_2380775154966815858 src: /10.251.107.50:33333 dest: /10.251.107.50:50010
081111 045224 20229 INFO dfs.DataNode$PacketResponder: Received block blk_3777389444004341563 of size 67108864 from /10.251.214.175
081111 045252 20089 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-4578850505370039622 terminating
081111 045430 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.6.214:50010 is added to blk_4893049344332234616 size 67108864
081111 045614 20587 INFO dfs.DataNode$DataXceiver: Receiving block blk_9131004320470011870 src: /10.251.123.132:52815 dest: /10.251.123.132:50010
081111 045623 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand5/_temporary/_task_200811101024_0011_m_000040_0/part-00040. blk_1253515519191706153
081111 045628 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.130:50010 is added to blk_-9056748866943837167 size 67108864
081111 045751 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.126.22:50010 is added to blk_-8298519990603402919 size 67108864
081111 045804 20338 INFO dfs.DataNode$PacketResponder: Received block blk_-337972286882621518 of size 67108864 from /10.250.10.144
081111 045834 19263 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-4411589101766563890 terminating
081111 045903 20579 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8838632285969241889 src: /10.251.43.21:35844 dest: /10.251.43.21:50010
081111 045904 20552 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-8225435781013159573 terminating
081111 045911 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.27.63:50010 is added to blk_-7985144765168396303 size 67108864
081111 050045 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand5/_temporary/_task_200811101024_0011_m_000541_0/part-00541. blk_-6841570414687342011
081111 050105 20507 INFO dfs.DataNode$PacketResponder: Received block blk_3924177422736829493 of size 67108864 from /10.251.66.102
081111 050307 20305 INFO dfs.DataNode$DataXceiver: Receiving block blk_9074726164158535746 src: /10.251.214.32:34409 dest: /10.251.214.32:50010
081111 050313 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.242:50010 is added to blk_-5773244829531118765 size 67108864
081111 050334 20441 INFO dfs.DataNode$DataXceiver: Received block blk_-4411589101766563890 src: /10.250.14.38:37362 dest: /10.250.14.38:50010 of size 67108864
081111 050346 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.90.64:50010 is added to blk_-8923923031063738809 size 67108864
081111 050414 20630 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8703611999267892995 src: /10.250.15.101:32889 dest: /10.250.15.101:50010
081111 050426 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.15.101:50010 is added to blk_-5352085087037370841 size 67108864
081111 050534 20462 INFO dfs.DataNode$PacketResponder: Received block blk_1411218759880637753 of size 67108864 from /10.251.123.20
081111 050710 20499 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-2268450354164990110 terminating
081111 051005 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.122.79:50010 is added to blk_-3622135859810774312 size 67108864
081111 051007 21019 INFO dfs.DataNode$DataXceiver: Receiving block blk_-2667669005094349452 src: /10.251.203.149:41751 dest: /10.251.203.149:50010
081111 051128 20647 INFO dfs.DataNode$PacketResponder: Received block blk_-2935178173527646581 of size 67108864 from /10.251.65.237
081111 051129 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.227:50010 is added to blk_5781791720343455145 size 67108864
081111 051130 20759 INFO dfs.DataNode$PacketResponder: Received block blk_49420377461855719 of size 67108864 from /10.251.70.112
081111 051331 20788 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-805262942087665153 terminating
081111 051411 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.225:50010 is added to blk_8323519561483823145 size 67108864
081111 051513 20482 INFO dfs.DataNode$DataXceiver: Receiving block blk_2208589726473895860 src: /10.251.199.245:33914 dest: /10.251.199.245:50010
081111 051517 20834 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6618090096819679591 src: /10.251.214.67:58610 dest: /10.251.214.67:50010
081111 051555 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.42.16:50010 is added to blk_-3253005280055520211 size 67108864
081111 051726 20622 INFO dfs.DataNode$PacketResponder: Received block blk_-6832799704680039005 of size 67108864 from /10.250.17.225
081111 051744 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.65.203:50010 is added to blk_4777657900314457675 size 3546628
081111 051819 20803 INFO dfs.DataNode$PacketResponder: Received block blk_-6464892000340112134 of size 67108864 from /10.250.5.161
081111 051953 20767 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-8435779070374485499 terminating
081111 052017 20779 INFO dfs.DataNode$PacketResponder: Received block blk_8993862241777591326 of size 67108864 from /10.250.15.240
081111 052103 20872 INFO dfs.DataNode$DataXceiver: Receiving block blk_8452214765874767801 src: /10.251.38.197:60409 dest: /10.251.38.197:50010
081111 052348 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.90.239:50010 is added to blk_-2281067667503210140 size 67108864
081111 052507 21174 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-4424971182856566289 terminating
081111 052633 21162 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2040222317555585426 terminating
081111 052716 20795 INFO dfs.DataNode$DataXceiver: Receiving block blk_-663653551898848389 src: /10.251.203.149:42756 dest: /10.251.203.149:50010
081111 052803 21053 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5836693155656083066 src: /10.251.214.67:41233 dest: /10.251.214.67:50010
081111 052812 21102 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7228803354694123939 src: /10.251.195.70:50389 dest: /10.251.195.70:50010
081111 052842 21012 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_2953550760670423417 terminating
081111 052846 21358 INFO dfs.DataNode$PacketResponder: Received block blk_5348548743130215090 of size 67108864 from /10.251.39.179
081111 052853 20914 INFO dfs.DataNode$PacketResponder: Received block blk_7413830214932763926 of size 67108864 from /10.251.125.174
081111 053001 20539 INFO dfs.DataNode$PacketResponder: Received block blk_6329639726609402522 of size 67108864 from /10.251.107.19
081111 053010 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand5/_temporary/_task_200811101024_0011_m_001408_0/part-01408. blk_6601059760261449131
081111 053158 21003 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5439682406172672049 src: /10.251.42.207:36091 dest: /10.251.42.207:50010
081111 053245 21296 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7492294065771709299 src: /10.251.203.149:41058 dest: /10.251.203.149:50010
081111 053325 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.144:50010 is added to blk_1526403491950199810 size 3534443
081111 053347 21024 INFO dfs.DataNode$PacketResponder: Received block blk_8902425017314766082 of size 67108864 from /10.251.26.131
081111 053359 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand5/_temporary/_task_200811101024_0011_m_001730_0/part-01730. blk_6400082566804273401
081111 053400 21069 INFO dfs.DataNode$DataXceiver: Receiving block blk_6400082566804273401 src: /10.251.38.214:42160 dest: /10.251.38.214:50010
081111 053431 20804 INFO dfs.DataNode$PacketResponder: Received block blk_4989329877491629543 of size 67108864 from /10.250.15.67
081111 053619 21091 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6608350831463251448 src: /10.251.107.196:43317 dest: /10.251.107.196:50010
081111 053916 21341 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-5580322334454533924 terminating
081111 053932 21163 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_7500282100357271699 terminating
081111 053958 21262 INFO dfs.DataNode$DataXceiver: Receiving block blk_8087142318511408355 src: /10.250.15.67:55334 dest: /10.250.15.67:50010
081111 054004 21237 INFO dfs.DataNode$PacketResponder: Received block blk_-997605125898553536 of size 67108864 from /10.251.30.179
081111 054018 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.37:50010 is added to blk_-4510259260131940043 size 3552141
081111 054135 21577 INFO dfs.DataNode$PacketResponder: Received block blk_2270593866672608129 of size 67108864 from /10.250.11.53
081111 054310 21173 INFO dfs.DataNode$PacketResponder: Received block blk_6685663454079607598 of size 10157271 from /10.251.66.3
081111 054323 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.30.179:50010 is added to blk_-4347054230277727863 size 67108864
081111 054342 20995 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1067234447809438340 terminating
081111 054443 21162 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1096621677333075077 terminating
081111 054449 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.125.193:50010 is added to blk_-8657800226489501791 size 3550839
081111 054504 21552 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_2151150262081352617 terminating
081111 054731 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3280859030919654043 is added to invalidSet of 10.251.214.130:50010
081111 054811 21215 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6811828069408410657 terminating
081111 054914 21234 INFO dfs.DataNode$PacketResponder: Received block blk_4967370797322420259 of size 67108864 from /10.251.39.192
081111 055147 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.127.243:50010 is added to blk_-719178972315562824 size 67108864
081111 055219 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.67.4:50010 is added to blk_9093176996492793041 size 67108864
081111 055444 21503 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-7200400342436668240 terminating
081111 055501 21608 INFO dfs.DataNode$PacketResponder: Received block blk_7213990003235926706 of size 67108864 from /10.250.11.100
081111 055640 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.32:50010 is added to blk_-2125743956855238111 size 67108864
081111 055936 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.244:50010 is added to blk_-4875138366845786590 size 67108864
081111 060015 21733 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2508619583759354778 terminating
081111 060043 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.244:50010 is added to blk_6095704128062577668 size 28484260
081111 060104 21535 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_6123232805286187512 terminating
081111 060154 21461 INFO dfs.DataNode$DataXceiver: Receiving block blk_6413965758113058023 src: /10.251.199.150:45184 dest: /10.251.199.150:50010
081111 060244 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.197.226:50010 is added to blk_7275457888263057111 size 67108864
081111 060510 21849 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_6818232325322327288 terminating
081111 061043 21809 INFO dfs.DataNode$PacketResponder: Received block blk_-5954527587435406374 of size 67108864 from /10.251.106.10
081111 061200 21837 INFO dfs.DataNode$PacketResponder: Received block blk_-4358842839092719327 of size 67108864 from /10.250.15.67
081111 061202 22063 INFO dfs.DataNode$PacketResponder: Received block blk_1489031087420498797 of size 67108864 from /10.251.71.193
081111 061534 21779 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-615578808743808725 terminating
081111 061549 21862 INFO dfs.DataNode$PacketResponder: Received block blk_4588471931728384541 of size 67108864 from /10.251.90.81
081111 061602 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt5/_temporary/_task_200811101024_0012_m_001192_0/part-01192. blk_7720864196601815614
081111 061707 21528 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-9098645110390099509 terminating
081111 061713 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.109.209:50010 is added to blk_1129715194117079802 size 67108864
081111 061730 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt5/_temporary/_task_200811101024_0012_m_000900_0/part-00900. blk_6690143611140381289
081111 061739 21895 INFO dfs.DataNode$PacketResponder: Received block blk_7463248820468902190 of size 67108864 from /10.251.91.15
081111 061857 17075 INFO dfs.DataNode$PacketResponder: Received block blk_3961831045048378097 of size 67108864 from /10.251.39.160
081111 061900 21890 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6220821375880228145 terminating
081111 061909 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.192:50010 is added to blk_-4405302133414008227 size 67108864
081111 061921 21913 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_8854625173826438305 terminating
081111 061926 22010 INFO dfs.DataNode$PacketResponder: Received block blk_8699194381234371727 of size 67108864 from /10.251.214.67
081111 062002 21694 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-8389657728641837382 terminating
081111 062029 21573 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5145508449157510071 src: /10.251.203.246:48508 dest: /10.251.203.246:50010
081111 062038 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.215.50:50010 is added to blk_6675543598756975559 size 67108864
081111 062102 22250 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-8364400144313355814 terminating
081111 062238 21812 INFO dfs.DataNode$DataXceiver: Receiving block blk_6108023147232847992 src: /10.251.42.9:43819 dest: /10.251.42.9:50010
081111 062311 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.102:50010 is added to blk_-1932467763723526131 size 67108864
081111 062353 22087 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8834022217832819265 src: /10.251.105.189:53216 dest: /10.251.105.189:50010
081111 062628 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt5/_temporary/_task_200811101024_0012_m_001314_0/part-01314. blk_5872617224009342667
081111 062735 22030 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_4877808040461609832 terminating
081111 062905 21843 INFO dfs.DataNode$PacketResponder: Received block blk_3049351456771045311 of size 67108864 from /10.251.199.159
081111 062911 22258 INFO dfs.DataNode$PacketResponder: Received block blk_1247599917761164561 of size 67108864 from /10.251.37.240
081111 063053 21803 INFO dfs.DataNode$PacketResponder: Received block blk_2616961363639592748 of size 67108864 from /10.251.202.134
081111 063138 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt5/_temporary/_task_200811101024_0012_m_001529_0/part-01529. blk_-826920518336899809
081111 063241 22074 INFO dfs.DataNode$PacketResponder: Received block blk_-3436382429367583573 of size 67108864 from /10.251.107.19
081111 063244 22352 INFO dfs.DataNode$DataXceiver: Receiving block blk_2824047463816148563 src: /10.251.42.84:35776 dest: /10.251.42.84:50010
081111 063302 21914 INFO dfs.DataNode$PacketResponder: Received block blk_-9211699406261033878 of size 67108864 from /10.251.30.85
081111 063305 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.67:50010 is added to blk_-5961895238114806886 size 28508403
081111 063316 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.19:50010 is added to blk_-2654726224018724510 size 67108864
081111 063323 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.67.225:50010 is added to blk_-3955287039966096707 size 67108864
081111 063402 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.75.143:50010 is added to blk_-1379379987510648393 size 67108864
081111 063413 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt5/_temporary/_task_200811101024_0012_m_001726_0/part-01726. blk_1858805942071850133
081111 063451 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt5/_temporary/_task_200811101024_0012_m_001554_0/part-01554. blk_5183831565578080242
081111 063459 21912 INFO dfs.DataNode$DataXceiver: Receiving block blk_3248835454898465560 src: /10.251.42.84:40950 dest: /10.251.42.84:50010
081111 063514 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt5/_temporary/_task_200811101024_0012_m_001709_0/part-01709. blk_3776895047834644694
081111 063636 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.192:50010 is added to blk_8521306385156958668 size 67108864
081111 063859 22169 INFO dfs.DataNode$DataXceiver: Receiving block blk_1606295458729964667 src: /10.251.214.32:47910 dest: /10.251.214.32:50010
081111 063923 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.149:50010 is added to blk_-7282356322111075290 size 67108864
081111 063944 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.122.65:50010 is added to blk_-5330522460969948971 size 67108864
081111 064011 22583 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_1077394056781488 terminating
081111 064106 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt5/_temporary/_task_200811101024_0012_m_001709_0/part-01709. blk_6054235487623373096
081111 064209 22299 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-7256345477493725857 terminating
081111 064322 6765 INFO dfs.DataNode$PacketResponder: Received block blk_3480684347183600958 of size 67108864 from /10.251.111.209
081111 064501 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt5/_temporary/_task_200811101024_0012_m_001635_0/part-01635. blk_9034993982993401678
081111 064603 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.31.5:50010 is added to blk_-1821104627978106102 size 67108864
081111 064615 22671 INFO dfs.DataNode$DataXceiver: Receiving block blk_8116683912654412228 src: /10.251.39.179:42019 dest: /10.251.39.179:50010
081111 064710 22306 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-8329840785892216949 terminating
081111 064806 22602 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_8254500345987409515 terminating
081111 064823 22556 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4657886075935807358 src: /10.251.31.160:53084 dest: /10.251.31.160:50010
081111 064835 22032 INFO dfs.DataNode$DataXceiver: Receiving block blk_8201813022998766557 src: /10.251.203.4:51404 dest: /10.251.203.4:50010
081111 065205 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-4438918035940270891 is added to invalidSet of 10.251.71.193:50010
081111 065205 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5616920288053661280 is added to invalidSet of 10.251.42.9:50010
081111 065206 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_1815340037591016775 is added to invalidSet of 10.250.9.207:50010
081111 065208 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2154193600525549460 is added to invalidSet of 10.251.43.147:50010
081111 065211 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5580254744888391593 is added to invalidSet of 10.251.201.204:50010
081111 065217 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1641081065757890722 is added to invalidSet of 10.251.107.242:50010
081111 065217 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5505696082121135363 is added to invalidSet of 10.251.214.130:50010
081111 065218 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5689009809369418749 is added to invalidSet of 10.251.91.84:50010
081111 065220 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_8155177385081669900 is added to invalidSet of 10.251.197.161:50010
081111 065224 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_247452006316846355 is added to invalidSet of 10.250.6.191:50010
081111 065224 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2657997043417110888 is added to invalidSet of 10.250.13.240:50010
081111 065225 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-323838382557775734 is added to invalidSet of 10.251.202.209:50010
081111 065225 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-719470845056640928 is added to invalidSet of 10.251.90.64:50010
081111 065225 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7999375661905777727 is added to invalidSet of 10.251.106.37:50010
081111 065226 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-2451769929123196359 is added to invalidSet of 10.251.199.159:50010
081111 065227 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3231389654886008065 is added to invalidSet of 10.251.195.52:50010
081111 065227 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3637945019239882309 is added to invalidSet of 10.251.198.196:50010
081111 065229 19 INFO dfs.FSDataset: Deleting block blk_-7423574498760490375 file /mnt/hadoop/dfs/data/current/subdir58/blk_-7423574498760490375
081111 065254 19 INFO dfs.FSNamesystem: BLOCK* ask 10.250.17.177:50010 to delete  blk_-8570780307468499817 blk_-9122557405432088649 blk_-4393063808227796056 blk_8767569714374844347 blk_7079754042611867581 blk_7608961006114219538 blk_-5017273584996436939 blk_-6537833125980536955 blk_7610838808763810123 blk_3300803097775546532 blk_-5120750586032922592 blk_1577274266662884430 blk_765879159867598347 blk_-9076085976403711202 blk_-3198963348573340497 blk_-4645750029177277209 blk_-5136142986912961316 blk_5677959846373741243 blk_2107477892986152528 blk_-4235116161537008844 blk_6082535783543982566 blk_-4809870147222033236 blk_8818706925296961012 blk_-5203577173046267127 blk_189089569009261656 blk_446299976487589160 blk_-3916247521166632303 blk_-3324962406687427922 blk_-1807424528783081572 blk_-6858401049333055963 blk_6036564204960295926 blk_-8140723044408248078 blk_-3800132731140204959 blk_1716344083117307767 blk_-5194808114606613364 blk_-5473871016976323232 blk_2920934363167004552 blk_8736689095894369097 blk_-7642734632751940776 blk_3408482260833769309 blk_118013751374560901 blk_7963891081239759520 blk_3813114133944383323 blk_3042818489384932576 blk_-4570173726231458270 blk_-1564644006975920581 blk_338095650783321996 blk_3150135312641203550 blk_4285859645577726288 blk_3438772130782939627 blk_2634772258588877972 blk_-6795664812575964130 blk_3923069610304693233 blk_-1782996202120067721 blk_2004418049430157212 blk_1932147224007687756 blk_-582901062969027153 blk_5072240701440032119 blk_-7919006477393039068 blk_-7318022361288598312 blk_-6974693594143537436 blk_-5435767047126325206 blk_-5805500288959332434 blk_-7109885589081848850 blk_2161580591957523893 blk_7240227881194993860 blk_-8298405680648445349 blk_-4253026248821272215 blk_8377661448601579317 blk_8029153852899017155 blk_-8754388319080705916 blk_-7844092300527332901 blk_710178463364063355 blk_-5136849989188547884 blk_8393887138377503163 blk_-6950176077776664217 blk_-6488701068659548195 blk_2537458728254532453 blk_364441107933628577 blk_6207861897580168557 blk_8814943807366894581 blk_-4150682644311695471 blk_9174833667156726933 blk_649427218152856001 blk_-7403541028238011236 blk_-334982586592048773 blk_61908781908925992 blk_6385574357371832424 blk_-66376131060945541 blk_1372596948297458670 blk_-3389135155401857220 blk_-6035411221441929663 blk_-5127580069634421247 blk_-5685246533892022418 blk_4977937528993040451 blk_5680538862600094527 blk_-8378747462487962732 blk_425101290285860876 blk_6306622708327890839 blk_-1067866602168873257
081111 065258 19 INFO dfs.FSDataset: Deleting block blk_-9169228974826183399 file /mnt/hadoop/dfs/data/current/subdir5/blk_-9169228974826183399
081111 065303 19 INFO dfs.FSNamesystem: BLOCK* ask 10.250.10.213:50010 to delete  blk_4029139044660806713 blk_-5471189807977280544 blk_6708643067868168687 blk_-500678958150296008 blk_-8597840983621849778 blk_-3610057702150392748 blk_-1709606535283888232 blk_-4154362211643572668 blk_-8892080524136798472 blk_5356427838869009345 blk_-6987238639050161133 blk_-5215128860160823363 blk_7186692462976470823 blk_-6538449588297475521 blk_-2165930080589343952 blk_-5524899010031625427 blk_6384439316405471171 blk_-2965258329365213675 blk_118950937507976810 blk_-1717088081766373300 blk_-3911466865418055820 blk_1237334407720045724 blk_-760015977981369567 blk_-6802007379650646616 blk_-7667535133893574689 blk_6865645438678864855 blk_4633996820313194570 blk_7225301266481603731 blk_-4930257130609958866 blk_-4124845864570823487 blk_4927011145115127531 blk_7234346856930822716 blk_7159969052744592746 blk_1296823600557793869 blk_2209319141644287774 blk_-622218131799806364 blk_-8154516246083521409 blk_4466433199471909449 blk_8406894133999850666 blk_991075908349619367 blk_-2081474832657208733 blk_-5573393775847919985 blk_2004177185950968695 blk_4041319486058127641 blk_6449230045010995668 blk_5978265573904271474 blk_-4813738732036414715 blk_4389340532803855247 blk_-857151863616763327 blk_-7200136644339435027 blk_-1454962873426270839 blk_-5012294311590635938 blk_7112727670634942639 blk_3335012758760643328 blk_3382627815322561484 blk_825124020036421636 blk_-8040559034239258688 blk_-5415591001139074826 blk_-1052513063506891954 blk_-1155882018729560343 blk_-5679835604685169040 blk_-4498808851217768984 blk_8345415947062862337 blk_8521655806854586696 blk_7602939593939794410 blk_-4833650023923869528 blk_7237730029042141635 blk_2860897425785746911 blk_-1937193099911148343 blk_5740615689780260922 blk_963252337613423037 blk_5537011318013544619 blk_2626057344048606017 blk_8296499240199635880 blk_7211071078501521087 blk_8823112510768971040 blk_-3366974935992288326 blk_-2947778702643296262 blk_7693891282153136044 blk_4644812717442758529 blk_-5724970555730638200 blk_-3039294462945223064 blk_-1729755380346651221 blk_-6448673813272428418 blk_-7724282460846954976 blk_2698691234887375588 blk_-4043525878322523713 blk_-5195120009388265 blk_8879208244602324204 blk_-5784376901556131897 blk_-5201149273969117873 blk_5253889604362640423 blk_7067050654303940677 blk_8992626816092659826 blk_-488462739843441981 blk_8543991617360374935 blk_1943146154560599630 blk_-9194660123773136535 blk_3351984198891394382 blk_-6759123807563555545
081111 065339 22547 INFO dfs.DataNode$PacketResponder: Received block blk_5406084276349645828 of size 67108864 from /10.250.10.223
081111 065404 22999 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_2016964533702776852 terminating
081111 065418 18 INFO dfs.FSDataset: Deleting block blk_-1306900105984505600 file /mnt/hadoop/dfs/data/current/subdir31/blk_-1306900105984505600
081111 065420 19 INFO dfs.FSDataset: Deleting block blk_-1312981516354612257 file /mnt/hadoop/dfs/data/current/subdir29/blk_-1312981516354612257
081111 065420 19 INFO dfs.FSDataset: Deleting block blk_-2658615574293326723 file /mnt/hadoop/dfs/data/current/subdir37/blk_-2658615574293326723
081111 065423 19 INFO dfs.FSDataset: Deleting block blk_2107487634698646045 file /mnt/hadoop/dfs/data/current/subdir3/blk_2107487634698646045
081111 065426 19 INFO dfs.FSDataset: Deleting block blk_-253282915050290778 file /mnt/hadoop/dfs/data/current/subdir23/blk_-253282915050290778
081111 065435 19 INFO dfs.FSDataset: Deleting block blk_1587615395786683981 file /mnt/hadoop/dfs/data/current/subdir14/blk_1587615395786683981
081111 065437 19 INFO dfs.FSDataset: Deleting block blk_3706543205492061794 file /mnt/hadoop/dfs/data/current/subdir1/blk_3706543205492061794
081111 065440 19 INFO dfs.FSDataset: Deleting block blk_3862032846413436284 file /mnt/hadoop/dfs/data/current/subdir28/blk_3862032846413436284
081111 065444 18 INFO dfs.FSDataset: Deleting block blk_4632283243694149854 file /mnt/hadoop/dfs/data/current/subdir40/blk_4632283243694149854
081111 065444 19 INFO dfs.FSDataset: Deleting block blk_1018961650823645078 file /mnt/hadoop/dfs/data/current/subdir60/blk_1018961650823645078
081111 065444 19 INFO dfs.FSDataset: Deleting block blk_1213582438865696738 file /mnt/hadoop/dfs/data/current/subdir56/blk_1213582438865696738
081111 065445 19 INFO dfs.FSDataset: Deleting block blk_2473550781612396886 file /mnt/hadoop/dfs/data/current/subdir54/blk_2473550781612396886
081111 065500 19 INFO dfs.FSDataset: Deleting block blk_8102707766842966459 file /mnt/hadoop/dfs/data/current/subdir29/blk_8102707766842966459
081111 065500 22388 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-352190670750948803 terminating
081111 065503 19 INFO dfs.FSDataset: Deleting block blk_6491888411266994867 file /mnt/hadoop/dfs/data/current/subdir39/blk_6491888411266994867
081111 065504 18 INFO dfs.FSDataset: Deleting block blk_3336810835691486095 file /mnt/hadoop/dfs/data/current/subdir11/blk_3336810835691486095
081111 065510 19 INFO dfs.FSDataset: Deleting block blk_4367982328407767477 file /mnt/hadoop/dfs/data/current/subdir28/blk_4367982328407767477
081111 065511 19 INFO dfs.FSDataset: Deleting block blk_6943254721518837570 file /mnt/hadoop/dfs/data/current/subdir41/blk_6943254721518837570
081111 065515 19 INFO dfs.FSDataset: Deleting block blk_6077744985764117617 file /mnt/hadoop/dfs/data/current/subdir17/blk_6077744985764117617
081111 065526 22651 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_6975937964598933356 terminating
081111 065538 19 INFO dfs.FSDataset: Deleting block blk_5771906433100975293 file /mnt/hadoop/dfs/data/current/subdir37/blk_5771906433100975293
081111 065605 22827 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3884471885525630813 src: /10.251.106.50:60327 dest: /10.251.106.50:50010
081111 065705 22525 INFO dfs.DataNode$DataXceiver: Receiving block blk_8596624696139957935 src: /10.251.214.175:41468 dest: /10.251.214.175:50010
081111 065705 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_000153_0/part-00153. blk_8596624696139957935
081111 065803 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.67:50010 is added to blk_-8483420270408348004 size 67108864
081111 065804 19 INFO dfs.FSDataset: Deleting block blk_4506604798892399878 file /mnt/hadoop/dfs/data/current/subdir0/blk_4506604798892399878
081111 065920 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_000197_0/part-00197. blk_1832308388558480
081111 065936 22801 INFO dfs.DataNode$PacketResponder: Received block blk_2247786307216183697 of size 67108864 from /10.251.67.4
081111 070022 22093 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-4289592006812254261 terminating
081111 070040 22706 INFO dfs.DataNode$DataXceiver: Receiving block blk_5989505185938770383 src: /10.251.67.4:53429 dest: /10.251.67.4:50010
081111 070055 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.121.224:50010 is added to blk_3134656542322449478 size 67108864
081111 070232 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-8050165297538775793
081111 070246 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.90.134:50010 is added to blk_-3184700311670257792 size 67108864
081111 070258 22788 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_5114572025256210948 terminating
081111 070301 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_000451_0/part-00451. blk_-8744988583359134013
081111 070326 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_000079_0/part-00079. blk_4225726256421431188
081111 070458 22933 INFO dfs.DataNode$PacketResponder: Received block blk_1060814796407897179 of size 67108864 from /10.251.109.209
081111 070505 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.25.237:50010 is added to blk_-1496429201748767714 size 67108864
081111 070519 22971 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-4684475480143348818 terminating
081111 070519 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_000077_0/part-00077. blk_608957629753483727
081111 070635 22203 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-7937864120968665564 terminating
081111 070735 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.130:50010 is added to blk_-4427507243535536615 size 67108864
081111 070823 22537 INFO dfs.DataNode$PacketResponder: Received block blk_7830067201550857864 of size 67108864 from /10.251.105.189
081111 070836 22330 INFO dfs.DataNode$PacketResponder: Received block blk_-2194976790278647102 of size 67108864 from /10.251.110.8
081111 070836 23168 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_8459274965060101250 terminating
081111 070912 22946 INFO dfs.DataNode$PacketResponder: Received block blk_1561008813520212225 of size 67108864 from /10.250.7.230
081111 070918 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.6.214:50010 is added to blk_5963655886023015963 size 67108864
081111 070928 23133 INFO dfs.DataNode$DataXceiver: Receiving block blk_2377944115560974127 src: /10.251.75.163:55681 dest: /10.251.75.163:50010
081111 071020 22762 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_6030936529326346827 terminating
081111 071027 22865 INFO dfs.DataNode$DataXceiver: Receiving block blk_3322875843051058970 src: /10.250.7.244:40195 dest: /10.250.7.244:50010
081111 071037 21663 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-7269672042149855641 terminating
081111 071042 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.30.134:50010 is added to blk_-5214060566378896158 size 67108864
081111 071112 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_000523_0/part-00523. blk_-4801855948459616884
081111 071205 23083 INFO dfs.DataNode$PacketResponder: Received block blk_-4846710336436578833 of size 67108864 from /10.251.203.129
081111 071331 22956 INFO dfs.DataNode$PacketResponder: Received block blk_-1522107359378428427 of size 67108864 from /10.251.125.174
081111 071510 22974 INFO dfs.DataNode$PacketResponder: Received block blk_-1846853350696018836 of size 67108864 from /10.250.15.67
081111 071611 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_000997_0/part-00997. blk_-500534246236005335
081111 071713 23240 INFO dfs.DataNode$PacketResponder: Received block blk_-1236077201457689973 of size 67108864 from /10.251.35.1
081111 071731 23039 INFO dfs.DataNode$PacketResponder: Received block blk_8474920544984206742 of size 67108864 from /10.251.91.229
081111 071746 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.16:50010 is added to blk_-6464763289185102366 size 67108864
081111 071904 22560 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7646004878694144580 src: /10.251.202.181:47130 dest: /10.251.202.181:50010
081111 071912 23232 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-41928108292416895 terminating
081111 071939 22914 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7389979618829759505 src: /10.251.127.47:58395 dest: /10.251.127.47:50010
081111 072053 23570 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_2096030750186708493 terminating
081111 072059 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.30.6:50010 is added to blk_-2345355483730155757 size 67108864
081111 072214 23052 INFO dfs.DataNode$PacketResponder: Received block blk_5834218936335592624 of size 67108864 from /10.251.111.228
081111 072224 23062 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-5504500392858491441 terminating
081111 072300 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.17.177:50010 is added to blk_4385764838827095986 size 3541872
081111 072355 22702 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-3710747952520609225 terminating
081111 072356 23300 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_707166530951154301 terminating
081111 072356 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.126.22:50010 is added to blk_707166530951154301 size 67108864
081111 072403 23051 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2241654010576170013 terminating
081111 072447 23098 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8475679647841427708 src: /10.251.75.49:58661 dest: /10.251.75.49:50010
081111 072523 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.27.63:50010 is added to blk_-7794786562417612249 size 67108864
081111 072710 22924 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-2068111211565768713 terminating
081111 072733 23446 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7238035741206373733 src: /10.251.202.209:59561 dest: /10.251.202.209:50010
081111 072818 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_001195_0/part-01195. blk_5708405953850477535
081111 072857 23371 INFO dfs.DataNode$PacketResponder: Received block blk_2253084440592362960 of size 67108864 from /10.251.199.19
081111 072901 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.91.32:50010 is added to blk_-8127580564011467092 size 67108864
081111 072903 23158 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-8568673600040025636 terminating
081111 072921 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.242:50010 is added to blk_621685330749869453 size 67108864
081111 072948 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.230:50010 is added to blk_8663640697928077697 size 67108864
081111 072949 23644 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_1508705816376906895 terminating
081111 072956 23082 INFO dfs.DataNode$PacketResponder: Received block blk_-6441360440068187612 of size 67108864 from /10.250.10.100
081111 073005 23505 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_2128110944185709079 terminating
081111 073013 23372 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_871974742726663441 terminating
081111 073059 23319 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8049943837789435742 terminating
081111 073109 23004 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-8246207776580662770 terminating
081111 073148 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.100:50010 is added to blk_3406384548372443486 size 67108864
081111 073230 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.193.175:50010 is added to blk_2460145905812664251 size 67108864
081111 073302 23438 INFO dfs.DataNode$PacketResponder: Received block blk_-7207514537719465756 of size 67108864 from /10.251.75.143
081111 073314 23367 INFO dfs.DataNode$DataXceiver: Receiving block blk_5216491057643255876 src: /10.251.105.189:55126 dest: /10.251.105.189:50010
081111 073350 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.7.96:50010 is added to blk_4641147544753118391 size 67108864
081111 073403 23490 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-85872290979354091 terminating
081111 073406 23140 INFO dfs.DataNode$PacketResponder: Received block blk_-8310554294841246175 of size 67108864 from /10.251.123.132
081111 073441 17869 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7293871043749246063 src: /10.251.193.175:59757 dest: /10.251.193.175:50010
081111 073507 23329 INFO dfs.DataNode$PacketResponder: Received block blk_3107521534415119817 of size 67108864 from /10.251.193.224
081111 073525 23203 INFO dfs.DataNode$PacketResponder: Received block blk_7194943883245628915 of size 67108864 from /10.251.71.193
081111 073610 23484 INFO dfs.DataNode$PacketResponder: Received block blk_-3816036740130815667 of size 3547785 from /10.250.19.227
081111 073704 23532 INFO dfs.DataNode$PacketResponder: Received block blk_-5450224419450631495 of size 67108864 from /10.251.42.84
081111 073746 23412 INFO dfs.DataNode$DataXceiver: Receiving block blk_5881782374981838094 src: /10.251.214.18:43091 dest: /10.251.214.18:50010
081111 073802 23624 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_7078766226078740026 terminating
081111 073914 23367 INFO dfs.DataNode$PacketResponder: Received block blk_-7380871572601322244 of size 67108864 from /10.251.121.224
081111 073943 23467 INFO dfs.DataNode$DataXceiver: Receiving block blk_3846795603907041885 src: /10.250.10.176:47296 dest: /10.250.10.176:50010
081111 073949 23599 INFO dfs.DataNode$PacketResponder: Received block blk_7922331746517419368 of size 67108864 from /10.251.199.19
081111 074108 23144 INFO dfs.DataNode$PacketResponder: Received block blk_155320394753274773 of size 67108864 from /10.251.201.204
081111 074200 23325 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-5722491454804415982 terminating
081111 074309 23736 INFO dfs.DataNode$PacketResponder: Received block blk_-3482997549732691785 of size 67108864 from /10.251.122.79
081111 074409 23194 INFO dfs.DataNode$DataXceiver: Receiving block blk_6417614801998326532 src: /10.251.214.67:48694 dest: /10.251.214.67:50010
081111 074431 23560 INFO dfs.DataNode$PacketResponder: Received block blk_-1090213273274684471 of size 67108864 from /10.251.214.130
081111 074449 23551 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3738901473077208093 src: /10.251.199.150:49679 dest: /10.251.199.150:50010
081111 074540 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_001850_0/part-01850. blk_-4312777901444596370
081111 074548 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_001835_0/part-01835. blk_6178964531181542074
081111 074653 23823 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_8550326614414622861 terminating
081111 074955 23635 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-6525012344542022819 terminating
081111 074958 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_001983_0/part-01983. blk_8861349372992394289
081111 075054 23885 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1049340855430710153 terminating
081111 075114 23523 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_703049809255765647 terminating
081111 075129 23397 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7460532841070848624 src: /10.250.15.67:55076 dest: /10.250.15.67:50010
081111 075151 23925 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7623732795385061487 src: /10.251.122.65:40641 dest: /10.251.122.65:50010
081111 075158 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_200811101024_0013_m_002009_1/part-02009. blk_-7367386750462511488
081111 075212 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.228:50010 is added to blk_-1237539644269634790 size 67108864
081111 075317 23851 INFO dfs.DataNode$DataXceiver: Receiving block blk_7713504868233504097 src: /10.251.127.191:36437 dest: /10.251.127.191:50010
081111 075345 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.65.203:50010 is added to blk_8005746184358076390 size 67108864
081111 075408 23362 INFO dfs.DataNode$PacketResponder: Received block blk_955083461510572332 of size 41287838 from /10.251.107.242
081111 075537 23945 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_283922378823540530 terminating
081111 075616 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_4471044334742081825 is added to invalidSet of 10.250.15.198:50010
081111 075619 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3671953658029337449 is added to invalidSet of 10.250.14.143:50010
081111 075623 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-75893538967225087 is added to invalidSet of 10.250.15.198:50010
081111 075626 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_805187982300776163 is added to invalidSet of 10.251.73.188:50010
081111 075628 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_4631777956354958682 is added to invalidSet of 10.251.30.134:50010
081111 075629 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7595375726951801956 is added to invalidSet of 10.251.122.79:50010
081111 075631 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-4681205519208992498 is added to invalidSet of 10.250.6.191:50010
081111 075632 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5702487846232575705 is added to invalidSet of 10.250.14.38:50010
081111 075634 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_4834069779372284635 is added to invalidSet of 10.251.71.97:50010
081111 075636 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3735517274901501821 is added to invalidSet of 10.250.6.4:50010
081111 075637 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_5018797996887008111 is added to invalidSet of 10.251.89.155:50010
081111 075638 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-566178736644429906 is added to invalidSet of 10.251.70.37:50010
081111 075640 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8121498678081035000 is added to invalidSet of 10.250.11.100:50010
081111 075657 23748 INFO dfs.DataNode$DataXceiver: Receiving block blk_7996616083434301906 src: /10.251.123.195:53947 dest: /10.251.123.195:50010
081111 075727 19 INFO dfs.FSDataset: Deleting block blk_6888300867578983331 file /mnt/hadoop/dfs/data/current/subdir46/blk_6888300867578983331
081111 075743 23596 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_7625391056418655045 terminating
081111 075746 19 INFO dfs.FSDataset: Deleting block blk_-8443921001690182522 file /mnt/hadoop/dfs/data/current/subdir33/blk_-8443921001690182522
081111 075747 23844 INFO dfs.DataNode$DataXceiver: Receiving block blk_706826531153199550 src: /10.250.5.161:58254 dest: /10.250.5.161:50010
081111 075753 19 INFO dfs.FSDataset: Deleting block blk_-7446091384471755694 file /mnt/hadoop/dfs/data/current/subdir30/blk_-7446091384471755694
081111 075759 19 INFO dfs.FSDataset: Deleting block blk_-4352313989800650828 file /mnt/hadoop/dfs/data/current/subdir56/blk_-4352313989800650828
081111 075809 19 INFO dfs.FSDataset: Deleting block blk_-6292499669518281202 file /mnt/hadoop/dfs/data/current/subdir26/blk_-6292499669518281202
081111 075809 23659 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-4582442491366282435 terminating
081111 075810 19 INFO dfs.FSDataset: Deleting block blk_-6750694665033046901 file /mnt/hadoop/dfs/data/current/subdir63/blk_-6750694665033046901
081111 075814 19 INFO dfs.FSDataset: Deleting block blk_-6131345369657245178 file /mnt/hadoop/dfs/data/current/blk_-6131345369657245178
081111 075824 19 INFO dfs.FSDataset: Deleting block blk_-3575718350717794894 file /mnt/hadoop/dfs/data/current/blk_-3575718350717794894
081111 075833 19 INFO dfs.FSDataset: Deleting block blk_-4380385204018751771 file /mnt/hadoop/dfs/data/current/subdir38/blk_-4380385204018751771
081111 075838 19 INFO dfs.FSDataset: Deleting block blk_-3594701313167635091 file /mnt/hadoop/dfs/data/current/subdir54/blk_-3594701313167635091
081111 075844 18 INFO dfs.FSDataset: Deleting block blk_-1693418636466747515 file /mnt/hadoop/dfs/data/current/subdir16/blk_-1693418636466747515
081111 075854 19 INFO dfs.FSDataset: Deleting block blk_5806397523737304814 file /mnt/hadoop/dfs/data/current/subdir34/blk_5806397523737304814
081111 075901 18 INFO dfs.FSDataset: Deleting block blk_-1345585044778938153 file /mnt/hadoop/dfs/data/current/subdir45/blk_-1345585044778938153
081111 075901 19 INFO dfs.FSDataset: Deleting block blk_2489882700357182381 file /mnt/hadoop/dfs/data/current/blk_2489882700357182381
081111 075902 19 INFO dfs.FSDataset: Deleting block blk_-1800458850981311953 file /mnt/hadoop/dfs/data/current/subdir57/blk_-1800458850981311953
081111 075905 19 INFO dfs.FSDataset: Deleting block blk_6373260430960091026 file /mnt/hadoop/dfs/data/current/subdir43/blk_6373260430960091026
081111 075907 19 INFO dfs.FSDataset: Deleting block blk_-1278725622357466169 file /mnt/hadoop/dfs/data/current/subdir0/blk_-1278725622357466169
081111 075911 19 INFO dfs.FSDataset: Deleting block blk_4513772639980123655 file /mnt/hadoop/dfs/data/current/blk_4513772639980123655
081111 075914 19 INFO dfs.FSDataset: Deleting block blk_1838564958520120941 file /mnt/hadoop/dfs/data/current/subdir39/blk_1838564958520120941
081111 075915 19 INFO dfs.FSDataset: Deleting block blk_7985460426680885040 file /mnt/hadoop/dfs/data/current/blk_7985460426680885040
081111 075920 19 INFO dfs.FSDataset: Deleting block blk_2215517976324634098 file /mnt/hadoop/dfs/data/current/subdir4/blk_2215517976324634098
081111 075920 19 INFO dfs.FSDataset: Deleting block blk_5453150794205420798 file /mnt/hadoop/dfs/data/current/subdir24/blk_5453150794205420798
081111 075933 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.194.102:50010 is added to blk_-7522572236891389139 size 67108864
081111 075957 19 INFO dfs.FSDataset: Deleting block blk_6163400028286940197 file /mnt/hadoop/dfs/data/current/subdir7/blk_6163400028286940197
081111 080006 19 INFO dfs.FSDataset: Deleting block blk_7583807825342916938 file /mnt/hadoop/dfs/data/current/subdir13/blk_7583807825342916938
081111 080019 23787 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5957060034695163774 src: /10.251.29.239:56334 dest: /10.251.29.239:50010
081111 080108 24040 INFO dfs.DataNode$DataXceiver: Receiving block blk_-2250942073136893488 src: /10.251.126.22:59179 dest: /10.251.126.22:50010
081111 080118 23798 INFO dfs.DataNode$PacketResponder: Received block blk_-3693786175267111588 of size 67108864 from /10.251.71.97
081111 080151 19 INFO dfs.FSDataset: Deleting block blk_8384967715738359346 file /mnt/hadoop/dfs/data/current/subdir27/blk_8384967715738359346
081111 080256 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.223:50010 is added to blk_-1948505589144820414 size 67108864
081111 080309 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.202.209:50010 is added to blk_-6224982681533811277 size 67108864
081111 080329 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.196:50010 is added to blk_-2843780069100181052 size 67108864
081111 080505 23478 INFO dfs.DataNode$DataXceiver: Receiving block blk_8083872765147994327 src: /10.250.15.67:41990 dest: /10.250.15.67:50010
081111 080554 24194 INFO dfs.DataNode$PacketResponder: Received block blk_-6502143621885580798 of size 67108864 from /10.251.39.179
081111 080645 23938 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8323785930877970805 src: /10.250.13.240:40589 dest: /10.250.13.240:50010
081111 080654 23600 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-7587189852169913514 terminating
081111 080710 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.74.192:50010 is added to blk_7439201509602018752 size 67108864
081111 080803 24149 INFO dfs.DataNode$PacketResponder: Received block blk_3224184805694764748 of size 67108864 from /10.250.18.114
081111 080934 19 INFO dfs.FSNamesystem: BLOCK* ask 10.250.14.38:50010 to replicate blk_-7571492020523929240 to datanode(s) 10.251.122.38:50010
081111 081027 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.30.6:50010 is added to blk_1119272693889755844 size 67108864
081111 081029 24268 INFO dfs.DataNode$PacketResponder: Received block blk_1600228140214754078 of size 67108864 from /10.251.90.81
081111 081055 24136 INFO dfs.DataNode$DataXceiver: Received block blk_1473949624670719319 src: /10.251.29.239:35617 dest: /10.251.29.239:50010 of size 67108864
081111 081145 23823 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7227576651324043655 src: /10.251.107.227:40740 dest: /10.251.107.227:50010
081111 081406 18 INFO dfs.FSDataset: Deleting block blk_-7606467001548719462 file /mnt/hadoop/dfs/data/current/subdir2/blk_-7606467001548719462
081111 081416 24027 INFO dfs.DataNode$PacketResponder: Received block blk_2771018387166611008 of size 67108864 from /10.251.26.131
081111 081534 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.110.160:50010 is added to blk_-2852294661215451806 size 67108864
081111 081606 23925 INFO dfs.DataNode$DataXceiver: Receiving block blk_7891129742741763034 src: /10.251.107.227:57000 dest: /10.251.107.227:50010
081111 081617 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.15.240:50010 is added to blk_8512974147315737844 size 67108864
081111 081640 23985 INFO dfs.DataNode$PacketResponder: Received block blk_427477290315080955 of size 67108864 from /10.251.123.132
081111 081702 24075 INFO dfs.DataNode$PacketResponder: Received block blk_-7630699912570186723 of size 67108864 from /10.251.71.240
081111 081847 24224 INFO dfs.DataNode$DataXceiver: Receiving block blk_1512136249403454074 src: /10.250.7.244:47958 dest: /10.250.7.244:50010
081111 081938 23963 INFO dfs.DataNode$PacketResponder: Received block blk_8655748951361667770 of size 67108864 from /10.251.109.236
081111 081954 24553 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8487226415276391460 terminating
081111 082005 23974 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8162657657061473938 src: /10.251.26.131:35484 dest: /10.251.26.131:50010
081111 082039 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.66.3:50010 is added to blk_-4970213618915792510 size 67108864
081111 082108 24348 INFO dfs.DataNode$DataXceiver: Receiving block blk_9034824096129858680 src: /10.250.6.223:45371 dest: /10.250.6.223:50010
081111 082322 24292 INFO dfs.DataNode$PacketResponder: Received block blk_-4775386099844009890 of size 67108864 from /10.251.107.19
081111 082527 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_200811101024_0014_m_001049_0/part-01049. blk_-1533191386601391937
081111 082528 24365 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2096199637324483321 terminating
081111 082541 24284 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_3433046099598424314 terminating
081111 082650 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.13.240:50010 is added to blk_4170936278386288557 size 67108864
081111 082730 24531 INFO dfs.DataNode$PacketResponder: Received block blk_-3023983271692989109 of size 67108864 from /10.251.110.68
081111 082809 24442 INFO dfs.DataNode$DataXceiver: Receiving block blk_469126362137371425 src: /10.251.199.245:53897 dest: /10.251.199.245:50010
081111 082902 24331 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_3922791077126964673 terminating
081111 082952 24363 INFO dfs.DataNode$PacketResponder: Received block blk_5813923864071791162 of size 67108864 from /10.251.125.193
081111 082955 24432 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_7412933088374169877 terminating
081111 083120 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.192:50010 is added to blk_-3888538223380979891 size 67108864
081111 083143 24261 INFO dfs.DataNode$PacketResponder: Received block blk_-5970165518655489569 of size 67108864 from /10.250.14.196
081111 083148 24344 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-7319442762668598889 terminating
081111 083212 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_200811101024_0014_m_000973_0/part-00973. blk_5844313985011885282
081111 083219 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_200811101024_0014_m_001348_0/part-01348. blk_4531288786306744275
081111 083336 24444 INFO dfs.DataNode$PacketResponder: Received block blk_-3845286823494684378 of size 67108864 from /10.251.107.196
081111 083338 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.129:50010 is added to blk_6170586659371027531 size 67108864
081111 083418 24490 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-808401720390253471 terminating
081111 083426 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.123.33:50010 is added to blk_1447281351164457761 size 67108864
081111 083427 24635 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7911408976820434495 src: /10.251.42.191:35834 dest: /10.251.42.191:50010
081111 083439 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.89.155:50010 is added to blk_-5455316994110506037 size 67108864
081111 083443 24635 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-917268749136585019 terminating
081111 083506 24609 INFO dfs.DataNode$DataXceiver: Receiving block blk_1122812501843755360 src: /10.251.198.196:51036 dest: /10.251.198.196:50010
081111 083742 24575 INFO dfs.DataNode$DataXceiver: Receiving block blk_5983149244832683042 src: /10.251.42.191:52748 dest: /10.251.42.191:50010
081111 083753 24539 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-5263787695149681326 terminating
081111 083755 24394 INFO dfs.DataNode$DataXceiver: Receiving block blk_5781019224640206392 src: /10.250.11.194:37060 dest: /10.250.11.194:50010
081111 083839 24534 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_4786794545180705888 terminating
081111 083839 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_1303107722036031233 size 67108864
081111 083955 24547 INFO dfs.DataNode$PacketResponder: Received block blk_4539833452749813987 of size 67108864 from /10.251.42.16
081111 084003 24758 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_703387617548630438 terminating
081111 084102 24677 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_5689516499354445029 terminating
081111 084148 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.100:50010 is added to blk_-4595094649363595358 size 67108864
081111 084149 24657 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1733026139299872187 src: /10.251.29.239:52800 dest: /10.251.29.239:50010
081111 084214 24555 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1910608563003942187 terminating
081111 084228 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_200811101024_0014_m_001394_0/part-01394. blk_-1101091099241306483
081111 084233 24661 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_7901008236334420364 terminating
081111 084322 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.42.207:50010 is added to blk_2119401975697524206 size 67108864
081111 084405 24786 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_6204056282927975796 terminating
081111 084423 24772 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_215466486430758887 terminating
081111 084619 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_200811101024_0014_m_001736_0/part-01736. blk_-3055230140330616860
081111 084721 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_4473390551542202999 size 67108864
081111 084728 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_200811101024_0014_m_001649_0/part-01649. blk_-5581540863377925517
081111 084732 24650 INFO dfs.DataNode$PacketResponder: Received block blk_7864850143895703065 of size 67108864 from /10.251.107.98
081111 084757 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.91.32:50010 is added to blk_423916008836295480 size 67108864
081111 084759 24704 INFO dfs.DataNode$DataXceiver: Receiving block blk_2166777223671724319 src: /10.251.214.175:52741 dest: /10.251.214.175:50010
081111 084818 25055 INFO dfs.DataNode$PacketResponder: Received block blk_4031738712701565209 of size 67108864 from /10.251.199.150
081111 084831 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_200811101024_0014_m_001940_0/part-01940. blk_1910319264393500537
081111 084842 24787 INFO dfs.DataNode$PacketResponder: Received block blk_-8789402864074203800 of size 67108864 from /10.251.193.224
081111 084906 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.17.177:50010 is added to blk_5733038550092620376 size 67108864
081111 085059 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_200811101024_0014_m_001642_0/part-01642. blk_8215843708467112607
081111 085146 24834 INFO dfs.DataNode$PacketResponder: Received block blk_-954856062493437881 of size 67108864 from /10.251.106.10
081111 085221 24607 INFO dfs.DataNode$PacketResponder: Received block blk_241585185788373386 of size 67108864 from /10.251.111.80
081111 085351 24958 INFO dfs.DataNode$DataXceiver: Receiving block blk_3808164640986377400 src: /10.251.75.79:36685 dest: /10.251.75.79:50010
081111 085429 25314 INFO dfs.DataNode$PacketResponder: Received block blk_-6719337521981113643 of size 3538321 from /10.250.15.198
081111 085432 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.197.226:50010 is added to blk_-23492739449820023 size 67108864
081111 085502 24832 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-8969188698416564850 terminating
081111 085529 24695 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-5144548163293169177 terminating
081111 085620 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.122.79:50010 is added to blk_-2895171018912807248 size 67108864
081111 085634 24881 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5892603381347650390 src: /10.250.14.143:57484 dest: /10.250.14.143:50010
081111 085634 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_200811101024_0014_m_002007_0/part-02007. blk_7908524857056299716
081111 085859 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_3492869167398569662 is added to invalidSet of 10.250.5.161:50010
081111 085906 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3894803026385053344 is added to invalidSet of 10.251.43.21:50010
081111 085906 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7873581794069372986 is added to invalidSet of 10.251.194.147:50010
081111 085907 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2060553104861908174 is added to invalidSet of 10.251.201.204:50010
081111 085909 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6783506473540833076 is added to invalidSet of 10.251.202.209:50010
081111 085910 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8863130543753323383 is added to invalidSet of 10.251.123.1:50010
081111 085911 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8769398708439834546 is added to invalidSet of 10.251.127.243:50010
081111 085912 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-3316519570098743145 is added to invalidSet of 10.251.70.211:50010
081111 085912 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6068451475575444798 is added to invalidSet of 10.251.43.210:50010
081111 085912 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7709713535298137702 is added to invalidSet of 10.251.215.50:50010
081111 085914 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1030832046197982436 is added to invalidSet of 10.251.89.155:50010
081111 085914 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_4738781172941319823 is added to invalidSet of 10.251.91.32:50010
081111 085916 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2869833078504290822 is added to invalidSet of 10.251.123.132:50010
081111 085916 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-6261136177206340449 is added to invalidSet of 10.251.70.37:50010
081111 085917 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1755309160709170318 is added to invalidSet of 10.251.39.209:50010
081111 085917 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-8312238797251504127 is added to invalidSet of 10.251.89.155:50010
081111 085922 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_2713941272199175167 is added to invalidSet of 10.251.38.53:50010
081111 085922 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-7846283874350947646 is added to invalidSet of 10.251.199.150:50010
081111 085924 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-793506372912437289 is added to invalidSet of 10.250.17.225:50010
081111 085926 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-5293091556505218722 is added to invalidSet of 10.251.125.237:50010
081111 085929 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_4516306414837452219 is added to invalidSet of 10.251.127.47:50010
081111 090008 19 INFO dfs.FSDataset: Deleting block blk_-8792837405349224102 file /mnt/hadoop/dfs/data/current/subdir35/blk_-8792837405349224102
081111 090031 19 INFO dfs.FSDataset: Deleting block blk_-8131422413299851907 file /mnt/hadoop/dfs/data/current/subdir46/blk_-8131422413299851907
081111 090034 24870 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-2268477281422551537 terminating
081111 090056 19 INFO dfs.FSDataset: Deleting block blk_-6177194341369822651 file /mnt/hadoop/dfs/data/current/subdir38/blk_-6177194341369822651
081111 090113 18 INFO dfs.FSDataset: Deleting block blk_1369784431092292099 file /mnt/hadoop/dfs/data/current/subdir36/blk_1369784431092292099
081111 090113 19 INFO dfs.FSDataset: Deleting block blk_-8060252929692927663 file /mnt/hadoop/dfs/data/current/subdir21/blk_-8060252929692927663
081111 090118 19 INFO dfs.FSDataset: Deleting block blk_-5344266425789048231 file /mnt/hadoop/dfs/data/current/subdir47/blk_-5344266425789048231
081111 090126 19 INFO dfs.FSDataset: Deleting block blk_-2518425063392446785 file /mnt/hadoop/dfs/data/current/subdir6/blk_-2518425063392446785
081111 090129 19 INFO dfs.FSDataset: Deleting block blk_-5598693099021996971 file /mnt/hadoop/dfs/data/current/subdir19/blk_-5598693099021996971
081111 090131 19 INFO dfs.FSDataset: Deleting block blk_901963876338651405 file /mnt/hadoop/dfs/data/current/subdir1/blk_901963876338651405
081111 090136 19 INFO dfs.FSDataset: Deleting block blk_-2722768686407017386 file /mnt/hadoop/dfs/data/current/subdir36/blk_-2722768686407017386
081111 090138 19 INFO dfs.FSDataset: Deleting block blk_92946806844541836 file /mnt/hadoop/dfs/data/current/subdir59/blk_92946806844541836
081111 090139 19 INFO dfs.FSDataset: Deleting block blk_1824928871191121429 file /mnt/hadoop/dfs/data/current/subdir2/blk_1824928871191121429
081111 090140 18 INFO dfs.FSDataset: Deleting block blk_-4301178988618507084 file /mnt/hadoop/dfs/data/current/subdir46/blk_-4301178988618507084
081111 090140 19 INFO dfs.FSDataset: Deleting block blk_-2098161025946048013 file /mnt/hadoop/dfs/data/current/subdir17/blk_-2098161025946048013
081111 090149 19 INFO dfs.FSDataset: Deleting block blk_-1451819169209310732 file /mnt/hadoop/dfs/data/current/subdir23/blk_-1451819169209310732
081111 090150 19 INFO dfs.FSDataset: Deleting block blk_2403370969415186087 file /mnt/hadoop/dfs/data/current/subdir6/blk_2403370969415186087
081111 090154 19 INFO dfs.FSDataset: Deleting block blk_5701964561533648583 file /mnt/hadoop/dfs/data/current/subdir10/blk_5701964561533648583
081111 090159 19 INFO dfs.FSDataset: Deleting block blk_4007982883396598426 file /mnt/hadoop/dfs/data/current/subdir13/blk_4007982883396598426
081111 090207 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.197.161:50010 is added to blk_-3724350946705225881 size 67108864
081111 090208 19 INFO dfs.FSDataset: Deleting block blk_6051144263842600010 file /mnt/hadoop/dfs/data/current/subdir42/blk_6051144263842600010
081111 090223 19 INFO dfs.FSDataset: Deleting block blk_8436095180790908547 file /mnt/hadoop/dfs/data/current/subdir43/blk_8436095180790908547
081111 090229 18 INFO dfs.FSDataset: Deleting block blk_5100975846124291571 file /mnt/hadoop/dfs/data/current/subdir38/blk_5100975846124291571
081111 090246 19 INFO dfs.FSDataset: Deleting block blk_-4941547144875557718 file /mnt/hadoop/dfs/data/current/subdir61/blk_-4941547144875557718
081111 090339 24904 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_6791628131402590260 terminating
081111 090816 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.196:50010 is added to blk_-6308508598639278892 size 67108864
081111 090818 25109 INFO dfs.DataNode$DataXceiver: Receiving block blk_6585429064641693815 src: /10.250.10.6:38158 dest: /10.250.10.6:50010
081111 091002 25162 INFO dfs.DataNode$PacketResponder: Received block blk_6000837850922531821 of size 67108864 from /10.251.197.226
081111 091027 25209 INFO dfs.DataNode$PacketResponder: Received block blk_7330803663603700184 of size 67108864 from /10.251.42.246
081111 091200 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_000480_0/part-00480. blk_-5613694521084380933
081111 091244 19683 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4967783186426171722 src: /10.251.39.192:57105 dest: /10.251.39.192:50010
081111 091317 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.100:50010 is added to blk_-1970947032352756307 size 67108864
081111 091351 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_000737_0/part-00737. blk_5894088883762961800
081111 091439 25116 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4861262918414696307 src: /10.251.198.196:34168 dest: /10.251.198.196:50010
081111 091555 25051 INFO dfs.DataNode$DataXceiver: Receiving block blk_2583600095045026513 src: /10.250.15.67:59629 dest: /10.250.15.67:50010
081111 091647 25353 INFO dfs.DataNode$PacketResponder: Received block blk_7872746846750105239 of size 67108864 from /10.251.43.21
081111 091728 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.39.160:50010 is added to blk_-3510473878877779134 size 67108864
081111 091733 19 INFO dfs.FSNamesystem: BLOCK* ask 10.251.126.5:50010 to delete  blk_-9016567407076718172 blk_-8695715290502978219 blk_-7168328752988473716 blk_-4355192005224403537 blk_-3757501769775889193 blk_-154600013573668394 blk_167132135416677587 blk_2654596473569751784 blk_5202581916713319258
081111 091742 19 INFO dfs.FSDataset: Deleting block blk_-2808875502459981198 file /mnt/hadoop/dfs/data/current/subdir3/blk_-2808875502459981198
081111 091837 24935 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_34905976850091068 terminating
081111 092003 25315 INFO dfs.DataNode$PacketResponder: Received block blk_4774383661473695119 of size 67108864 from /10.251.110.68
081111 092023 25367 INFO dfs.DataNode$DataXceiver: Receiving block blk_465845235850106928 src: /10.251.70.112:39499 dest: /10.251.70.112:50010
081111 092213 25595 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_8882415653769775762 terminating
081111 092239 25403 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_4207359722112667226 terminating
081111 092311 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_000882_0/part-00882. blk_-3152434225121092058
081111 092425 25331 INFO dfs.DataNode$DataXceiver: Receiving block blk_4820650745157199554 src: /10.251.127.243:49726 dest: /10.251.127.243:50010
081111 092436 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.6.223:50010 is added to blk_-6390296685077811803 size 3546314
081111 092617 19 INFO dfs.FSDataset: Deleting block blk_9173199815015538212 file /mnt/hadoop/dfs/data/current/subdir24/blk_9173199815015538212
081111 092649 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_001033_0/part-01033. blk_-8948316319329891645
081111 092720 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.42.207:50010 is added to blk_-6314365697827006048 size 67108864
081111 092751 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_000939_0/part-00939. blk_-3522956055443924791
081111 092821 25409 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_5454423648092670683 terminating
081111 092915 25283 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_4565822428702518889 terminating
081111 093026 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.29.239:50010 is added to blk_-409558225151784282 size 67108864
081111 093208 25332 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-3967261531426400230 terminating
081111 093316 25596 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4655280636272058732 src: /10.251.26.8:48892 dest: /10.251.26.8:50010
081111 093350 25537 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-5799866343936729403 terminating
081111 093407 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_001294_0/part-01294. blk_4214952053910973364
081111 093429 25813 INFO dfs.DataNode$DataXceiver: Receiving block blk_-6135061626100136978 src: /10.251.31.180:48900 dest: /10.251.31.180:50010
081111 093513 25793 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5057534532247887258 src: /10.250.5.237:45766 dest: /10.250.5.237:50010
081111 093550 25550 INFO dfs.DataNode$PacketResponder: Received block blk_9103049989653819836 of size 67108864 from /10.250.10.223
081111 093657 25789 INFO dfs.DataNode$DataXceiver: Receiving block blk_-4365612131648152536 src: /10.251.107.227:57736 dest: /10.251.107.227:50010
081111 093701 24237 INFO dfs.DataNode$DataXceiver: Receiving block blk_3598226258881465924 src: /10.251.199.159:41752 dest: /10.251.199.159:50010
081111 093703 25622 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_3378157549705391333 terminating
081111 093742 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-8347949825960763812
081111 093814 25929 INFO dfs.DataNode$PacketResponder: Received block blk_-2934265883100282310 of size 67108864 from /10.251.39.144
081111 093823 25642 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_1259893634804870815 terminating
081111 093913 25312 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_412956398445037066 terminating
081111 093952 25849 INFO dfs.DataNode$PacketResponder: Received block blk_-9167880724995702718 of size 67108864 from /10.251.214.225
081111 094008 26005 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7764040409370932316 src: /10.251.125.237:58496 dest: /10.251.125.237:50010
081111 094052 25242 INFO dfs.DataNode$DataXceiver: Receiving block blk_931297048899943584 src: /10.251.67.113:54324 dest: /10.251.67.113:50010
081111 094221 25770 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_2833019093942278701 terminating
081111 094319 25340 INFO dfs.DataNode$PacketResponder: Received block blk_-5785220384376578831 of size 67108864 from /10.251.203.80
081111 094331 26028 INFO dfs.DataNode$PacketResponder: Received block blk_3386670145292909259 of size 67108864 from /10.251.110.68
081111 094415 25901 INFO dfs.DataNode$DataXceiver: Receiving block blk_1859772752119590543 src: /10.251.42.246:57741 dest: /10.251.42.246:50010
081111 094439 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_001425_0/part-01425. blk_2630223487595425525
081111 094633 25926 INFO dfs.DataNode$PacketResponder: Received block blk_-5385105076403314787 of size 67108864 from /10.251.42.9
081111 094713 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_001757_0/part-01757. blk_-5343533992609553091
081111 094825 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_001473_0/part-01473. blk_-4390972361898978900
081111 094832 26039 INFO dfs.DataNode$PacketResponder: Received block blk_1190791818440776717 of size 67108864 from /10.251.43.147
081111 094918 25951 INFO dfs.DataNode$PacketResponder: Received block blk_-7202857144668308473 of size 67108864 from /10.251.43.21
081111 094921 25993 INFO dfs.DataNode$DataXceiver: Receiving block blk_5565611615515783341 src: /10.251.126.5:35656 dest: /10.251.126.5:50010
081111 094930 26178 INFO dfs.DataNode$PacketResponder: Received block blk_-3283153974358895314 of size 67108864 from /10.251.111.228
081111 094936 25537 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1484647023667047838 terminating
081111 094939 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.6.223:50010 is added to blk_7474737521119026445 size 67108864
081111 095020 25962 INFO dfs.DataNode$DataXceiver: Receiving block blk_-8571819028995448536 src: /10.251.197.226:34801 dest: /10.251.197.226:50010
081111 095035 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.214.67:50010 is added to blk_-8428590803682810851 size 67108864
081111 095039 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_001860_0/part-01860. blk_8716294289715825928
081111 095238 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.123.132:50010 is added to blk_8527475857502481768 size 67108864
081111 095309 26010 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_406835586147450451 terminating
081111 095434 26090 INFO dfs.DataNode$PacketResponder: Received block blk_7294821275446427348 of size 67108864 from /10.251.43.210
081111 095535 28 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.127.243:50010 is added to blk_1793140687921032046 size 67108864
081111 095618 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.126.5:50010 is added to blk_4361294871479973840 size 67108864
081111 095632 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.6.4:50010 is added to blk_-6945615463687647586 size 67108864
081111 095636 26319 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_1216611589160220108 terminating
081111 095653 25890 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3265479347842446682 src: /10.250.14.224:47278 dest: /10.250.14.224:50010
081111 095702 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk_8527562124953828227 size 67108864
081111 095726 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.30.134:50010 is added to blk_2749066163012162435 size 67108864
081111 095733 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.228:50010 is added to blk_-1305222006484630743 size 67108864
081111 095813 26362 INFO dfs.DataNode$PacketResponder: Received block blk_-3702595599317472079 of size 67108864 from /10.251.25.237
081111 095840 26225 INFO dfs.DataNode$PacketResponder: Received block blk_6446927133528675675 of size 67108864 from /10.251.39.209
081111 095844 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.215.192:50010 is added to blk_2015610615789582788 size 67108864
081111 095957 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.75.49:50010 is added to blk_-2959268996938658555 size 15715490
081111 100210 19 INFO dfs.FSDataset: Deleting block blk_-1082541280306680938 file /mnt/hadoop/dfs/data/current/subdir38/blk_-1082541280306680938
081111 100223 26181 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-191333338640084691 terminating
081111 100226 26261 INFO dfs.DataNode$DataXceiver: Receiving block blk_3972778210951456006 src: /10.251.121.224:56526 dest: /10.251.121.224:50010
081111 100245 26152 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_1408672604432845193 terminating
081111 100323 14118 INFO dfs.DataNode$PacketResponder: Received block blk_7679838117000095334 of size 67108864 from /10.251.30.85
081111 100350 32 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.13.240:50010 is added to blk_2593937801738981947 size 67108864
081111 100414 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.193:50010 is added to blk_5489815612272797790 size 67108864
081111 100417 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.30.101:50010 is added to blk_6451403582950672007 size 67108864
081111 100646 26268 INFO dfs.DataNode$DataXceiver: Receiving block blk_-7048088870427586736 src: /10.250.10.100:56512 dest: /10.250.10.100:50010
081111 100729 26320 INFO dfs.DataNode$PacketResponder: Received block blk_7589872946955471867 of size 67108864 from /10.251.195.70
081111 100752 26527 INFO dfs.DataNode$DataXceiver: Receiving block blk_-178934379749864379 src: /10.251.71.97:55517 dest: /10.251.71.97:50010
081111 100820 26329 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_2026200052147887341 terminating
081111 100824 26391 INFO dfs.DataNode$DataXceiver: Receiving block blk_8303284829424905326 src: /10.251.70.37:47359 dest: /10.251.70.37:50010
081111 100903 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.130:50010 is added to blk_5646792755154529338 size 67108864
081111 101115 26281 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_712730845180531820 terminating
081111 101117 26526 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8418106412701718933 terminating
081111 101120 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.67.225:50010 is added to blk_-6325232815283133921 size 67108864
081111 101131 26402 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-800664075087524591 terminating
20171223-22:15:29:606|Step_LSC|30002312|onStandStepChanged 3579
20171223-22:15:29:615|Step_LSC|30002312|onExtend:1514038530000 14 0 4
20171223-22:15:29:633|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:15:29:635|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:15:29:635|Step_StandStepCounter|30002312|flush sensor data
20171223-22:15:29:635|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##6993##548365##8661##12266##27164404
20171223-22:15:29:636|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7007##548365##8661##12361##27173954
20171223-22:15:29:636|Step_LSC|30002312|onStandStepChanged 3579
20171223-22:15:29:645|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=126775
20171223-22:15:29:648|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:29:649|Step_StandReportReceiver|30002312|REPORT : 7007 5002 150089 240
20171223-22:15:29:737|Step_LSC|30002312|onExtend:1514038530000 0 0 4
20171223-22:15:29:738|Step_LSC|30002312|onStandStepChanged 3579
20171223-22:15:29:792|Step_LSC|30002312|onStandStepChanged 3580
20171223-22:15:29:800|Step_LSC|30002312|onExtend:1514038530000 1 0 4
20171223-22:15:29:950|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7007##548365##8661##12361##27173954
20171223-22:15:29:950|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7008##548365##8661##12456##27174269
20171223-22:15:29:959|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=126797
20171223-22:15:29:962|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:29:962|Step_StandReportReceiver|30002312|REPORT : 7008 5003 150111 240
20171223-22:15:30:331|Step_LSC|30002312|onStandStepChanged 3581
20171223-22:15:30:335|Step_LSC|30002312|onExtend:1514038531000 1 0 4
20171223-22:15:30:632|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7008##548365##8661##12456##27174269
20171223-22:15:30:632|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7009##548365##8661##12551##27174951
20171223-22:15:30:639|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=126818
20171223-22:15:30:641|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:30:642|Step_StandReportReceiver|30002312|REPORT : 7009 5004 150132 240
20171223-22:15:30:841|Step_LSC|30002312|onStandStepChanged 3583
20171223-22:15:30:858|Step_LSC|30002312|onExtend:1514038531000 2 0 4
20171223-22:15:31:142|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7009##548365##8661##12551##27174951
20171223-22:15:31:143|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7011##548365##8661##12646##27175461
20171223-22:15:31:157|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=126861
20171223-22:15:31:160|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:31:160|Step_StandReportReceiver|30002312|REPORT : 7011 5005 150175 240
20171223-22:15:31:841|Step_LSC|30002312|onStandStepChanged 3584
20171223-22:15:31:862|Step_LSC|30002312|onExtend:1514038532000 1 0 4
20171223-22:15:32:145|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7011##548365##8661##12646##27175461
20171223-22:15:32:147|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7012##548365##8661##12741##27176464
20171223-22:15:32:156|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=126882
20171223-22:15:32:162|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:32:163|Step_StandReportReceiver|30002312|REPORT : 7012 5006 150197 240
20171223-22:15:32:340|Step_LSC|30002312|onStandStepChanged 3585
20171223-22:15:32:354|Step_LSC|30002312|onExtend:1514038533000 1 0 4
20171223-22:15:32:647|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7012##548365##8661##12741##27176464
20171223-22:15:32:648|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7013##548365##8661##12836##27176966
20171223-22:15:32:656|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=126904
20171223-22:15:32:658|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:32:659|Step_StandReportReceiver|30002312|REPORT : 7013 5007 150218 240
20171223-22:15:32:840|Step_LSC|30002312|onStandStepChanged 3586
20171223-22:15:32:846|Step_LSC|30002312|onExtend:1514038533000 1 0 4
20171223-22:15:33:144|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7013##548365##8661##12836##27176966
20171223-22:15:33:144|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7014##548365##8661##12931##27177463
20171223-22:15:33:148|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=126925
20171223-22:15:33:149|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:33:149|Step_StandReportReceiver|30002312|REPORT : 7014 5007 150239 240
20171223-22:15:33:341|Step_LSC|30002312|onStandStepChanged 3587
20171223-22:15:33:370|Step_LSC|30002312|onExtend:1514038534000 1 0 4
20171223-22:15:33:643|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7014##548365##8661##12931##27177463
20171223-22:15:33:644|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7015##548365##8661##13026##27177962
20171223-22:15:33:658|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=126947
20171223-22:15:33:662|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:33:663|Step_StandReportReceiver|30002312|REPORT : 7015 5008 150261 240
20171223-22:15:34:711|Step_LSC|30002312|onStandStepChanged 3588
20171223-22:15:34:723|Step_LSC|30002312|onExtend:1514038535000 1 0 4
20171223-22:15:34:723|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:15:34:848|Step_LSC|30002312|onStandStepChanged 3589
20171223-22:15:34:851|Step_LSC|30002312|onExtend:1514038535000 1 0 4
20171223-22:15:35:11|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7015##548365##8661##13026##27177962
20171223-22:15:35:12|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7017##548365##8661##13121##27179330
20171223-22:15:35:20|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=126989
20171223-22:15:35:22|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:35:23|Step_ScreenUtil|30002312|isScreenOn true
20171223-22:15:35:23|Step_StandReportReceiver|30002312|screen status unknown,think screen on
20171223-22:15:35:23|Step_StandReportReceiver|30002312|REPORT : 7017 5010 150304 240
20171223-22:15:35:96|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:15:35:97|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:15:35:97|Step_StandStepCounter|30002312|flush sensor data
20171223-22:15:35:98|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7017##548365##8661##13121##27179330
20171223-22:15:35:98|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7017##548365##8661##13216##27179417
20171223-22:15:35:98|Step_LSC|30002312|onStandStepChanged 3589
20171223-22:15:35:104|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=126989
20171223-22:15:35:112|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:35:112|Step_StandReportReceiver|30002312|REPORT : 7017 5010 150304 240
20171223-22:15:35:198|Step_LSC|30002312|onStandStepChanged 3589
20171223-22:15:35:201|Step_LSC|30002312|onExtend:1514038536000 0 0 4
20171223-22:15:35:348|Step_LSC|30002312|onStandStepChanged 3590
20171223-22:15:35:352|Step_LSC|30002312|onExtend:1514038536000 1 0 4
20171223-22:15:35:413|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7017##548365##8661##13216##27179417
20171223-22:15:35:413|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7018##548365##8661##13311##27179732
20171223-22:15:35:418|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127011
20171223-22:15:35:421|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:35:421|Step_StandReportReceiver|30002312|REPORT : 7018 5010 150325 240
20171223-22:15:35:848|Step_LSC|30002312|onStandStepChanged 3591
20171223-22:15:35:852|Step_LSC|30002312|onExtend:1514038536000 1 0 4
20171223-22:15:36:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7018##548365##8661##13311##27179732
20171223-22:15:36:149|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7019##548365##8661##13406##27180468
20171223-22:15:36:154|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127032
20171223-22:15:36:157|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:36:157|Step_StandReportReceiver|30002312|REPORT : 7019 5011 150346 240
20171223-22:15:36:356|Step_LSC|30002312|onStandStepChanged 3592
20171223-22:15:36:365|Step_LSC|30002312|onExtend:1514038537000 1 0 4
20171223-22:15:36:657|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7019##548365##8661##13406##27180468
20171223-22:15:36:657|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7020##548365##8661##13501##27180976
20171223-22:15:36:668|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127054
20171223-22:15:36:672|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:36:673|Step_StandReportReceiver|30002312|REPORT : 7020 5012 150368 240
20171223-22:15:37:351|Step_LSC|30002312|onStandStepChanged 3593
20171223-22:15:37:361|Step_LSC|30002312|onExtend:1514038538000 1 0 4
20171223-22:15:37:652|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7020##548365##8661##13501##27180976
20171223-22:15:37:653|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7021##548365##8661##13596##27181971
20171223-22:15:37:666|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127075
20171223-22:15:37:670|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:37:671|Step_StandReportReceiver|30002312|REPORT : 7021 5012 150389 240
20171223-22:15:37:848|Step_LSC|30002312|onStandStepChanged 3594
20171223-22:15:37:858|Step_LSC|30002312|onExtend:1514038538000 1 0 4
20171223-22:15:38:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7021##548365##8661##13596##27181971
20171223-22:15:38:149|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7022##548365##8661##13691##27182468
20171223-22:15:38:159|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127097
20171223-22:15:38:164|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:38:164|Step_StandReportReceiver|30002312|REPORT : 7022 5013 150411 240
20171223-22:15:38:349|Step_LSC|30002312|onStandStepChanged 3595
20171223-22:15:38:361|Step_LSC|30002312|onExtend:1514038539000 1 0 4
20171223-22:15:38:653|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7022##548365##8661##13691##27182468
20171223-22:15:38:654|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7023##548365##8661##13786##27182973
20171223-22:15:38:665|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127118
20171223-22:15:38:669|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:38:669|Step_StandReportReceiver|30002312|REPORT : 7023 5014 150432 240
20171223-22:15:38:850|Step_LSC|30002312|onStandStepChanged 3596
20171223-22:15:38:868|Step_LSC|30002312|onExtend:1514038539000 1 0 4
20171223-22:15:39:150|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7023##548365##8661##13786##27182973
20171223-22:15:39:151|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7024##548365##8661##13881##27183469
20171223-22:15:39:155|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127139
20171223-22:15:39:156|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:39:156|Step_StandReportReceiver|30002312|REPORT : 7024 5015 150454 240
20171223-22:15:39:348|Step_LSC|30002312|onStandStepChanged 3597
20171223-22:15:39:354|Step_LSC|30002312|onExtend:1514038540000 1 0 4
20171223-22:15:39:655|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7024##548365##8661##13881##27183469
20171223-22:15:39:656|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7025##548365##8661##13976##27183974
20171223-22:15:39:665|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127161
20171223-22:15:39:669|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:39:669|Step_StandReportReceiver|30002312|REPORT : 7025 5015 150475 240
20171223-22:15:40:349|Step_LSC|30002312|onStandStepChanged 3598
20171223-22:15:40:365|Step_LSC|30002312|onExtend:1514038541000 1 0 4
20171223-22:15:40:652|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7025##548365##8661##13976##27183974
20171223-22:15:40:653|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7026##548365##8661##14071##27184972
20171223-22:15:40:668|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127182
20171223-22:15:40:675|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:40:676|Step_StandReportReceiver|30002312|REPORT : 7026 5016 150496 240
20171223-22:15:40:848|Step_LSC|30002312|onStandStepChanged 3599
20171223-22:15:40:852|Step_LSC|30002312|onExtend:1514038541000 1 0 4
20171223-22:15:41:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7026##548365##8661##14071##27184972
20171223-22:15:41:149|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7027##548365##8661##14166##27185467
20171223-22:15:41:152|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127204
20171223-22:15:41:153|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:41:153|Step_StandReportReceiver|30002312|REPORT : 7027 5017 150518 240
20171223-22:15:41:353|Step_LSC|30002312|onStandStepChanged 3600
20171223-22:15:41:358|Step_LSC|30002312|onExtend:1514038542000 1 0 4
20171223-22:15:41:658|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7027##548365##8661##14166##27185467
20171223-22:15:41:658|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7028##548365##8661##14261##27185977
20171223-22:15:41:667|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127225
20171223-22:15:41:670|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:41:672|Step_StandReportReceiver|30002312|REPORT : 7028 5017 150539 240
20171223-22:15:41:848|Step_LSC|30002312|onStandStepChanged 3601
20171223-22:15:41:856|Step_LSC|30002312|onExtend:1514038542000 1 0 4
20171223-22:15:42:150|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7028##548365##8661##14261##27185977
20171223-22:15:42:151|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7029##548365##8661##14356##27186469
20171223-22:15:42:160|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127246
20171223-22:15:42:164|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:42:165|Step_StandReportReceiver|30002312|REPORT : 7029 5018 150561 240
20171223-22:15:42:848|Step_LSC|30002312|onStandStepChanged 3602
20171223-22:15:42:852|Step_LSC|30002312|onExtend:1514038543000 1 0 4
20171223-22:15:43:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7029##548365##8661##14356##27186469
20171223-22:15:43:149|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7030##548365##8661##14451##27187468
20171223-22:15:43:160|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127268
20171223-22:15:43:163|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:43:164|Step_StandReportReceiver|30002312|REPORT : 7030 5019 150582 240
20171223-22:15:43:348|Step_LSC|30002312|onStandStepChanged 3603
20171223-22:15:43:354|Step_LSC|30002312|onExtend:1514038544000 1 0 4
20171223-22:15:43:649|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7030##548365##8661##14451##27187468
20171223-22:15:43:650|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7031##548365##8661##14546##27187968
20171223-22:15:43:677|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127289
20171223-22:15:43:679|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:43:679|Step_StandReportReceiver|30002312|REPORT : 7031 5020 150604 240
20171223-22:15:43:850|Step_LSC|30002312|onStandStepChanged 3604
20171223-22:15:43:861|Step_LSC|30002312|onExtend:1514038544000 1 0 4
20171223-22:15:44:152|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7031##548365##8661##14546##27187968
20171223-22:15:44:152|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7032##548365##8661##14641##27188471
20171223-22:15:44:163|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127311
20171223-22:15:44:166|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:44:170|Step_StandReportReceiver|30002312|REPORT : 7032 5020 150625 240
20171223-22:15:44:849|Step_LSC|30002312|onStandStepChanged 3605
20171223-22:15:44:863|Step_LSC|30002312|onExtend:1514038545000 1 0 4
20171223-22:15:45:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7032##548365##8661##14641##27188471
20171223-22:15:45:150|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7033##548365##8661##14736##27189469
20171223-22:15:45:158|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127332
20171223-22:15:45:161|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:45:164|Step_StandReportReceiver|30002312|REPORT : 7033 5021 150646 240
20171223-22:15:45:348|Step_LSC|30002312|onStandStepChanged 3606
20171223-22:15:45:361|Step_LSC|30002312|onExtend:1514038546000 1 0 4
20171223-22:15:45:649|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7033##548365##8661##14736##27189469
20171223-22:15:45:650|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7034##548365##8661##14831##27189969
20171223-22:15:45:656|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127354
20171223-22:15:45:658|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:45:659|Step_StandReportReceiver|30002312|REPORT : 7034 5022 150668 240
20171223-22:15:45:848|Step_LSC|30002312|onStandStepChanged 3607
20171223-22:15:45:851|Step_LSC|30002312|onExtend:1514038546000 1 0 4
20171223-22:15:46:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7034##548365##8661##14831##27189969
20171223-22:15:46:150|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7035##548365##8661##14926##27190469
20171223-22:15:46:157|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127375
20171223-22:15:46:160|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:46:164|Step_StandReportReceiver|30002312|REPORT : 7035 5022 150689 240
20171223-22:15:46:349|Step_LSC|30002312|onStandStepChanged 3608
20171223-22:15:46:355|Step_LSC|30002312|onExtend:1514038547000 1 0 4
20171223-22:15:46:650|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7035##548365##8661##14926##27190469
20171223-22:15:46:651|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7036##548365##8661##15021##27190970
20171223-22:15:46:674|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127396
20171223-22:15:46:679|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:46:684|Step_StandReportReceiver|30002312|REPORT : 7036 5023 150711 240
20171223-22:15:47:349|Step_LSC|30002312|onStandStepChanged 3609
20171223-22:15:47:370|Step_LSC|30002312|onExtend:1514038548000 1 0 4
20171223-22:15:47:650|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7036##548365##8661##15021##27190970
20171223-22:15:47:650|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7037##548365##8661##15116##27191969
20171223-22:15:47:658|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127418
20171223-22:15:47:661|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:47:661|Step_StandReportReceiver|30002312|REPORT : 7037 5024 150732 240
20171223-22:15:47:857|Step_LSC|30002312|onStandStepChanged 3610
20171223-22:15:47:860|Step_LSC|30002312|onExtend:1514038548000 1 0 4
20171223-22:15:48:158|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7037##548365##8661##15116##27191969
20171223-22:15:48:159|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7038##548365##8661##15211##27192478
20171223-22:15:48:167|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127439
20171223-22:15:48:169|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:48:174|Step_StandReportReceiver|30002312|REPORT : 7038 5025 150753 240
20171223-22:15:48:350|Step_LSC|30002312|onStandStepChanged 3611
20171223-22:15:48:368|Step_LSC|30002312|onExtend:1514038549000 1 0 4
20171223-22:15:48:652|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7038##548365##8661##15211##27192478
20171223-22:15:48:653|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7039##548365##8661##15306##27192971
20171223-22:15:48:668|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127461
20171223-22:15:48:676|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:48:679|Step_StandReportReceiver|30002312|REPORT : 7039 5025 150775 240
20171223-22:15:49:350|Step_LSC|30002312|onStandStepChanged 3612
20171223-22:15:49:366|Step_LSC|30002312|onExtend:1514038550000 1 0 4
20171223-22:15:49:652|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7039##548365##8661##15306##27192971
20171223-22:15:49:653|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7040##548365##8661##15401##27193971
20171223-22:15:49:668|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127482
20171223-22:15:49:675|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:49:676|Step_StandReportReceiver|30002312|REPORT : 7040 5026 150796 240
20171223-22:15:49:858|Step_LSC|30002312|onStandStepChanged 3613
20171223-22:15:49:864|Step_LSC|30002312|onExtend:1514038550000 1 0 4
20171223-22:15:50:160|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7040##548365##8661##15401##27193971
20171223-22:15:50:161|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7041##548365##8661##15496##27194480
20171223-22:15:50:176|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127504
20171223-22:15:50:181|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:50:182|Step_StandReportReceiver|30002312|REPORT : 7041 5027 150818 240
20171223-22:15:50:850|Step_LSC|30002312|onStandStepChanged 3614
20171223-22:15:50:864|Step_LSC|30002312|onExtend:1514038551000 1 0 4
20171223-22:15:51:151|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7041##548365##8661##15496##27194480
20171223-22:15:51:152|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7042##548365##8661##15591##27195470
20171223-22:15:51:169|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127525
20171223-22:15:51:180|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:51:181|Step_StandReportReceiver|30002312|REPORT : 7042 5027 150839 240
20171223-22:15:51:859|Step_LSC|30002312|onStandStepChanged 3616
20171223-22:15:51:868|Step_LSC|30002312|onExtend:1514038552000 2 0 4
20171223-22:15:52:160|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7042##548365##8661##15591##27195470
20171223-22:15:52:161|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7044##548365##8661##15686##27196480
20171223-22:15:52:175|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127568
20171223-22:15:52:183|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:52:183|Step_StandReportReceiver|30002312|REPORT : 7044 5029 150882 240
20171223-22:15:52:349|Step_LSC|30002312|onStandStepChanged 3617
20171223-22:15:52:363|Step_LSC|30002312|onExtend:1514038553000 1 0 4
20171223-22:15:52:651|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7044##548365##8661##15686##27196480
20171223-22:15:52:652|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7045##548365##8661##15781##27196970
20171223-22:15:52:666|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127589
20171223-22:15:52:674|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:52:674|Step_StandReportReceiver|30002312|REPORT : 7045 5030 150903 240
20171223-22:15:53:349|Step_LSC|30002312|onStandStepChanged 3619
20171223-22:15:53:358|Step_LSC|30002312|onExtend:1514038554000 2 0 4
20171223-22:15:53:653|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7045##548365##8661##15781##27196970
20171223-22:15:53:653|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7047##548365##8661##15876##27197972
20171223-22:15:53:665|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127632
20171223-22:15:53:673|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:53:675|Step_StandReportReceiver|30002312|REPORT : 7047 5031 150946 240
20171223-22:15:54:350|Step_LSC|30002312|onStandStepChanged 3620
20171223-22:15:54:359|Step_LSC|30002312|onExtend:1514038555000 1 0 4
20171223-22:15:54:651|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7047##548365##8661##15876##27197972
20171223-22:15:54:651|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7048##548365##8661##15971##27198970
20171223-22:15:54:657|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127653
20171223-22:15:54:660|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:54:661|Step_StandReportReceiver|30002312|REPORT : 7048 5032 150968 240
20171223-22:15:55:349|Step_LSC|30002312|onStandStepChanged 3622
20171223-22:15:55:359|Step_LSC|30002312|onExtend:1514038556000 2 0 4
20171223-22:15:55:650|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7048##548365##8661##15971##27198970
20171223-22:15:55:650|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7050##548365##8661##16066##27199969
20171223-22:15:55:657|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127696
20171223-22:15:55:659|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:55:660|Step_StandReportReceiver|30002312|REPORT : 7050 5033 151011 240
20171223-22:15:55:848|Step_LSC|30002312|onStandStepChanged 3624
20171223-22:15:55:855|Step_LSC|30002312|onExtend:1514038556000 2 0 4
20171223-22:15:56:150|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7050##548365##8661##16066##27199969
20171223-22:15:56:151|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7052##548365##8661##16161##27200470
20171223-22:15:56:163|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127739
20171223-22:15:56:169|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:56:170|Step_StandReportReceiver|30002312|REPORT : 7052 5035 151053 240
20171223-22:15:57:851|Step_LSC|30002312|onStandStepChanged 3625
20171223-22:15:57:863|Step_LSC|30002312|onExtend:1514038558000 1 0 4
20171223-22:15:58:155|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7052##548365##8661##16161##27200470
20171223-22:15:58:156|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7053##548365##8661##16256##27202475
20171223-22:15:58:170|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127761
20171223-22:15:58:174|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:15:58:175|Step_StandReportReceiver|30002312|REPORT : 7053 5035 151075 240
20171223-22:15:59:849|Step_LSC|30002312|onStandStepChanged 3626
20171223-22:15:59:865|Step_LSC|30002312|onExtend:1514038560000 1 0 4
20171223-22:16:0:119|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:16:0:150|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038440000##7053##548365##8661##16256##27202475
20171223-22:16:0:150|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038500000##7054##548458##8661##16256##27204469
20171223-22:16:0:159|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127782
20171223-22:16:0:161|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:16:0:162|Step_StandReportReceiver|30002312|REPORT : 7054 5036 151096 240
20171223-22:16:0:849|Step_LSC|30002312|onStandStepChanged 3626
20171223-22:16:0:859|Step_LSC|30002312|onExtend:1514038561000 0 0 0
20171223-22:16:1:150|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038500000##7054##548458##8661##16256##27204469
20171223-22:16:1:150|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038500000##7054##548551##8661##16256##27205469
20171223-22:16:1:165|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127782
20171223-22:16:1:172|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:0:123|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:17:18:850|Step_LSC|30002312|onStandStepChanged 3626
20171223-22:17:18:864|Step_LSC|30002312|onExtend:1514038639000 0 0 4
20171223-22:17:19:151|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038500000##7054##548551##8661##16256##27205469
20171223-22:17:19:152|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7054##548552##8661##16256##27283470
20171223-22:17:19:167|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127782
20171223-22:17:19:174|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:20:348|Step_LSC|30002312|onStandStepChanged 3628
20171223-22:17:20:358|Step_LSC|30002312|onExtend:1514038641000 2 0 4
20171223-22:17:20:652|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7054##548552##8661##16256##27283470
20171223-22:17:20:652|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7056##548553##8661##16256##27284971
20171223-22:17:20:662|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127824
20171223-22:17:20:669|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:20:675|Step_StandReportReceiver|30002312|REPORT : 7056 5037 151139 240
20171223-22:17:20:853|Step_LSC|30002312|onStandStepChanged 3630
20171223-22:17:20:859|Step_LSC|30002312|onExtend:1514038641000 2 0 4
20171223-22:17:21:156|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7056##548553##8661##16256##27284971
20171223-22:17:21:156|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7058##548554##8661##16256##27285475
20171223-22:17:21:162|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127867
20171223-22:17:21:164|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:21:165|Step_StandReportReceiver|30002312|REPORT : 7058 5039 151182 240
20171223-22:17:21:348|Step_LSC|30002312|onStandStepChanged 3632
20171223-22:17:21:353|Step_LSC|30002312|onExtend:1514038642000 2 0 4
20171223-22:17:21:649|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7058##548554##8661##16256##27285475
20171223-22:17:21:649|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7060##548555##8661##16256##27285968
20171223-22:17:21:659|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127910
20171223-22:17:21:663|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:21:668|Step_StandReportReceiver|30002312|REPORT : 7060 5040 151225 240
20171223-22:17:21:850|Step_LSC|30002312|onStandStepChanged 3634
20171223-22:17:21:859|Step_LSC|30002312|onExtend:1514038642000 2 0 4
20171223-22:17:22:151|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7060##548555##8661##16256##27285968
20171223-22:17:22:152|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7062##548556##8661##16256##27286470
20171223-22:17:22:162|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127953
20171223-22:17:22:166|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:22:170|Step_StandReportReceiver|30002312|REPORT : 7062 5042 151268 240
20171223-22:17:22:348|Step_LSC|30002312|onStandStepChanged 3636
20171223-22:17:22:363|Step_LSC|30002312|onExtend:1514038643000 2 0 4
20171223-22:17:22:650|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7062##548556##8661##16256##27286470
20171223-22:17:22:651|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7064##548557##8661##16256##27286970
20171223-22:17:22:661|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=127996
20171223-22:17:22:665|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:22:665|Step_StandReportReceiver|30002312|REPORT : 7064 5043 151310 240
20171223-22:17:22:849|Step_LSC|30002312|onStandStepChanged 3638
20171223-22:17:22:859|Step_LSC|30002312|onExtend:1514038643000 2 0 4
20171223-22:17:23:150|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7064##548557##8661##16256##27286970
20171223-22:17:23:150|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7066##548558##8661##16256##27287469
20171223-22:17:23:160|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128039
20171223-22:17:23:165|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:23:165|Step_StandReportReceiver|30002312|REPORT : 7066 5045 151353 240
20171223-22:17:23:348|Step_LSC|30002312|onStandStepChanged 3640
20171223-22:17:23:358|Step_LSC|30002312|onExtend:1514038644000 2 0 4
20171223-22:17:23:653|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7066##548558##8661##16256##27287469
20171223-22:17:23:653|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7068##548559##8661##16256##27287972
20171223-22:17:23:662|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128081
20171223-22:17:23:667|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:23:668|Step_StandReportReceiver|30002312|REPORT : 7068 5046 151396 240
20171223-22:17:23:851|Step_LSC|30002312|onStandStepChanged 3642
20171223-22:17:23:861|Step_LSC|30002312|onExtend:1514038644000 2 0 4
20171223-22:17:24:151|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7068##548559##8661##16256##27287972
20171223-22:17:24:152|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7070##548560##8661##16256##27288471
20171223-22:17:24:162|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128124
20171223-22:17:24:166|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:24:167|Step_StandReportReceiver|30002312|REPORT : 7070 5047 151439 240
20171223-22:17:24:348|Step_LSC|30002312|onStandStepChanged 3643
20171223-22:17:24:359|Step_LSC|30002312|onExtend:1514038645000 1 0 4
20171223-22:17:24:654|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7070##548560##8661##16256##27288471
20171223-22:17:24:654|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7071##548561##8661##16256##27288973
20171223-22:17:24:664|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128146
20171223-22:17:24:668|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:24:668|Step_StandReportReceiver|30002312|REPORT : 7071 5048 151460 240
20171223-22:17:24:848|Step_LSC|30002312|onStandStepChanged 3646
20171223-22:17:24:860|Step_LSC|30002312|onExtend:1514038645000 3 0 4
20171223-22:17:25:150|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7071##548561##8661##16256##27288973
20171223-22:17:25:151|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7074##548562##8661##16256##27289470
20171223-22:17:25:159|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128210
20171223-22:17:25:162|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:25:163|Step_StandReportReceiver|30002312|REPORT : 7074 5050 151525 240
20171223-22:17:27:855|Step_LSC|30002312|onStandStepChanged 3646
20171223-22:17:27:859|Step_LSC|30002312|onExtend:1514038648000 0 0 0
20171223-22:17:28:156|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7074##548562##8661##16256##27289470
20171223-22:17:28:156|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7074##548563##8661##16256##27292475
20171223-22:17:28:166|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128210
20171223-22:17:28:170|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:44:849|Step_LSC|30002312|onStandStepChanged 3647
20171223-22:17:44:869|Step_LSC|30002312|onExtend:1514038665000 1 0 4
20171223-22:17:45:151|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7074##548563##8661##16256##27292475
20171223-22:17:45:151|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7075##548564##8661##16256##27309470
20171223-22:17:45:162|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128231
20171223-22:17:45:166|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:45:169|Step_StandReportReceiver|30002312|REPORT : 7075 5051 151546 240
20171223-22:17:45:348|Step_LSC|30002312|onStandStepChanged 3649
20171223-22:17:45:366|Step_LSC|30002312|onExtend:1514038666000 2 0 4
20171223-22:17:45:649|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7075##548564##8661##16256##27309470
20171223-22:17:45:650|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7077##548565##8661##16256##27309969
20171223-22:17:45:663|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128274
20171223-22:17:45:668|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:45:674|Step_StandReportReceiver|30002312|REPORT : 7077 5052 151589 240
20171223-22:17:45:851|Step_LSC|30002312|onStandStepChanged 3651
20171223-22:17:45:856|Step_LSC|30002312|onExtend:1514038666000 2 0 4
20171223-22:17:46:154|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7077##548565##8661##16256##27309969
20171223-22:17:46:154|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7079##548566##8661##16256##27310473
20171223-22:17:46:163|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128317
20171223-22:17:46:167|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:46:170|Step_StandReportReceiver|30002312|REPORT : 7079 5054 151632 240
20171223-22:17:46:349|Step_LSC|30002312|onStandStepChanged 3653
20171223-22:17:46:364|Step_LSC|30002312|onExtend:1514038667000 2 0 4
20171223-22:17:46:650|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7079##548566##8661##16256##27310473
20171223-22:17:46:651|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7081##548567##8661##16256##27310970
20171223-22:17:46:659|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128360
20171223-22:17:46:661|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:46:662|Step_StandReportReceiver|30002312|REPORT : 7081 5055 151675 240
20171223-22:17:46:848|Step_LSC|30002312|onStandStepChanged 3655
20171223-22:17:46:858|Step_LSC|30002312|onExtend:1514038667000 2 0 4
20171223-22:17:47:153|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7081##548567##8661##16256##27310970
20171223-22:17:47:154|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7083##548568##8661##16256##27311472
20171223-22:17:47:163|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128403
20171223-22:17:47:165|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:47:166|Step_StandReportReceiver|30002312|REPORT : 7083 5057 151717 240
20171223-22:17:47:349|Step_LSC|30002312|onStandStepChanged 3656
20171223-22:17:47:366|Step_LSC|30002312|onExtend:1514038668000 1 0 4
20171223-22:17:47:651|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7083##548568##8661##16256##27311472
20171223-22:17:47:652|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7084##548569##8661##16256##27311970
20171223-22:17:47:659|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128424
20171223-22:17:47:662|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:47:662|Step_StandReportReceiver|30002312|REPORT : 7084 5057 151739 240
20171223-22:17:47:848|Step_LSC|30002312|onStandStepChanged 3658
20171223-22:17:47:854|Step_LSC|30002312|onExtend:1514038668000 2 0 4
20171223-22:17:48:148|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7084##548569##8661##16256##27311970
20171223-22:17:48:149|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7086##548570##8661##16256##27312467
20171223-22:17:48:157|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128467
20171223-22:17:48:162|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:48:164|Step_StandReportReceiver|30002312|REPORT : 7086 5059 151782 240
20171223-22:17:48:348|Step_LSC|30002312|onStandStepChanged 3660
20171223-22:17:48:357|Step_LSC|30002312|onExtend:1514038669000 2 0 4
20171223-22:17:48:648|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7086##548570##8661##16256##27312467
20171223-22:17:48:649|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7088##548571##8661##16256##27312967
20171223-22:17:48:656|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128510
20171223-22:17:48:661|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:48:661|Step_StandReportReceiver|30002312|REPORT : 7088 5060 151824 240
20171223-22:17:48:848|Step_LSC|30002312|onStandStepChanged 3662
20171223-22:17:48:853|Step_LSC|30002312|onExtend:1514038669000 2 0 4
20171223-22:17:49:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7088##548571##8661##16256##27312967
20171223-22:17:49:149|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7090##548572##8661##16256##27313468
20171223-22:17:49:162|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128553
20171223-22:17:49:166|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:49:166|Step_StandReportReceiver|30002312|REPORT : 7090 5062 151867 240
20171223-22:17:49:353|Step_LSC|30002312|onStandStepChanged 3664
20171223-22:17:49:358|Step_LSC|30002312|onExtend:1514038670000 2 0 4
20171223-22:17:49:655|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7090##548572##8661##16256##27313468
20171223-22:17:49:655|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7092##548573##8661##16256##27313974
20171223-22:17:49:661|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128595
20171223-22:17:49:664|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:49:664|Step_StandReportReceiver|30002312|REPORT : 7092 5063 151910 240
20171223-22:17:49:848|Step_LSC|30002312|onStandStepChanged 3666
20171223-22:17:49:854|Step_LSC|30002312|onExtend:1514038670000 2 0 4
20171223-22:17:50:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7092##548573##8661##16256##27313974
20171223-22:17:50:149|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7094##548574##8661##16256##27314468
20171223-22:17:50:156|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128638
20171223-22:17:50:158|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:50:160|Step_StandReportReceiver|30002312|REPORT : 7094 5065 151953 240
20171223-22:17:50:348|Step_LSC|30002312|onStandStepChanged 3667
20171223-22:17:50:354|Step_LSC|30002312|onExtend:1514038671000 1 0 4
20171223-22:17:50:649|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7094##548574##8661##16256##27314468
20171223-22:17:50:649|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7095##548575##8661##16256##27314968
20171223-22:17:50:658|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128660
20171223-22:17:50:661|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:50:661|Step_StandReportReceiver|30002312|REPORT : 7095 5065 151974 240
20171223-22:17:50:849|Step_LSC|30002312|onStandStepChanged 3669
20171223-22:17:50:856|Step_LSC|30002312|onExtend:1514038671000 2 0 4
20171223-22:17:51:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7095##548575##8661##16256##27314968
20171223-22:17:51:150|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7097##548576##8661##16256##27315469
20171223-22:17:51:158|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128703
20171223-22:17:51:161|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:51:165|Step_StandReportReceiver|30002312|REPORT : 7097 5067 152017 240
20171223-22:17:51:362|Step_LSC|30002312|onStandStepChanged 3671
20171223-22:17:51:370|Step_LSC|30002312|onExtend:1514038672000 2 0 4
20171223-22:17:51:664|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7097##548576##8661##16256##27315469
20171223-22:17:51:664|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7099##548577##8661##16256##27315983
20171223-22:17:51:673|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128745
20171223-22:17:51:677|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:51:681|Step_StandReportReceiver|30002312|REPORT : 7099 5068 152060 240
20171223-22:17:51:848|Step_LSC|30002312|onStandStepChanged 3673
20171223-22:17:51:854|Step_LSC|30002312|onExtend:1514038672000 2 0 4
20171223-22:17:52:151|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7099##548577##8661##16256##27315983
20171223-22:17:52:152|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7101##548578##8661##16256##27316470
20171223-22:17:52:162|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128788
20171223-22:17:52:169|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:52:169|Step_StandReportReceiver|30002312|REPORT : 7101 5070 152103 240
20171223-22:17:52:848|Step_LSC|30002312|onStandStepChanged 3674
20171223-22:17:52:855|Step_LSC|30002312|onExtend:1514038673000 1 0 4
20171223-22:17:53:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7101##548578##8661##16256##27316470
20171223-22:17:53:149|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7102##548579##8661##16256##27317468
20171223-22:17:53:163|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128810
20171223-22:17:53:168|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:53:169|Step_StandReportReceiver|30002312|REPORT : 7102 5070 152124 240
20171223-22:17:53:849|Step_LSC|30002312|onStandStepChanged 3675
20171223-22:17:53:859|Step_LSC|30002312|onExtend:1514038674000 1 0 4
20171223-22:17:54:152|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7102##548579##8661##16256##27317468
20171223-22:17:54:153|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7103##548580##8661##16256##27318472
20171223-22:17:54:167|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128831
20171223-22:17:54:175|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:54:176|Step_StandReportReceiver|30002312|REPORT : 7103 5071 152146 240
20171223-22:17:54:849|Step_LSC|30002312|onStandStepChanged 3676
20171223-22:17:54:864|Step_LSC|30002312|onExtend:1514038675000 1 0 4
20171223-22:17:55:153|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7103##548580##8661##16256##27318472
20171223-22:17:55:154|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7104##548581##8661##16256##27319473
20171223-22:17:55:162|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128853
20171223-22:17:55:166|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:55:169|Step_StandReportReceiver|30002312|REPORT : 7104 5072 152167 240
20171223-22:17:55:348|Step_LSC|30002312|onStandStepChanged 3677
20171223-22:17:55:381|Step_LSC|30002312|onExtend:1514038676000 1 0 4
20171223-22:17:55:649|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7104##548581##8661##16256##27319473
20171223-22:17:55:649|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7105##548582##8661##16256##27319968
20171223-22:17:55:658|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128874
20171223-22:17:55:660|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:55:664|Step_StandReportReceiver|30002312|REPORT : 7105 5072 152189 240
20171223-22:17:56:349|Step_LSC|30002312|onStandStepChanged 3678
20171223-22:17:56:361|Step_LSC|30002312|onExtend:1514038677000 1 0 4
20171223-22:17:56:650|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7105##548582##8661##16256##27319968
20171223-22:17:56:651|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7106##548583##8661##16256##27320969
20171223-22:17:56:666|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128895
20171223-22:17:56:671|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:56:678|Step_StandReportReceiver|30002312|REPORT : 7106 5073 152210 240
20171223-22:17:57:349|Step_LSC|30002312|onStandStepChanged 3679
20171223-22:17:57:372|Step_LSC|30002312|onExtend:1514038678000 1 0 4
20171223-22:17:57:650|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7106##548583##8661##16256##27320969
20171223-22:17:57:650|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7107##548584##8661##16256##27321969
20171223-22:17:57:659|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128917
20171223-22:17:57:662|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:57:662|Step_StandReportReceiver|30002312|REPORT : 7107 5074 152231 240
20171223-22:17:57:849|Step_LSC|30002312|onStandStepChanged 3681
20171223-22:17:57:862|Step_LSC|30002312|onExtend:1514038678000 2 0 4
20171223-22:17:58:150|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7107##548584##8661##16256##27321969
20171223-22:17:58:151|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038560000##7109##548585##8661##16256##27322469
20171223-22:17:58:159|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=128960
20171223-22:17:58:162|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:17:58:167|Step_StandReportReceiver|30002312|REPORT : 7109 5075 152274 240
20171223-22:17:59:858|Step_LSC|30002312|onStandStepChanged 3683
20171223-22:17:59:872|Step_LSC|30002312|onExtend:1514038680000 2 0 4
20171223-22:18:0:134|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:18:0:158|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038560000##7109##548585##8661##16256##27322469
20171223-22:18:0:159|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038620000##7111##548640##8661##16256##27324478
20171223-22:18:0:168|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129002
20171223-22:18:0:171|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:18:0:172|Step_StandReportReceiver|30002312|REPORT : 7111 5077 152317 240
20171223-22:18:1:349|Step_LSC|30002312|onStandStepChanged 3684
20171223-22:18:1:361|Step_LSC|30002312|onExtend:1514038682000 1 0 4
20171223-22:18:1:649|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038620000##7111##548640##8661##16256##27324478
20171223-22:18:1:650|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038620000##7112##548695##8661##16256##27325968
20171223-22:18:1:659|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129024
20171223-22:18:1:663|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:18:1:664|Step_StandReportReceiver|30002312|REPORT : 7112 5077 152339 240
20171223-22:18:2:349|Step_LSC|30002312|onStandStepChanged 3685
20171223-22:18:2:363|Step_LSC|30002312|onExtend:1514038683000 1 0 4
20171223-22:18:2:649|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038620000##7112##548695##8661##16256##27325968
20171223-22:18:2:650|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038620000##7113##548750##8661##16256##27326969
20171223-22:18:2:659|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129045
20171223-22:18:2:661|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:18:2:662|Step_StandReportReceiver|30002312|REPORT : 7113 5078 152360 240
20171223-22:18:5:351|Step_LSC|30002312|onStandStepChanged 3686
20171223-22:18:5:358|Step_LSC|30002312|onExtend:1514038686000 1 0 4
20171223-22:18:5:653|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038620000##7113##548750##8661##16256##27326969
20171223-22:18:5:653|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038620000##7114##548805##8661##16256##27329972
20171223-22:18:5:673|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129067
20171223-22:18:5:684|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:18:5:684|Step_StandReportReceiver|30002312|REPORT : 7114 5079 152381 240
20171223-22:18:5:849|Step_LSC|30002312|onStandStepChanged 3686
20171223-22:18:5:854|Step_LSC|30002312|onExtend:1514038686000 0 0 0
20171223-22:18:6:150|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038620000##7114##548805##8661##16256##27329972
20171223-22:18:6:151|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038620000##7114##548860##8661##16256##27330470
20171223-22:18:6:164|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129067
20171223-22:18:6:168|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:0:138|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:19:7:854|Step_LSC|30002312|onStandStepChanged 3689
20171223-22:19:7:865|Step_LSC|30002312|onExtend:1514038748000 3 0 4
20171223-22:19:8:155|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038620000##7114##548860##8661##16256##27330470
20171223-22:19:8:156|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7117##548865##8661##16256##27392475
20171223-22:19:8:163|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129131
20171223-22:19:8:165|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:8:167|Step_StandReportReceiver|30002312|REPORT : 7117 5081 152446 240
20171223-22:19:8:357|Step_LSC|30002312|onStandStepChanged 3694
20171223-22:19:8:365|Step_LSC|30002312|onExtend:1514038749000 5 0 4
20171223-22:19:8:658|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7117##548865##8661##16256##27392475
20171223-22:19:8:660|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7122##548870##8661##16256##27392978
20171223-22:19:8:675|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129238
20171223-22:19:8:686|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:8:686|Step_StandReportReceiver|30002312|REPORT : 7122 5085 152553 240
20171223-22:19:8:849|Step_LSC|30002312|onStandStepChanged 3696
20171223-22:19:8:862|Step_LSC|30002312|onExtend:1514038749000 2 0 4
20171223-22:19:9:150|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7122##548870##8661##16256##27392978
20171223-22:19:9:151|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7124##548875##8661##16256##27393470
20171223-22:19:9:158|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129281
20171223-22:19:9:163|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:9:169|Step_StandReportReceiver|30002312|REPORT : 7124 5086 152596 240
20171223-22:19:9:348|Step_LSC|30002312|onStandStepChanged 3698
20171223-22:19:9:359|Step_LSC|30002312|onExtend:1514038750000 2 0 4
20171223-22:19:9:652|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7124##548875##8661##16256##27393470
20171223-22:19:9:652|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7126##548880##8661##16256##27393971
20171223-22:19:9:662|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129324
20171223-22:19:9:668|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:9:671|Step_StandReportReceiver|30002312|REPORT : 7126 5087 152638 240
20171223-22:19:9:849|Step_LSC|30002312|onStandStepChanged 3700
20171223-22:19:9:859|Step_LSC|30002312|onExtend:1514038750000 2 0 4
20171223-22:19:10:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7126##548880##8661##16256##27393971
20171223-22:19:10:150|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7128##548885##8661##16256##27394469
20171223-22:19:10:160|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129366
20171223-22:19:10:164|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:10:165|Step_StandReportReceiver|30002312|REPORT : 7128 5089 152681 240
20171223-22:19:10:348|Step_LSC|30002312|onStandStepChanged 3702
20171223-22:19:10:362|Step_LSC|30002312|onExtend:1514038751000 2 0 4
20171223-22:19:10:650|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7128##548885##8661##16256##27394469
20171223-22:19:10:650|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7130##548890##8661##16256##27394969
20171223-22:19:10:658|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129409
20171223-22:19:10:661|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:10:664|Step_StandReportReceiver|30002312|REPORT : 7130 5090 152724 240
20171223-22:19:10:848|Step_LSC|30002312|onStandStepChanged 3704
20171223-22:19:10:858|Step_LSC|30002312|onExtend:1514038751000 2 0 4
20171223-22:19:11:149|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7130##548890##8661##16256##27394969
20171223-22:19:11:150|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7132##548895##8661##16256##27395468
20171223-22:19:11:160|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129452
20171223-22:19:11:164|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:11:164|Step_StandReportReceiver|30002312|REPORT : 7132 5092 152767 240
20171223-22:19:11:349|Step_LSC|30002312|onStandStepChanged 3707
20171223-22:19:11:358|Step_LSC|30002312|onExtend:1514038752000 3 0 4
20171223-22:19:11:650|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7132##548895##8661##16256##27395468
20171223-22:19:11:651|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7135##548900##8661##16256##27395969
20171223-22:19:11:658|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129516
20171223-22:19:11:661|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:11:661|Step_StandReportReceiver|30002312|REPORT : 7135 5094 152831 240
20171223-22:19:11:848|Step_LSC|30002312|onStandStepChanged 3711
20171223-22:19:11:857|Step_LSC|30002312|onExtend:1514038752000 4 0 4
20171223-22:19:12:153|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7135##548900##8661##16256##27395969
20171223-22:19:12:154|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7139##548905##8661##16256##27396473
20171223-22:19:12:162|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129602
20171223-22:19:12:165|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:12:167|Step_StandReportReceiver|30002312|REPORT : 7139 5097 152917 240
20171223-22:19:12:349|Step_LSC|30002312|onStandStepChanged 3712
20171223-22:19:12:364|Step_LSC|30002312|onExtend:1514038753000 1 0 4
20171223-22:19:12:650|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7139##548905##8661##16256##27396473
20171223-22:19:12:650|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7140##548910##8661##16256##27396969
20171223-22:19:12:658|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129623
20171223-22:19:12:662|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:12:663|Step_StandReportReceiver|30002312|REPORT : 7140 5097 152938 240
20171223-22:19:14:349|Step_LSC|30002312|onStandStepChanged 3726
20171223-22:19:14:363|Step_LSC|30002312|onExtend:1514038755000 14 0 4
20171223-22:19:14:655|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7140##548910##8661##16256##27396969
20171223-22:19:14:656|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7154##548915##8661##16256##27398975
20171223-22:19:14:677|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129923
20171223-22:19:14:686|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:14:688|Step_StandReportReceiver|30002312|REPORT : 7154 5107 153238 240
20171223-22:19:15:855|Step_LSC|30002312|onStandStepChanged 3726
20171223-22:19:15:858|Step_LSC|30002312|onExtend:1514038756000 0 0 0
20171223-22:19:16:156|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7154##548915##8661##16256##27398975
20171223-22:19:16:157|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7154##548920##8661##16256##27400475
20171223-22:19:16:171|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=129923
20171223-22:19:16:177|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:43:719|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:19:57:751|Step_LSC|30002312|onStandStepChanged 3732
20171223-22:19:57:755|Step_LSC|30002312|onExtend:1514038756000 6 0 4
20171223-22:19:57:758|Step_LSC|30002312|onStandStepChanged 3732
20171223-22:19:57:763|Step_LSC|30002312|onExtend:1514038798000 0 0 4
20171223-22:19:57:775|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:19:57:776|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:19:57:776|Step_StandStepCounter|30002312|flush sensor data
20171223-22:19:57:777|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7154##548920##8661##16256##27400475
20171223-22:19:57:778|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7160##548925##8661##16256##27442097
20171223-22:19:57:780|Step_LSC|30002312|onStandStepChanged 3732
20171223-22:19:57:789|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130052
20171223-22:19:57:796|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:57:796|Step_StandReportReceiver|30002312|REPORT : 7160 5112 153367 240
20171223-22:19:57:878|Step_LSC|30002312|onStandStepChanged 3732
20171223-22:19:57:879|Step_LSC|30002312|onExtend:1514038798000 0 0 4
20171223-22:19:58:97|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7160##548925##8661##16256##27442097
20171223-22:19:58:97|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7160##548930##8661##16256##27442416
20171223-22:19:58:109|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130052
20171223-22:19:58:112|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:58:363|Step_LSC|30002312|onStandStepChanged 3735
20171223-22:19:58:363|Step_LSC|30002312|flushTempCacheToDB by stand
20171223-22:19:58:366|Step_LSC|30002312|Alarm uploadStaticsToDB totalSteps=7163Calories:153367Floor:240Distance:5112
20171223-22:19:58:366|Step_FlushableStepDataCache|30002312|writeDataToDB size 327
20171223-22:19:58:366|Step_FlushableStepDataCache|30002312|upLoadOneMinuteDataToEngine time=25233975,0,93,0,20002
20171223-22:19:58:366|Step_FlushableStepDataCache|30002312|upLoadOneMinuteDataToEngine time=25233976,0,1,0,20002
20171223-22:19:58:366|Step_FlushableStepDataCache|30002312|upLoadOneMinuteDataToEngine time=25233977,0,55,0,20002
20171223-22:19:58:366|Step_FlushableStepDataCache|30002312|upLoadOneMinuteDataToEngine time=25233978,0,5,0,20002
20171223-22:19:58:367|Step_FlushableStepDataCache|30002312|upLoadOneMinuteDataToEngine time=25233979,0,46,0,20002
20171223-22:19:58:369|HiH_HiAppUtil|30002312|getBinderPackageName packageName = com.huawei.health
20171223-22:19:58:370|HiH_HiAppUtil|30002312|getBinderPackageName packageName = com.huawei.health
20171223-22:19:58:371|HiH_HiHealthBinder|30002312|getAppContext() isAppValid health or wear, packageName = com.huawei.health
20171223-22:19:58:373|Step_LSC|30002312|onExtend:1514038799000 3 0 4
20171223-22:19:58:374|HiH_HiAppUtil|30002312|getBinderPackageName packageName = com.huawei.health
20171223-22:19:58:375|HiH_HiAppUtil|30002312|getBinderPackageName packageName = com.huawei.health
20171223-22:19:58:375|HiH_HiHealthBinder|30002312|insertHiHealthData() checkAppType  0 appID = 1
20171223-22:19:58:376|HiH_HiHealthBinder|30002312|insertHiHealthData() size = 4,app = 1,One Data Type = 40002,packageName = com.huawei.health,writeStatType = 0
20171223-22:19:58:377|HiH_HiHealthDataInsertStore|30002312|saveHealthDetailData() deviceID = 2,clientID=1,id=1
20171223-22:19:58:378|HiH_HiHealthDataInsertStore|30002312|saveStatData() type =40002,time = 1513958400000,statClient = 2,who is 1
20171223-22:19:58:378|HiH_DataStatManager|30002312|new date =20171223, type=40002,7163.0,old=6983.0
20171223-22:19:58:379|HiH_HiHealthDataInsertStore|30002312|saveStatData() type =40003,time = 1513958400000,statClient = 2,who is 1
20171223-22:19:58:380|HiH_DataStatManager|30002312|new date =20171223, type=40003,153367.0,old=210654.54000000004
20171223-22:19:58:380|HiH_HiHealthDataInsertStore|30002312|saveHealthDetailData() saveOneDetailData fail hiHealthData = 1513958400000,type = 40003
20171223-22:19:58:380|HiH_HiHealthDataInsertStore|30002312|saveStatData() type =40005,time = 1513958400000,statClient = 2,who is 1
20171223-22:19:58:381|HiH_DataStatManager|30002312|new date =20171223, type=40005,240.0,old=330.0
20171223-22:19:58:381|HiH_HiHealthDataInsertStore|30002312|saveHealthDetailData() saveOneDetailData fail hiHealthData = 1513958400000,type = 40005
20171223-22:19:58:381|HiH_HiHealthDataInsertStore|30002312|saveStatData() type =40004,time = 1513958400000,statClient = 2,who is 1
20171223-22:19:58:382|HiH_DataStatManager|30002312|new date =20171223, type=40004,5112.0,old=4985.0
20171223-22:19:58:406|HiH_HiHealthDataInsertStore|30002312|bulkSaveDetailHiHealthData() size = 4,totalTime = 30
20171223-22:19:58:407|HiH_ListenerManager|30002312|startListenerChange subscribeList = [1]
20171223-22:19:58:411|HiH_HiHealthBinder|30002312|insertHiHealthData() bulkSaveDetailHiHealthData fail errorCode = 4,errorMessage = ERR_DATA_INSERT 
20171223-22:19:58:411|HiH_HiHealthBinder|30002312|insertHiHealthData() end totalTime = 45
20171223-22:19:58:411|Step_LSC|30002312|uploadStaticsToDB() onResult  type = 4 obj=true
20171223-22:19:58:411|Step_LSC|30002312|uploadStaticsToDB failed message=true
20171223-22:19:58:413|HiH_HiSyncControl|30002312|checkInsertStatus stepSum or calorieSum is enough
20171223-22:19:58:413|HiH_HiAppUtil|30002312|getBinderPackageName packageName = com.huawei.health
20171223-22:19:58:415|HiH_HiAppUtil|30002312|getBinderPackageName packageName = com.huawei.health
20171223-22:19:58:415|HiH_HiHealthBinder|30002312|getAppContext() isAppValid health or wear, packageName = com.huawei.health
20171223-22:19:58:415|HiH_HiSyncControl|30002312|checkInsertStatus stepStatSum or calorieStatSum is enough
20171223-22:19:58:416|HiH_HiSyncControl|30002312|stepSyncOrNot appSynTimes is 0, statsyncTimes is 0
20171223-22:19:58:416|HiH_HiSyncControl|30002312|startInsertSportSync start auto sync,app is 1
20171223-22:19:58:417|HiH_HiAppUtil|30002312|getBinderPackageName packageName = com.huawei.health
20171223-22:19:58:417|HiH_HiSyncUtil|30002312|checkFirstSyncByType no such data in db ,type is 1 deviceCode is 0
20171223-22:19:58:417|HiH_HiSyncControl|30002312|startInsertSportSync first 500 steps sync,do all sync
20171223-22:19:58:419|HiH_HiAppUtil|30002312|getBinderPackageName packageName = com.huawei.health
20171223-22:19:58:419|HiH_HiSyncControl|30002312|startSync hiSyncOption = HiSyncOption{syncAction=2, syncMethod=2, syncScope=0, syncDataType=20000, syncModel=2, pushAction=0},app = 1 who = 1
20171223-22:19:58:419|HiH_HiHealthBinder|30002312|insertHiHealthData() checkAppType  0 appID = 1
20171223-22:19:58:419|HiH_HiHealthBinder|30002312|insertHiHealthData() size = 20,app = 1,One Data Type = 2,packageName = com.huawei.health,writeStatType = 0
20171223-22:19:58:423|HiH_HiSyncControl|30002312|needAutoSync autoSyncSwitch is open
20171223-22:19:58:423|HiH_HiHealthDataInsertStore|30002312|saveHealthDetailData() deviceID = 2,clientID=1,id=1
20171223-22:19:58:424|HiH_HiSyncControl|30002312|initDataPrivacy the dataPrivacy switch is open, start push health data!
20171223-22:19:58:424|HiH_|30002312|initDataPrivacy the dataPrivacy is true
20171223-22:19:58:428|HiH_HiSyncControl|30002312|initUserPrivacy the userPrivacy switch is open, start push user data!
20171223-22:19:58:428|HiH_|30002312|initUserPrivacy the userPrivacy is true
20171223-22:19:58:428|HiH_HiSyncControl|30002312|ifCanSync not! no cloud version
20171223-22:19:58:428|HiH_HiBroadcastUtil|30002312|sendSyncFailedBroadcast
20171223-22:19:58:453|HiH_HiHealthDataInsertStore|30002312|bulkSaveDetailHiHealthData() size = 20,totalTime = 34
20171223-22:19:58:466|HiH_DataStatManager|30002312|new date =20171223, type=40002,6724.0,old=7163.0
20171223-22:19:58:468|HiH_DataStatManager|30002312|new date =20171223, type=40004,4800.935999999999,old=5112.0
20171223-22:19:58:469|HiH_DataStatManager|30002312|new date =20171223, type=40003,214445.88000000006,old=210654.54000000004
20171223-22:19:58:471|HiH_DataStatManager|30002312|new date =20171223, type=40005,330.0,old=330.0
20171223-22:19:58:480|HiH_DataStatManager|30002312|new date =20171223, type=40011,6314.0,old=6137.0
20171223-22:19:58:481|HiH_DataStatManager|30002312|new date =20171223, type=40031,4508.196000000001,old=4381.818
20171223-22:19:58:482|HiH_DataStatManager|30002312|new date =20171223, type=40021,135245.88,old=131454.53999999995
20171223-22:19:58:494|HiH_DataStatManager|30002312|new date =20171223, type=40013,410.0,old=410.0
20171223-22:19:58:495|HiH_DataStatManager|30002312|new date =20171223, type=40034,292.73999999999995,old=292.73999999999995
20171223-22:19:58:495|HiH_DataStatManager|30002312|new date =20171223, type=40024,79200.0,old=79200.0
20171223-22:19:58:499|HiH_DataStatManager|30002312|new date =20171223, type=40041,6720.0,old=6480.0
20171223-22:19:58:501|HiH_DataStatManager|30002312|new date =20171223, type=40044,420.0,old=420.0
20171223-22:19:58:501|HiH_DataStatManager|30002312|new date =20171223, type=40006,7140.0,old=6900.0
20171223-22:19:58:502|HiH_HiHealthDataInsertStore|30002312|saveRealTimeHealthDatasStat() size = 1,totalTime = 48
20171223-22:19:58:505|HiH_ListenerManager|30002312|startListenerChange subscribeList = [1]
20171223-22:19:58:505|HiH_HiHealthBinder|30002312|insertHiHealthData() end totalTime = 94
20171223-22:19:58:505|Step_FlushableStepDataCache|30002312|InsertCallBack() onSuccess  type = 0 data=true
20171223-22:19:58:505|Step_FlushableStepDataCache|30002312|InsertEvent success begin:25233975 end:25233979
20171223-22:19:58:505|Step_SPUtils|30002312|setWriteDBLastDataMinute=25233979
20171223-22:19:58:508|Step_SPUtils|30002312|setWriteDBLastDataMinute success
20171223-22:19:58:508|Step_LSC|30002312|flush2DB result success
20171223-22:19:58:508|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7160##548930##8661##16256##27442416
20171223-22:19:58:508|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7163##548935##8661##16256##27442827
20171223-22:19:58:512|HiH_HiSyncControl|30002312|checkInsertStatus stepSum or calorieSum is enough
20171223-22:19:58:513|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130116
20171223-22:19:58:514|HiH_HiSyncControl|30002312|checkInsertStatus stepStatSum or calorieStatSum is enough
20171223-22:19:58:514|HiH_HiSyncControl|30002312|stepSyncOrNot appSynTimes is 0, statsyncTimes is 0
20171223-22:19:58:514|HiH_HiSyncControl|30002312|startInsertSportSync start auto sync,app is 1
20171223-22:19:58:515|HiH_HiSyncUtil|30002312|checkFirstSyncByType no such data in db ,type is 1 deviceCode is 0
20171223-22:19:58:515|HiH_HiSyncControl|30002312|startInsertSportSync first 500 steps sync,do all sync
20171223-22:19:58:516|HiH_HiSyncControl|30002312|startSync hiSyncOption = HiSyncOption{syncAction=2, syncMethod=2, syncScope=0, syncDataType=20000, syncModel=2, pushAction=0},app = 1 who = 1
20171223-22:19:58:517|HiH_HiSyncControl|30002312|needAutoSync autoSyncSwitch is open
20171223-22:19:58:517|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:58:518|HiH_HiSyncControl|30002312|initDataPrivacy the dataPrivacy switch is open, start push health data!
20171223-22:19:58:518|HiH_|30002312|initDataPrivacy the dataPrivacy is true
20171223-22:19:58:518|HiH_HiSyncControl|30002312|initUserPrivacy the userPrivacy switch is open, start push user data!
20171223-22:19:58:518|HiH_|30002312|initUserPrivacy the userPrivacy is true
20171223-22:19:58:518|HiH_HiSyncControl|30002312|ifCanSync not! no cloud version
20171223-22:19:58:518|HiH_HiBroadcastUtil|30002312|sendSyncFailedBroadcast
20171223-22:19:58:519|Step_StandReportReceiver|30002312|REPORT : 7163 5114 153431 240
20171223-22:19:58:863|Step_LSC|30002312|onStandStepChanged 3737
20171223-22:19:58:872|Step_LSC|30002312|onExtend:1514038799000 2 0 4
20171223-22:19:59:164|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7163##548935##8661##16256##27442827
20171223-22:19:59:166|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7165##548940##8661##16256##27443484
20171223-22:19:59:175|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130159
20171223-22:19:59:178|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:59:182|Step_StandReportReceiver|30002312|REPORT : 7165 5115 153474 240
20171223-22:19:59:365|Step_LSC|30002312|onStandStepChanged 3739
20171223-22:19:59:373|Step_LSC|30002312|onExtend:1514038800000 2 0 4
20171223-22:19:59:667|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7165##548940##8661##16256##27443484
20171223-22:19:59:667|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038680000##7167##548945##8661##16256##27443986
20171223-22:19:59:677|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130201
20171223-22:19:59:681|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:19:59:682|Step_StandReportReceiver|30002312|REPORT : 7167 5117 153517 240
20171223-22:19:59:862|Step_LSC|30002312|onStandStepChanged 3742
20171223-22:19:59:871|Step_LSC|30002312|onExtend:1514038800000 3 0 4
20171223-22:20:0:130|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:20:0:165|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038680000##7167##548945##8661##16256##27443986
20171223-22:20:0:165|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7170##548996##8661##16256##27444484
20171223-22:20:0:174|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130266
20171223-22:20:0:177|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:0:179|Step_StandReportReceiver|30002312|REPORT : 7170 5119 153581 240
20171223-22:20:0:363|Step_LSC|30002312|onStandStepChanged 3745
20171223-22:20:0:374|Step_LSC|30002312|onExtend:1514038801000 3 0 4
20171223-22:20:0:666|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7170##548996##8661##16256##27444484
20171223-22:20:0:666|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7173##549047##8661##16256##27444985
20171223-22:20:0:679|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130330
20171223-22:20:0:688|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:0:693|Step_StandReportReceiver|30002312|REPORT : 7173 5121 153645 240
20171223-22:20:0:863|Step_LSC|30002312|onStandStepChanged 3746
20171223-22:20:0:872|Step_LSC|30002312|onExtend:1514038801000 1 0 4
20171223-22:20:1:166|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7173##549047##8661##16256##27444985
20171223-22:20:1:166|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7174##549098##8661##16256##27445485
20171223-22:20:1:181|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130351
20171223-22:20:1:186|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:1:186|Step_StandReportReceiver|30002312|REPORT : 7174 5122 153667 240
20171223-22:20:1:363|Step_LSC|30002312|onStandStepChanged 3748
20171223-22:20:1:378|Step_LSC|30002312|onExtend:1514038802000 2 0 4
20171223-22:20:1:664|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7174##549098##8661##16256##27445485
20171223-22:20:1:665|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7176##549149##8661##16256##27445983
20171223-22:20:1:679|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130394
20171223-22:20:1:687|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:1:692|Step_StandReportReceiver|30002312|REPORT : 7176 5123 153709 240
20171223-22:20:1:863|Step_LSC|30002312|onStandStepChanged 3751
20171223-22:20:1:873|Step_LSC|30002312|onExtend:1514038802000 3 0 4
20171223-22:20:2:164|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7176##549149##8661##16256##27445983
20171223-22:20:2:165|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7179##549200##8661##16256##27446483
20171223-22:20:2:174|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130458
20171223-22:20:2:178|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:2:179|Step_StandReportReceiver|30002312|REPORT : 7179 5125 153774 240
20171223-22:20:2:365|Step_LSC|30002312|onStandStepChanged 3753
20171223-22:20:2:375|Step_LSC|30002312|onExtend:1514038803000 2 0 4
20171223-22:20:2:670|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7179##549200##8661##16256##27446483
20171223-22:20:2:671|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7181##549251##8661##16256##27446989
20171223-22:20:2:678|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130501
20171223-22:20:2:693|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:2:695|Step_StandReportReceiver|30002312|REPORT : 7181 5127 153817 240
20171223-22:20:2:863|Step_LSC|30002312|onStandStepChanged 3755
20171223-22:20:2:874|Step_LSC|30002312|onExtend:1514038803000 2 0 4
20171223-22:20:3:164|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7181##549251##8661##16256##27446989
20171223-22:20:3:165|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7183##549302##8661##16256##27447483
20171223-22:20:3:175|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130544
20171223-22:20:3:179|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:3:180|Step_StandReportReceiver|30002312|REPORT : 7183 5128 153859 240
20171223-22:20:3:862|Step_LSC|30002312|onStandStepChanged 3757
20171223-22:20:3:873|Step_LSC|30002312|onExtend:1514038804000 2 0 4
20171223-22:20:4:166|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7183##549302##8661##16256##27447483
20171223-22:20:4:167|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7185##549353##8661##16256##27448486
20171223-22:20:4:187|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130587
20171223-22:20:4:193|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:4:193|Step_StandReportReceiver|30002312|REPORT : 7185 5130 153902 240
20171223-22:20:4:362|Step_LSC|30002312|onStandStepChanged 3758
20171223-22:20:4:373|Step_LSC|30002312|onExtend:1514038805000 1 0 4
20171223-22:20:4:664|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7185##549353##8661##16256##27448486
20171223-22:20:4:665|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7186##549404##8661##16256##27448983
20171223-22:20:4:673|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130608
20171223-22:20:4:676|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:4:680|Step_StandReportReceiver|30002312|REPORT : 7186 5130 153924 240
20171223-22:20:5:362|Step_LSC|30002312|onStandStepChanged 3759
20171223-22:20:5:377|Step_LSC|30002312|onExtend:1514038806000 1 0 4
20171223-22:20:5:662|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7186##549404##8661##16256##27448983
20171223-22:20:5:664|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7187##549455##8661##16256##27449982
20171223-22:20:5:672|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130630
20171223-22:20:5:675|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:5:675|Step_StandReportReceiver|30002312|REPORT : 7187 5131 153945 240
20171223-22:20:7:364|Step_LSC|30002312|onStandStepChanged 3760
20171223-22:20:7:378|Step_LSC|30002312|onExtend:1514038808000 1 0 4
20171223-22:20:7:665|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7187##549455##8661##16256##27449982
20171223-22:20:7:666|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7188##549506##8661##16256##27451985
20171223-22:20:7:681|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130651
20171223-22:20:7:686|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:7:690|Step_StandReportReceiver|30002312|REPORT : 7188 5132 153966 240
20171223-22:20:7:864|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:20:7:875|Step_LSC|30002312|onExtend:1514038808000 1 0 4
20171223-22:20:8:172|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7188##549506##8661##16256##27451985
20171223-22:20:8:173|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7189##549557##8661##16256##27452491
20171223-22:20:8:180|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:20:8:182|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:8:185|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:20:12:863|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:20:12:874|Step_LSC|30002312|onExtend:1514038813000 0 0 0
20171223-22:20:13:164|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7189##549557##8661##16256##27452491
20171223-22:20:13:165|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7189##549608##8661##16256##27457484
20171223-22:20:13:180|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:20:13:184|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:15:873|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:20:15:885|Step_LSC|30002312|onExtend:1514038816000 0 0 5
20171223-22:20:16:175|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7189##549608##8661##16256##27457484
20171223-22:20:16:175|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038740000##7189##549659##8661##16256##27460494
20171223-22:20:16:186|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:20:16:194|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:20:18:586|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:22:37:638|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:23:41:831|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:24:22:157|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:24:22:165|Step_LSC|30002312|onExtend:1514038819000 0 0 5
20171223-22:24:22:165|Step_LSC|30002312|onExtend:1514038879000 0 0 5
20171223-22:24:22:165|Step_LSC|30002312|onExtend:1514038939000 0 0 5
20171223-22:24:22:165|Step_LSC|30002312|onExtend:1514038999000 0 0 5
20171223-22:24:22:165|Step_LSC|30002312|onExtend:1514039059000 0 0 0
20171223-22:24:22:204|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:24:22:206|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:24:22:206|Step_StandStepCounter|30002312|flush sensor data
20171223-22:24:22:207|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038740000##7189##549659##8661##16256##27460494
20171223-22:24:22:207|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038980000##7189##549659##8661##16256##27706526
20171223-22:24:22:208|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:24:22:214|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:24:22:215|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:24:22:215|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:24:22:309|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:24:22:310|Step_LSC|30002312|onExtend:1514039062000 0 0 0
20171223-22:24:22:344|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:24:22:516|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038980000##7189##549659##8661##16256##27706526
20171223-22:24:22:516|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038980000##7189##549659##8661##16256##27706835
20171223-22:24:22:524|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:24:22:526|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:24:51:883|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:24:51:892|Step_LSC|30002312|onExtend:1514039091000 0 0 5
20171223-22:24:52:184|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038980000##7189##549659##8661##16256##27706835
20171223-22:24:52:185|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038980000##7189##549659##8661##16256##27736503
20171223-22:24:52:200|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:24:52:210|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:24:53:884|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:24:53:897|Step_LSC|30002312|onExtend:1514039093000 0 0 0
20171223-22:24:54:185|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038980000##7189##549659##8661##16256##27736503
20171223-22:24:54:186|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038980000##7189##549659##8661##16256##27738504
20171223-22:24:54:201|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:24:54:205|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:24:58:889|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:24:58:890|Step_LSC|30002312|onExtend:1514039098000 0 0 5
20171223-22:24:59:190|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038980000##7189##549659##8661##16256##27738504
20171223-22:24:59:191|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038980000##7189##549659##8661##16256##27743510
20171223-22:24:59:206|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:24:59:210|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:25:0:162|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:25:13:8|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:28:38:344|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:29:8:492|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:30:12:959|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:30:23:988|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:30:23:993|Step_LSC|30002312|onExtend:1514039112000 0 0 5
20171223-22:30:23:993|Step_LSC|30002312|onExtend:1514039172000 0 0 5
20171223-22:30:23:993|Step_LSC|30002312|onExtend:1514039232000 0 0 5
20171223-22:30:23:993|Step_LSC|30002312|onExtend:1514039292000 0 0 5
20171223-22:30:23:993|Step_LSC|30002312|onExtend:1514039352000 0 0 5
20171223-22:30:23:993|Step_LSC|30002312|onExtend:1514039412000 0 0 0
20171223-22:30:24:25|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:30:24:27|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:30:24:27|Step_StandStepCounter|30002312|flush sensor data
20171223-22:30:24:28|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514038980000##7189##549659##8661##16256##27743510
20171223-22:30:24:28|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039340000##7189##549659##8661##16256##28068346
20171223-22:30:24:28|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:30:24:40|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:30:24:45|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:30:24:46|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:30:24:129|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:30:24:132|Step_LSC|30002312|onExtend:1514039424000 0 0 0
20171223-22:30:24:347|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039340000##7189##549659##8661##16256##28068346
20171223-22:30:24:348|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039340000##7189##549659##8661##16256##28068666
20171223-22:30:24:355|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:30:24:358|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:31:0:136|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:31:31:924|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:31:31:940|Step_LSC|30002312|onExtend:1514039492000 0 0 5
20171223-22:31:32:225|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039340000##7189##549659##8661##16256##28068666
20171223-22:31:32:226|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039400000##7189##549659##8661##16256##28136544
20171223-22:31:32:241|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:31:32:246|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:31:54:515|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:31:59:613|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:31:59:618|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:31:59:619|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:31:59:619|Step_StandStepCounter|30002312|flush sensor data
20171223-22:31:59:620|Step_LSC|30002312|onExtend:1514039515000 0 0 0
20171223-22:31:59:620|Step_LSC|30002312|onExtend:1514039575000 0 0 0
20171223-22:31:59:620|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039400000##7189##549659##8661##16256##28136544
20171223-22:31:59:621|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039400000##7189##549659##8661##16256##28163940
20171223-22:31:59:621|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:31:59:652|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:31:59:655|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:31:59:656|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:31:59:723|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:31:59:725|Step_LSC|30002312|onExtend:1514039520000 0 0 0
20171223-22:31:59:725|Step_LSC|30002312|timeStamp back,extendReportTimeStamp=1514039575000
20171223-22:31:59:956|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039400000##7189##549659##8661##16256##28163940
20171223-22:31:59:957|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039400000##7189##549659##8661##16256##28164276
20171223-22:31:59:965|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:31:59:968|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:32:0:95|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:32:28:791|HiH_HiSyncControl|30002312|startTimer start autoSync
20171223-22:32:28:796|HiH_HiSyncControl|30002312|startSync hiSyncOption = HiSyncOption{syncAction=1, syncMethod=2, syncScope=0, syncDataType=20000, syncModel=2, pushAction=0},app = 1 who = 1
20171223-22:32:28:796|HiH_HiSyncControl|30002312|stepSyncOrNot appSynTimes is 0
20171223-22:32:28:798|HiH_HiSyncControl|30002312|needAutoSync autoSyncSwitch is open
20171223-22:32:28:800|HiH_HiSyncControl|30002312|initDataPrivacy the dataPrivacy switch is open, start push health data!
20171223-22:32:28:800|HiH_|30002312|initDataPrivacy the dataPrivacy is true
20171223-22:32:28:801|HiH_HiSyncControl|30002312|initUserPrivacy the userPrivacy switch is open, start push user data!
20171223-22:32:28:801|HiH_|30002312|initUserPrivacy the userPrivacy is true
20171223-22:32:28:801|HiH_HiSyncControl|30002312|ifCanSync not! no cloud version
20171223-22:32:28:801|HiH_HiBroadcastUtil|30002312|sendSyncFailedBroadcast
20171223-22:33:0:103|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:33:9:926|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:33:9:938|Step_LSC|30002312|onExtend:1514039590000 0 0 5
20171223-22:33:10:228|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039400000##7189##549659##8661##16256##28164276
20171223-22:33:10:229|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039520000##7189##549659##8661##16256##28234547
20171223-22:33:10:244|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:33:10:248|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:33:30:315|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:33:46:274|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:33:46:279|Step_LSC|30002312|onExtend:1514039626000 0 0 0
20171223-22:33:46:301|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:33:46:303|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:33:46:303|Step_StandStepCounter|30002312|flush sensor data
20171223-22:33:46:304|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039520000##7189##549659##8661##16256##28234547
20171223-22:33:46:304|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039520000##7189##549659##8661##16256##28270623
20171223-22:33:46:305|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:33:46:311|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:33:46:314|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:33:46:315|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:33:46:405|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:33:46:406|Step_LSC|30002312|onExtend:1514039626000 0 0 0
20171223-22:33:46:616|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039520000##7189##549659##8661##16256##28270623
20171223-22:33:46:616|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039520000##7189##549659##8661##16256##28270935
20171223-22:33:46:622|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:33:46:624|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:34:0:126|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:35:0:49|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:36:0:138|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:37:0:122|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:37:14:939|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:37:14:951|Step_LSC|30002312|onExtend:1514039834000 0 0 5
20171223-22:37:15:240|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039520000##7189##549659##8661##16256##28270935
20171223-22:37:15:240|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039760000##7189##549659##8661##16256##28479559
20171223-22:37:15:249|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:37:15:253|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:37:37:466|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:38:31:368|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:39:25:260|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:39:25:264|Step_LSC|30002312|onExtend:1514039857000 0 0 5
20171223-22:39:25:265|Step_LSC|30002312|onExtend:1514039917000 0 0 5
20171223-22:39:25:265|Step_LSC|30002312|onExtend:1514039977000 0 0 0
20171223-22:39:25:300|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:39:25:301|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:39:25:301|Step_StandStepCounter|30002312|flush sensor data
20171223-22:39:25:301|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039760000##7189##549659##8661##16256##28479559
20171223-22:39:25:302|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039880000##7189##549659##8661##16256##28609620
20171223-22:39:25:306|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:39:25:310|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:39:25:313|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:39:25:314|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:39:25:402|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:39:25:406|Step_LSC|30002312|onExtend:1514039965000 0 0 0
20171223-22:39:25:406|Step_LSC|30002312|timeStamp back,extendReportTimeStamp=1514039977000
20171223-22:39:25:446|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:39:25:614|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039880000##7189##549659##8661##16256##28609620
20171223-22:39:25:614|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514039880000##7189##549659##8661##16256##28609933
20171223-22:39:25:625|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:39:25:628|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:40:0:151|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:41:0:151|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:42:0:137|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:43:0:96|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:44:0:47|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:45:0:94|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:45:48:956|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:45:48:970|Step_LSC|30002312|onExtend:1514040348000 0 0 5
20171223-22:45:49:258|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514039880000##7189##549659##8661##16256##28609933
20171223-22:45:49:259|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040240000##7189##549659##8661##16256##28993578
20171223-22:45:49:273|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:45:49:278|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:45:54:954|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:45:54:974|Step_LSC|30002312|onExtend:1514040354000 0 0 0
20171223-22:45:55:256|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040240000##7189##549659##8661##16256##28993578
20171223-22:45:55:257|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040240000##7189##549659##8661##16256##28999575
20171223-22:45:55:268|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:45:55:272|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:46:0:142|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:47:0:157|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:48:0:132|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:49:0:139|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:50:0:142|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:51:0:128|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:51:2:965|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:51:2:978|Step_LSC|30002312|onExtend:1514040662000 0 0 5
20171223-22:51:3:266|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040240000##7189##549659##8661##16256##28999575
20171223-22:51:3:266|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040600000##7189##549659##8661##16256##29307585
20171223-22:51:3:275|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:51:3:278|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:51:24:638|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:51:37:252|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:51:37:260|Step_LSC|30002312|onExtend:1514040697000 0 0 0
20171223-22:51:37:290|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:51:37:292|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:51:37:292|Step_StandStepCounter|30002312|flush sensor data
20171223-22:51:37:292|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040600000##7189##549659##8661##16256##29307585
20171223-22:51:37:293|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040600000##7189##549659##8661##16256##29341612
20171223-22:51:37:295|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:51:37:302|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:51:37:306|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:51:37:307|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:51:37:393|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:51:37:394|Step_LSC|30002312|onExtend:1514040697000 0 0 0
20171223-22:51:37:608|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040600000##7189##549659##8661##16256##29341612
20171223-22:51:37:608|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040600000##7189##549659##8661##16256##29341927
20171223-22:51:37:615|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:51:37:618|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:51:40:331|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:51:40:340|Step_LSC|30002312|onExtend:1514040700000 0 0 0
20171223-22:51:40:632|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040600000##7189##549659##8661##16256##29341927
20171223-22:51:40:632|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040600000##7189##549659##8661##16256##29344951
20171223-22:51:40:641|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:51:40:644|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:51:48:980|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:51:48:993|Step_LSC|30002312|onExtend:1514040708000 0 0 5
20171223-22:51:49:282|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040600000##7189##549659##8661##16256##29344951
20171223-22:51:49:283|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040600000##7189##549659##8661##16256##29353602
20171223-22:51:49:298|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:51:49:315|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:52:0:146|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:52:8:790|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:52:45:412|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:52:45:415|Step_LSC|30002312|onExtend:1514040766000 0 0 5
20171223-22:52:45:485|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:52:45:486|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:52:45:486|Step_StandStepCounter|30002312|flush sensor data
20171223-22:52:45:487|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040600000##7189##549659##8661##16256##29353602
20171223-22:52:45:487|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040660000##7189##549659##8661##16256##29409806
20171223-22:52:45:487|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:52:45:494|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:52:45:497|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:52:45:498|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:52:45:587|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:52:45:588|Step_LSC|30002312|onExtend:1514040766000 0 0 5
20171223-22:52:45:800|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040660000##7189##549659##8661##16256##29409806
20171223-22:52:45:802|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040660000##7189##549659##8661##16256##29410120
20171223-22:52:45:810|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:52:45:813|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:52:45:985|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:52:45:987|Step_LSC|30002312|onExtend:1514040766000 0 0 0
20171223-22:52:46:286|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040660000##7189##549659##8661##16256##29410120
20171223-22:52:46:287|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040660000##7189##549659##8661##16256##29410605
20171223-22:52:46:296|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:52:46:301|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:52:51:987|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:52:51:993|Step_LSC|30002312|onExtend:1514040772000 0 0 5
20171223-22:52:52:287|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040660000##7189##549659##8661##16256##29410605
20171223-22:52:52:288|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040660000##7189##549659##8661##16256##29416607
20171223-22:52:52:301|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:52:52:306|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:52:55:718|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:53:1:177|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:53:1:185|Step_LSC|30002312|onExtend:1514040776000 0 0 5
20171223-22:53:1:185|Step_LSC|30002312|onExtend:1514040836000 0 0 0
20171223-22:53:1:211|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:53:1:213|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:53:1:213|Step_StandStepCounter|30002312|flush sensor data
20171223-22:53:1:214|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040660000##7189##549659##8661##16256##29416607
20171223-22:53:1:215|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040720000##7189##549659##8661##16256##29425533
20171223-22:53:1:218|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:53:1:219|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:53:1:226|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:53:1:227|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:53:1:315|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:53:1:320|Step_LSC|30002312|onExtend:1514040781000 0 0 0
20171223-22:53:1:320|Step_LSC|30002312|timeStamp back,extendReportTimeStamp=1514040836000
20171223-22:53:1:325|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:53:1:528|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040720000##7189##549659##8661##16256##29425533
20171223-22:53:1:529|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040720000##7189##549659##8661##16256##29425847
20171223-22:53:1:539|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:53:1:544|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:53:6:996|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:53:7:6|Step_LSC|30002312|onExtend:1514040786000 0 0 5
20171223-22:53:7:299|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040720000##7189##549659##8661##16256##29425847
20171223-22:53:7:299|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040720000##7189##549659##8661##16256##29431618
20171223-22:53:7:309|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:53:7:314|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:53:11:351|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:53:12:698|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:53:12:704|Step_LSC|30002312|onExtend:1514040793000 0 0 5
20171223-22:53:12:714|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:53:12:716|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:53:12:716|Step_StandStepCounter|30002312|flush sensor data
20171223-22:53:12:718|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040720000##7189##549659##8661##16256##29431618
20171223-22:53:12:718|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040720000##7189##549659##8661##16256##29437037
20171223-22:53:12:720|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:53:12:731|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:53:12:737|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:53:12:737|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:53:12:818|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:53:12:820|Step_LSC|30002312|onExtend:1514040793000 0 0 5
20171223-22:53:13:15|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:53:13:16|Step_LSC|30002312|onExtend:1514040793000 0 0 0
20171223-22:53:13:38|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040720000##7189##549659##8661##16256##29437037
20171223-22:53:13:39|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040720000##7189##549659##8661##16256##29437358
20171223-22:53:13:45|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:53:13:48|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:54:0:118|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:54:55:20|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:54:55:29|Step_LSC|30002312|onExtend:1514040895000 0 0 5
20171223-22:54:55:330|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040720000##7189##549659##8661##16256##29437358
20171223-22:54:55:331|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040780000##7189##549659##8661##16256##29539649
20171223-22:54:55:346|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:54:55:361|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:55:0:140|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:55:20:126|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:55:55:699|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:55:55:702|Step_LSC|30002312|onExtend:1514040956000 0 0 5
20171223-22:55:55:730|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-22:55:55:733|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-22:55:55:733|Step_StandStepCounter|30002312|flush sensor data
20171223-22:55:55:734|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040780000##7189##549659##8661##16256##29539649
20171223-22:55:55:735|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040840000##7189##549659##8661##16256##29600053
20171223-22:55:55:735|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:55:55:746|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:55:55:748|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:55:55:748|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-22:55:55:834|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:55:55:835|Step_LSC|30002312|onExtend:1514040956000 0 0 5
20171223-22:55:56:24|Step_LSC|30002312|onStandStepChanged 3761
20171223-22:55:56:27|Step_LSC|30002312|onExtend:1514040956000 0 0 0
20171223-22:55:56:53|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040840000##7189##549659##8661##16256##29600053
20171223-22:55:56:53|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514040840000##7189##549659##8661##16256##29600372
20171223-22:55:56:62|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-22:55:56:64|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-22:56:0:140|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:56:6:185|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-22:57:46:409|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-22:59:36:579|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:1:5:778|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:2:31:995|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:3:16:894|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:5:7:189|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:6:7:350|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:7:6:75|Step_LSC|30002312|onStandStepChanged 3761
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514040966000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041026000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041086000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041146000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041206000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041266000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041326000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041386000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041446000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041506000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041566000 0 0 5
20171223-23:7:6:86|Step_LSC|30002312|onExtend:1514041626000 0 0 5
20171223-23:7:6:124|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-23:7:6:125|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-23:7:6:125|Step_StandStepCounter|30002312|flush sensor data
20171223-23:7:6:126|Step_LSC|30002312|onStandStepChanged 3761
20171223-23:7:6:126|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514040840000##7189##549659##8661##16256##29600372
20171223-23:7:6:126|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041560000##7189##549659##8661##16256##30270445
20171223-23:7:6:137|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-23:7:6:142|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:7:6:145|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-23:7:6:226|Step_LSC|30002312|onStandStepChanged 3761
20171223-23:7:6:232|Step_LSC|30002312|onExtend:1514041627000 0 0 5
20171223-23:7:6:309|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:7:6:445|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041560000##7189##549659##8661##16256##30270445
20171223-23:7:6:446|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041560000##7189##549659##8661##16256##30270764
20171223-23:7:6:453|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-23:7:6:455|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:7:7:71|Step_LSC|30002312|onStandStepChanged 3761
20171223-23:7:7:71|Step_LSC|30002312|onExtend:1514041627000 0 0 0
20171223-23:7:7:373|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041560000##7189##549659##8661##16256##30270764
20171223-23:7:7:374|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041560000##7189##549659##8661##16256##30271693
20171223-23:7:7:391|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-23:7:7:400|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:7:50:264|Step_LSC|30002312|getTodaySportData mStepsRecordManager=com.huawei.health.g.c@8a2a501
20171223-23:7:50:266|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-23:7:50:266|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041560000##7189##549659##8661##16256##30271693
20171223-23:7:50:267|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041560000##7189##549659##8661##16256##30314585
20171223-23:7:50:274|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-23:7:50:279|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:7:50:304|Step_LSC|30002312|getTodaySportData mStepsRecordManager=com.huawei.health.g.c@8a2a501
20171223-23:7:51:222|Step_LSC|30002312|getTodaySportData mStepsRecordManager=com.huawei.health.g.c@8a2a501
20171223-23:7:51:224|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-23:7:51:225|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041560000##7189##549659##8661##16256##30314585
20171223-23:7:51:228|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041560000##7189##549659##8661##16256##30315546
20171223-23:7:51:235|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-23:7:51:238|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:7:51:314|Step_LSC|30002312|getTodaySportData mStepsRecordManager=com.huawei.health.g.c@8a2a501
20171223-23:8:0:112|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:8:9:229|Step_LSC|30002312|getTodaySportData mStepsRecordManager=com.huawei.health.g.c@8a2a501
20171223-23:8:9:233|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-23:8:9:235|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041560000##7189##549659##8661##16256##30315546
20171223-23:8:9:235|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041620000##7189##549659##8661##16256##30333554
20171223-23:8:9:248|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-23:8:9:253|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:8:9:293|Step_LSC|30002312|getTodaySportData mStepsRecordManager=com.huawei.health.g.c@8a2a501
20171223-23:8:19:101|Step_LSC|30002312|onStandStepChanged 3761
20171223-23:8:19:116|Step_LSC|30002312|onExtend:1514041699000 0 0 5
20171223-23:8:19:403|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041620000##7189##549659##8661##16256##30333554
20171223-23:8:19:404|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041620000##7189##549659##8661##16256##30343722
20171223-23:8:19:419|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-23:8:19:432|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:8:41:127|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-23:10:37:857|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:11:38:10|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:12:28:131|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:13:47:8|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:14:2:860|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:14:24:380|Step_LSC|30002312|onStandStepChanged 3761
20171223-23:14:24:384|Step_LSC|30002312|onExtend:1514041722000 0 0 5
20171223-23:14:24:384|Step_LSC|30002312|onExtend:1514041782000 0 0 5
20171223-23:14:24:384|Step_LSC|30002312|onExtend:1514041842000 0 0 5
20171223-23:14:24:384|Step_LSC|30002312|onExtend:1514041902000 0 0 5
20171223-23:14:24:384|Step_LSC|30002312|onExtend:1514041962000 0 0 5
20171223-23:14:24:384|Step_LSC|30002312|onExtend:1514042022000 0 0 5
20171223-23:14:24:384|Step_LSC|30002312|onExtend:1514042082000 0 0 5
20171223-23:14:24:389|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-23:14:24:390|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-23:14:24:390|Step_StandStepCounter|30002312|flush sensor data
20171223-23:14:24:391|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041620000##7189##549659##8661##16256##30343722
20171223-23:14:24:391|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7189##549659##8661##16256##30708710
20171223-23:14:24:392|Step_LSC|30002312|onStandStepChanged 3761
20171223-23:14:24:399|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-23:14:24:401|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:24:409|Step_StandReportReceiver|30002312|REPORT : 7189 5132 153988 240
20171223-23:14:24:494|Step_LSC|30002312|onStandStepChanged 3761
20171223-23:14:24:499|Step_LSC|30002312|onExtend:1514042064000 0 0 5
20171223-23:14:24:499|Step_LSC|30002312|timeStamp back,extendReportTimeStamp=1514042082000
20171223-23:14:24:711|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7189##549659##8661##16256##30708710
20171223-23:14:24:711|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7189##549659##8661##16256##30709030
20171223-23:14:24:721|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-23:14:24:724|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:25:106|Step_LSC|30002312|onStandStepChanged 3761
20171223-23:14:25:113|Step_LSC|30002312|onExtend:1514042064000 0 0 0
20171223-23:14:25:407|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7189##549659##8661##16256##30709030
20171223-23:14:25:408|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7189##549659##8661##16256##30709727
20171223-23:14:25:422|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130673
20171223-23:14:25:434|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:42:607|Step_LSC|30002312|onStandStepChanged 3763
20171223-23:14:42:620|Step_LSC|30002312|onExtend:1514042082000 2 0 0
20171223-23:14:42:909|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7189##549659##8661##16256##30709727
20171223-23:14:42:910|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7191##549659##8661##16256##30727229
20171223-23:14:42:925|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130715
20171223-23:14:42:930|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:42:933|Step_StandReportReceiver|30002312|REPORT : 7191 5134 154031 240
20171223-23:14:43:108|Step_LSC|30002312|onStandStepChanged 3764
20171223-23:14:43:122|Step_LSC|30002312|onExtend:1514042082000 1 0 4
20171223-23:14:43:414|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7191##549659##8661##16256##30727229
20171223-23:14:43:414|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7192##549659##8661##16256##30727733
20171223-23:14:43:426|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130737
20171223-23:14:43:432|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:43:434|Step_StandReportReceiver|30002312|REPORT : 7192 5135 154052 240
20171223-23:14:43:611|Step_LSC|30002312|onStandStepChanged 3765
20171223-23:14:43:622|Step_LSC|30002312|onExtend:1514042083000 1 0 4
20171223-23:14:43:913|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7192##549659##8661##16256##30727733
20171223-23:14:43:914|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7193##549659##8661##16256##30728232
20171223-23:14:43:929|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130758
20171223-23:14:43:934|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:43:938|Step_StandReportReceiver|30002312|REPORT : 7193 5135 154074 240
20171223-23:14:44:108|Step_LSC|30002312|onStandStepChanged 3768
20171223-23:14:44:118|Step_LSC|30002312|onExtend:1514042083000 3 0 4
20171223-23:14:44:409|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7193##549659##8661##16256##30728232
20171223-23:14:44:410|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7196##549659##8661##16256##30728728
20171223-23:14:44:425|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130822
20171223-23:14:44:431|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:44:437|Step_StandReportReceiver|30002312|REPORT : 7196 5137 154138 240
20171223-23:14:44:610|Step_LSC|30002312|onStandStepChanged 3770
20171223-23:14:44:617|Step_LSC|30002312|onExtend:1514042084000 2 0 4
20171223-23:14:44:913|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7196##549659##8661##16256##30728728
20171223-23:14:44:914|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7198##549659##8661##16256##30729233
20171223-23:14:44:930|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130865
20171223-23:14:44:946|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:44:954|Step_StandReportReceiver|30002312|REPORT : 7198 5139 154181 240
20171223-23:14:45:108|Step_LSC|30002312|onStandStepChanged 3772
20171223-23:14:45:118|Step_LSC|30002312|onExtend:1514042084000 2 0 4
20171223-23:14:45:413|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7198##549659##8661##16256##30729233
20171223-23:14:45:414|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7200##549659##8661##16256##30729733
20171223-23:14:45:429|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130908
20171223-23:14:45:434|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:45:438|Step_StandReportReceiver|30002312|REPORT : 7200 5140 154223 240
20171223-23:14:45:607|Step_LSC|30002312|onStandStepChanged 3774
20171223-23:14:45:618|Step_LSC|30002312|onExtend:1514042085000 2 0 4
20171223-23:14:45:908|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7200##549659##8661##16256##30729733
20171223-23:14:45:909|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7202##549659##8661##16256##30730227
20171223-23:14:45:916|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=130951
20171223-23:14:45:919|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:45:924|Step_StandReportReceiver|30002312|REPORT : 7202 5142 154266 240
20171223-23:14:46:106|Step_LSC|30002312|onStandStepChanged 3777
20171223-23:14:46:115|Step_LSC|30002312|onExtend:1514042085000 3 0 4
20171223-23:14:46:409|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7202##549659##8661##16256##30730227
20171223-23:14:46:409|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7205##549659##8661##16256##30730728
20171223-23:14:46:425|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131015
20171223-23:14:46:431|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:46:445|Step_StandReportReceiver|30002312|REPORT : 7205 5144 154331 240
20171223-23:14:46:608|Step_LSC|30002312|onStandStepChanged 3780
20171223-23:14:46:618|Step_LSC|30002312|onExtend:1514042086000 3 0 4
20171223-23:14:46:912|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7205##549659##8661##16256##30730728
20171223-23:14:46:914|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7208##549659##8661##16256##30731232
20171223-23:14:46:928|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131079
20171223-23:14:46:932|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:46:933|Step_StandReportReceiver|30002312|REPORT : 7208 5146 154395 240
20171223-23:14:47:109|Step_LSC|30002312|onStandStepChanged 3783
20171223-23:14:47:118|Step_LSC|30002312|onExtend:1514042086000 3 0 4
20171223-23:14:47:411|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7208##549659##8661##16256##30731232
20171223-23:14:47:412|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7211##549659##8661##16256##30731730
20171223-23:14:47:421|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131144
20171223-23:14:47:432|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:47:441|Step_StandReportReceiver|30002312|REPORT : 7211 5148 154459 240
20171223-23:14:48:107|Step_LSC|30002312|onStandStepChanged 3784
20171223-23:14:48:117|Step_LSC|30002312|onExtend:1514042087000 1 0 4
20171223-23:14:48:409|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7211##549659##8661##16256##30731730
20171223-23:14:48:410|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7212##549659##8661##16256##30732729
20171223-23:14:48:418|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131165
20171223-23:14:48:421|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:48:422|Step_StandReportReceiver|30002312|REPORT : 7212 5149 154481 240
20171223-23:14:48:606|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:14:48:618|Step_LSC|30002312|onExtend:1514042088000 2 0 4
20171223-23:14:48:906|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7212##549659##8661##16256##30732729
20171223-23:14:48:907|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7214##549659##8661##16256##30733226
20171223-23:14:48:922|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:14:48:932|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:14:48:937|Step_StandReportReceiver|30002312|REPORT : 7214 5150 154523 240
20171223-23:14:52:109|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:14:52:125|Step_LSC|30002312|onExtend:1514042091000 0 0 0
20171223-23:14:52:410|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7214##549659##8661##16256##30733226
20171223-23:14:52:411|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514041980000##7214##549659##8661##16256##30736730
20171223-23:14:52:421|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:14:52:424|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:15:0:121|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:16:0:120|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:16:23:109|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:16:23:120|Step_LSC|30002312|onExtend:1514042182000 0 0 5
20171223-23:16:23:410|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514041980000##7214##549659##8661##16256##30736730
20171223-23:16:23:411|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042100000##7214##549659##8661##16256##30827730
20171223-23:16:23:420|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:16:23:424|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:16:45:587|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-23:17:40:773|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:17:40:780|Step_LSC|30002312|onExtend:1514042205000 0 0 5
20171223-23:17:40:780|Step_LSC|30002312|onExtend:1514042265000 0 0 0
20171223-23:17:40:993|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-23:17:40:995|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-23:17:40:995|Step_StandStepCounter|30002312|flush sensor data
20171223-23:17:40:996|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042100000##7214##549659##8661##16256##30827730
20171223-23:17:40:996|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042160000##7214##549659##8661##16256##30905315
20171223-23:17:40:997|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:17:41:3|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:17:41:4|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:17:41:5|Step_StandReportReceiver|30002312|REPORT : 7214 5150 154523 240
20171223-23:17:41:97|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:17:41:100|Step_LSC|30002312|onExtend:1514042261000 0 0 0
20171223-23:17:41:100|Step_LSC|30002312|timeStamp back,extendReportTimeStamp=1514042265000
20171223-23:17:41:306|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042160000##7214##549659##8661##16256##30905315
20171223-23:17:41:306|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042160000##7214##549659##8661##16256##30905625
20171223-23:17:41:309|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:17:41:310|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:17:41:344|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:17:41:692|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:17:41:694|Step_LSC|30002312|onExtend:1514042262000 0 0 0
20171223-23:17:41:716|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171223-23:17:41:987|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_ON
20171223-23:17:41:989|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.SCREEN_ON
20171223-23:17:41:989|Step_StandStepCounter|30002312|flush sensor data
20171223-23:17:41:991|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:17:41:991|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042160000##7214##549659##8661##16256##30905625
20171223-23:17:41:991|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042160000##7214##549659##8661##16256##30906310
20171223-23:17:42:1|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:17:42:4|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:17:42:5|Step_StandReportReceiver|30002312|REPORT : 7214 5150 154523 240
20171223-23:17:42:91|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:17:42:93|Step_LSC|30002312|onExtend:1514042262000 0 0 0
20171223-23:17:42:391|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042160000##7214##549659##8661##16256##30906310
20171223-23:17:42:392|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042160000##7214##549659##8661##16256##30906710
20171223-23:17:42:397|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:17:42:401|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:18:0:115|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:19:0:120|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:19:20:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:19:20:149|Step_LSC|30002312|onExtend:1514042360000 0 0 5
20171223-23:19:20:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042160000##7214##549659##8661##16256##30906710
20171223-23:19:20:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042280000##7214##549659##8661##16256##31004759
20171223-23:19:20:456|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:19:20:461|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:19:21:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:19:21:148|Step_LSC|30002312|onExtend:1514042361000 0 0 0
20171223-23:19:21:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042280000##7214##549659##8661##16256##31004759
20171223-23:19:21:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042280000##7214##549659##8661##16256##31005759
20171223-23:19:21:455|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:19:21:464|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:20:0:112|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:21:0:125|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:22:0:118|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:23:15:139|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:23:15:149|Step_LSC|30002312|onExtend:1514042595000 0 0 5
20171223-23:23:15:441|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042280000##7214##549659##8661##16256##31005759
20171223-23:23:15:441|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042520000##7214##549659##8661##16256##31239760
20171223-23:23:15:466|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:23:15:473|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:23:19:141|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:23:19:149|Step_LSC|30002312|onExtend:1514042599000 0 0 0
20171223-23:23:19:446|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042520000##7214##549659##8661##16256##31239760
20171223-23:23:19:447|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042520000##7214##549659##8661##16256##31243766
20171223-23:23:19:463|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:23:19:467|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:23:50:452|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:24:0:103|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:25:0:122|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:26:0:140|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:27:0:143|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:28:0:124|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:29:0:116|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:29:4:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:29:4:145|Step_LSC|30002312|onExtend:1514042944000 0 0 5
20171223-23:29:4:438|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042520000##7214##549659##8661##16256##31243766
20171223-23:29:4:438|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042880000##7214##549659##8661##16256##31588757
20171223-23:29:4:456|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:29:4:459|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:29:5:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:29:5:148|Step_LSC|30002312|onExtend:1514042945000 0 0 0
20171223-23:29:5:438|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042880000##7214##549659##8661##16256##31588757
20171223-23:29:5:438|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514042880000##7214##549659##8661##16256##31589757
20171223-23:29:5:448|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:29:5:452|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:30:0:165|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:31:0:123|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:32:0:140|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:32:28:792|HiH_HiSyncControl|30002312|startTimer start autoSync
20171223-23:32:28:794|HiH_HiSyncControl|30002312|startSync hiSyncOption = HiSyncOption{syncAction=1, syncMethod=2, syncScope=0, syncDataType=20000, syncModel=2, pushAction=0},app = 1 who = 1
20171223-23:32:28:794|HiH_HiSyncControl|30002312|stepSyncOrNot appSynTimes is 0
20171223-23:32:28:795|HiH_HiSyncControl|30002312|needAutoSync autoSyncSwitch is open
20171223-23:32:28:795|HiH_HiSyncControl|30002312|initDataPrivacy the dataPrivacy switch is open, start push health data!
20171223-23:32:28:796|HiH_|30002312|initDataPrivacy the dataPrivacy is true
20171223-23:32:28:796|HiH_HiSyncControl|30002312|initUserPrivacy the userPrivacy switch is open, start push user data!
20171223-23:32:28:796|HiH_|30002312|initUserPrivacy the userPrivacy is true
20171223-23:32:28:796|HiH_HiSyncControl|30002312|ifCanSync not! no cloud version
20171223-23:32:28:796|HiH_HiBroadcastUtil|30002312|sendSyncFailedBroadcast
20171223-23:33:0:134|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:34:0:84|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:35:0:131|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:36:0:147|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:37:0:167|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:37:14:139|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:37:14:147|Step_LSC|30002312|onExtend:1514043434000 0 0 5
20171223-23:37:14:440|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514042880000##7214##549659##8661##16256##31589757
20171223-23:37:14:441|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514043360000##7214##549659##8661##16256##32078759
20171223-23:37:14:451|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:37:14:456|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:37:16:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:37:16:149|Step_LSC|30002312|onExtend:1514043436000 0 0 0
20171223-23:37:16:440|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514043360000##7214##549659##8661##16256##32078759
20171223-23:37:16:441|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514043360000##7214##549659##8661##16256##32080760
20171223-23:37:16:456|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:37:16:460|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:38:0:131|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:39:0:132|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:40:0:134|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:40:10:144|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:40:10:150|Step_LSC|30002312|onExtend:1514043610000 0 0 5
20171223-23:40:10:445|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514043360000##7214##549659##8661##16256##32080760
20171223-23:40:10:446|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514043540000##7214##549659##8661##16256##32254765
20171223-23:40:10:463|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:40:10:472|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:40:14:139|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:40:14:153|Step_LSC|30002312|onExtend:1514043614000 0 0 0
20171223-23:40:14:440|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514043540000##7214##549659##8661##16256##32254765
20171223-23:40:14:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514043540000##7214##549659##8661##16256##32258759
20171223-23:40:14:457|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:40:14:462|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:41:0:155|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:42:0:134|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:42:38:906|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:42:38:915|Step_LSC|30002312|onExtend:1514043759000 0 0 0
20171223-23:42:39:207|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514043540000##7214##549659##8661##16256##32258759
20171223-23:42:39:207|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514043660000##7214##549659##8661##16256##32403526
20171223-23:42:39:221|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:42:39:225|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:43:0:139|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:44:0:163|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:45:0:128|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:46:0:137|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:46:7:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:46:7:148|Step_LSC|30002312|onExtend:1514043967000 0 0 5
20171223-23:46:7:444|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514043660000##7214##549659##8661##16256##32403526
20171223-23:46:7:444|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514043900000##7214##549659##8661##16256##32611763
20171223-23:46:7:464|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:46:7:467|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:46:14:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:46:14:150|Step_LSC|30002312|onExtend:1514043974000 0 0 0
20171223-23:46:14:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514043900000##7214##549659##8661##16256##32611763
20171223-23:46:14:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514043900000##7214##549659##8661##16256##32618758
20171223-23:46:14:457|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:46:14:462|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:47:0:156|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:48:0:156|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:49:0:142|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:50:0:130|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:50:48:406|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:50:48:414|Step_LSC|30002312|onExtend:1514044249000 0 0 0
20171223-23:50:48:707|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514043900000##7214##549659##8661##16256##32618758
20171223-23:50:48:708|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044140000##7214##549659##8661##16256##32893026
20171223-23:50:48:718|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:50:48:722|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:51:0:123|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:52:0:154|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:52:57:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:52:57:155|Step_LSC|30002312|onExtend:1514044377000 0 0 5
20171223-23:52:57:444|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044140000##7214##549659##8661##16256##32893026
20171223-23:52:57:444|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044260000##7214##549659##8661##16256##33021763
20171223-23:52:57:453|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:52:57:456|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:53:0:133|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:53:0:141|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:53:0:147|Step_LSC|30002312|onExtend:1514044380000 0 0 0
20171223-23:53:0:441|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044260000##7214##549659##8661##16256##33021763
20171223-23:53:0:442|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044320000##7214##549659##8661##16256##33024761
20171223-23:53:0:450|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:53:0:453|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:53:15:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:53:15:153|Step_LSC|30002312|onExtend:1514044395000 0 0 5
20171223-23:53:15:438|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044320000##7214##549659##8661##16256##33024761
20171223-23:53:15:439|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044320000##7214##549659##8661##16256##33039758
20171223-23:53:15:455|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:53:15:464|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:53:17:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:53:17:156|Step_LSC|30002312|onExtend:1514044397000 0 0 0
20171223-23:53:17:443|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044320000##7214##549659##8661##16256##33039758
20171223-23:53:17:444|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044320000##7214##549659##8661##16256##33041763
20171223-23:53:17:467|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:53:17:472|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:53:25:139|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:53:25:146|Step_LSC|30002312|onExtend:1514044405000 0 0 5
20171223-23:53:25:440|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044320000##7214##549659##8661##16256##33041763
20171223-23:53:25:441|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044320000##7214##549659##8661##16256##33049759
20171223-23:53:25:452|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:53:25:456|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:53:31:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:53:31:151|Step_LSC|30002312|onExtend:1514044411000 0 0 0
20171223-23:53:31:441|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044320000##7214##549659##8661##16256##33049759
20171223-23:53:31:441|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044320000##7214##549659##8661##16256##33055760
20171223-23:53:31:452|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:53:31:458|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:53:40:143|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:53:40:152|Step_LSC|30002312|onExtend:1514044420000 0 0 5
20171223-23:53:40:447|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044320000##7214##549659##8661##16256##33055760
20171223-23:53:40:448|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044320000##7214##549659##8661##16256##33064766
20171223-23:53:40:466|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:53:40:475|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:53:42:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:53:42:149|Step_LSC|30002312|onExtend:1514044422000 0 0 0
20171223-23:53:42:440|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044320000##7214##549659##8661##16256##33064766
20171223-23:53:42:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044320000##7214##549659##8661##16256##33066759
20171223-23:53:42:456|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:53:42:460|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:53:51:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:53:51:153|Step_LSC|30002312|onExtend:1514044431000 0 0 5
20171223-23:53:51:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044320000##7214##549659##8661##16256##33066759
20171223-23:53:51:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044320000##7214##549659##8661##16256##33075759
20171223-23:53:51:456|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:53:51:470|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:53:57:143|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:53:57:153|Step_LSC|30002312|onExtend:1514044437000 0 0 0
20171223-23:53:57:445|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044320000##7214##549659##8661##16256##33075759
20171223-23:53:57:446|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044320000##7214##549659##8661##16256##33081765
20171223-23:53:57:459|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:53:57:463|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:54:0:148|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:54:28:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:54:28:160|Step_LSC|30002312|onExtend:1514044468000 0 0 5
20171223-23:54:28:452|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044320000##7214##549659##8661##16256##33081765
20171223-23:54:28:452|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044380000##7214##549659##8661##16256##33112771
20171223-23:54:28:462|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:54:28:467|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:54:29:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:54:29:149|Step_LSC|30002312|onExtend:1514044469000 0 0 0
20171223-23:54:29:438|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044380000##7214##549659##8661##16256##33112771
20171223-23:54:29:439|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044380000##7214##549659##8661##16256##33113758
20171223-23:54:29:456|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:54:29:459|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:54:34:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:54:34:147|Step_LSC|30002312|onExtend:1514044474000 0 0 5
20171223-23:54:34:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044380000##7214##549659##8661##16256##33113758
20171223-23:54:34:439|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044380000##7214##549659##8661##16256##33118758
20171223-23:54:34:448|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:54:34:452|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:54:35:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:54:35:147|Step_LSC|30002312|onExtend:1514044475000 0 0 0
20171223-23:54:35:442|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044380000##7214##549659##8661##16256##33118758
20171223-23:54:35:442|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044380000##7214##549659##8661##16256##33119761
20171223-23:54:35:460|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:54:35:465|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:54:45:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:54:45:151|Step_LSC|30002312|onExtend:1514044485000 0 0 5
20171223-23:54:45:448|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044380000##7214##549659##8661##16256##33119761
20171223-23:54:45:449|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044380000##7214##549659##8661##16256##33129767
20171223-23:54:45:465|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:54:45:470|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:54:47:146|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:54:47:155|Step_LSC|30002312|onExtend:1514044487000 0 0 0
20171223-23:54:47:453|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044380000##7214##549659##8661##16256##33129767
20171223-23:54:47:454|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044380000##7214##549659##8661##16256##33131773
20171223-23:54:47:465|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:54:47:469|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:54:54:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:54:54:163|Step_LSC|30002312|onExtend:1514044494000 0 0 5
20171223-23:54:54:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044380000##7214##549659##8661##16256##33131773
20171223-23:54:54:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044380000##7214##549659##8661##16256##33138759
20171223-23:54:54:448|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:54:54:451|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:54:56:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:54:56:157|Step_LSC|30002312|onExtend:1514044496000 0 0 0
20171223-23:54:56:451|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044380000##7214##549659##8661##16256##33138759
20171223-23:54:56:452|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044380000##7214##549659##8661##16256##33140770
20171223-23:54:56:468|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:54:56:476|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:55:0:146|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:55:24:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:55:24:150|Step_LSC|30002312|onExtend:1514044524000 0 0 5
20171223-23:55:24:442|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044380000##7214##549659##8661##16256##33140770
20171223-23:55:24:443|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044440000##7214##549659##8661##16256##33168762
20171223-23:55:24:467|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:55:24:472|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:55:25:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:55:25:147|Step_LSC|30002312|onExtend:1514044525000 0 0 0
20171223-23:55:25:441|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044440000##7214##549659##8661##16256##33168762
20171223-23:55:25:442|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044440000##7214##549659##8661##16256##33169760
20171223-23:55:25:456|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:55:25:460|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:56:0:148|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:56:12:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:56:12:161|Step_LSC|30002312|onExtend:1514044572000 0 0 5
20171223-23:56:12:448|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044440000##7214##549659##8661##16256##33169760
20171223-23:56:12:449|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044500000##7214##549659##8661##16256##33216767
20171223-23:56:12:465|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:56:12:474|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:56:13:144|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:56:13:150|Step_LSC|30002312|onExtend:1514044573000 0 0 0
20171223-23:56:13:445|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044500000##7214##549659##8661##16256##33216767
20171223-23:56:13:445|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044500000##7214##549659##8661##16256##33217764
20171223-23:56:13:454|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:56:13:457|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:56:28:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:56:28:145|Step_LSC|30002312|onExtend:1514044588000 0 0 5
20171223-23:56:28:443|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044500000##7214##549659##8661##16256##33217764
20171223-23:56:28:444|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044500000##7214##549659##8661##16256##33232763
20171223-23:56:28:460|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:56:28:464|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:56:35:137|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:56:35:152|Step_LSC|30002312|onExtend:1514044595000 0 0 0
20171223-23:56:35:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044500000##7214##549659##8661##16256##33232763
20171223-23:56:35:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044500000##7214##549659##8661##16256##33239758
20171223-23:56:35:460|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:56:35:467|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:57:0:132|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:57:34:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:57:34:156|Step_LSC|30002312|onExtend:1514044654000 0 0 5
20171223-23:57:34:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044500000##7214##549659##8661##16256##33239758
20171223-23:57:34:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044560000##7214##549659##8661##16256##33298759
20171223-23:57:34:457|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:57:34:464|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:57:42:138|Step_LSC|30002312|onStandStepChanged 3786
20171223-23:57:42:147|Step_LSC|30002312|onExtend:1514044662000 0 0 0
20171223-23:57:42:443|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044560000##7214##549659##8661##16256##33298759
20171223-23:57:42:444|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044560000##7214##549659##8661##16256##33306763
20171223-23:57:42:464|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=131208
20171223-23:57:42:470|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=240
20171223-23:58:0:117|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171223-23:59:0:135|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:0:0:215|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:0:0:216|Step_StandStepDataManager|30002312|theDayChanged 1514044800216
20171224-0:0:0:218|Step_StandStepDataManager|30002312|initEnviroument
20171224-0:0:0:218|Step_SPUtils|30002312|getStepCounterStatus
20171224-0:0:0:218|Step_SPUtils|30002312| getStepCounterStatus= true
20171224-0:0:0:218|Step_StandStepCounter|30002312|reStartStepCounter
20171224-0:0:0:223|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:0:0:226|Step_StandStepCounter|30002312|registersensorsuccess: true
20171224-0:0:0:227|Step_DataCache|30002312|clear()
20171224-0:0:0:228|Step_SPUtils|30002312| getTodayBasicStandardSteps= 1514018661124##0##3428
20171224-0:0:0:229|Step_StandStepDataManager|30002312|tryToRecordAsBasicStepData bWrite true
20171224-0:0:0:229|Step_SPUtils|30002312| getDiffTotalSteps= 1513958400215##0
20171224-0:0:0:229|Step_SPUtils|30002312|setBasicStandardSteps basicStandardStep=3786 restartSteps=0
20171224-0:0:0:232|Step_SPUtils|30002312|setDiffTotalSteps=0
20171224-0:0:0:233|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044560000##7214##549659##8661##16256##33306763
20171224-0:0:0:233|Step_SPUtils|30002312| getTodayBasicStandardSteps= 1514044800223##3786##0
20171224-0:0:0:233|Step_SPUtils|30002312| getDiffTotalSteps= 1514044800231##0
20171224-0:0:0:234|Step_StandStepDataManager|30002312|tryToReloadTodayBasicSteps1514044800223|3786|0|0
20171224-0:0:0:234|Step_StandStepDataManager|30002312|reload : 3786 restar 0
20171224-0:0:0:234|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044560000##7214##549659##8661##16256##33306763
20171224-0:0:0:234|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044740000##0##549659##8661##16256##33444553
20171224-0:0:0:235|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:0:0:235|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:0:0:236|Step_StandReportReceiver|30002312|REPORT : 0 0 0 0
20171224-0:0:0:237|Step_HGNH|30002312|next day:steps0mLastReport7214
20171224-0:0:0:237|Step_SPUtils|30002312|setGoalNotifiShownRecord 1514044800237 false
20171224-0:0:0:240|Step_HGNH|30002312|closeNotification...
20171224-0:0:0:240|Step_NotificationUtil|30002312|deleteHealthNotification() 
20171224-0:0:0:323|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:0:0:329|Step_LSC|30002312|onExtend:1514044801000 0 0 0
20171224-0:0:0:624|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044740000##0##549659##8661##16256##33444553
20171224-0:0:0:624|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044740000##0##549659##8661##16256##33444943
20171224-0:0:0:628|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:0:0:628|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:0:5:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:0:5:152|Step_LSC|30002312|onExtend:1514044805000 0 0 5
20171224-0:0:5:438|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044740000##0##549659##8661##16256##33444943
20171224-0:0:5:438|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044740000##0##549659##8661##16256##33449757
20171224-0:0:5:444|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:0:5:444|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:0:6:144|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:0:6:149|Step_LSC|30002312|onExtend:1514044806000 0 0 0
20171224-0:0:6:445|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044740000##0##549659##8661##16256##33449757
20171224-0:0:6:445|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514044740000##0##549659##8661##16256##33450764
20171224-0:0:6:451|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:0:6:451|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:1:0:154|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:2:0:169|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:3:0:124|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:4:0:136|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:5:0:128|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:5:22:473|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:5:22:478|Step_LSC|30002312|onExtend:1514045123000 0 0 0
20171224-0:5:22:774|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514044740000##0##549659##8661##16256##33450764
20171224-0:5:22:774|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045040000##0##549659##8661##16256##33767093
20171224-0:5:22:779|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:5:22:779|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:6:0:153|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:6:16:562|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:6:16:574|Step_LSC|30002312|onExtend:1514045177000 0 0 0
20171224-0:6:16:863|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045040000##0##549659##8661##16256##33767093
20171224-0:6:16:864|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045100000##0##549659##8661##16256##33821183
20171224-0:6:16:871|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:6:16:871|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:7:0:125|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:7:2:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:7:2:150|Step_LSC|30002312|onExtend:1514045222000 0 0 5
20171224-0:7:2:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045100000##0##549659##8661##16256##33821183
20171224-0:7:2:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045160000##0##549659##8661##16256##33866758
20171224-0:7:2:445|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:7:2:445|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:7:4:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:7:4:153|Step_LSC|30002312|onExtend:1514045224000 0 0 0
20171224-0:7:4:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045160000##0##549659##8661##16256##33866758
20171224-0:7:4:439|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045160000##0##549659##8661##16256##33868758
20171224-0:7:4:447|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:7:4:447|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:7:20:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:7:20:149|Step_LSC|30002312|onExtend:1514045240000 0 0 5
20171224-0:7:20:438|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045160000##0##549659##8661##16256##33868758
20171224-0:7:20:439|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045160000##0##549659##8661##16256##33884757
20171224-0:7:20:444|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:7:20:444|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:7:21:138|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:7:21:149|Step_LSC|30002312|onExtend:1514045241000 0 0 0
20171224-0:7:21:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045160000##0##549659##8661##16256##33884757
20171224-0:7:21:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045160000##0##549659##8661##16256##33885759
20171224-0:7:21:447|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:7:21:447|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:8:0:141|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:9:0:135|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:9:4:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:9:4:148|Step_LSC|30002312|onExtend:1514045344000 0 0 5
20171224-0:9:4:440|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045160000##0##549659##8661##16256##33885759
20171224-0:9:4:441|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045280000##0##549659##8661##16256##33988760
20171224-0:9:4:460|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:9:4:460|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:9:6:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:9:6:148|Step_LSC|30002312|onExtend:1514045346000 0 0 0
20171224-0:9:6:442|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045280000##0##549659##8661##16256##33988760
20171224-0:9:6:442|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045280000##0##549659##8661##16256##33990761
20171224-0:9:6:450|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:9:6:450|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:9:34:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:9:34:153|Step_LSC|30002312|onExtend:1514045374000 0 0 5
20171224-0:9:34:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045280000##0##549659##8661##16256##33990761
20171224-0:9:34:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045280000##0##549659##8661##16256##34018758
20171224-0:9:34:446|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:9:34:446|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:9:35:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:9:35:148|Step_LSC|30002312|onExtend:1514045375000 0 0 0
20171224-0:9:35:440|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045280000##0##549659##8661##16256##34018758
20171224-0:9:35:441|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045280000##0##549659##8661##16256##34019759
20171224-0:9:35:448|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:9:35:448|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:10:0:183|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:10:9:136|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:10:9:143|Step_LSC|30002312|onExtend:1514045409000 0 0 5
20171224-0:10:9:438|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045280000##0##549659##8661##16256##34019759
20171224-0:10:9:438|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045340000##0##549659##8661##16256##34053757
20171224-0:10:9:443|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:10:9:443|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:10:10:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:10:10:148|Step_LSC|30002312|onExtend:1514045410000 0 0 0
20171224-0:10:10:439|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045340000##0##549659##8661##16256##34053757
20171224-0:10:10:440|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045340000##0##549659##8661##16256##34054759
20171224-0:10:10:446|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:10:10:446|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:10:42:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:10:42:149|Step_LSC|30002312|onExtend:1514045442000 0 0 5
20171224-0:10:42:438|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045340000##0##549659##8661##16256##34054759
20171224-0:10:42:438|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045340000##0##549659##8661##16256##34086757
20171224-0:10:42:445|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:10:42:445|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:10:43:138|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:10:43:153|Step_LSC|30002312|onExtend:1514045443000 0 0 0
20171224-0:10:43:440|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045340000##0##549659##8661##16256##34086757
20171224-0:10:43:441|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045340000##0##549659##8661##16256##34087759
20171224-0:10:43:447|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:10:43:448|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:11:0:122|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:11:56:143|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:11:56:151|Step_LSC|30002312|onExtend:1514045516000 0 0 5
20171224-0:11:56:445|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045340000##0##549659##8661##16256##34087759
20171224-0:11:56:446|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045400000##0##549659##8661##16256##34160765
20171224-0:11:56:453|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:11:56:454|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:11:57:141|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:11:57:149|Step_LSC|30002312|onExtend:1514045517000 0 0 0
20171224-0:11:57:442|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045400000##0##549659##8661##16256##34160765
20171224-0:11:57:443|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045400000##0##549659##8661##16256##34161762
20171224-0:11:57:449|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:11:57:450|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:12:0:138|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:13:0:151|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:14:0:160|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:15:0:147|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:15:52:983|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:15:52:990|Step_LSC|30002312|onExtend:1514045753000 0 0 0
20171224-0:15:53:285|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045400000##0##549659##8661##16256##34161762
20171224-0:15:53:286|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514045640000##0##549659##8661##16256##34397604
20171224-0:15:53:292|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:15:53:292|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:16:0:149|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:17:0:126|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:18:0:103|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:19:0:137|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:20:0:128|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:21:0:131|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:22:0:135|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:23:0:141|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:24:0:114|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:25:0:132|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:25:16:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:25:16:146|Step_LSC|30002312|onExtend:1514046316000 0 0 5
20171224-0:25:16:438|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514045640000##0##549659##8661##16256##34397604
20171224-0:25:16:439|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514046240000##0##549659##8661##16256##34960758
20171224-0:25:16:445|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:25:16:445|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:25:17:137|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:25:17:149|Step_LSC|30002312|onExtend:1514046317000 0 0 0
20171224-0:25:17:438|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514046240000##0##549659##8661##16256##34960758
20171224-0:25:17:438|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514046240000##0##549659##8661##16256##34961757
20171224-0:25:17:445|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:25:17:445|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:26:0:114|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:27:0:140|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:28:0:83|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:28:25:151|Step_LSC|30002312|onStandStepChanged 3786
20171224-0:28:25:159|Step_LSC|30002312|onExtend:1514046505000 0 0 5
20171224-0:28:25:453|Step_SPUtils|30002312| getTodayTotalDetailSteps = 1514046240000##0##549659##8661##16256##34961757
20171224-0:28:25:454|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514046420000##0##549659##8661##16256##35149773
20171224-0:28:25:461|Step_ExtSDM|30002312|calculateCaloriesWithCache totalCalories=0
20171224-0:28:25:461|Step_ExtSDM|30002312|calculateAltitudeWithCache totalAltitude=0
20171224-0:28:48:963|Step_StandReportReceiver|30002312|onReceive action: android.intent.action.SCREEN_OFF
20171224-0:31:52:120|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:32:28:791|HiH_HiSyncControl|30002312|startTimer start autoSync
20171224-0:32:28:797|HiH_HiSyncControl|30002312|startSync hiSyncOption = HiSyncOption{syncAction=1, syncMethod=2, syncScope=0, syncDataType=20000, syncModel=2, pushAction=0},app = 1 who = 1
20171224-0:32:28:799|HiH_HiSyncControl|30002312|checkCurrentDay a new day comes , reset basicSyncCondition, currentDay is 20171224 oldDay is 20171223
20171224-0:32:28:800|HiH_HiSyncControl|30002312|stepSyncOrNot appSynTimes is 0
20171224-0:32:28:803|HiH_HiSyncControl|30002312|needAutoSync autoSyncSwitch is open
20171224-0:32:28:804|HiH_HiSyncControl|30002312|initDataPrivacy the dataPrivacy switch is open, start push health data!
20171224-0:32:28:804|HiH_|30002312|initDataPrivacy the dataPrivacy is true
20171224-0:32:28:805|HiH_HiSyncControl|30002312|initUserPrivacy the userPrivacy switch is open, start push user data!
20171224-0:32:28:806|HiH_|30002312|initUserPrivacy the userPrivacy is true
20171224-0:32:28:806|HiH_HiSyncControl|30002312|ifCanSync not! no cloud version
20171224-0:32:28:806|HiH_HiBroadcastUtil|30002312|sendSyncFailedBroadcast
20171224-0:32:41:114|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:37:23:74|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:38:23:301|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:39:14:288|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:42:46:681|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:45:47:485|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:48:29:111|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:49:29:380|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:51:1:747|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:52:20:752|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:53:59:235|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:55:32:22|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:58:53:985|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-0:59:7:581|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-1:0:0:794|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-1:1:0:935|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
20171224-1:2:35:789|Step_LSC|30002312|processHandleBroadcastAction action:android.intent.action.TIME_TICK
081111 101153 26436 INFO dfs.DataNode$PacketResponder: Received block blk_6516880861186877710 of size 67108864 from /10.251.42.84
081111 101206 26380 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-3228470001178394592 terminating
081111 101225 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/randtxt9/_temporary/_task_200811101024_0016_m_000347_0/part-00347. blk_-8426741581316629266
081111 101230 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.29.239:50010 is added to blk_-762982068597249045 size 67108864
081111 101238 26399 INFO dfs.DataNode$DataXceiver: Receiving block blk_-5224756755359850354 src: /10.251.43.192:38028 dest: /10.251.43.192:50010
081111 101243 26312 INFO dfs.DataNode$PacketResponder: Received block blk_-1440254020029439248 of size 67108864 from /10.251.30.85
081111 101316 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.90.64:50010 is added to blk_1441315972355360459 size 67108864
081111 101356 26434 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-9001895211102825241 terminating
081111 101415 26449 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-2025617675484194470 terminating
081111 101444 26469 INFO dfs.DataNode$PacketResponder: Received block blk_-1592043549272047369 of size 67108864 from /10.251.105.189
081111 101455 26895 INFO dfs.DataNode$DataXceiver: Receiving block blk_2583125615128303019 src: /10.251.71.97:54431 dest: /10.251.71.97:50010
081111 101621 24902 INFO dfs.DataNode$DataXceiver: Receiving block blk_4198733391373026104 src: /10.251.106.10:46843 dest: /10.251.106.10:50010
081111 101735 26595 INFO dfs.DataNode$PacketResponder: Received block blk_-5815145248455404269 of size 67108864 from /10.251.121.224
081111 101804 26494 INFO dfs.DataNode$DataXceiver: Receiving block blk_-295306975763175640 src: /10.250.9.207:53270 dest: /10.250.9.207:50010
081111 101954 26414 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_5225719677049010638 terminating
081111 102017 26347 INFO dfs.DataNode$DataXceiver: Receiving block blk_4343207286455274569 src: /10.250.9.207:59759 dest: /10.250.9.207:50010
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:00.008 25746 INFO nova.osapi_compute.wsgi.server [req-38101a0b-2096-447d-96ea-a692162415ae 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2477829
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:00.272 25746 INFO nova.osapi_compute.wsgi.server [req-9bc36dd9-91c5-4314-898a-47625eb93b09 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2577181
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:01.551 25746 INFO nova.osapi_compute.wsgi.server [req-55db2d8d-cdb7-4b4b-993b-429be84c0c3e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2731631
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:01.813 25746 INFO nova.osapi_compute.wsgi.server [req-2a3dc421-6604-42a7-9390-a18dc824d5d6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2580249
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:03.091 25746 INFO nova.osapi_compute.wsgi.server [req-939eb332-c1c1-4e67-99b8-8695f8f1980a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2727931
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:03.358 25746 INFO nova.osapi_compute.wsgi.server [req-b6a4fa91-7414-432a-b725-52b5613d3ca3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2642131
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:04.500 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:04.562 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:04.693 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:04.789 25746 INFO nova.osapi_compute.wsgi.server [req-bbfc3fb8-7cb3-4ac8-801e-c893d1082762 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4256971
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:05.060 25746 INFO nova.osapi_compute.wsgi.server [req-31826992-8435-4e03-bc09-ba9cca2d8ef9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2661140
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:05.185 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:05.186 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:05.367 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:06.321 25746 INFO nova.osapi_compute.wsgi.server [req-7160b3e7-676b-498f-b147-7759d8eaea76 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2563808
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:06.584 25746 INFO nova.osapi_compute.wsgi.server [req-e46f1fc1-61ce-4673-b3c7-f8bd94554273 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2580891
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:07.864 25746 INFO nova.osapi_compute.wsgi.server [req-546e2e6a-b85e-434a-91dc-53a0a9124a4f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2733629
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:08.137 25746 INFO nova.osapi_compute.wsgi.server [req-e2c35e53-06d3-4feb-84b9-705c94d40e5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2694771
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:09.411 25746 INFO nova.osapi_compute.wsgi.server [req-ce9c8a59-c9ba-43b1-9735-318ceabc9216 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2692339
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:09.692 25746 INFO nova.osapi_compute.wsgi.server [req-e1da47c6-0f46-4ce8-940c-05397a5fab9e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2777061
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:10.279 25743 INFO nova.api.openstack.compute.server_external_events [req-ab451068-9756-4ad9-9d18-5ceaa6424627 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:e3871ffd-5cd5-4287-bddd-3529f7b59515 for instance b9000564-fe1a-409b-b8cc-1e88b294cd1d
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:10.285 25743 INFO nova.osapi_compute.wsgi.server [req-ab451068-9756-4ad9-9d18-5ceaa6424627 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0913219
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:10.296 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:10.302 2931 INFO nova.virt.libvirt.driver [-] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:10.303 2931 INFO nova.compute.manager [req-8e64797b-fb99-4c8a-87e5-9a8de673412f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] Took 19.05 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:10.416 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:10.417 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:10.421 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:10.424 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:10.470 2931 INFO nova.compute.manager [req-8e64797b-fb99-4c8a-87e5-9a8de673412f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] Took 19.84 seconds to build instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:10.600 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:10.978 25746 INFO nova.osapi_compute.wsgi.server [req-d81279b2-d9df-48b7-9c36-edab3801c067 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2808621
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:11.243 25746 INFO nova.osapi_compute.wsgi.server [req-22455aab-13cf-4045-92e8-65371ef51485 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2603891
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:13.658 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:14.265 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 1
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:14.266 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=2560MB phys_disk=15GB used_disk=20GB total_vcpus=16 used_vcpus=1 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:14.329 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:15.141 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:15.142 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:15.318 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:16.795 25783 INFO nova.metadata.wsgi.server [req-b40b44ea-c721-4bc4-b1cd-bb238982ede4 - - - - -] 10.11.21.122,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2451560
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:16.806 25783 INFO nova.metadata.wsgi.server [-] 10.11.21.122,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0008290
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:17.120 25786 INFO nova.metadata.wsgi.server [req-f9565d6d-171c-408f-8b5f-9e9792826f42 - - - - -] 10.11.21.122,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2197890
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:17.441 25793 INFO nova.metadata.wsgi.server [req-ed0b5830-26a8-4484-8164-feaebe737259 - - - - -] 10.11.21.122,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2368760
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:17.504 25746 INFO nova.osapi_compute.wsgi.server [req-c53a921a-16c7-422e-8c9d-c922a720d047 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/b9000564-fe1a-409b-b8cc-1e88b294cd1d HTTP/1.1" status: 204 len: 203 time: 0.2534380
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:17.531 25793 INFO nova.metadata.wsgi.server [-] 10.11.21.122,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0010660
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:17.541 2931 INFO nova.compute.manager [req-c53a921a-16c7-422e-8c9d-c922a720d047 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] Terminating instance
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:17.754 2931 INFO nova.virt.libvirt.driver [-] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:17.773 25746 INFO nova.osapi_compute.wsgi.server [req-430aaf51-6fd5-4ede-bab8-7ca540ec136c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2658210
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:17.861 25784 INFO nova.metadata.wsgi.server [req-567bc482-6358-4db5-99c5-b011692d6cf8 - - - - -] 10.11.21.122,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2349129
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:18.450 2931 INFO nova.virt.libvirt.driver [req-c53a921a-16c7-422e-8c9d-c922a720d047 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] Deleting instance files /var/lib/nova/instances/b9000564-fe1a-409b-b8cc-1e88b294cd1d_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:18.451 2931 INFO nova.virt.libvirt.driver [req-c53a921a-16c7-422e-8c9d-c922a720d047 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] Deletion of /var/lib/nova/instances/b9000564-fe1a-409b-b8cc-1e88b294cd1d_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:18.571 2931 INFO nova.compute.manager [req-c53a921a-16c7-422e-8c9d-c922a720d047 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] Took 1.03 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:18.994 25746 INFO nova.osapi_compute.wsgi.server [req-1dd5c6bd-1bda-4e6d-b896-80dc15ab8c56 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.2161739
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:19.050 2931 INFO nova.compute.manager [req-c53a921a-16c7-422e-8c9d-c922a720d047 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] Took 0.48 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:20.106 25746 INFO nova.osapi_compute.wsgi.server [req-750a3ab2-0fba-499a-bad0-f8584e777993 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.1072969
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:20.345 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:20.346 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:20.349 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:21.067 25746 INFO nova.api.openstack.wsgi [req-0b851395-2895-44b9-8265-a27d0bb52910 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:21.069 25746 INFO nova.osapi_compute.wsgi.server [req-0b851395-2895-44b9-8265-a27d0bb52910 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0793190
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:30.788 25746 INFO nova.osapi_compute.wsgi.server [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.6686139
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:30.979 25746 INFO nova.osapi_compute.wsgi.server [req-97738e9d-8df6-4948-89f0-afcd17e1f899 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1901591
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:31.092 2931 INFO nova.compute.claims [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:31.093 2931 INFO nova.compute.claims [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:31.093 2931 INFO nova.compute.claims [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:31.094 2931 INFO nova.compute.claims [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:31.094 2931 INFO nova.compute.claims [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:31.095 2931 INFO nova.compute.claims [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:31.095 2931 INFO nova.compute.claims [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:31.127 2931 INFO nova.compute.claims [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:31.162 25746 INFO nova.osapi_compute.wsgi.server [req-e0e308c0-7fe0-4d30-a7ec-07972df0447c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1796741
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:31.359 25746 INFO nova.osapi_compute.wsgi.server [req-84a068e2-7bf3-4fbf-b480-f41b090acc76 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/96abccce-8d1f-4e07-b6d1-4b2ab87e23b4 HTTP/1.1" status: 200 len: 1708 time: 0.1917260
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:31.699 2931 INFO nova.virt.libvirt.driver [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:32.738 25746 INFO nova.osapi_compute.wsgi.server [req-7a0b0b1d-c0a6-4b5e-b136-946e4779c49e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.3730500
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:32.974 2931 INFO nova.compute.manager [-] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:33.009 25746 INFO nova.osapi_compute.wsgi.server [req-37a1da35-0d55-4c2e-8689-a9c75f3d4f51 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2672291
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:34.280 25746 INFO nova.osapi_compute.wsgi.server [req-e0346c36-c199-4fb3-805c-30036a6a6bb8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2643161
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:34.547 25746 INFO nova.osapi_compute.wsgi.server [req-f1971a67-cc91-4a21-af01-43fcb2b23f5f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2641511
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:35.832 25746 INFO nova.osapi_compute.wsgi.server [req-7ee56f12-b2ee-4eec-a767-d06435b1b2c6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2796621
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:36.095 25746 INFO nova.osapi_compute.wsgi.server [req-e5a94766-dd02-47b2-8ee8-27d1f873a57c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2579432
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:37.363 25746 INFO nova.osapi_compute.wsgi.server [req-63dbb4ce-0c2f-4bdc-a33a-5d6828cbba7a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2629061
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:37.618 25746 INFO nova.osapi_compute.wsgi.server [req-80cf4c1c-dbce-4ad2-87df-2d1c410694d1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2494071
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:38.922 25746 INFO nova.osapi_compute.wsgi.server [req-f309f43f-4929-4345-92bf-d85babef55fd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2974470
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:39.193 25746 INFO nova.osapi_compute.wsgi.server [req-31312dd3-0875-43e3-a6c5-5869e0c7e953 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2665739
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:40.459 25746 INFO nova.osapi_compute.wsgi.server [req-dd5eea8f-ecd9-481e-aa07-ada6bea727cc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2614441
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:40.721 25746 INFO nova.osapi_compute.wsgi.server [req-9f55f8ef-5f3f-4087-9a62-af37a1d6ba75 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2563009
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:42.001 25746 INFO nova.osapi_compute.wsgi.server [req-7510d7d3-bc52-4241-93ea-b03036f20981 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2748721
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:42.271 25746 INFO nova.osapi_compute.wsgi.server [req-c6700d4f-9ed4-4baf-9ac9-ad0e7f66f155 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2646890
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:43.537 25746 INFO nova.osapi_compute.wsgi.server [req-b2aa258b-dbc8-4979-8029-ca24828e3c80 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2596920
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:43.801 25746 INFO nova.osapi_compute.wsgi.server [req-f6148ee1-e3f6-4bb3-a200-847fd8486d77 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2600410
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:44.514 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:44.582 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:44.697 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:45.080 25746 INFO nova.osapi_compute.wsgi.server [req-437ba7af-f981-4699-819a-700389a00f34 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2730091
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:45.249 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:45.250 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:45.356 25746 INFO nova.osapi_compute.wsgi.server [req-8ce73885-a9a6-4631-8b4e-5ff2c1b6f9ee 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2709410
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:45.444 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:46.634 25746 INFO nova.osapi_compute.wsgi.server [req-60c3da91-bd1c-4693-991e-fa3580fa22c2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2722521
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:46.894 25746 INFO nova.osapi_compute.wsgi.server [req-410ed8a3-3cb0-47ca-920a-653be17f284e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2555740
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:48.329 25746 INFO nova.osapi_compute.wsgi.server [req-2d4c68d5-0961-4f37-8cc3-ceb9268e1419 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4287961
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:48.577 25746 INFO nova.osapi_compute.wsgi.server [req-177f6889-979d-4f28-94cc-c911dfd47a30 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2432280
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:49.831 25746 INFO nova.osapi_compute.wsgi.server [req-f632ae73-7d87-49f4-bdbf-3a0f06d197ed 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2486479
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:50.096 25746 INFO nova.osapi_compute.wsgi.server [req-ec90bc75-c4b2-4022-a072-9c9aa8fd7fc5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2611251
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:50.142 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:50.143 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:50.321 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:51.361 25746 INFO nova.osapi_compute.wsgi.server [req-0ba05e2a-7142-4fa0-859c-8573119de464 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2593911
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:51.608 25746 INFO nova.osapi_compute.wsgi.server [req-f3305c24-054a-4bd7-b2cb-959552349f8b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2426131
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:51.633 25743 INFO nova.api.openstack.compute.server_external_events [req-3e22b75e-1c4f-4579-8a73-7ede40d3f955 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:a208479c-c0e3-4730-a5a0-d75e8afd0252 for instance 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:51.638 25743 INFO nova.osapi_compute.wsgi.server [req-3e22b75e-1c4f-4579-8a73-7ede40d3f955 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0901799
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:51.650 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:51.658 2931 INFO nova.virt.libvirt.driver [-] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:51.659 2931 INFO nova.compute.manager [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Took 19.96 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:51.770 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:51.771 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:51.794 2931 INFO nova.compute.manager [req-6a763803-4838-49c7-814e-eaefbaddee9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Took 20.71 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:52.886 25746 INFO nova.osapi_compute.wsgi.server [req-007c1773-4e52-4805-a67c-6b8b4df633f4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2721858
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:53.138 25746 INFO nova.osapi_compute.wsgi.server [req-6d417741-b7d6-40b7-8468-1c87adfebbb8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2486420
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:55.377 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:55.378 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:55.557 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-scheduler.log.1.2017-05-16_13:53:08 2017-05-16 00:00:57.129 25998 INFO nova.scheduler.host_manager [req-d724a3bd-e314-4f81-a41c-460aa91f24ae - - - - -] Successfully synced instances from host 'cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us'.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:58.177 25783 INFO nova.metadata.wsgi.server [req-e78e9555-b566-46e4-8753-94827f0c2910 - - - - -] 10.11.21.123,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2205780
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:58.189 25783 INFO nova.metadata.wsgi.server [-] 10.11.21.123,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0019679
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:58.430 25776 INFO nova.metadata.wsgi.server [req-61196723-e034-487d-82e0-9cc159c8572c - - - - -] 10.11.21.123,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2335150
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:58.445 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.123,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0008781
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:58.766 25778 INFO nova.metadata.wsgi.server [req-f1603c09-bec5-418f-9b75-0f3393afea8a - - - - -] 10.11.21.123,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.2285759
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:59.100 25799 INFO nova.metadata.wsgi.server [req-2d24fed7-3524-414c-9783-e3e042c71543 - - - - -] 10.11.21.123,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2275620
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:59.114 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.123,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0010512
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:59.130 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.123,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.0009170
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:59.145 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.123,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ HTTP/1.1" status: 200 len: 124 time: 0.0008931
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:59.157 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.123,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ami HTTP/1.1" status: 200 len: 119 time: 0.0009408
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:59.172 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.123,10.11.10.1 "GET /latest/meta-data/block-device-mapping/root HTTP/1.1" status: 200 len: 124 time: 0.0018420
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:59.410 25746 INFO nova.osapi_compute.wsgi.server [req-d473bea3-588a-441a-8b2a-a137806f8786 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/96abccce-8d1f-4e07-b6d1-4b2ab87e23b4 HTTP/1.1" status: 204 len: 203 time: 0.2604880
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:59.446 2931 INFO nova.compute.manager [req-d473bea3-588a-441a-8b2a-a137806f8786 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:59.567 25795 INFO nova.metadata.wsgi.server [req-240372b7-bd9c-4cdd-977c-d2dc10bf8526 - - - - -] 10.11.21.123,10.11.10.1 "GET /latest/meta-data/placement/ HTTP/1.1" status: 200 len: 134 time: 0.3841610
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:59.678 25746 INFO nova.osapi_compute.wsgi.server [req-0dd87940-76ef-4556-bfb6-3cb42c58fc82 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2644901
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:59.733 2931 INFO nova.compute.manager [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Running instance usage audit for host cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us from 2017-05-16 05:00:00 to 2017-05-16 06:00:00. 87 instances.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:59.739 2931 INFO nova.virt.libvirt.driver [-] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Instance destroyed successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:00.419 2931 INFO nova.virt.libvirt.driver [req-d473bea3-588a-441a-8b2a-a137806f8786 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Deleting instance files /var/lib/nova/instances/96abccce-8d1f-4e07-b6d1-4b2ab87e23b4_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:00.421 2931 INFO nova.virt.libvirt.driver [req-d473bea3-588a-441a-8b2a-a137806f8786 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Deletion of /var/lib/nova/instances/96abccce-8d1f-4e07-b6d1-4b2ab87e23b4_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:00.563 2931 INFO nova.compute.manager [req-d473bea3-588a-441a-8b2a-a137806f8786 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Took 1.11 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:01.033 25746 INFO nova.osapi_compute.wsgi.server [req-a9b72c1a-65a9-44a5-bcb3-db8c2aa6d404 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.3479111
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:01.080 2931 INFO nova.compute.manager [req-d473bea3-588a-441a-8b2a-a137806f8786 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] Took 0.52 seconds to deallocate network for instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:01.698 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:01.699 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:01.700 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:02.141 25746 INFO nova.osapi_compute.wsgi.server [req-cd5e40af-8fe5-4bd7-ae38-b5fb546d31ff 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.1034541
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:03.114 25746 INFO nova.api.openstack.wsgi [req-fff6fe1a-cbb6-4b38-806a-afee069d7c13 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:03.116 25746 INFO nova.osapi_compute.wsgi.server [req-fff6fe1a-cbb6-4b38-806a-afee069d7c13 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0886950
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:12.693 25746 INFO nova.osapi_compute.wsgi.server [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.5442920
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:12.885 25746 INFO nova.osapi_compute.wsgi.server [req-4e83daf7-a24c-4ab4-96ff-1da5848255ad 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1878400
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:12.998 2931 INFO nova.compute.claims [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:12.999 2931 INFO nova.compute.claims [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:12.999 2931 INFO nova.compute.claims [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:13.000 2931 INFO nova.compute.claims [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:13.000 2931 INFO nova.compute.claims [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:13.001 2931 INFO nova.compute.claims [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:13.002 2931 INFO nova.compute.claims [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:13.033 2931 INFO nova.compute.claims [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:13.086 25746 INFO nova.osapi_compute.wsgi.server [req-b7948826-c8b0-4d2c-9933-1580f19fdf93 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.1955159
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:13.179 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:13.291 25746 INFO nova.osapi_compute.wsgi.server [req-9bc8b0af-864a-421b-a2bc-b0472952e7c1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/b562ef10-ba2d-48ae-bf4a-18666cba4a51 HTTP/1.1" status: 200 len: 1708 time: 0.2013841
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:13.569 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 1
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:13.570 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=2560MB phys_disk=15GB used_disk=20GB total_vcpus=16 used_vcpus=1 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:13.623 2931 INFO nova.virt.libvirt.driver [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Creating image
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:13.632 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:14.560 25746 INFO nova.osapi_compute.wsgi.server [req-b543ced1-368e-4997-bd19-bfe484087365 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2633560
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:14.735 2931 INFO nova.compute.manager [-] [instance: 96abccce-8d1f-4e07-b6d1-4b2ab87e23b4] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:14.819 25746 INFO nova.osapi_compute.wsgi.server [req-428f28af-b5be-46d2-a334-c261235ec80d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2561328
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:16.100 25746 INFO nova.osapi_compute.wsgi.server [req-45f80172-398a-4dc5-be8b-2d5a3509a95c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2740800
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:16.368 25746 INFO nova.osapi_compute.wsgi.server [req-ccf53bd0-3d1f-4951-858c-c5b0416e7ccf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2638359
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:17.654 25746 INFO nova.osapi_compute.wsgi.server [req-e89e7c8f-91d3-41c6-8483-ca10554084c7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2793601
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:17.920 25746 INFO nova.osapi_compute.wsgi.server [req-9e6e6ab0-4a0e-4d51-aed1-028d418862ad 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2611539
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:19.202 25746 INFO nova.osapi_compute.wsgi.server [req-6b8f0c42-a8aa-4947-918c-385855184318 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2773159
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:19.475 25746 INFO nova.osapi_compute.wsgi.server [req-d133dda3-dd32-4846-bca9-96dfeb3e9657 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2688291
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:20.752 25746 INFO nova.osapi_compute.wsgi.server [req-aeff33d0-dd58-4d76-bfec-d25b843a0521 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2706490
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:21.044 25746 INFO nova.osapi_compute.wsgi.server [req-f0ebc071-5c5d-4bfa-aea6-9a6d1704b71b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2866180
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:22.317 25746 INFO nova.osapi_compute.wsgi.server [req-7909bf67-0040-4290-a996-c16ad91da03b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2673891
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:22.584 25746 INFO nova.osapi_compute.wsgi.server [req-2c6c341b-b271-4b81-8187-e4b32d472053 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2630482
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:23.863 25746 INFO nova.osapi_compute.wsgi.server [req-3d954aae-f4f5-47a6-9cf8-a8e00beeab93 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2733018
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:24.133 25746 INFO nova.osapi_compute.wsgi.server [req-11200554-a543-43c3-ac95-d855b3bd8b32 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2652190
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:25.141 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:25.142 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:25.331 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:25.388 25746 INFO nova.osapi_compute.wsgi.server [req-f19e6425-c9a9-46d2-9439-7e9b0d955ed5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2491391
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:25.647 25746 INFO nova.osapi_compute.wsgi.server [req-cebeeef5-8804-4bfc-beca-716acf433be4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2543209
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:26.460 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:26.524 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:26.643 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:26.930 25746 INFO nova.osapi_compute.wsgi.server [req-f2287178-8d2a-4421-a098-c154341ae628 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2777491
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:27.193 25746 INFO nova.osapi_compute.wsgi.server [req-370681d7-1260-4326-b10a-112d8c56c41e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2589149
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:28.558 25746 INFO nova.osapi_compute.wsgi.server [req-1cd04999-4cd8-418c-9117-ecaf3e129b33 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3592770
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:28.807 25746 INFO nova.osapi_compute.wsgi.server [req-1ff7c697-95a2-4bf6-9198-158d16989796 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2463491
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:30.095 25746 INFO nova.osapi_compute.wsgi.server [req-b413cc90-4f89-4c1c-8049-0931c23830a3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2825699
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:30.142 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:30.142 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:30.319 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:30.358 25746 INFO nova.osapi_compute.wsgi.server [req-5e81acc6-75f4-4110-ac6a-3adf677d4d9f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2592661
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:31.811 25746 INFO nova.osapi_compute.wsgi.server [req-1c33bdb8-a945-437c-a2a4-836c25c502ef 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4467819
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:32.073 25746 INFO nova.osapi_compute.wsgi.server [req-bababf49-3a32-4c04-937b-14c494b95f3d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2577350
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:32.963 25743 INFO nova.api.openstack.compute.server_external_events [req-01451338-4c87-48c4-9dbc-a9831f8f6d41 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:d1ce4c5a-2811-41c3-8a5a-39f9e6defcb8 for instance b562ef10-ba2d-48ae-bf4a-18666cba4a51
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:32.969 25743 INFO nova.osapi_compute.wsgi.server [req-01451338-4c87-48c4-9dbc-a9831f8f6d41 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.1043310
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:32.985 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:32.992 2931 INFO nova.virt.libvirt.driver [-] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:32.992 2931 INFO nova.compute.manager [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Took 19.37 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:33.103 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:33.105 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:33.126 2931 INFO nova.compute.manager [req-97fcea79-42f7-4241-9b9e-63fe193c1929 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Took 20.14 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:33.365 25746 INFO nova.osapi_compute.wsgi.server [req-8f929d23-bb12-4951-9b49-f8df8837aa5e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2870619
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:33.618 25746 INFO nova.osapi_compute.wsgi.server [req-05bac4af-55c4-4d90-bd9e-f782858bd4d1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2505140
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:35.147 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:35.148 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:35.325 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:39.421 25775 INFO nova.metadata.wsgi.server [req-35ea282d-a35a-408a-b01f-a07c9d2957da - - - - -] 10.11.21.124,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2212229
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:39.734 25799 INFO nova.metadata.wsgi.server [req-b6d4a270-accb-4c3a-8179-9611e52e1768 - - - - -] 10.11.21.124,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.2249990
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:39.821 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.124,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0006270
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:39.835 25775 INFO nova.metadata.wsgi.server [-] 10.11.21.124,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0007789
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:39.847 25775 INFO nova.metadata.wsgi.server [-] 10.11.21.124,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0010281
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:39.885 25746 INFO nova.osapi_compute.wsgi.server [req-31d09752-6f82-4fc5-ac97-416b9c865af4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/b562ef10-ba2d-48ae-bf4a-18666cba4a51 HTTP/1.1" status: 204 len: 203 time: 0.2582572
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:39.924 2931 INFO nova.compute.manager [req-31d09752-6f82-4fc5-ac97-416b9c865af4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:40.090 25776 INFO nova.metadata.wsgi.server [req-04d59c69-59a1-47ec-8179-f86938f6da66 - - - - -] 10.11.21.124,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2326000
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:40.141 2931 INFO nova.virt.libvirt.driver [-] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:40.157 25746 INFO nova.osapi_compute.wsgi.server [req-96c3ec98-21a0-4af2-84a8-d4989512413e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2677610
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:40.382 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:40.383 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:40.597 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:40.859 2931 INFO nova.virt.libvirt.driver [req-31d09752-6f82-4fc5-ac97-416b9c865af4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Deleting instance files /var/lib/nova/instances/b562ef10-ba2d-48ae-bf4a-18666cba4a51_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:40.861 2931 INFO nova.virt.libvirt.driver [req-31d09752-6f82-4fc5-ac97-416b9c865af4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Deletion of /var/lib/nova/instances/b562ef10-ba2d-48ae-bf4a-18666cba4a51_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:40.979 2931 INFO nova.compute.manager [req-31d09752-6f82-4fc5-ac97-416b9c865af4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Took 1.05 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:41.372 25746 INFO nova.osapi_compute.wsgi.server [req-40c6c6d9-caf9-426e-b739-a5562aa4043a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.2106130
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:41.435 2931 INFO nova.compute.manager [req-31d09752-6f82-4fc5-ac97-416b9c865af4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] Took 0.45 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:42.476 25746 INFO nova.osapi_compute.wsgi.server [req-4cb2bae3-c745-454c-bdca-bd5f7c53340f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0977290
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:43.495 25746 INFO nova.api.openstack.wsgi [req-a0893d5a-dc60-4c49-82e5-b9f7dfc2f6ab f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:43.497 25746 INFO nova.osapi_compute.wsgi.server [req-a0893d5a-dc60-4c49-82e5-b9f7dfc2f6ab f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.1146111
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:45.116 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:45.117 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:45.119 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Base or swap file too young to remove: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:50.149 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:50.150 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:50.152 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:52.949 25746 INFO nova.osapi_compute.wsgi.server [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.4586949
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:53.157 25746 INFO nova.osapi_compute.wsgi.server [req-64dca5a6-dde8-45f1-9e76-e001afd2caa9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.2041020
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:53.256 2931 INFO nova.compute.claims [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:53.257 2931 INFO nova.compute.claims [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:53.258 2931 INFO nova.compute.claims [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:53.259 2931 INFO nova.compute.claims [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:53.259 2931 INFO nova.compute.claims [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:53.260 2931 INFO nova.compute.claims [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:53.261 2931 INFO nova.compute.claims [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:53.293 2931 INFO nova.compute.claims [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:53.338 25746 INFO nova.osapi_compute.wsgi.server [req-de9d35cf-5e55-41c8-85ed-eae0a39f53f4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.1767941
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:53.538 25746 INFO nova.osapi_compute.wsgi.server [req-323b974a-761f-4406-b306-b93e2bb5e478 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/78dc1847-8848-49cc-933e-9239b12c9dcf HTTP/1.1" status: 200 len: 1708 time: 0.1951439
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:53.893 2931 INFO nova.virt.libvirt.driver [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:54.818 25746 INFO nova.osapi_compute.wsgi.server [req-e7b18c45-b875-4590-b598-617401f382c3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2754171
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:55.100 25746 INFO nova.osapi_compute.wsgi.server [req-93ebec7f-31c0-4efe-bd3c-b139f832b447 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2773421
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:01:55.266 2931 INFO nova.compute.manager [-] [instance: b562ef10-ba2d-48ae-bf4a-18666cba4a51] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:56.385 25746 INFO nova.osapi_compute.wsgi.server [req-cd862c89-18a0-4954-a1a0-f3f26df71d39 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2794621
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:56.636 25746 INFO nova.osapi_compute.wsgi.server [req-ddab777b-9479-4ebb-b0a0-459ea7fd50b7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2469411
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:57.910 25746 INFO nova.osapi_compute.wsgi.server [req-09b59776-544a-4963-b0ae-b19efe78d211 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2686541
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:58.172 25746 INFO nova.osapi_compute.wsgi.server [req-1a086eaf-4641-48d5-bcd7-32c91302df8f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2569268
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:59.428 25746 INFO nova.osapi_compute.wsgi.server [req-87fb3150-d67d-4752-9f09-376914ef72ec 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2497211
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:01:59.664 25746 INFO nova.osapi_compute.wsgi.server [req-e584ef57-6eeb-4444-bd9a-e1bf5ea3149b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2322080
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:00.935 25746 INFO nova.osapi_compute.wsgi.server [req-bce85102-ddb3-4323-aa2f-073373c4af50 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2648559
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:01.197 25746 INFO nova.osapi_compute.wsgi.server [req-45d4275d-2214-4159-b61e-168da60fc29d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2567871
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:02.464 25746 INFO nova.osapi_compute.wsgi.server [req-5ef90541-0ddc-426b-a12a-3ca3cb0b9b81 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2612560
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:02.726 25746 INFO nova.osapi_compute.wsgi.server [req-f7783f57-038a-4d59-af27-9c554d8fd072 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2585540
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:03.997 25746 INFO nova.osapi_compute.wsgi.server [req-ee142be0-90ed-45f1-a82c-1f2097469a7c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2649550
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:04.277 25746 INFO nova.osapi_compute.wsgi.server [req-d4353962-c926-4d9c-87ea-4cc0834b395a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2747011
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:05.159 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:05.160 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:05.358 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:05.653 25746 INFO nova.osapi_compute.wsgi.server [req-77d6ad13-1155-4342-a655-eca5f333bf88 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3706589
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:05.920 25746 INFO nova.osapi_compute.wsgi.server [req-91f43d98-99af-4c3c-ae42-753ae149ae99 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2630000
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:07.196 25746 INFO nova.osapi_compute.wsgi.server [req-f75f4c92-16e4-4a6c-9ab5-fc2155379490 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2694011
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:07.339 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:07.403 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] VM Paused (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:07.471 25746 INFO nova.osapi_compute.wsgi.server [req-55ebb1b1-6e88-44d5-be79-fe8b37e64ecf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2707460
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:07.518 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:08.740 25746 INFO nova.osapi_compute.wsgi.server [req-b4692194-e82b-4c3b-9f95-afd1bcb5d768 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2636631
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:09.012 25746 INFO nova.osapi_compute.wsgi.server [req-44701b65-e05c-4855-8fd1-ffb90c9b978e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2675440
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:10.284 25746 INFO nova.osapi_compute.wsgi.server [req-91380ad7-eb23-433c-aba8-4bd0747e1c66 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2670941
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:10.413 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:10.414 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:10.601 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:10.715 25746 INFO nova.osapi_compute.wsgi.server [req-a6d2fe15-c840-4b5f-b058-88ee45b5a7fe 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4255710
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:11.992 25746 INFO nova.osapi_compute.wsgi.server [req-3cea2065-89b0-4f75-ad1a-338464ef9cd3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2708280
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:12.275 25746 INFO nova.osapi_compute.wsgi.server [req-deb9d948-8712-41cb-b575-9d67db2bfc62 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2789159
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:13.540 25746 INFO nova.osapi_compute.wsgi.server [req-23ece7f9-4204-4654-a696-11f5ecfb3f17 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2591121
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:13.815 25746 INFO nova.osapi_compute.wsgi.server [req-1271f5ec-7a4e-4176-b507-2fc69a6142c9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2703741
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:14.170 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:14.310 25743 INFO nova.api.openstack.compute.server_external_events [req-74354c38-3eb0-4bde-a63d-5a53b008030c f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:b26c4935-6377-4e96-9ba1-ec6985d42d5d for instance 78dc1847-8848-49cc-933e-9239b12c9dcf
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:14.315 25743 INFO nova.osapi_compute.wsgi.server [req-74354c38-3eb0-4bde-a63d-5a53b008030c f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0921431
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:14.351 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:14.362 2931 INFO nova.virt.libvirt.driver [-] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:14.362 2931 INFO nova.compute.manager [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Took 20.47 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:14.476 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:14.477 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:14.496 2931 INFO nova.compute.manager [req-caeb3818-dab6-4e8d-9ea6-aceb23905ebc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Took 21.25 seconds to build instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:14.720 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 1
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:14.720 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=2560MB phys_disk=15GB used_disk=20GB total_vcpus=16 used_vcpus=1 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:14.775 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:15.095 25746 INFO nova.osapi_compute.wsgi.server [req-c55c1814-b208-4043-9018-b4f8851ac580 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2735250
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:15.138 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:15.139 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:15.307 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:15.358 25746 INFO nova.osapi_compute.wsgi.server [req-9d2c1d24-f157-464f-aa8c-17ebcdaac12b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2577691
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:20.366 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:20.367 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:20.549 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:20.779 25788 INFO nova.metadata.wsgi.server [req-60baf898-c32b-4e88-916e-2bf4c115fa33 - - - - -] 10.11.21.125,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2202821
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:21.082 25793 INFO nova.metadata.wsgi.server [req-1f64aad7-bba8-415a-8664-7c6d747c60e4 - - - - -] 10.11.21.125,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.2119691
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:21.327 25791 INFO nova.metadata.wsgi.server [req-e420b0c2-4377-47db-85dc-dec44f304472 - - - - -] 10.11.21.125,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2296579
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:21.625 25746 INFO nova.osapi_compute.wsgi.server [req-06631678-1e19-4e4e-bddf-a588d8ea6217 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/78dc1847-8848-49cc-933e-9239b12c9dcf HTTP/1.1" status: 204 len: 203 time: 0.2560370
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:21.664 2931 INFO nova.compute.manager [req-06631678-1e19-4e4e-bddf-a588d8ea6217 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:21.763 25776 INFO nova.metadata.wsgi.server [req-f642cadd-7508-4db5-a059-07998751face - - - - -] 10.11.21.125,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2418430
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:21.879 2931 INFO nova.virt.libvirt.driver [-] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:21.913 25746 INFO nova.osapi_compute.wsgi.server [req-136a1713-e90c-489c-b626-cabb0f68302a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2837551
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:22.569 2931 INFO nova.virt.libvirt.driver [req-06631678-1e19-4e4e-bddf-a588d8ea6217 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Deleting instance files /var/lib/nova/instances/78dc1847-8848-49cc-933e-9239b12c9dcf_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:22.571 2931 INFO nova.virt.libvirt.driver [req-06631678-1e19-4e4e-bddf-a588d8ea6217 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Deletion of /var/lib/nova/instances/78dc1847-8848-49cc-933e-9239b12c9dcf_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:22.693 2931 INFO nova.compute.manager [req-06631678-1e19-4e4e-bddf-a588d8ea6217 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Took 1.02 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:23.131 25746 INFO nova.osapi_compute.wsgi.server [req-f0bffbc3-5ab0-4916-91c1-0a61dd7d4ec2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.2131531
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:23.171 2931 INFO nova.compute.manager [req-06631678-1e19-4e4e-bddf-a588d8ea6217 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] Took 0.48 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:24.244 25746 INFO nova.osapi_compute.wsgi.server [req-16984fba-a60d-4234-9a5c-2f5f9bb45081 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.1063731
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:25.118 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:25.119 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:25.120 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Base or swap file too young to remove: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:25.212 25746 INFO nova.api.openstack.wsgi [req-3fa8a45e-031e-4f3b-b327-b9cd4210f3ba f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:25.214 25746 INFO nova.osapi_compute.wsgi.server [req-3fa8a45e-031e-4f3b-b327-b9cd4210f3ba f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.1029620
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:30.114 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:30.115 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:30.118 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:34.773 25746 INFO nova.osapi_compute.wsgi.server [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.5169401
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:34.974 25746 INFO nova.osapi_compute.wsgi.server [req-878d8f43-04d4-4bce-a27a-3feca48ba5eb 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1964839
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:35.071 2931 INFO nova.compute.claims [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:35.072 2931 INFO nova.compute.claims [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:35.073 2931 INFO nova.compute.claims [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:35.074 2931 INFO nova.compute.claims [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:35.075 2931 INFO nova.compute.claims [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:35.076 2931 INFO nova.compute.claims [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:35.076 2931 INFO nova.compute.claims [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:35.109 2931 INFO nova.compute.claims [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:35.170 25746 INFO nova.osapi_compute.wsgi.server [req-bb86cdcb-2b6e-410f-b422-7209c14f060d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.1929309
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:35.348 25746 INFO nova.osapi_compute.wsgi.server [req-07bc4660-d894-4b67-8bb7-0b1ebe315af2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/95960536-049b-41f6-9049-05fc479b6a7c HTTP/1.1" status: 200 len: 1708 time: 0.1721809
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:35.942 2931 INFO nova.virt.libvirt.driver [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:36.629 25746 INFO nova.osapi_compute.wsgi.server [req-89a2dec8-a82e-4108-ab4a-eda9c9bc896d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2763519
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:36.925 25746 INFO nova.osapi_compute.wsgi.server [req-723faf03-db43-4131-a339-c792974c7721 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2896700
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:37.163 2931 INFO nova.compute.manager [-] [instance: 78dc1847-8848-49cc-933e-9239b12c9dcf] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:38.214 25746 INFO nova.osapi_compute.wsgi.server [req-008605fa-fa0a-483e-b596-a17329409763 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2833471
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:38.477 25746 INFO nova.osapi_compute.wsgi.server [req-c58e5c9b-8211-4631-a4dc-8be93e96a0d2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2586210
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:39.736 25746 INFO nova.osapi_compute.wsgi.server [req-44d3573a-0d2c-4610-85f3-f228ac82a1d1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2529991
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:40.006 25746 INFO nova.osapi_compute.wsgi.server [req-40d207ea-99bc-47cd-b37b-d2bb7e7e030c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2665081
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:41.268 25746 INFO nova.osapi_compute.wsgi.server [req-1756bc65-fb0b-450c-9e51-57bf965737b7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2555981
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:41.518 25746 INFO nova.osapi_compute.wsgi.server [req-713cc4cb-1c01-47b1-b2d5-e5c54f7e9d21 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2455752
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:42.794 25746 INFO nova.osapi_compute.wsgi.server [req-35722e9e-e5fb-47d9-8e20-d5237bd7440c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2711270
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:43.184 25746 INFO nova.osapi_compute.wsgi.server [req-39477c65-5728-4dd9-8e3e-d8c89db36e91 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3852520
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:44.461 25746 INFO nova.osapi_compute.wsgi.server [req-6dd4cb0c-c909-4935-af86-a978ccb6a461 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2695961
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:44.710 25746 INFO nova.osapi_compute.wsgi.server [req-7bf56606-1c53-49cb-9d8d-e6afa1920cc1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2449391
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:45.986 25746 INFO nova.osapi_compute.wsgi.server [req-77b6a0fd-dcab-47dd-8465-13cbe5434d40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2681870
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:46.262 25746 INFO nova.osapi_compute.wsgi.server [req-9d8cd0a4-1fa9-4eb1-ad29-42d2756029e0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2719712
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:47.528 25746 INFO nova.osapi_compute.wsgi.server [req-100e1170-b42e-4840-8c18-618eec0dbb92 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2595811
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:47.792 25746 INFO nova.osapi_compute.wsgi.server [req-80d1efbd-1f02-4091-9827-adafc6003e3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2598870
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:48.985 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] VM Started (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:49.074 25746 INFO nova.osapi_compute.wsgi.server [req-ff142455-b2d5-4551-ad81-56244e8b4246 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2751970
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:49.156 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:49.290 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:49.351 25746 INFO nova.osapi_compute.wsgi.server [req-bb582317-a5f0-432d-b7a1-604ce4024a61 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2720761
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:50.202 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:50.203 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:50.381 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:50.764 25746 INFO nova.osapi_compute.wsgi.server [req-6e4f4f57-ce56-4885-956a-dc1235c5afb2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4072819
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:51.020 25746 INFO nova.osapi_compute.wsgi.server [req-964322ee-29e9-4c5d-a07c-9d30a2f01ee2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2523489
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:52.299 25746 INFO nova.osapi_compute.wsgi.server [req-6264d86e-7a74-4647-a528-8e476a256e94 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2726710
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:52.572 25746 INFO nova.osapi_compute.wsgi.server [req-08190bc9-9cd6-4d14-a825-06421a17de6c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2684422
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:53.856 25746 INFO nova.osapi_compute.wsgi.server [req-17ee8196-ec23-4adc-acc9-4c24686d8b68 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2791021
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:54.125 25746 INFO nova.osapi_compute.wsgi.server [req-efe668bd-0162-4b21-b1c0-d962abc9f6ba 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2633080
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:55.399 25746 INFO nova.osapi_compute.wsgi.server [req-1ea0f0f5-7810-4564-b77e-b6729aec28e8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2678630
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:55.541 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:55.542 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:55.621 25743 INFO nova.api.openstack.compute.server_external_events [req-e265ea4c-ba4b-44f0-b086-7420e34e17cb f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:1cf11429-1563-4f6c-823c-91ba6e0f6675 for instance 95960536-049b-41f6-9049-05fc479b6a7c
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:55.626 25743 INFO nova.osapi_compute.wsgi.server [req-e265ea4c-ba4b-44f0-b086-7420e34e17cb f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0867331
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:55.637 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:55.646 2931 INFO nova.virt.libvirt.driver [-] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:55.647 2931 INFO nova.compute.manager [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Took 19.71 seconds to spawn the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:55.673 25746 INFO nova.osapi_compute.wsgi.server [req-4115658f-3cd0-4dc0-9025-dbf0e4581302 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2711699
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:55.733 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:55.757 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:55.757 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:02:55.778 2931 INFO nova.compute.manager [req-d38f479d-9bb9-4276-9688-52607e8fd350 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Took 20.72 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:56.955 25746 INFO nova.osapi_compute.wsgi.server [req-62ff267f-85a7-45e5-be31-1b23edc89a22 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2752779
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:02:57.225 25746 INFO nova.osapi_compute.wsgi.server [req-e0e52eae-603e-45e1-9b5e-06c76033bfd5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2668512
nova-scheduler.log.1.2017-05-16_13:53:08 2017-05-16 00:02:58.484 25998 INFO nova.scheduler.host_manager [req-53427a68-ae6f-4b07-aace-853b0890e9b3 - - - - -] Successfully synced instances from host 'cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us'.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:00.138 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:00.139 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:00.303 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:01.916 25778 INFO nova.metadata.wsgi.server [req-cb87c9f0-3bb5-4102-b97e-6066266f5cf9 - - - - -] 10.11.21.126,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2225840
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:01.929 25778 INFO nova.metadata.wsgi.server [-] 10.11.21.126,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0006402
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:02.250 25799 INFO nova.metadata.wsgi.server [req-2886b8ba-64e7-4ee7-862d-0e6af9fb9fe9 - - - - -] 10.11.21.126,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2317870
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:02.262 25778 INFO nova.metadata.wsgi.server [-] 10.11.21.126,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0009999
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:02.276 25778 INFO nova.metadata.wsgi.server [-] 10.11.21.126,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0010130
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:02.503 25775 INFO nova.metadata.wsgi.server [req-386447cd-167b-4202-aeaa-fd90d85b53b8 - - - - -] 10.11.21.126,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2158179
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:02.587 25775 INFO nova.metadata.wsgi.server [-] 10.11.21.126,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0012150
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:02.678 25778 INFO nova.metadata.wsgi.server [-] 10.11.21.126,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.0008578
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:03.094 25788 INFO nova.metadata.wsgi.server [req-d7729064-6e3e-4d44-b65f-5b329ba18a74 - - - - -] 10.11.21.126,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ HTTP/1.1" status: 200 len: 124 time: 0.4023941
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:03.110 25788 INFO nova.metadata.wsgi.server [-] 10.11.21.126,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ami HTTP/1.1" status: 200 len: 119 time: 0.0029080
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:03.343 25791 INFO nova.metadata.wsgi.server [req-27e91939-3ba4-4d14-80e1-47d8fbb230ff - - - - -] 10.11.21.126,10.11.10.1 "GET /latest/meta-data/block-device-mapping/root HTTP/1.1" status: 200 len: 124 time: 0.2206309
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:03.497 25746 INFO nova.osapi_compute.wsgi.server [req-a6b9779d-b384-4e84-b3d0-bac079760244 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/95960536-049b-41f6-9049-05fc479b6a7c HTTP/1.1" status: 204 len: 203 time: 0.2639730
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:03.534 2931 INFO nova.compute.manager [req-a6b9779d-b384-4e84-b3d0-bac079760244 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:03.582 25779 INFO nova.metadata.wsgi.server [req-68a0fcfe-55d9-4f80-8fef-d9915932d2ef - - - - -] 10.11.21.126,10.11.10.1 "GET /latest/meta-data/placement/ HTTP/1.1" status: 200 len: 134 time: 0.2258282
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:03.750 2931 INFO nova.virt.libvirt.driver [-] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:03.767 25746 INFO nova.osapi_compute.wsgi.server [req-0f4ba393-7607-4e33-aac9-bca155244bd5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2653639
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:04.425 2931 INFO nova.virt.libvirt.driver [req-a6b9779d-b384-4e84-b3d0-bac079760244 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Deleting instance files /var/lib/nova/instances/95960536-049b-41f6-9049-05fc479b6a7c_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:04.427 2931 INFO nova.virt.libvirt.driver [req-a6b9779d-b384-4e84-b3d0-bac079760244 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Deletion of /var/lib/nova/instances/95960536-049b-41f6-9049-05fc479b6a7c_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:04.543 2931 INFO nova.compute.manager [req-a6b9779d-b384-4e84-b3d0-bac079760244 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Took 1.00 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:04.968 25746 INFO nova.osapi_compute.wsgi.server [req-f00e1f15-d28f-491d-95ad-07d51491f3af 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.1953011
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:05.004 2931 INFO nova.compute.manager [req-a6b9779d-b384-4e84-b3d0-bac079760244 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] Took 0.46 seconds to deallocate network for instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:05.181 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:05.182 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:05.287 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:06.075 25746 INFO nova.osapi_compute.wsgi.server [req-9bba2bda-1367-48fa-906a-e1632f23d1fc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.1013889
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:07.021 25746 INFO nova.api.openstack.wsgi [req-c4d0c20c-cfe8-4e66-b280-b083419d4967 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:07.022 25746 INFO nova.osapi_compute.wsgi.server [req-c4d0c20c-cfe8-4e66-b280-b083419d4967 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0833371
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:10.318 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:10.319 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:10.320 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:16.161 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:16.610 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 0
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:16.611 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=512MB phys_disk=15GB used_disk=0GB total_vcpus=16 used_vcpus=0 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:16.676 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:16.800 25746 INFO nova.osapi_compute.wsgi.server [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.7116742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:17.097 25746 INFO nova.osapi_compute.wsgi.server [req-49348d37-0ea0-4804-b3d1-fe84b2fdfa12 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.2927220
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:17.220 2931 INFO nova.compute.claims [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:17.221 2931 INFO nova.compute.claims [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:17.222 2931 INFO nova.compute.claims [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:17.223 2931 INFO nova.compute.claims [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:17.224 2931 INFO nova.compute.claims [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:17.224 2931 INFO nova.compute.claims [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:17.225 2931 INFO nova.compute.claims [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:17.260 2931 INFO nova.compute.claims [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:17.286 25746 INFO nova.osapi_compute.wsgi.server [req-c77665c9-8474-4c2c-94e9-da8b5eccc4a4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1848371
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:17.473 25746 INFO nova.osapi_compute.wsgi.server [req-2a800172-ad24-4031-9a51-d3495146717f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5 HTTP/1.1" status: 200 len: 1572 time: 0.1838732
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:17.931 2931 INFO nova.virt.libvirt.driver [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:18.775 25746 INFO nova.osapi_compute.wsgi.server [req-1357c966-8c54-46e4-be1d-b9651e26fa66 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2973850
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:19.075 25746 INFO nova.osapi_compute.wsgi.server [req-08b757f2-16eb-4ebd-8cef-1b922ef51b5e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2967250
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:19.284 2931 INFO nova.compute.manager [-] [instance: 95960536-049b-41f6-9049-05fc479b6a7c] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:20.359 25746 INFO nova.osapi_compute.wsgi.server [req-9b15f752-78b7-4fc5-8d2b-54f619e0e2b5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2771640
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:20.627 25746 INFO nova.osapi_compute.wsgi.server [req-19e7ad40-3594-41b0-a83a-40333f767ebd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2650268
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:21.908 25746 INFO nova.osapi_compute.wsgi.server [req-e6e405d0-64f1-4959-b359-ba64936c6144 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2754779
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:22.173 25746 INFO nova.osapi_compute.wsgi.server [req-5e19ca64-9d79-46dc-af69-0bfe959dadd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2620330
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:23.451 25746 INFO nova.osapi_compute.wsgi.server [req-86997220-e959-4cef-8315-723f20d4f1de 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2708549
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:23.710 25746 INFO nova.osapi_compute.wsgi.server [req-32d71807-a737-4853-b385-bed9ed2bfb49 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2557549
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:25.003 25746 INFO nova.osapi_compute.wsgi.server [req-4ffeb467-110b-4615-a5a8-11f1ccb2bf27 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2863061
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:25.263 25746 INFO nova.osapi_compute.wsgi.server [req-36edee2a-9189-477f-be64-1ff36bf2933b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2571361
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:26.537 25746 INFO nova.osapi_compute.wsgi.server [req-aa55b411-2ba0-4b61-9fd9-26d40aeb9423 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2682309
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:26.806 25746 INFO nova.osapi_compute.wsgi.server [req-308202e3-7228-440e-aa2e-70558059a68b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2644999
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:28.089 25746 INFO nova.osapi_compute.wsgi.server [req-af7be5c1-cb29-4dba-b7eb-d008e01c9690 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2762442
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:28.355 25746 INFO nova.osapi_compute.wsgi.server [req-77cc3c0e-d34a-49e1-b132-eef9a76b64aa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2619240
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:29.621 25746 INFO nova.osapi_compute.wsgi.server [req-bafa2157-685a-4091-b1e9-eb8635e92273 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2600479
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:29.879 25746 INFO nova.osapi_compute.wsgi.server [req-206aa9c3-b7b4-41e7-92a2-70cff6538462 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2538040
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:30.161 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:30.162 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:30.357 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:31.011 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:31.081 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] VM Paused (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:31.161 25746 INFO nova.osapi_compute.wsgi.server [req-135542a4-6100-4643-83f4-c3d3271ac898 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2776730
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:31.202 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:31.429 25746 INFO nova.osapi_compute.wsgi.server [req-213c462a-869c-4e96-95c0-5d9b49fd5f79 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2630680
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:32.684 25746 INFO nova.osapi_compute.wsgi.server [req-385c4fd4-fc22-4fc7-b1c9-ff7fea96847c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2493019
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:32.958 25746 INFO nova.osapi_compute.wsgi.server [req-e28bae73-ef07-429a-af86-3b7b89a80422 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2702570
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:34.385 25746 INFO nova.osapi_compute.wsgi.server [req-f5f0905b-6c4a-42d1-b5c3-6b1da21ed657 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4212601
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:34.646 25746 INFO nova.osapi_compute.wsgi.server [req-41e63879-5cf5-4846-8bb8-4caf07985a8c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2571011
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:35.416 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:35.417 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:35.602 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:35.914 25746 INFO nova.osapi_compute.wsgi.server [req-826238fb-3373-47c4-b6ef-2576b3bc286f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2608051
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:36.172 25746 INFO nova.osapi_compute.wsgi.server [req-0b3194a7-fbae-4214-88fe-39afd63e25a7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2536151
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:36.979 25743 INFO nova.api.openstack.compute.server_external_events [req-3be0ba1e-d96d-483e-b14d-e3c6c34f36f7 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:85754923-baff-45ad-9e35-a3055c898234 for instance 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:36.986 25743 INFO nova.osapi_compute.wsgi.server [req-3be0ba1e-d96d-483e-b14d-e3c6c34f36f7 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.1039391
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:36.996 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:37.002 2931 INFO nova.virt.libvirt.driver [-] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:37.003 2931 INFO nova.compute.manager [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Took 19.07 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:37.113 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:37.115 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:37.133 2931 INFO nova.compute.manager [req-2d658d2c-7eff-414e-a68f-3f1c75c9e874 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Took 19.92 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:37.446 25746 INFO nova.osapi_compute.wsgi.server [req-f32a2c19-2c81-45f7-8506-f8e539929139 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2687280
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:37.708 25746 INFO nova.osapi_compute.wsgi.server [req-f204b576-164a-45e0-b31f-e57265e8864a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2570262
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:40.655 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:40.656 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:40.829 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:43.324 25779 INFO nova.metadata.wsgi.server [req-258a0b43-37d6-4f22-98b8-e7863a77ef43 - - - - -] 10.11.21.127,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2254272
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:43.635 25791 INFO nova.metadata.wsgi.server [req-1cbcfc19-86a6-45ba-be47-f13c3af17b9e - - - - -] 10.11.21.127,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.2243600
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:43.963 25774 INFO nova.metadata.wsgi.server [req-851b5cec-0231-43f7-a3f1-47e8d1a1f2b5 - - - - -] 10.11.21.127,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2370040
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:43.998 25746 INFO nova.osapi_compute.wsgi.server [req-c6d4eab2-e008-4384-a149-8ff001ca4cb6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5 HTTP/1.1" status: 204 len: 203 time: 0.2801199
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:44.031 2931 INFO nova.compute.manager [req-c6d4eab2-e008-4384-a149-8ff001ca4cb6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:44.221 25778 INFO nova.metadata.wsgi.server [req-ecbd525f-38ad-49a5-a65d-6b43af9227ab - - - - -] 10.11.21.127,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2475102
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:44.247 2931 INFO nova.virt.libvirt.driver [-] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:44.384 25746 INFO nova.osapi_compute.wsgi.server [req-330a4477-cfaf-485f-a5e9-69e0e9be0a4f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.3824501
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:44.908 2931 INFO nova.virt.libvirt.driver [req-c6d4eab2-e008-4384-a149-8ff001ca4cb6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Deleting instance files /var/lib/nova/instances/7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:44.910 2931 INFO nova.virt.libvirt.driver [req-c6d4eab2-e008-4384-a149-8ff001ca4cb6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Deletion of /var/lib/nova/instances/7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:45.022 2931 INFO nova.compute.manager [req-c6d4eab2-e008-4384-a149-8ff001ca4cb6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Took 0.99 seconds to destroy the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:45.162 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:45.163 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:45.270 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:45.565 2931 INFO nova.compute.manager [req-c6d4eab2-e008-4384-a149-8ff001ca4cb6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] Took 0.54 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:45.605 25746 INFO nova.osapi_compute.wsgi.server [req-bc2d4d95-02a9-48df-9933-d07c2e4a7ba3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.2152700
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:46.701 25746 INFO nova.osapi_compute.wsgi.server [req-bdd0d3f5-8b43-4b08-ad83-4753e5dd25c2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0908029
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:47.601 25746 INFO nova.api.openstack.wsgi [req-a567979f-c5a4-42af-ae34-4707d45e2d19 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:47.602 25746 INFO nova.osapi_compute.wsgi.server [req-a567979f-c5a4-42af-ae34-4707d45e2d19 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0918391
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:50.300 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:50.301 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:50.302 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:57.214 25746 INFO nova.osapi_compute.wsgi.server [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.5000288
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:57.417 25746 INFO nova.osapi_compute.wsgi.server [req-61ab7fb1-ea13-4170-9529-ed2c20312112 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1983159
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:57.509 2931 INFO nova.compute.claims [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:57.510 2931 INFO nova.compute.claims [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:57.511 2931 INFO nova.compute.claims [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:57.512 2931 INFO nova.compute.claims [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:57.512 2931 INFO nova.compute.claims [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:57.513 2931 INFO nova.compute.claims [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:57.514 2931 INFO nova.compute.claims [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:57.551 2931 INFO nova.compute.claims [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:57.603 25746 INFO nova.osapi_compute.wsgi.server [req-4581b3d6-dfb3-4463-be9b-bff865be3b7c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.1811130
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:57.799 25746 INFO nova.osapi_compute.wsgi.server [req-ee54c54f-9e2f-4f81-b6d9-4a1d5f2d3143 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/af5f7392-f7d4-4298-b647-c98924c64aa1 HTTP/1.1" status: 200 len: 1708 time: 0.1915350
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:58.129 2931 INFO nova.virt.libvirt.driver [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:59.088 25746 INFO nova.osapi_compute.wsgi.server [req-9c29c95b-af64-4c50-b94c-5e9c11ed5386 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2832489
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:03:59.244 2931 INFO nova.compute.manager [-] [instance: 7e7cc42f-3cb9-4d91-804c-f5a32d54f1c5] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:03:59.378 25746 INFO nova.osapi_compute.wsgi.server [req-6edf1bc7-00ce-410b-9215-df6910965afa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2856178
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:00.643 25746 INFO nova.osapi_compute.wsgi.server [req-2545b8fd-40f0-4e23-988d-b56699acc006 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2583179
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:00.910 25746 INFO nova.osapi_compute.wsgi.server [req-0cfdf7d7-2973-4aa9-a729-01793f1e0211 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2635262
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:02.178 25746 INFO nova.osapi_compute.wsgi.server [req-52403199-280b-4272-82f8-8de3f18c3c47 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2608440
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:02.437 25746 INFO nova.osapi_compute.wsgi.server [req-53a10b92-552c-4e90-9f52-b1a8c3ced361 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2543302
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:03.712 25746 INFO nova.osapi_compute.wsgi.server [req-eb2674b8-424e-4647-8551-ac3f76b8158f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2697911
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:03.984 25746 INFO nova.osapi_compute.wsgi.server [req-b19871ff-db0a-46c5-8eef-a1edef1e2de8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2688050
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:05.251 25746 INFO nova.osapi_compute.wsgi.server [req-b205f1b9-cbfe-4e2a-9ba7-37286386e48c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2615452
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:05.524 25746 INFO nova.osapi_compute.wsgi.server [req-cf54ca47-9d41-4346-80df-3b12a85567a4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2689090
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:06.778 25746 INFO nova.osapi_compute.wsgi.server [req-707b6cbd-f9dd-4e7f-905b-f6cda1e13ce8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2474949
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:07.061 25746 INFO nova.osapi_compute.wsgi.server [req-5de95d80-f71b-490c-8d14-92b6f68286f6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2795429
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:08.324 25746 INFO nova.osapi_compute.wsgi.server [req-7e612509-4ae5-4741-b376-839bd045d819 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2575760
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:08.586 25746 INFO nova.osapi_compute.wsgi.server [req-a48835ba-fc76-4d9f-870b-5d7f0956db98 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2599471
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:09.850 25746 INFO nova.osapi_compute.wsgi.server [req-038072fc-4104-4e19-a0d5-f29a252fd887 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2583978
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:10.117 25746 INFO nova.osapi_compute.wsgi.server [req-1451b9ac-5cf6-4d69-a2e1-52175b593775 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2634912
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:10.190 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:10.190 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:10.371 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:11.255 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:11.320 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] VM Paused (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:11.407 25746 INFO nova.osapi_compute.wsgi.server [req-2aa79807-3ca7-40ed-92cf-b00558282857 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2837162
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:11.456 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:11.703 25746 INFO nova.osapi_compute.wsgi.server [req-2fea3fa7-dd69-4371-b956-e6edcdbf175b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2923260
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:13.089 25746 INFO nova.osapi_compute.wsgi.server [req-ec9d2f4e-9c80-4752-bd12-de53bdea6321 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3798852
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:13.503 25746 INFO nova.osapi_compute.wsgi.server [req-ea3a5f31-5950-484f-be55-67a9df287f2e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4113560
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:14.766 25746 INFO nova.osapi_compute.wsgi.server [req-5b3396e6-da74-4756-8c45-fb22ed4200f4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2566571
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:15.044 25746 INFO nova.osapi_compute.wsgi.server [req-9397f963-fad4-47eb-96e8-39f4f4552343 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2735012
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:15.177 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:15.178 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:15.363 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:16.306 25746 INFO nova.osapi_compute.wsgi.server [req-fea0b5a8-c597-4190-af41-69c4c624c06a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2578111
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:16.569 25746 INFO nova.osapi_compute.wsgi.server [req-9120b63e-fdd9-4dc6-9ccd-7888f3d38fcd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2586780
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:17.372 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:17.828 25746 INFO nova.osapi_compute.wsgi.server [req-54356141-cc49-4eb1-a69e-3f6833ea9a63 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2530420
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:17.948 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 1
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:17.949 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=2560MB phys_disk=15GB used_disk=20GB total_vcpus=16 used_vcpus=1 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:18.012 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:18.104 25746 INFO nova.osapi_compute.wsgi.server [req-5060aaf4-0f21-48ed-892a-9f8918c76841 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2708540
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:18.445 25743 INFO nova.api.openstack.compute.server_external_events [req-0b7fefce-6f00-464a-969a-d6799e8bba35 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:7bdba89d-dd80-489c-ab06-11d494c5c478 for instance af5f7392-f7d4-4298-b647-c98924c64aa1
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:18.450 25743 INFO nova.osapi_compute.wsgi.server [req-0b7fefce-6f00-464a-969a-d6799e8bba35 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0926709
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:18.462 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:18.473 2931 INFO nova.virt.libvirt.driver [-] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:18.473 2931 INFO nova.compute.manager [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Took 20.35 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:18.581 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:18.582 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:18.614 2931 INFO nova.compute.manager [req-d6986b54-3735-4a42-9074-0ba7d9717de9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Took 21.11 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:19.365 25746 INFO nova.osapi_compute.wsgi.server [req-76572632-711f-443c-97fd-3910faa34e1f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2556660
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:19.634 25746 INFO nova.osapi_compute.wsgi.server [req-434ba9f2-b0d5-4cfd-8a12-16024f09e6eb 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2654181
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:20.141 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:20.142 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:20.332 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:24.819 25778 INFO nova.metadata.wsgi.server [req-b1e650c2-d691-43dc-bd4b-43a1cc250f36 - - - - -] 10.11.21.128,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2413990
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:25.052 25783 INFO nova.metadata.wsgi.server [req-1e7b94f6-9cde-4749-9677-71235d2b0cad - - - - -] 10.11.21.128,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.2219090
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:25.372 25788 INFO nova.metadata.wsgi.server [req-ffe3e7b2-f553-4460-aa21-9f437d384256 - - - - -] 10.11.21.128,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2238111
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:25.389 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:25.389 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:25.577 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:25.614 25776 INFO nova.metadata.wsgi.server [req-129bcb0b-f78e-42bf-8553-a34ff5884e77 - - - - -] 10.11.21.128,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2283430
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:25.923 25746 INFO nova.osapi_compute.wsgi.server [req-7c98765b-5005-4eb1-b863-0e66d8c312c4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/af5f7392-f7d4-4298-b647-c98924c64aa1 HTTP/1.1" status: 204 len: 203 time: 0.2809131
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:25.952 25786 INFO nova.metadata.wsgi.server [req-7f28b5ca-c3e5-4e98-876e-cc3c0065dcb6 - - - - -] 10.11.21.128,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.2495749
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:25.958 2931 INFO nova.compute.manager [req-7c98765b-5005-4eb1-b863-0e66d8c312c4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Terminating instance
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:26.173 2931 INFO nova.virt.libvirt.driver [-] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:26.204 25746 INFO nova.osapi_compute.wsgi.server [req-4bdf00b0-3bbc-44fa-bd84-de85ec43c8a2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2769570
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:26.267 25790 INFO nova.metadata.wsgi.server [req-e8586e79-0c6d-45b1-a1d2-3d8133877961 - - - - -] 10.11.21.128,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2267981
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:26.853 2931 INFO nova.virt.libvirt.driver [req-7c98765b-5005-4eb1-b863-0e66d8c312c4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Deleting instance files /var/lib/nova/instances/af5f7392-f7d4-4298-b647-c98924c64aa1_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:26.855 2931 INFO nova.virt.libvirt.driver [req-7c98765b-5005-4eb1-b863-0e66d8c312c4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Deletion of /var/lib/nova/instances/af5f7392-f7d4-4298-b647-c98924c64aa1_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:26.975 2931 INFO nova.compute.manager [req-7c98765b-5005-4eb1-b863-0e66d8c312c4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Took 1.01 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:27.388 25746 INFO nova.osapi_compute.wsgi.server [req-2bf7cfee-a236-42f3-8fb1-96fefab0b302 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.1794369
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:27.457 2931 INFO nova.compute.manager [req-7c98765b-5005-4eb1-b863-0e66d8c312c4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] Took 0.48 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:28.489 25746 INFO nova.osapi_compute.wsgi.server [req-dc3bb50f-58cf-4aab-ae70-8d41b2009f52 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0951850
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:29.480 25746 INFO nova.api.openstack.wsgi [req-60f50a9d-827b-4fc8-b8c7-dc0bbe15c936 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:29.482 25746 INFO nova.osapi_compute.wsgi.server [req-60f50a9d-827b-4fc8-b8c7-dc0bbe15c936 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0870681
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:30.114 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:30.115 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:30.117 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Base or swap file too young to remove: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:35.143 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:35.144 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:35.145 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:38.992 25746 INFO nova.osapi_compute.wsgi.server [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.4953768
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:39.184 25746 INFO nova.osapi_compute.wsgi.server [req-969a61db-496a-4350-8b5b-ff1bc11eb114 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1885760
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:39.301 2931 INFO nova.compute.claims [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:39.301 2931 INFO nova.compute.claims [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:39.302 2931 INFO nova.compute.claims [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:39.302 2931 INFO nova.compute.claims [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:39.303 2931 INFO nova.compute.claims [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:39.303 2931 INFO nova.compute.claims [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:39.304 2931 INFO nova.compute.claims [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:39.339 2931 INFO nova.compute.claims [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:39.373 25746 INFO nova.osapi_compute.wsgi.server [req-c8ccbfa8-315f-4095-83c4-b30936e668ad 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1856170
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:39.566 25746 INFO nova.osapi_compute.wsgi.server [req-aaefd74f-734e-484c-872f-cc6fe0a37a8f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/ae3a1b5d-eec1-45bb-b76a-c59d83b1471f HTTP/1.1" status: 200 len: 1708 time: 0.1892228
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:39.920 2931 INFO nova.virt.libvirt.driver [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:40.849 25746 INFO nova.osapi_compute.wsgi.server [req-e6509c84-a275-459c-b564-25fdbaa58cdc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2789471
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:41.156 25746 INFO nova.osapi_compute.wsgi.server [req-596345be-48b5-46f8-a2d8-79378eb77a59 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.3028941
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:41.170 2931 INFO nova.compute.manager [-] [instance: af5f7392-f7d4-4298-b647-c98924c64aa1] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:42.437 25746 INFO nova.osapi_compute.wsgi.server [req-3a90789d-5475-4e5d-8013-cfe3f8388818 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2747879
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:42.710 25746 INFO nova.osapi_compute.wsgi.server [req-a4c7cd49-f302-49d5-b690-07572719c6a0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2691431
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:43.976 25746 INFO nova.osapi_compute.wsgi.server [req-30cef5d6-4405-4d45-8372-bb6e7d306da7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2594419
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:44.252 25746 INFO nova.osapi_compute.wsgi.server [req-3fdad801-640b-40e9-8554-508a00a681ad 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2723150
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:45.518 25746 INFO nova.osapi_compute.wsgi.server [req-8db66943-ce30-49e8-998c-7235e8f971c2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2594271
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:45.792 25746 INFO nova.osapi_compute.wsgi.server [req-b2ffcdcc-26b5-4e4a-9d90-2922ac5b9b02 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2687330
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:47.064 25746 INFO nova.osapi_compute.wsgi.server [req-be718301-57f1-4762-937d-8171d3785f5d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2661371
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:47.328 25746 INFO nova.osapi_compute.wsgi.server [req-5e6e042b-f9e8-4ab4-add4-b4809623b13a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2591050
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:48.611 25746 INFO nova.osapi_compute.wsgi.server [req-72a5e10c-fdb5-447b-8ece-adf07b6992ac 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2774739
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:48.881 25746 INFO nova.osapi_compute.wsgi.server [req-68e17e36-b9b8-4b5d-9096-6bd81ce33bbc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2648020
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:50.198 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:50.200 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:50.280 25746 INFO nova.osapi_compute.wsgi.server [req-ec4801c6-49fe-4c54-8978-87ec1dca9133 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3924651
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:50.385 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:50.550 25746 INFO nova.osapi_compute.wsgi.server [req-5e38f94a-fd4a-4898-93e0-48033a8a2a1c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2656732
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:51.812 25746 INFO nova.osapi_compute.wsgi.server [req-e3697d4c-fc1d-4871-a2ca-bf214a413669 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2555609
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:52.092 25746 INFO nova.osapi_compute.wsgi.server [req-aad2bb5d-8811-4355-9590-ea1c90040ac5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2747231
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:52.697 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:52.767 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:52.893 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:53.530 25746 INFO nova.osapi_compute.wsgi.server [req-61b292c7-db97-46b5-820a-3e3c7c22493a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4331501
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:53.807 25746 INFO nova.osapi_compute.wsgi.server [req-1a40db47-25b1-456f-82a6-62af7d642a7b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2719719
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:55.080 25746 INFO nova.osapi_compute.wsgi.server [req-9023d31d-10a0-4c42-808c-25a63f1f7380 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2676129
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:55.338 25746 INFO nova.osapi_compute.wsgi.server [req-2bd7010e-8e5a-4ed3-afbe-9cfb2918fce7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2540510
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:55.435 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:55.436 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:04:55.615 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:56.613 25746 INFO nova.osapi_compute.wsgi.server [req-aed90710-a0a8-43a6-bd76-ae5572354e8d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2693179
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:56.885 25746 INFO nova.osapi_compute.wsgi.server [req-5cb84c28-f466-4fad-949b-23da89895861 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2679729
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:58.163 25746 INFO nova.osapi_compute.wsgi.server [req-93b87e0b-d97f-4334-9d9f-44974b048f51 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2718840
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:58.430 25746 INFO nova.osapi_compute.wsgi.server [req-242354ec-cd50-4661-9a66-be40b6b679cb 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2633321
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:58.630 25751 INFO nova.osapi_compute.wsgi.server [req-ec851719-5052-4fee-8b0a-3c33b586efd1 d16a600c5e2a47fe98aee00ee4cb9743 e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "GET /v2/e9746973ac574c6b8a9e8857f56a7608/servers/detail?all_tenants=True&changes-since=2017-05-16T05%3A54%3A58.530160%2B00%3A00 HTTP/1.1" status: 200 len: 23370 time: 0.3273299
nova-scheduler.log.1.2017-05-16_13:53:08 2017-05-16 00:04:59.397 25998 INFO nova.scheduler.host_manager [req-8ac8250d-9ea8-45a4-b1d5-c19ea4219fed - - - - -] The instance sync for host 'cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us' did not match. Re-created its InstanceList.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:59.701 25746 INFO nova.osapi_compute.wsgi.server [req-ce3a7cb6-1b77-4a95-a38a-4a886e8be2a8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2661889
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:59.977 25746 INFO nova.osapi_compute.wsgi.server [req-5f1c2027-e1b0-43f1-b6d3-694f483a69be 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2732208
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:59.989 25743 INFO nova.api.openstack.compute.server_external_events [req-7159484f-4b91-464c-a6f6-c7820ded6511 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:98dbd0a6-013e-4cd9-9986-fa32e5d6dd32 for instance ae3a1b5d-eec1-45bb-b76a-c59d83b1471f
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:04:59.993 25743 INFO nova.osapi_compute.wsgi.server [req-7159484f-4b91-464c-a6f6-c7820ded6511 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.2715590
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:00.004 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:00.012 2931 INFO nova.virt.libvirt.driver [-] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:00.012 2931 INFO nova.compute.manager [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Took 20.09 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:00.119 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:00.120 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:00.144 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:00.146 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:00.183 2931 INFO nova.compute.manager [req-d82fab16-60f8-4c9f-bde8-f362f57bdd40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Took 20.89 seconds to build instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:00.323 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:01.254 25746 INFO nova.osapi_compute.wsgi.server [req-8913af36-e6b2-4bb8-8efc-38328f8df4f2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2723920
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:01.523 25746 INFO nova.osapi_compute.wsgi.server [req-f2c16e44-ddd0-4d4b-8b26-7659b6a68386 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2646050
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:05.133 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:05.134 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:05.309 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:06.317 25775 INFO nova.metadata.wsgi.server [req-89097afe-bee0-455e-8b96-cf6531c50564 - - - - -] 10.11.21.129,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2234900
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:06.628 25790 INFO nova.metadata.wsgi.server [req-920159e6-7067-4753-b15a-8791d87dd456 - - - - -] 10.11.21.129,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.2278371
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:06.641 25790 INFO nova.metadata.wsgi.server [-] 10.11.21.129,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0009260
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:06.887 25786 INFO nova.metadata.wsgi.server [req-87cd83be-e801-4d38-9256-f3308d3178b4 - - - - -] 10.11.21.129,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2354860
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:06.898 25786 INFO nova.metadata.wsgi.server [-] 10.11.21.129,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0006950
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:07.230 25795 INFO nova.metadata.wsgi.server [req-691a651a-31b1-4d8a-87d4-f6a1577b69fa - - - - -] 10.11.21.129,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2376790
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:07.244 25795 INFO nova.metadata.wsgi.server [-] 10.11.21.129,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0012221
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:07.583 25788 INFO nova.metadata.wsgi.server [req-de9c326d-8d13-4a2f-9d79-0ef565d09903 - - - - -] 10.11.21.129,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.3263230
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:07.601 25775 INFO nova.metadata.wsgi.server [-] 10.11.21.129,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ HTTP/1.1" status: 200 len: 124 time: 0.0010171
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:07.614 25775 INFO nova.metadata.wsgi.server [-] 10.11.21.129,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ami HTTP/1.1" status: 200 len: 119 time: 0.0008659
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:07.797 25746 INFO nova.osapi_compute.wsgi.server [req-d20b3fad-09d8-47c2-81f2-8b392fdbfbd5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/ae3a1b5d-eec1-45bb-b76a-c59d83b1471f HTTP/1.1" status: 204 len: 203 time: 0.2648540
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:07.836 2931 INFO nova.compute.manager [req-d20b3fad-09d8-47c2-81f2-8b392fdbfbd5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:07.858 25791 INFO nova.metadata.wsgi.server [req-3e10a534-1a8e-4ae8-a49a-df5cb9b87fb0 - - - - -] 10.11.21.129,10.11.10.1 "GET /latest/meta-data/block-device-mapping/root HTTP/1.1" status: 200 len: 124 time: 0.2325919
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:08.050 2931 INFO nova.virt.libvirt.driver [-] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:08.075 25746 INFO nova.osapi_compute.wsgi.server [req-0f33968d-d618-4346-9501-8f382cd571d9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2731459
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:08.735 2931 INFO nova.virt.libvirt.driver [req-d20b3fad-09d8-47c2-81f2-8b392fdbfbd5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Deleting instance files /var/lib/nova/instances/ae3a1b5d-eec1-45bb-b76a-c59d83b1471f_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:08.737 2931 INFO nova.virt.libvirt.driver [req-d20b3fad-09d8-47c2-81f2-8b392fdbfbd5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Deletion of /var/lib/nova/instances/ae3a1b5d-eec1-45bb-b76a-c59d83b1471f_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:08.859 2931 INFO nova.compute.manager [req-d20b3fad-09d8-47c2-81f2-8b392fdbfbd5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Took 1.02 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:09.263 25746 INFO nova.osapi_compute.wsgi.server [req-3f659609-fd63-4882-8044-6a95fc6ca75d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.1815979
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:09.309 2931 INFO nova.compute.manager [req-d20b3fad-09d8-47c2-81f2-8b392fdbfbd5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] Took 0.45 seconds to deallocate network for instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:10.337 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:10.338 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:10.339 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:10.371 25746 INFO nova.osapi_compute.wsgi.server [req-08ef7873-7b28-46f6-953e-c1f5d19ffffa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.1004288
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:11.328 25746 INFO nova.api.openstack.wsgi [req-5cf6a402-2c6f-4dd4-96ee-de51ad620633 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:11.329 25746 INFO nova.osapi_compute.wsgi.server [req-5cf6a402-2c6f-4dd4-96ee-de51ad620633 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0835118
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:11.798 25740 INFO nova.osapi_compute.wsgi.server [req-1c0b524e-3746-4307-b884-a4ae0ee3bac9 d16a600c5e2a47fe98aee00ee4cb9743 e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.2 "GET /v2/e9746973ac574c6b8a9e8857f56a7608/servers/detail?all_tenants=True&changes-since=2017-05-16T05%3A55%3A11.521167%2B00%3A00&host=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us HTTP/1.1" status: 200 len: 23222 time: 0.2740111
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:11.861 25740 INFO nova.osapi_compute.wsgi.server [req-56339001-1664-4389-b65c-2a5328d70e88 d16a600c5e2a47fe98aee00ee4cb9743 e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.2 "GET /v2/e9746973ac574c6b8a9e8857f56a7608/flavors/2 HTTP/1.1" status: 200 len: 604 time: 0.0573232
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:12.019 25740 INFO nova.osapi_compute.wsgi.server [req-b066b9b3-3cb8-4dff-bb8a-5717a1e6192a d16a600c5e2a47fe98aee00ee4cb9743 e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.2 "GET /v2/e9746973ac574c6b8a9e8857f56a7608/images/0673dd71-34c5-4fbb-86c4-40623fbe45b4 HTTP/1.1" status: 200 len: 868 time: 0.1525230
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:17.156 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:17.499 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 0
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:17.499 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=512MB phys_disk=15GB used_disk=0GB total_vcpus=16 used_vcpus=0 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:17.558 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:20.936 25746 INFO nova.osapi_compute.wsgi.server [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.5533919
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:21.125 25746 INFO nova.osapi_compute.wsgi.server [req-903eebb0-3535-4b76-80bf-7b37baa4daa0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1841609
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:21.240 2931 INFO nova.compute.claims [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:21.241 2931 INFO nova.compute.claims [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:21.241 2931 INFO nova.compute.claims [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:21.242 2931 INFO nova.compute.claims [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:21.242 2931 INFO nova.compute.claims [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:21.242 2931 INFO nova.compute.claims [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:21.243 2931 INFO nova.compute.claims [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:21.281 2931 INFO nova.compute.claims [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:21.317 25746 INFO nova.osapi_compute.wsgi.server [req-af33e530-53f7-4a12-83de-a2297101103f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.1883159
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:21.526 25746 INFO nova.osapi_compute.wsgi.server [req-c1a9a7e5-bc9c-40eb-8ec5-5a07a611b661 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/43204226-2f87-4da7-b7ee-4d20cc66e846 HTTP/1.1" status: 200 len: 1708 time: 0.2040591
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:21.867 2931 INFO nova.virt.libvirt.driver [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:22.925 25746 INFO nova.osapi_compute.wsgi.server [req-5bb78d63-0f3e-4f7e-b545-272cd7cbe517 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.3940661
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:23.100 2931 INFO nova.compute.manager [-] [instance: ae3a1b5d-eec1-45bb-b76a-c59d83b1471f] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:23.217 25746 INFO nova.osapi_compute.wsgi.server [req-e9c83d27-a59d-4538-a732-7dab65fb4130 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2882659
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:24.498 25746 INFO nova.osapi_compute.wsgi.server [req-5c5129a0-c3fa-4de4-bd08-4cbc5d7964b9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2748799
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:24.771 25746 INFO nova.osapi_compute.wsgi.server [req-5dc52138-28f0-4afb-95e0-7fff9d651004 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2686439
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:26.024 25746 INFO nova.osapi_compute.wsgi.server [req-efbd124c-7597-41ac-8a22-78c44eb4fcbb 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2458200
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:26.294 25746 INFO nova.osapi_compute.wsgi.server [req-a9fc7a8f-1f8c-4a69-90f4-5e5dc260051e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2646010
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:27.562 25746 INFO nova.osapi_compute.wsgi.server [req-19db83be-773c-4b1c-8615-7f1bf20500fa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2618630
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:27.820 25746 INFO nova.osapi_compute.wsgi.server [req-777cc5b2-6b84-4afb-8ee2-a1517f1b1eae 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2524779
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:29.091 25746 INFO nova.osapi_compute.wsgi.server [req-15f26609-7890-441c-8de5-d04abd093485 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2659788
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:29.345 25746 INFO nova.osapi_compute.wsgi.server [req-8ec5e22b-e672-4e01-8b83-5e00108a6950 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2512300
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:30.612 25746 INFO nova.osapi_compute.wsgi.server [req-6fe0e366-f20b-454e-ade1-771deb7a911f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2608259
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:30.865 25746 INFO nova.osapi_compute.wsgi.server [req-e796fbdd-a70f-4bae-883d-b2098996e4a7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2494860
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:32.134 25746 INFO nova.osapi_compute.wsgi.server [req-912661ba-4a2e-4f08-9b80-ad089e9b411c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2634768
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:32.394 25746 INFO nova.osapi_compute.wsgi.server [req-6c8b83e2-6f28-48d3-a8ab-71da45452ed9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2556951
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:33.674 25746 INFO nova.osapi_compute.wsgi.server [req-88946337-1b76-4bc9-b24f-586f5a78abe2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2740719
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:33.931 25746 INFO nova.osapi_compute.wsgi.server [req-60307fcf-4b28-4649-ba54-227bbbf75c6a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2517381
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:34.911 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:34.979 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:35.105 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:35.188 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:35.189 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:35.203 25746 INFO nova.osapi_compute.wsgi.server [req-1902ca72-a41e-4900-898d-7d5c2feae591 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2677131
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:35.374 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:35.478 25746 INFO nova.osapi_compute.wsgi.server [req-5289331a-b4c1-48d6-b35f-9edfcacccef0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2714450
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:36.733 25746 INFO nova.osapi_compute.wsgi.server [req-bc51e9df-dfc1-4406-8bbb-6d4b4d6e0e20 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2498078
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:37.021 25746 INFO nova.osapi_compute.wsgi.server [req-7fe775ef-77ca-494b-8b7b-ca9e05ab4015 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2847130
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:38.459 25746 INFO nova.osapi_compute.wsgi.server [req-fc64a697-9990-4170-bd07-734525c5d666 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4322081
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:38.708 25746 INFO nova.osapi_compute.wsgi.server [req-c4aa3830-6060-410f-b743-97f55cb918f8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2457409
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:40.098 25746 INFO nova.osapi_compute.wsgi.server [req-b9a11572-cb40-4192-9bbb-0aa909c3e0f6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3834350
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:40.144 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:40.145 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:40.330 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:40.373 25746 INFO nova.osapi_compute.wsgi.server [req-2a243770-f5c3-4a9b-8bd5-13063d31a644 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2696431
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:41.210 25743 INFO nova.api.openstack.compute.server_external_events [req-297ec716-683d-434e-8b23-eb29cdd762cb f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:60df1c66-a513-4973-aeed-095809a52794 for instance 43204226-2f87-4da7-b7ee-4d20cc66e846
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:41.216 25743 INFO nova.osapi_compute.wsgi.server [req-297ec716-683d-434e-8b23-eb29cdd762cb f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0912890
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:41.229 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:41.236 2931 INFO nova.virt.libvirt.driver [-] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:41.236 2931 INFO nova.compute.manager [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Took 19.37 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:41.351 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:41.352 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:41.371 2931 INFO nova.compute.manager [req-29a09cdb-3169-4c40-8bd1-552636286362 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Took 20.14 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:41.636 25746 INFO nova.osapi_compute.wsgi.server [req-7f1448b8-38fe-4dc3-b1d7-ca0058b598bd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2568409
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:41.897 25746 INFO nova.osapi_compute.wsgi.server [req-a20bd580-2378-436b-91fc-79d1ce871713 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2556732
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:45.383 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:45.384 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:45.547 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:47.492 25797 INFO nova.metadata.wsgi.server [req-82d56826-e751-467c-b922-97a8f782f84f - - - - -] 10.11.21.130,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2336938
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:47.502 25797 INFO nova.metadata.wsgi.server [-] 10.11.21.130,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0007410
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:47.731 25778 INFO nova.metadata.wsgi.server [req-89545a1f-55c0-4475-9183-2f74dfc5fb04 - - - - -] 10.11.21.130,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2199168
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:47.743 25778 INFO nova.metadata.wsgi.server [-] 10.11.21.130,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0006940
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:47.983 25786 INFO nova.metadata.wsgi.server [req-a189d358-853f-4bdb-8972-dcc23dfa73a8 - - - - -] 10.11.21.130,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.2292249
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:48.197 25746 INFO nova.osapi_compute.wsgi.server [req-31453286-67b1-4c86-89bc-445a668da2d9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/43204226-2f87-4da7-b7ee-4d20cc66e846 HTTP/1.1" status: 204 len: 203 time: 0.2904482
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:48.237 2931 INFO nova.compute.manager [req-31453286-67b1-4c86-89bc-445a668da2d9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:48.242 25774 INFO nova.metadata.wsgi.server [req-e0ed36e7-7f48-4cf0-b1c1-8f2d0147a296 - - - - -] 10.11.21.130,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2446721
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:48.454 2931 INFO nova.virt.libvirt.driver [-] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:48.479 25746 INFO nova.osapi_compute.wsgi.server [req-75bc6269-8527-4b6a-8925-67611b9e00d9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2785730
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:48.488 25793 INFO nova.metadata.wsgi.server [req-c1795e2c-0a17-4aa0-b47e-19e4218b049e - - - - -] 10.11.21.130,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2323599
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:49.129 2931 INFO nova.virt.libvirt.driver [req-31453286-67b1-4c86-89bc-445a668da2d9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Deleting instance files /var/lib/nova/instances/43204226-2f87-4da7-b7ee-4d20cc66e846_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:49.131 2931 INFO nova.virt.libvirt.driver [req-31453286-67b1-4c86-89bc-445a668da2d9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Deletion of /var/lib/nova/instances/43204226-2f87-4da7-b7ee-4d20cc66e846_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:49.254 2931 INFO nova.compute.manager [req-31453286-67b1-4c86-89bc-445a668da2d9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Took 1.01 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:49.673 25746 INFO nova.osapi_compute.wsgi.server [req-7d3fe371-e3ea-4f93-be96-eb4d1c93b951 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.1882880
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:49.714 2931 INFO nova.compute.manager [req-31453286-67b1-4c86-89bc-445a668da2d9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] Took 0.46 seconds to deallocate network for instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:50.114 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:50.115 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:50.116 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Base or swap file too young to remove: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:50.775 25746 INFO nova.osapi_compute.wsgi.server [req-4633379f-e44c-47c6-a6d0-2aa71067f9db 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0956130
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:51.742 25746 INFO nova.api.openstack.wsgi [req-3e88afc7-0acc-498b-b3ab-9a74aad4e4fa f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:05:51.745 25746 INFO nova.osapi_compute.wsgi.server [req-3e88afc7-0acc-498b-b3ab-9a74aad4e4fa f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0880730
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:55.145 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:55.146 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:05:55.147 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:01.300 25746 INFO nova.osapi_compute.wsgi.server [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.5126011
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:01.495 25746 INFO nova.osapi_compute.wsgi.server [req-24665636-9f41-4a8b-8158-2967540c88a6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1911871
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:01.609 2931 INFO nova.compute.claims [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:01.609 2931 INFO nova.compute.claims [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:01.610 2931 INFO nova.compute.claims [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:01.610 2931 INFO nova.compute.claims [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:01.611 2931 INFO nova.compute.claims [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:01.611 2931 INFO nova.compute.claims [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:01.612 2931 INFO nova.compute.claims [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:01.647 2931 INFO nova.compute.claims [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:01.692 25746 INFO nova.osapi_compute.wsgi.server [req-871a45c2-0c1b-4c81-9fac-cb018a99a9ff 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1919560
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:01.889 25746 INFO nova.osapi_compute.wsgi.server [req-e133e2c7-a7a8-453c-b34c-ea8e1e300efe 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/fecdd5a9-3ca0-4c82-9336-63b7774f738e HTTP/1.1" status: 200 len: 1708 time: 0.1929550
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:02.243 2931 INFO nova.virt.libvirt.driver [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:03.161 25746 INFO nova.osapi_compute.wsgi.server [req-8f756dca-2eff-4af6-ba75-9a79b10195a3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2669790
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:03.427 25746 INFO nova.osapi_compute.wsgi.server [req-86b60c9f-5825-4447-9b26-c90aa4eb5986 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2631650
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:03.772 2931 INFO nova.compute.manager [-] [instance: 43204226-2f87-4da7-b7ee-4d20cc66e846] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:04.698 25746 INFO nova.osapi_compute.wsgi.server [req-9ae643e5-6c72-44f6-a04f-db3339d55f20 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2647748
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:04.976 25746 INFO nova.osapi_compute.wsgi.server [req-fe9ef402-d3b4-4d8c-8093-17b49aba2993 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2738080
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:06.255 25746 INFO nova.osapi_compute.wsgi.server [req-2d362d97-cb67-4511-9150-17dccfff1ffc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2728651
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:06.529 25746 INFO nova.osapi_compute.wsgi.server [req-83ffe6d1-b745-4902-832b-44a5e5b480d0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2706349
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:07.793 25746 INFO nova.osapi_compute.wsgi.server [req-acb68414-8b02-462e-b5e0-0f35e15e1e69 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2575791
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:08.048 25746 INFO nova.osapi_compute.wsgi.server [req-ae221290-3f43-4590-a89a-386bf9616794 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2508280
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:09.312 25746 INFO nova.osapi_compute.wsgi.server [req-3db4d5f7-9334-45de-9e4f-41fb8fbe9c3a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2585869
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:09.579 25746 INFO nova.osapi_compute.wsgi.server [req-a0cfddf4-71b7-4e21-9fcd-ddf39b33b35d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2614942
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:10.849 25746 INFO nova.osapi_compute.wsgi.server [req-3bb31561-435a-48b5-806e-5ef157eab7d6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2627311
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:11.132 25746 INFO nova.osapi_compute.wsgi.server [req-226723dd-2517-44e1-8964-528ef2fad42f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2796719
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:12.404 25746 INFO nova.osapi_compute.wsgi.server [req-981d3f9b-fbb6-47e5-9c90-9d503ada7bd7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2654428
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:12.673 25746 INFO nova.osapi_compute.wsgi.server [req-93aa1075-ea03-4919-a278-ebd71dd0701a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2659068
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:13.928 25746 INFO nova.osapi_compute.wsgi.server [req-bb1ab8b2-c10d-40e5-889d-21d7e739ad43 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2491281
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:14.193 25746 INFO nova.osapi_compute.wsgi.server [req-f9ac2f26-39d4-4979-982a-1d2c4baee0e2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2600241
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:15.172 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:15.173 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:15.356 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:15.572 25746 INFO nova.osapi_compute.wsgi.server [req-b9fc4b7e-0550-40d7-affa-7939509a0ef2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3734829
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:15.831 25746 INFO nova.osapi_compute.wsgi.server [req-896c8e56-99d8-4dad-8eeb-cf3effe66eca 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2536900
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:15.870 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:15.931 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:16.048 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:17.111 25746 INFO nova.osapi_compute.wsgi.server [req-1d647fe1-d879-4988-889e-d860ef5b8338 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2757139
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:17.522 25746 INFO nova.osapi_compute.wsgi.server [req-89a9360c-984b-4eea-b18f-619d5715cd1d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4072108
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:17.556 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:18.101 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 1
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:18.102 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=2560MB phys_disk=15GB used_disk=20GB total_vcpus=16 used_vcpus=1 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:18.159 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:18.774 25746 INFO nova.osapi_compute.wsgi.server [req-44948c02-9c44-4f95-b4dd-08d7e4df65da 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2457120
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:19.045 25746 INFO nova.osapi_compute.wsgi.server [req-a0e8716b-4556-4eda-8b06-0b9b11f65e74 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2657661
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:20.134 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:20.135 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:20.317 25746 INFO nova.osapi_compute.wsgi.server [req-093db94a-7c2d-442f-be65-7f28c92b858d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2675610
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:20.320 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:20.576 25746 INFO nova.osapi_compute.wsgi.server [req-7ddd7d81-0ab9-4cd7-8791-d29551028f51 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2544250
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:21.840 25746 INFO nova.osapi_compute.wsgi.server [req-96502cb9-998c-4726-aca3-c3de19a2c3c1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2585871
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:22.105 25746 INFO nova.osapi_compute.wsgi.server [req-ce6ebea3-0123-4aae-a535-4058e0fceb0c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2613189
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:22.540 25743 INFO nova.api.openstack.compute.server_external_events [req-5825bf9a-4201-4b20-a68a-f3a7e1addf40 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:8b683414-2896-4251-aa05-980752c9b9b3 for instance fecdd5a9-3ca0-4c82-9336-63b7774f738e
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:22.547 25743 INFO nova.osapi_compute.wsgi.server [req-5825bf9a-4201-4b20-a68a-f3a7e1addf40 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0999198
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:22.558 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:22.565 2931 INFO nova.virt.libvirt.driver [-] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:22.566 2931 INFO nova.compute.manager [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Took 20.32 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:22.689 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:22.690 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:22.709 2931 INFO nova.compute.manager [req-1162e278-3bf2-4b32-93b5-9c7ec218365e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Took 21.11 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:23.372 25746 INFO nova.osapi_compute.wsgi.server [req-d30bc321-f408-400a-9c25-4425620164a0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2625270
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:23.638 25746 INFO nova.osapi_compute.wsgi.server [req-dfef2312-3d98-4d1a-bbb9-65069ce80588 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2610891
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:25.372 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:25.373 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:25.549 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:28.943 25793 INFO nova.metadata.wsgi.server [req-875ce34e-697a-49a6-83d1-d50427627932 - - - - -] 10.11.21.131,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2973430
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:29.185 25776 INFO nova.metadata.wsgi.server [req-56b3881f-cc53-4c33-a96e-41d2d6b2ed59 - - - - -] 10.11.21.131,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.2318211
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:29.503 25774 INFO nova.metadata.wsgi.server [req-410908b9-815e-4f81-be5c-3fca98c283de - - - - -] 10.11.21.131,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2301619
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:29.830 25778 INFO nova.metadata.wsgi.server [req-a12a3398-3d5e-4ac5-ad15-7e88582d0bb7 - - - - -] 10.11.21.131,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2361510
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:29.844 25778 INFO nova.metadata.wsgi.server [-] 10.11.21.131,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0015490
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:29.860 25778 INFO nova.metadata.wsgi.server [-] 10.11.21.131,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0012500
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:29.912 25746 INFO nova.osapi_compute.wsgi.server [req-79fc63ab-432b-40e7-b612-fe4bb62bb25f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/fecdd5a9-3ca0-4c82-9336-63b7774f738e HTTP/1.1" status: 204 len: 203 time: 0.2630820
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:29.949 2931 INFO nova.compute.manager [req-79fc63ab-432b-40e7-b612-fe4bb62bb25f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:30.111 25784 INFO nova.metadata.wsgi.server [req-b16e8403-55ff-4459-93fe-3bba6e77f57c - - - - -] 10.11.21.131,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2374320
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:30.140 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:30.141 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:30.173 2931 INFO nova.virt.libvirt.driver [-] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:30.194 25746 INFO nova.osapi_compute.wsgi.server [req-127a13a3-306e-435a-af4c-7bc12e7a8d3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2779312
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:30.334 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:30.863 2931 INFO nova.virt.libvirt.driver [req-79fc63ab-432b-40e7-b612-fe4bb62bb25f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Deleting instance files /var/lib/nova/instances/fecdd5a9-3ca0-4c82-9336-63b7774f738e_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:30.865 2931 INFO nova.virt.libvirt.driver [req-79fc63ab-432b-40e7-b612-fe4bb62bb25f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Deletion of /var/lib/nova/instances/fecdd5a9-3ca0-4c82-9336-63b7774f738e_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:30.996 2931 INFO nova.compute.manager [req-79fc63ab-432b-40e7-b612-fe4bb62bb25f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Took 1.04 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:31.393 25746 INFO nova.osapi_compute.wsgi.server [req-4d05bae9-8a34-42b6-98be-991eae0f992c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.1925120
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:31.538 2931 INFO nova.compute.manager [req-79fc63ab-432b-40e7-b612-fe4bb62bb25f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] Took 0.54 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:32.499 25746 INFO nova.osapi_compute.wsgi.server [req-29e61e24-e666-401a-a46b-70449016967f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.1012239
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:33.575 25746 INFO nova.api.openstack.wsgi [req-f34ca27d-611a-4561-9ed2-e54664bf0514 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:33.577 25746 INFO nova.osapi_compute.wsgi.server [req-f34ca27d-611a-4561-9ed2-e54664bf0514 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0926130
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:35.367 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:35.368 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:35.370 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:42.990 25746 INFO nova.osapi_compute.wsgi.server [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.4763680
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:43.185 25746 INFO nova.osapi_compute.wsgi.server [req-05bc2099-282b-4260-9a9c-1fa4f55e65bf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1909380
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:43.297 2931 INFO nova.compute.claims [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:43.298 2931 INFO nova.compute.claims [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:43.298 2931 INFO nova.compute.claims [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:43.298 2931 INFO nova.compute.claims [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:43.299 2931 INFO nova.compute.claims [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:43.299 2931 INFO nova.compute.claims [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:43.300 2931 INFO nova.compute.claims [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:43.333 2931 INFO nova.compute.claims [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:43.365 25746 INFO nova.osapi_compute.wsgi.server [req-0502af14-8377-4e21-9cce-d9138bd27e32 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1754098
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:43.564 25746 INFO nova.osapi_compute.wsgi.server [req-d5fd3b8a-3323-4f9f-9453-d74ab24c21d3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/63a0d960-70b6-44c6-b606-491478a5cadf HTTP/1.1" status: 200 len: 1708 time: 0.1946359
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:43.915 2931 INFO nova.virt.libvirt.driver [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:44.852 25746 INFO nova.osapi_compute.wsgi.server [req-9339970e-e303-4833-a6b6-31e4b360624c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2833371
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:45.143 25746 INFO nova.osapi_compute.wsgi.server [req-d7e60bdc-85aa-4e2f-b45c-82adbb1a5a4e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2857559
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:45.169 2931 INFO nova.compute.manager [-] [instance: fecdd5a9-3ca0-4c82-9336-63b7774f738e] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:46.407 25746 INFO nova.osapi_compute.wsgi.server [req-0faa8465-ec0e-43b5-8b0e-ff014563ba05 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2582810
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:46.672 25746 INFO nova.osapi_compute.wsgi.server [req-cfb0b245-947c-47d7-84e0-826ee5b5cda3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2609611
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:47.947 25746 INFO nova.osapi_compute.wsgi.server [req-424209a4-ace9-489b-b79d-86686ef48d00 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2697830
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:48.230 25746 INFO nova.osapi_compute.wsgi.server [req-945d1f31-a2e5-496a-a1da-21a0c88b6b61 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2775040
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:49.503 25746 INFO nova.osapi_compute.wsgi.server [req-ca134311-b7a4-47ee-9f59-3f6fa9745739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2671990
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:49.771 25746 INFO nova.osapi_compute.wsgi.server [req-e48f0708-5059-4eb3-8742-55fb03247cd9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2630441
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:51.040 25746 INFO nova.osapi_compute.wsgi.server [req-048f93d6-3858-482b-972d-48640f16bed1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2640278
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:51.293 25746 INFO nova.osapi_compute.wsgi.server [req-ad038d3b-0d3c-4abc-971c-b2bd82f6405c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2512250
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:52.560 25746 INFO nova.osapi_compute.wsgi.server [req-dc2db9dc-6a63-40a1-8dc4-b0194200ffb3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2617428
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:52.821 25746 INFO nova.osapi_compute.wsgi.server [req-b3dbbd16-100c-4a9e-a7cd-e2911d09417d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2583311
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:54.185 25746 INFO nova.osapi_compute.wsgi.server [req-1d1c4773-caa1-4408-8cc7-8475d9bb358e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3579462
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:54.456 25746 INFO nova.osapi_compute.wsgi.server [req-137d902f-6187-4b26-a6e2-9b16f72e5f89 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2673008
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:55.195 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:55.196 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:55.380 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:55.893 25746 INFO nova.osapi_compute.wsgi.server [req-2e68b02f-6c9b-4ff6-86ae-96c9bfd618c5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4313569
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:56.140 25746 INFO nova.osapi_compute.wsgi.server [req-52c0125b-347b-4b3a-88f4-4d87d4f024b3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2426732
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:56.714 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:56.781 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:06:56.905 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:57.408 25746 INFO nova.osapi_compute.wsgi.server [req-c860e116-eadb-455a-921a-fedfa8e998a2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2615430
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:57.665 25746 INFO nova.osapi_compute.wsgi.server [req-86058deb-b5e1-4cc2-96b2-4b1f0ca81306 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2519629
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:58.945 25746 INFO nova.osapi_compute.wsgi.server [req-8720bc4b-6afa-4ed0-8211-0a1c4eeb3626 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2753210
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:06:59.197 25746 INFO nova.osapi_compute.wsgi.server [req-db18b8a9-ee02-437c-840b-5ef03afab0cc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2491229
nova-scheduler.log.1.2017-05-16_13:53:08 2017-05-16 00:07:00.405 25998 INFO nova.scheduler.host_manager [req-27843cc7-d4d3-4583-b8dc-c2b484e85dc8 - - - - -] The instance sync for host 'cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us' did not match. Re-created its InstanceList.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:00.468 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:00.469 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:00.473 25746 INFO nova.osapi_compute.wsgi.server [req-1375293f-4b18-49f6-a38e-2bad37034b98 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2700448
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:00.640 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:00.740 25746 INFO nova.osapi_compute.wsgi.server [req-178e4577-d563-4131-a9cb-021a4ef7b10c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2630880
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:02.012 25746 INFO nova.osapi_compute.wsgi.server [req-0327658a-755d-47f2-93c8-b96bfeadd80f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2663522
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:02.276 25746 INFO nova.osapi_compute.wsgi.server [req-b88d307a-b459-47f5-b4a3-ef2e78868ebb 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2594640
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:03.568 25746 INFO nova.osapi_compute.wsgi.server [req-74ee69bd-fd92-497e-8fce-3049300a465d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2857480
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:03.840 25746 INFO nova.osapi_compute.wsgi.server [req-767559d9-3156-4388-a6c3-feb6533e4e13 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2680290
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:03.922 25743 INFO nova.api.openstack.compute.server_external_events [req-543e2dd0-b199-4751-933a-2b0e5a6d48b5 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:e1bf6353-ae58-4e46-b2d1-65a69b992bed for instance 63a0d960-70b6-44c6-b606-491478a5cadf
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:03.927 25743 INFO nova.osapi_compute.wsgi.server [req-543e2dd0-b199-4751-933a-2b0e5a6d48b5 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0965390
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:03.936 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:03.945 2931 INFO nova.virt.libvirt.driver [-] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:03.945 2931 INFO nova.compute.manager [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Took 20.03 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:04.064 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:04.066 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:04.084 2931 INFO nova.compute.manager [req-72b4858f-049e-49e1-b31e-b562c5018eaf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Took 20.80 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:05.138 25746 INFO nova.osapi_compute.wsgi.server [req-11c5a3c1-ff74-4def-a7bc-3925b7f53c24 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2920430
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:05.153 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:05.154 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:05.324 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:05.395 25746 INFO nova.osapi_compute.wsgi.server [req-9174a757-0149-4e23-94ae-8e316c8ea14f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2539101
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:10.292 25776 INFO nova.metadata.wsgi.server [req-7d3eeb2d-3948-433a-8002-d969d2f86863 - - - - -] 10.11.21.132,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2316251
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:10.305 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0006261
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:10.380 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:10.381 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:10.539 25799 INFO nova.metadata.wsgi.server [req-2c3352c8-820a-4166-ab18-cdc88a711208 - - - - -] 10.11.21.132,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2239330
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:10.551 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0007501
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:10.556 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:10.563 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0011230
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:10.578 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0009539
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.016 25786 INFO nova.metadata.wsgi.server [req-d1ae2038-d065-471d-9607-0c3547d44422 - - - - -] 10.11.21.132,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2430041
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.032 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.0013649
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.271 25783 INFO nova.metadata.wsgi.server [req-60ac28f2-bde9-4da6-9542-7b7a345b5363 - - - - -] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ HTTP/1.1" status: 200 len: 124 time: 0.2251949
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.285 25783 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ami HTTP/1.1" status: 200 len: 119 time: 0.0010529
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.298 25783 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/block-device-mapping/root HTTP/1.1" status: 200 len: 124 time: 0.0007141
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.311 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/placement/ HTTP/1.1" status: 200 len: 134 time: 0.0007560
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.324 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/placement/availability-zone HTTP/1.1" status: 200 len: 120 time: 0.0009980
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.335 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/local-ipv4 HTTP/1.1" status: 200 len: 129 time: 0.0006690
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.346 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/reservation-id HTTP/1.1" status: 200 len: 127 time: 0.0007050
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.358 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/local-hostname HTTP/1.1" status: 200 len: 130 time: 0.0007689
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.370 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/security-groups HTTP/1.1" status: 200 len: 123 time: 0.0007679
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.382 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/ami-launch-index HTTP/1.1" status: 200 len: 117 time: 0.0007291
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.671 25746 INFO nova.osapi_compute.wsgi.server [req-74b000e1-54e1-4761-ba25-545a517f42f9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/63a0d960-70b6-44c6-b606-491478a5cadf HTTP/1.1" status: 204 len: 203 time: 0.2671530
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.702 25788 INFO nova.metadata.wsgi.server [req-5295ab7c-ca83-4234-a5ed-6a67d7d38b4b - - - - -] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/public-hostname HTTP/1.1" status: 200 len: 130 time: 0.2245400
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:11.710 2931 INFO nova.compute.manager [req-74b000e1-54e1-4761-ba25-545a517f42f9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.718 25788 INFO nova.metadata.wsgi.server [-] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/hostname HTTP/1.1" status: 200 len: 130 time: 0.0011861
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:11.927 2931 INFO nova.virt.libvirt.driver [-] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.953 25746 INFO nova.osapi_compute.wsgi.server [req-1480c231-74f8-46d8-9467-9bc9f1645c87 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2783649
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:11.968 25790 INFO nova.metadata.wsgi.server [req-9dfe357d-81fd-45bb-b2b1-3a0ead0a0770 - - - - -] 10.11.21.132,10.11.10.1 "GET /latest/meta-data/ami-id HTTP/1.1" status: 200 len: 129 time: 0.2386379
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:12.634 2931 INFO nova.virt.libvirt.driver [req-74b000e1-54e1-4761-ba25-545a517f42f9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Deleting instance files /var/lib/nova/instances/63a0d960-70b6-44c6-b606-491478a5cadf_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:12.637 2931 INFO nova.virt.libvirt.driver [req-74b000e1-54e1-4761-ba25-545a517f42f9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Deletion of /var/lib/nova/instances/63a0d960-70b6-44c6-b606-491478a5cadf_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:12.751 2931 INFO nova.compute.manager [req-74b000e1-54e1-4761-ba25-545a517f42f9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Took 1.04 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:13.157 25746 INFO nova.osapi_compute.wsgi.server [req-4535b71b-250e-41e3-9fef-ea48e83e93d3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.1995878
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:13.214 2931 INFO nova.compute.manager [req-74b000e1-54e1-4761-ba25-545a517f42f9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] Took 0.46 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:14.263 25746 INFO nova.osapi_compute.wsgi.server [req-d4f13b7d-5438-4b7b-a8be-68d64e065378 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.1000040
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:15.235 25746 INFO nova.api.openstack.wsgi [req-2828889b-97b9-4cc6-9cf7-44c8f4e856b9 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:15.237 25746 INFO nova.osapi_compute.wsgi.server [req-2828889b-97b9-4cc6-9cf7-44c8f4e856b9 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0839748
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:15.581 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:15.582 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:15.584 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:17.165 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:17.507 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 0
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:17.508 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=512MB phys_disk=15GB used_disk=0GB total_vcpus=16 used_vcpus=0 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:17.567 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:24.789 25746 INFO nova.osapi_compute.wsgi.server [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.5130808
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:24.994 25746 INFO nova.osapi_compute.wsgi.server [req-79b9f95a-26b4-4b98-9f57-1896c54a0a33 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.2017291
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:25.186 25746 INFO nova.osapi_compute.wsgi.server [req-11fa0afd-fbbf-4d33-b4a3-eadb918ccd07 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1875880
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:25.202 2931 INFO nova.compute.claims [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:25.203 2931 INFO nova.compute.claims [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:25.204 2931 INFO nova.compute.claims [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:25.205 2931 INFO nova.compute.claims [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:25.206 2931 INFO nova.compute.claims [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:25.207 2931 INFO nova.compute.claims [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:25.208 2931 INFO nova.compute.claims [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:25.239 2931 INFO nova.compute.claims [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:25.394 25746 INFO nova.osapi_compute.wsgi.server [req-3bc17f6d-1354-46d7-9366-675d3c38eba9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c HTTP/1.1" status: 200 len: 1572 time: 0.2030520
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:25.935 2931 INFO nova.virt.libvirt.driver [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:26.669 25746 INFO nova.osapi_compute.wsgi.server [req-43e0f44d-b7b7-4458-849c-184dfc5d16e9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2717040
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:26.956 25746 INFO nova.osapi_compute.wsgi.server [req-59b0af54-2013-4474-a1c6-9d78e6f052a2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2835078
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:27.105 2931 INFO nova.compute.manager [-] [instance: 63a0d960-70b6-44c6-b606-491478a5cadf] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:28.214 25746 INFO nova.osapi_compute.wsgi.server [req-450cfe03-d536-4ad6-8d45-109150d3fe46 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2515070
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:28.493 25746 INFO nova.osapi_compute.wsgi.server [req-bd42eb24-d22e-4fef-b92e-3a70d8d83d15 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2747049
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:29.768 25746 INFO nova.osapi_compute.wsgi.server [req-9c9d1d32-84c4-469c-8092-ba8d95d524b7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2688880
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:30.040 25746 INFO nova.osapi_compute.wsgi.server [req-b9718cd8-f65e-49cc-8349-6cf7122af137 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2675118
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:31.313 25746 INFO nova.osapi_compute.wsgi.server [req-387f80a3-539e-4716-8595-9913cb1d4116 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2672651
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:31.557 25746 INFO nova.osapi_compute.wsgi.server [req-dc75766e-f0ca-4e46-aa86-93fa2924a379 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2399080
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:32.929 25746 INFO nova.osapi_compute.wsgi.server [req-ab030d68-61de-4726-9836-33469b7135dd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3666670
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:33.184 25746 INFO nova.osapi_compute.wsgi.server [req-2e32a4b6-281d-4b23-8783-e23e909271ae 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2499630
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:34.457 25746 INFO nova.osapi_compute.wsgi.server [req-78acc268-1d58-4b31-967a-c31e22012a78 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2668920
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:34.727 25746 INFO nova.osapi_compute.wsgi.server [req-e78ed642-d348-4419-931e-8d1380eeb864 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2662079
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:35.996 25746 INFO nova.osapi_compute.wsgi.server [req-2d3fdf84-c2d9-47cc-ad3c-8c309b96d32d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2640860
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:36.262 25746 INFO nova.osapi_compute.wsgi.server [req-5d311501-92ce-4342-99ac-863f9b53a5cb 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2633319
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:37.513 25746 INFO nova.osapi_compute.wsgi.server [req-0013db4a-a7f2-4013-a135-314a1fbb97e8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2449338
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:37.774 25746 INFO nova.osapi_compute.wsgi.server [req-d69dd2fe-057c-420c-afea-ba8597cd9982 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2567449
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:39.068 25746 INFO nova.osapi_compute.wsgi.server [req-89ca6e78-b521-4038-b5da-c3a9d694d5e1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2878940
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:39.323 25746 INFO nova.osapi_compute.wsgi.server [req-a9ca2993-416e-4268-a0c5-a1ff1214fcb7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2501280
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:39.377 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:39.443 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:39.561 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:40.137 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:40.138 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:40.314 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:40.762 25746 INFO nova.osapi_compute.wsgi.server [req-d68496f5-904c-4250-841e-b6efeba3bad2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4324191
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:41.041 25746 INFO nova.osapi_compute.wsgi.server [req-5ff477a1-4d19-44b2-bd80-bc948fd71044 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2753420
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:42.324 25746 INFO nova.osapi_compute.wsgi.server [req-91545d8f-3e15-4a9d-869f-5e9af23a108d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2779088
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:42.587 25746 INFO nova.osapi_compute.wsgi.server [req-de162460-0015-4ef5-ad34-36cdaf269be6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2570632
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:43.853 25746 INFO nova.osapi_compute.wsgi.server [req-e8a40430-24a0-4317-859b-555bb76fb798 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2604382
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:44.113 25746 INFO nova.osapi_compute.wsgi.server [req-3ab9508b-f853-4fdc-a5d3-7fe02d8f6a77 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2551761
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:45.226 25743 INFO nova.api.openstack.compute.server_external_events [req-4c5a4bf8-3ffd-47e6-bd27-6cc8aa71b618 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:f53e6f35-05f7-4896-9abf-6df58d3e69b8 for instance d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:45.232 25743 INFO nova.osapi_compute.wsgi.server [req-4c5a4bf8-3ffd-47e6-bd27-6cc8aa71b618 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.1034019
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:45.243 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:45.253 2931 INFO nova.virt.libvirt.driver [-] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:45.253 2931 INFO nova.compute.manager [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Took 19.32 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:45.371 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:45.372 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:45.391 25746 INFO nova.osapi_compute.wsgi.server [req-6d8d14b0-9baf-4de0-b02a-abd9d158342d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2724118
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:45.415 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:45.423 2931 INFO nova.compute.manager [req-01d570b0-78a7-4719-b7a3-429fd7dc5a3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Took 20.23 seconds to build instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:45.547 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:45.664 25746 INFO nova.osapi_compute.wsgi.server [req-3c7d7e41-e930-4301-9889-6509c7142e16 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2694650
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:46.949 25746 INFO nova.osapi_compute.wsgi.server [req-88051a36-ba5a-44d8-90a4-dfd5a30c2f8a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2790279
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:47.217 25746 INFO nova.osapi_compute.wsgi.server [req-2d183073-77ff-47b9-95eb-07be7fafc897 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2651579
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:50.141 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:50.142 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:50.311 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:51.603 25795 INFO nova.metadata.wsgi.server [req-bcebb9de-a387-44e2-b784-db819fff540d - - - - -] 10.11.21.133,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2296522
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:51.926 25793 INFO nova.metadata.wsgi.server [req-c754e29a-bcc8-48d9-b29d-9c375a27275a - - - - -] 10.11.21.133,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.2319870
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:52.240 25779 INFO nova.metadata.wsgi.server [req-e7dbbfbb-8c21-4c78-86d5-d207f180455f - - - - -] 10.11.21.133,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2253911
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:52.253 25779 INFO nova.metadata.wsgi.server [-] 10.11.21.133,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0008950
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:52.482 25790 INFO nova.metadata.wsgi.server [req-3a439e39-d7f5-4cdf-a08c-c530a5bc9b23 - - - - -] 10.11.21.133,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.2186041
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:52.494 25790 INFO nova.metadata.wsgi.server [-] 10.11.21.133,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0011051
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:52.912 25784 INFO nova.metadata.wsgi.server [req-d04bb0bd-f965-40e1-90d3-b1b01f160b06 - - - - -] 10.11.21.133,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.4052589
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:53.161 25791 INFO nova.metadata.wsgi.server [req-32ecf06f-cf33-4487-baa9-740cb3b4fc82 - - - - -] 10.11.21.133,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.2367840
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:53.407 25774 INFO nova.metadata.wsgi.server [req-259797db-8cb1-4b12-b243-5269bdd6dc13 - - - - -] 10.11.21.133,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ HTTP/1.1" status: 200 len: 124 time: 0.2289269
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:53.512 25746 INFO nova.osapi_compute.wsgi.server [req-ae7c1466-8f74-4112-bb31-d2e2652275de 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c HTTP/1.1" status: 204 len: 203 time: 0.2855930
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:53.554 2931 INFO nova.compute.manager [req-ae7c1466-8f74-4112-bb31-d2e2652275de 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Terminating instance
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:53.772 2931 INFO nova.virt.libvirt.driver [-] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:53.818 25746 INFO nova.osapi_compute.wsgi.server [req-bcb231a7-f705-44c7-ada9-18959debb717 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.3035021
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:53.887 25797 INFO nova.metadata.wsgi.server [req-a3b733a0-a9ce-4a98-8d03-33b07da80a72 - - - - -] 10.11.21.133,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ami HTTP/1.1" status: 200 len: 119 time: 0.4668469
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:54.463 2931 INFO nova.virt.libvirt.driver [req-ae7c1466-8f74-4112-bb31-d2e2652275de 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Deleting instance files /var/lib/nova/instances/d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:54.466 2931 INFO nova.virt.libvirt.driver [req-ae7c1466-8f74-4112-bb31-d2e2652275de 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Deletion of /var/lib/nova/instances/d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:54.585 2931 INFO nova.compute.manager [req-ae7c1466-8f74-4112-bb31-d2e2652275de 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Took 1.02 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:55.034 25746 INFO nova.osapi_compute.wsgi.server [req-54ffbd34-3bea-4d1a-9433-db04e8dbb2af 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.2087660
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:55.052 2931 INFO nova.compute.manager [req-ae7c1466-8f74-4112-bb31-d2e2652275de 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] Took 0.47 seconds to deallocate network for instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:55.337 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:55.337 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:07:55.338 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:56.135 25746 INFO nova.osapi_compute.wsgi.server [req-e0003c57-829b-4caf-bcde-8593b829ff56 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0959971
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:57.076 25746 INFO nova.api.openstack.wsgi [req-acc5bb0f-42fe-4bbc-ac8b-0bace2d3b906 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:07:57.077 25746 INFO nova.osapi_compute.wsgi.server [req-acc5bb0f-42fe-4bbc-ac8b-0bace2d3b906 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0836670
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:06.646 25746 INFO nova.osapi_compute.wsgi.server [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.4964380
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:06.848 25746 INFO nova.osapi_compute.wsgi.server [req-9d415292-078d-4325-9e67-e1ed14e1c957 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1959479
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:06.960 2931 INFO nova.compute.claims [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:06.960 2931 INFO nova.compute.claims [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:06.961 2931 INFO nova.compute.claims [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:06.961 2931 INFO nova.compute.claims [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:06.962 2931 INFO nova.compute.claims [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:06.962 2931 INFO nova.compute.claims [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:06.963 2931 INFO nova.compute.claims [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:06.999 2931 INFO nova.compute.claims [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:07.046 25746 INFO nova.osapi_compute.wsgi.server [req-8117222a-0c51-49b8-8ea5-6090b6314d9c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.1933501
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:07.228 25746 INFO nova.osapi_compute.wsgi.server [req-1fc1761c-fded-443a-814a-e48c5f5d3567 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/17288ea8-cbf4-4f0e-94fe-853fd2735f29 HTTP/1.1" status: 200 len: 1708 time: 0.1789858
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:07.620 2931 INFO nova.virt.libvirt.driver [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:08.510 25746 INFO nova.osapi_compute.wsgi.server [req-eb045220-2c05-468d-95c0-390ba6b3dc33 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2756851
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:08.769 2931 INFO nova.compute.manager [-] [instance: d54b44eb-2d1a-4aa2-ba6b-074d35f8f12c] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:08.800 25746 INFO nova.osapi_compute.wsgi.server [req-87e35b57-2f3f-4c30-a490-d4711539d535 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2846689
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:10.205 25746 INFO nova.osapi_compute.wsgi.server [req-f86b0a78-1dbe-4610-824e-b80f7f687b3c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3997040
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:10.469 25746 INFO nova.osapi_compute.wsgi.server [req-665a67f1-74e7-4ffe-8ffa-9a81977fe6df 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2598279
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:11.740 25746 INFO nova.osapi_compute.wsgi.server [req-010217f3-bf1a-48b7-9077-bd3cf46faf82 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2640989
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:12.006 25746 INFO nova.osapi_compute.wsgi.server [req-14c5efcb-3268-41d1-aeb2-c73cae0553a2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2620471
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:13.268 25746 INFO nova.osapi_compute.wsgi.server [req-c51ad426-c9e8-4939-9e18-3e7b2eaab0db 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2563319
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:13.533 25746 INFO nova.osapi_compute.wsgi.server [req-5e98f980-a45f-41ad-ab47-6a5adc12de51 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2608740
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:14.818 25746 INFO nova.osapi_compute.wsgi.server [req-f6913476-4049-499b-ba84-2c7dd2640cbd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2802150
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:15.086 25746 INFO nova.osapi_compute.wsgi.server [req-bc4a646e-41af-4ec9-b74c-341d98b4912e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2636530
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:16.334 25746 INFO nova.osapi_compute.wsgi.server [req-e06b0cad-8215-47ba-add9-89ec1daeffb9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2420769
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:16.602 25746 INFO nova.osapi_compute.wsgi.server [req-32b14e4d-90cd-4ecb-9171-0ede02691b36 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2646151
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:17.860 25746 INFO nova.osapi_compute.wsgi.server [req-7504342b-b277-44f1-a711-06a0ebacca4e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2528281
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:18.116 25746 INFO nova.osapi_compute.wsgi.server [req-0f3adb60-b6c6-4d59-ac1e-28b756a66886 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2527301
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:18.146 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:18.553 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 1
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:18.554 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=2560MB phys_disk=15GB used_disk=20GB total_vcpus=16 used_vcpus=1 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:18.616 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:19.392 25746 INFO nova.osapi_compute.wsgi.server [req-ebd68e1c-457d-484c-9bc7-e813b34f0c7e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2695768
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:19.658 25746 INFO nova.osapi_compute.wsgi.server [req-f894c9ea-bf2b-40c2-98df-e60c969bb8eb 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2627070
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:20.180 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:20.182 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:20.238 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:20.330 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:20.372 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:20.455 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:20.938 25746 INFO nova.osapi_compute.wsgi.server [req-b0e44ae8-e497-4e7f-971d-fec7ab6cdc78 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2749891
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:21.207 25746 INFO nova.osapi_compute.wsgi.server [req-79805ef8-d822-4dd2-badb-4a56c8687410 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2645309
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:22.485 25746 INFO nova.osapi_compute.wsgi.server [req-355185de-7074-4448-a197-a632db9c5cbd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2717261
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:22.754 25746 INFO nova.osapi_compute.wsgi.server [req-2567652e-8b67-4fdb-ae28-88239694577a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2652020
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:24.184 25746 INFO nova.osapi_compute.wsgi.server [req-97f723ce-8b78-4bdd-9d45-f7fa2e7707de 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4219470
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:24.448 25746 INFO nova.osapi_compute.wsgi.server [req-4beb51c0-21fe-4609-b459-ffc91c724c5c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2611339
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:25.426 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:25.427 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:25.599 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:25.730 25746 INFO nova.osapi_compute.wsgi.server [req-25d4598c-81f0-4e79-87fd-228f3dc94aa7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2755768
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:25.989 25746 INFO nova.osapi_compute.wsgi.server [req-4d246567-e421-4c7a-a3bb-25a9aca95a74 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2553751
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:26.577 25743 INFO nova.api.openstack.compute.server_external_events [req-a27af937-85b3-4903-abf5-6c809b6cde67 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:899a49ed-0154-4308-949e-f59ac7acc66d for instance 17288ea8-cbf4-4f0e-94fe-853fd2735f29
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:26.583 25743 INFO nova.osapi_compute.wsgi.server [req-a27af937-85b3-4903-abf5-6c809b6cde67 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0935540
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:26.594 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:26.601 2931 INFO nova.virt.libvirt.driver [-] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:26.601 2931 INFO nova.compute.manager [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Took 18.98 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:26.713 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:26.714 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:26.735 2931 INFO nova.compute.manager [req-868a5460-dbb6-416b-b4c4-a98abae6c847 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Took 19.79 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:27.260 25746 INFO nova.osapi_compute.wsgi.server [req-c3a9d75b-ddc5-4987-9989-6925aa53232a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2654040
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:27.521 25746 INFO nova.osapi_compute.wsgi.server [req-525a532e-ea93-4b66-ab16-33fbf450d6e6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2577920
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:30.367 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:30.369 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:30.547 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:33.033 25797 INFO nova.metadata.wsgi.server [req-610fdd1e-f55d-480e-866a-6effcc9d83ae - - - - -] 10.11.21.134,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2398081
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:33.381 25799 INFO nova.metadata.wsgi.server [req-816e73b8-912b-4ce2-802f-2ac72c996398 - - - - -] 10.11.21.134,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.2231328
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:33.394 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.134,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0007591
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:33.629 25774 INFO nova.metadata.wsgi.server [req-b0c8e9d7-4c07-4875-80da-ee5c41be4832 - - - - -] 10.11.21.134,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2222700
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:33.802 25746 INFO nova.osapi_compute.wsgi.server [req-c00498e9-c4a0-4e46-b680-c8e4927369f7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/17288ea8-cbf4-4f0e-94fe-853fd2735f29 HTTP/1.1" status: 204 len: 203 time: 0.2726481
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:33.843 2931 INFO nova.compute.manager [req-c00498e9-c4a0-4e46-b680-c8e4927369f7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:33.944 25783 INFO nova.metadata.wsgi.server [req-49f8125c-481f-4d59-b390-607c59629638 - - - - -] 10.11.21.134,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.2255361
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:34.059 2931 INFO nova.virt.libvirt.driver [-] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:34.080 25746 INFO nova.osapi_compute.wsgi.server [req-c825e8ea-d765-4f39-9175-93cb0321be3f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2741730
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:34.741 2931 INFO nova.virt.libvirt.driver [req-c00498e9-c4a0-4e46-b680-c8e4927369f7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Deleting instance files /var/lib/nova/instances/17288ea8-cbf4-4f0e-94fe-853fd2735f29_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:34.742 2931 INFO nova.virt.libvirt.driver [req-c00498e9-c4a0-4e46-b680-c8e4927369f7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Deletion of /var/lib/nova/instances/17288ea8-cbf4-4f0e-94fe-853fd2735f29_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:34.853 2931 INFO nova.compute.manager [req-c00498e9-c4a0-4e46-b680-c8e4927369f7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Took 1.00 seconds to destroy the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:35.145 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:35.146 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:35.247 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:35.282 25746 INFO nova.osapi_compute.wsgi.server [req-2d69546b-af37-4bab-b9b9-be2a92f487a2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.1977990
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:35.420 2931 INFO nova.compute.manager [req-c00498e9-c4a0-4e46-b680-c8e4927369f7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] Took 0.57 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:36.381 25746 INFO nova.osapi_compute.wsgi.server [req-c52a31aa-4db4-4cc1-ae33-1134f503fb3a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0929339
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:37.454 25746 INFO nova.api.openstack.wsgi [req-ffb8f145-8d6f-4f36-a8ff-c93c67f00c73 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:37.455 25746 INFO nova.osapi_compute.wsgi.server [req-ffb8f145-8d6f-4f36-a8ff-c93c67f00c73 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0967801
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:40.276 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:40.277 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:40.278 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:47.085 25746 INFO nova.osapi_compute.wsgi.server [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.6913249
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:47.274 25746 INFO nova.osapi_compute.wsgi.server [req-f25b814c-14fb-49a7-8bdd-1f2ffb3483e6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1850250
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:47.371 2931 INFO nova.compute.claims [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:47.372 2931 INFO nova.compute.claims [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:47.373 2931 INFO nova.compute.claims [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:47.374 2931 INFO nova.compute.claims [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:47.375 2931 INFO nova.compute.claims [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:47.376 2931 INFO nova.compute.claims [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:47.377 2931 INFO nova.compute.claims [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:47.408 2931 INFO nova.compute.claims [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:47.574 25746 INFO nova.osapi_compute.wsgi.server [req-981d4d23-d12e-47a6-b1d7-590f5a586156 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.2966871
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:47.766 25746 INFO nova.osapi_compute.wsgi.server [req-03a0519f-9124-4409-bb35-88b9fae7ea0e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/70c1714b-c11b-4c88-b300-239afe1f5ff8 HTTP/1.1" status: 200 len: 1708 time: 0.1883068
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:47.999 2931 INFO nova.virt.libvirt.driver [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:49.041 25746 INFO nova.osapi_compute.wsgi.server [req-40b49896-8426-467a-8d7f-de6c1cd123d4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2690799
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:08:49.186 2931 INFO nova.compute.manager [-] [instance: 17288ea8-cbf4-4f0e-94fe-853fd2735f29] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:49.319 25746 INFO nova.osapi_compute.wsgi.server [req-e2337654-a816-4dff-9610-c34d22b07019 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2732439
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:50.578 25746 INFO nova.osapi_compute.wsgi.server [req-2c9c783f-3c7a-4844-87b1-e207a38c74ab 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2527661
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:50.840 25746 INFO nova.osapi_compute.wsgi.server [req-7e26e9a0-f683-46fb-a738-a31eb8cb0765 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2576380
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:52.115 25746 INFO nova.osapi_compute.wsgi.server [req-fac2c33c-ee61-4d6a-9875-408a6a0be215 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2699261
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:52.383 25746 INFO nova.osapi_compute.wsgi.server [req-0dbc8793-cf63-4757-9f9a-7f48ea19367d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2641821
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:53.665 25746 INFO nova.osapi_compute.wsgi.server [req-54202d96-8c90-4b84-96bc-36a64cd83102 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2769511
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:53.929 25746 INFO nova.osapi_compute.wsgi.server [req-70429066-8c1a-4b28-a3aa-7d8b43d9b9f7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2594910
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:55.213 25746 INFO nova.osapi_compute.wsgi.server [req-8a13853b-8daa-420b-a028-d49e438497b5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2778099
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:55.484 25746 INFO nova.osapi_compute.wsgi.server [req-31a940b9-3604-4c3f-8aec-386824bc1e9d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2664509
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:56.757 25746 INFO nova.osapi_compute.wsgi.server [req-82610fd9-87cb-4433-a9d1-73a9f1bba99b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2669392
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:57.037 25746 INFO nova.osapi_compute.wsgi.server [req-0b5ced27-6fb3-449b-a87f-1d9beaae3f62 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2769639
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:58.306 25746 INFO nova.osapi_compute.wsgi.server [req-aded094f-044c-41ef-b827-8a8a69bd0e73 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2634399
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:58.570 25746 INFO nova.osapi_compute.wsgi.server [req-0a48ad90-076f-4367-9334-0b865814f9dd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2591650
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:08:59.836 25746 INFO nova.osapi_compute.wsgi.server [req-bb193d66-759c-4753-9b98-792dd882f3a0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2596231
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:00.109 25746 INFO nova.osapi_compute.wsgi.server [req-12db2bbc-6755-4401-99d8-450e84703abd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2685480
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:00.178 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:00.179 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:00.355 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:01.018 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:01.081 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:01.203 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:01.373 25746 INFO nova.osapi_compute.wsgi.server [req-408beb7b-9353-4b15-be97-7bdcdcb86011 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2582371
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:01.635 25746 INFO nova.osapi_compute.wsgi.server [req-245e4301-a338-44d2-8fdd-3b80b5679f24 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2575240
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:02.993 25746 INFO nova.osapi_compute.wsgi.server [req-69484de8-4761-43a1-9246-3751ba4eef5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3519812
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:03.421 25746 INFO nova.osapi_compute.wsgi.server [req-caad057d-79a5-41b4-a223-4c1a0d757007 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4240189
nova-scheduler.log.1.2017-05-16_13:53:08 2017-05-16 00:09:04.153 25998 INFO nova.scheduler.host_manager [req-702e2535-33f8-4590-bd45-7c7fbd5b8c1f - - - - -] The instance sync for host 'cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us' did not match. Re-created its InstanceList.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:04.691 25746 INFO nova.osapi_compute.wsgi.server [req-60b39154-68cf-4345-85ba-a56c897b7d66 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2642689
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:04.964 25746 INFO nova.osapi_compute.wsgi.server [req-bf5f82ea-8ca0-4437-b4d6-f914db1fb477 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2676780
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:05.140 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:05.140 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:05.321 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:06.227 25746 INFO nova.osapi_compute.wsgi.server [req-5e914d03-3281-48b4-9dee-bf7664c6b59c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2578239
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:06.497 25746 INFO nova.osapi_compute.wsgi.server [req-94248553-16cd-4c90-944b-22c39d61951c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2661738
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:07.779 25746 INFO nova.osapi_compute.wsgi.server [req-243447da-80ee-4808-8d5d-11bd590c2711 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2768469
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:08.029 25743 INFO nova.api.openstack.compute.server_external_events [req-2f1a8f6f-b2fc-48af-92c3-8a37d40f54dc f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:1bb8c7a3-effa-4568-8cf7-2b9a55011df9 for instance 70c1714b-c11b-4c88-b300-239afe1f5ff8
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:08.034 25743 INFO nova.osapi_compute.wsgi.server [req-2f1a8f6f-b2fc-48af-92c3-8a37d40f54dc f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0879810
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:08.044 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] VM Resumed (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:08.052 25746 INFO nova.osapi_compute.wsgi.server [req-3e79db14-65fd-4ace-abb1-7923c8fc654f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2679651
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:08.053 2931 INFO nova.virt.libvirt.driver [-] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:08.054 2931 INFO nova.compute.manager [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Took 20.05 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:08.161 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:08.163 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:08.188 2931 INFO nova.compute.manager [req-98474cd9-61e1-4afe-bd52-676a577b058f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Took 20.83 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:09.312 25746 INFO nova.osapi_compute.wsgi.server [req-623d521c-5e95-47e2-81e4-d6905cb6d4ad 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2556899
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:09.572 25746 INFO nova.osapi_compute.wsgi.server [req-619e0308-cfbb-4b47-b280-8dfa8b9a257f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2556121
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:10.135 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:10.137 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:10.315 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:14.377 25797 INFO nova.metadata.wsgi.server [req-ca350bb1-e8ca-4c16-8049-9ac51c2dd13e - - - - -] 10.11.21.135,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.3158371
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:14.605 25790 INFO nova.metadata.wsgi.server [req-e840cb3c-2e87-4b1c-8d42-7817a506acd7 - - - - -] 10.11.21.135,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.2177148
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:14.919 25791 INFO nova.metadata.wsgi.server [req-972078d5-4e3b-4133-b562-d6d29d9c0110 - - - - -] 10.11.21.135,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2238431
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:14.932 25791 INFO nova.metadata.wsgi.server [-] 10.11.21.135,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0010960
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.022 25790 INFO nova.metadata.wsgi.server [-] 10.11.21.135,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0011439
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.035 25790 INFO nova.metadata.wsgi.server [-] 10.11.21.135,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0009611
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.272 25774 INFO nova.metadata.wsgi.server [req-57347641-fcb9-4d81-a320-5b050607d370 - - - - -] 10.11.21.135,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2257981
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.287 25774 INFO nova.metadata.wsgi.server [-] 10.11.21.135,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.0007021
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:15.370 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:15.371 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.537 25799 INFO nova.metadata.wsgi.server [req-705dd9dc-0fad-4523-b0ef-0524abc6f12a - - - - -] 10.11.21.135,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ HTTP/1.1" status: 200 len: 124 time: 0.2319629
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:15.546 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.549 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.135,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ami HTTP/1.1" status: 200 len: 119 time: 0.0006430
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.642 25797 INFO nova.metadata.wsgi.server [-] 10.11.21.135,10.11.10.1 "GET /latest/meta-data/block-device-mapping/root HTTP/1.1" status: 200 len: 124 time: 0.0009499
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.655 25797 INFO nova.metadata.wsgi.server [-] 10.11.21.135,10.11.10.1 "GET /latest/meta-data/placement/ HTTP/1.1" status: 200 len: 134 time: 0.0008729
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.667 25797 INFO nova.metadata.wsgi.server [-] 10.11.21.135,10.11.10.1 "GET /latest/meta-data/placement/availability-zone HTTP/1.1" status: 200 len: 120 time: 0.0008440
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.679 25797 INFO nova.metadata.wsgi.server [-] 10.11.21.135,10.11.10.1 "GET /latest/meta-data/local-ipv4 HTTP/1.1" status: 200 len: 129 time: 0.0008481
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.842 25746 INFO nova.osapi_compute.wsgi.server [req-4df3e4ef-09e7-4806-a7e9-bbb5a052ab1b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/70c1714b-c11b-4c88-b300-239afe1f5ff8 HTTP/1.1" status: 204 len: 203 time: 0.2632701
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:15.880 2931 INFO nova.compute.manager [req-4df3e4ef-09e7-4806-a7e9-bbb5a052ab1b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:15.918 25788 INFO nova.metadata.wsgi.server [req-16b789d8-59a7-4979-8807-37686ec0144a - - - - -] 10.11.21.135,10.11.10.1 "GET /latest/meta-data/reservation-id HTTP/1.1" status: 200 len: 127 time: 0.2272091
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:16.094 2931 INFO nova.virt.libvirt.driver [-] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:16.114 25746 INFO nova.osapi_compute.wsgi.server [req-413607a3-2a36-4868-a07e-2dd451878744 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2678130
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:16.762 2931 INFO nova.virt.libvirt.driver [req-4df3e4ef-09e7-4806-a7e9-bbb5a052ab1b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Deleting instance files /var/lib/nova/instances/70c1714b-c11b-4c88-b300-239afe1f5ff8_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:16.764 2931 INFO nova.virt.libvirt.driver [req-4df3e4ef-09e7-4806-a7e9-bbb5a052ab1b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Deletion of /var/lib/nova/instances/70c1714b-c11b-4c88-b300-239afe1f5ff8_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:16.878 2931 INFO nova.compute.manager [req-4df3e4ef-09e7-4806-a7e9-bbb5a052ab1b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Took 0.99 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:17.324 25746 INFO nova.osapi_compute.wsgi.server [req-ac940fc4-e12a-4d3d-b262-e8ae3afad120 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.2029028
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:17.347 2931 INFO nova.compute.manager [req-4df3e4ef-09e7-4806-a7e9-bbb5a052ab1b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] Took 0.47 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:18.432 25746 INFO nova.osapi_compute.wsgi.server [req-5579c03c-54d9-46ef-bfc5-6edae7fa4094 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.1024609
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:19.366 25746 INFO nova.api.openstack.wsgi [req-ea4d9b17-3021-441c-a951-975ae6253a8b f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:19.368 25746 INFO nova.osapi_compute.wsgi.server [req-ea4d9b17-3021-441c-a951-975ae6253a8b f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0890410
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:19.471 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:19.788 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 0
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:19.789 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=512MB phys_disk=15GB used_disk=0GB total_vcpus=16 used_vcpus=0 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:19.850 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:20.112 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:20.113 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:20.114 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Base or swap file too young to remove: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:25.180 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:25.181 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:25.182 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:28.951 25746 INFO nova.osapi_compute.wsgi.server [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.5049269
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:29.164 25746 INFO nova.osapi_compute.wsgi.server [req-237bbb72-4dca-4476-879c-60be2ce9a95a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.2089801
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:29.271 2931 INFO nova.compute.claims [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:29.272 2931 INFO nova.compute.claims [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:29.273 2931 INFO nova.compute.claims [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:29.273 2931 INFO nova.compute.claims [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:29.274 2931 INFO nova.compute.claims [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:29.275 2931 INFO nova.compute.claims [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:29.276 2931 INFO nova.compute.claims [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:29.313 2931 INFO nova.compute.claims [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:29.369 25746 INFO nova.osapi_compute.wsgi.server [req-d95fcabf-b278-4a61-986a-1f17a76796c2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.2029319
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:29.569 25746 INFO nova.osapi_compute.wsgi.server [req-20e8e749-e3a8-4213-8093-8c63771bfba2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/bf8c824d-f099-4433-a41e-e3da7578262e HTTP/1.1" status: 200 len: 1708 time: 0.1956160
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:29.898 2931 INFO nova.virt.libvirt.driver [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:30.843 25746 INFO nova.osapi_compute.wsgi.server [req-e234332e-87e4-4b52-92cd-ce2165ef38ad 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2687941
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:31.092 2931 INFO nova.compute.manager [-] [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:31.130 25746 INFO nova.osapi_compute.wsgi.server [req-5fba3209-52f5-4586-822a-2ecea551f621 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2835598
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:32.392 25746 INFO nova.osapi_compute.wsgi.server [req-529aa531-a3ff-4ded-80ad-b001df794f5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2563291
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:32.661 25746 INFO nova.osapi_compute.wsgi.server [req-fd4a841b-8d46-4485-b466-117bf8e53a94 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2663469
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:33.924 25746 INFO nova.osapi_compute.wsgi.server [req-8d6c4fd5-5df1-4eb8-b38b-9dd170a7e4a7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2576051
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:34.246 25746 INFO nova.osapi_compute.wsgi.server [req-e28227ec-c953-412d-bfea-e9d1ee4387d0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3168991
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:35.504 25746 INFO nova.osapi_compute.wsgi.server [req-03cc7683-8508-42ad-b3bc-f1a9d0a3519d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2540560
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:35.765 25746 INFO nova.osapi_compute.wsgi.server [req-0482ad95-c90b-4dc3-b288-1960a118b189 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2574868
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:37.033 25746 INFO nova.osapi_compute.wsgi.server [req-612c8a52-3f24-45a6-ab9f-61dedce20abb 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2626150
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:37.289 25746 INFO nova.osapi_compute.wsgi.server [req-50e6e570-f8fb-4d9b-973d-b9ba8a260aa0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2513149
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:38.551 25746 INFO nova.osapi_compute.wsgi.server [req-285b8598-3e91-4f68-929a-c538179bfece 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2567759
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:38.820 25746 INFO nova.osapi_compute.wsgi.server [req-6a675cf4-c816-4ccd-ae9c-eb30dacc7512 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2645810
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:40.100 25746 INFO nova.osapi_compute.wsgi.server [req-493775eb-1dd2-40ef-b924-0bf937b83836 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2762039
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:40.189 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:40.190 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:40.367 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:40.370 25746 INFO nova.osapi_compute.wsgi.server [req-2b589ba2-b7e0-4ed4-8169-372247f99be8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2652750
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:41.638 25746 INFO nova.osapi_compute.wsgi.server [req-17d97474-1552-40e1-b424-7de7e498d150 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2607520
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:41.850 2931 WARNING nova.compute.manager [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] While synchronizing instance power states, found 1 instances in the database and 0 instances on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:41.903 25746 INFO nova.osapi_compute.wsgi.server [req-cacb633a-46b5-4f22-aad1-864352c7e20d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2598419
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:43.328 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] VM Started (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:43.355 25746 INFO nova.osapi_compute.wsgi.server [req-be61f06f-fcee-49d3-9438-719cedca6e55 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4461639
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:43.389 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:43.627 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:43.627 25746 INFO nova.osapi_compute.wsgi.server [req-dd3a90c6-4783-47d3-a7da-72b8a4ef98e3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2672410
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:44.892 25746 INFO nova.osapi_compute.wsgi.server [req-95aa6171-064c-4a8f-96f6-d43250af4408 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2598760
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:45.140 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:45.141 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:45.160 25746 INFO nova.osapi_compute.wsgi.server [req-d7f4eacc-a714-40ec-8b5b-b43ac80269f1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2638071
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:45.319 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:46.427 25746 INFO nova.osapi_compute.wsgi.server [req-8faad941-ddb6-41e5-85d5-813c88b1a880 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2605860
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:46.694 25746 INFO nova.osapi_compute.wsgi.server [req-482e36cc-6ff0-4a18-a72e-0768a622100b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2634540
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:47.962 25746 INFO nova.osapi_compute.wsgi.server [req-a70bb1f3-4825-4f34-a58f-d94ba87be696 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2621419
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:48.335 25746 INFO nova.osapi_compute.wsgi.server [req-1714e9d2-6899-439e-95de-0481a374037e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3674121
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:49.404 25743 INFO nova.api.openstack.compute.server_external_events [req-fed0e14d-7871-4f5c-8445-d4fbc93c48a3 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:5c0b217a-68a9-46d1-8936-23c5688aa673 for instance bf8c824d-f099-4433-a41e-e3da7578262e
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:49.409 25743 INFO nova.osapi_compute.wsgi.server [req-fed0e14d-7871-4f5c-8445-d4fbc93c48a3 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.1030841
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:49.420 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:49.429 2931 INFO nova.virt.libvirt.driver [-] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:49.429 2931 INFO nova.compute.manager [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Took 19.53 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:49.540 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:49.541 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:49.566 2931 INFO nova.compute.manager [req-a07ac654-8e81-416d-bfbb-189116b07969 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Took 20.31 seconds to build instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:49.599 2931 INFO nova.compute.manager [-] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:49.626 25746 INFO nova.osapi_compute.wsgi.server [req-f486a199-9e29-45b5-91c5-42e148ce5712 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2863400
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:49.884 25746 INFO nova.osapi_compute.wsgi.server [req-7c8e64e9-3213-4cb0-af0a-d211dd344679 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2544081
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:50.369 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:50.370 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:50.539 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:51.165 25746 INFO nova.osapi_compute.wsgi.server [req-5768d800-ecd4-4554-a48a-2b1671a84c8c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2755542
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:51.429 25746 INFO nova.osapi_compute.wsgi.server [req-395ef8c3-1b43-49ab-97ea-cbac197fb624 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2584429
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:55.140 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:55.141 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:55.323 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:55.813 25788 INFO nova.metadata.wsgi.server [req-64ef1894-4284-4723-8d5f-4f0d80b1e371 - - - - -] 10.11.21.136,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2157998
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:55.825 25788 INFO nova.metadata.wsgi.server [-] 10.11.21.136,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0013340
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:56.049 25774 INFO nova.metadata.wsgi.server [req-a910b888-17ef-40b7-9311-a75a73d748d1 - - - - -] 10.11.21.136,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2159710
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:56.281 25793 INFO nova.metadata.wsgi.server [req-5eee7c2e-9d80-4668-869d-1d8f70d9b074 - - - - -] 10.11.21.136,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2191730
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:56.587 25783 INFO nova.metadata.wsgi.server [req-a1ef568c-59da-48e0-ad9c-64a66da68a4c - - - - -] 10.11.21.136,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.2141590
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:56.679 25783 INFO nova.metadata.wsgi.server [-] 10.11.21.136,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0009689
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:56.917 25777 INFO nova.metadata.wsgi.server [req-8e7a76fa-e150-42ed-9775-906ec38f2389 - - - - -] 10.11.21.136,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2261841
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:57.013 25777 INFO nova.metadata.wsgi.server [-] 10.11.21.136,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.0023859
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:57.339 25786 INFO nova.metadata.wsgi.server [req-91801364-8068-4e06-88b0-2075c365d41a - - - - -] 10.11.21.136,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ HTTP/1.1" status: 200 len: 124 time: 0.2300491
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:57.351 25783 INFO nova.metadata.wsgi.server [-] 10.11.21.136,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ami HTTP/1.1" status: 200 len: 119 time: 0.0009940
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:57.602 25779 INFO nova.metadata.wsgi.server [req-c13bec68-eade-44d9-b3ce-0d584189dd68 - - - - -] 10.11.21.136,10.11.10.1 "GET /latest/meta-data/block-device-mapping/root HTTP/1.1" status: 200 len: 124 time: 0.2360520
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:57.616 25779 INFO nova.metadata.wsgi.server [-] 10.11.21.136,10.11.10.1 "GET /latest/meta-data/placement/ HTTP/1.1" status: 200 len: 134 time: 0.0015550
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:57.701 25746 INFO nova.osapi_compute.wsgi.server [req-ea160a5d-14a4-4637-b413-119173854b09 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/bf8c824d-f099-4433-a41e-e3da7578262e HTTP/1.1" status: 204 len: 203 time: 0.2631681
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:57.739 2931 INFO nova.compute.manager [req-ea160a5d-14a4-4637-b413-119173854b09 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:57.846 25778 INFO nova.metadata.wsgi.server [req-c22c0e7f-ff55-4389-9c48-bf72dda87ce7 - - - - -] 10.11.21.136,10.11.10.1 "GET /latest/meta-data/placement/availability-zone HTTP/1.1" status: 200 len: 120 time: 0.2186198
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:57.957 2931 INFO nova.virt.libvirt.driver [-] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:57.970 25746 INFO nova.osapi_compute.wsgi.server [req-a36c9659-b804-4c4f-952a-693b747decac 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2665620
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:58.622 2931 INFO nova.virt.libvirt.driver [req-ea160a5d-14a4-4637-b413-119173854b09 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Deleting instance files /var/lib/nova/instances/bf8c824d-f099-4433-a41e-e3da7578262e_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:58.623 2931 INFO nova.virt.libvirt.driver [req-ea160a5d-14a4-4637-b413-119173854b09 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Deletion of /var/lib/nova/instances/bf8c824d-f099-4433-a41e-e3da7578262e_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:58.744 2931 INFO nova.compute.manager [req-ea160a5d-14a4-4637-b413-119173854b09 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Took 1.00 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:09:59.202 25746 INFO nova.osapi_compute.wsgi.server [req-ee8bc8ba-9265-4280-9215-dbe000a41209 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.2280791
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:09:59.276 2931 INFO nova.compute.manager [req-ea160a5d-14a4-4637-b413-119173854b09 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] Took 0.53 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:00.303 25746 INFO nova.osapi_compute.wsgi.server [req-4895c258-b2f8-488f-a2a3-4fae63982e48 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0968180
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:00.349 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:00.349 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:00.350 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:01.301 25746 INFO nova.api.openstack.wsgi [req-a237c1a8-c823-4154-927b-ebcd741f6fbf f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:01.303 25746 INFO nova.osapi_compute.wsgi.server [req-a237c1a8-c823-4154-927b-ebcd741f6fbf f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0892961
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:10.771 25746 INFO nova.osapi_compute.wsgi.server [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.4532349
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:10.965 25746 INFO nova.osapi_compute.wsgi.server [req-fbfbe68a-3fb6-4132-b139-e707f5534120 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1882150
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:11.073 2931 INFO nova.compute.claims [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:11.074 2931 INFO nova.compute.claims [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:11.075 2931 INFO nova.compute.claims [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:11.076 2931 INFO nova.compute.claims [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:11.076 2931 INFO nova.compute.claims [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:11.077 2931 INFO nova.compute.claims [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:11.077 2931 INFO nova.compute.claims [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:11.111 2931 INFO nova.compute.claims [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:11.169 25746 INFO nova.osapi_compute.wsgi.server [req-33e71032-1e09-4ea2-9571-c1c1f705552d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.1996739
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:11.360 25746 INFO nova.osapi_compute.wsgi.server [req-c0a17f48-be13-4aef-becf-c37a92070a7d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/be793e89-2cc3-4f99-9884-9c6a624a84bc HTTP/1.1" status: 200 len: 1708 time: 0.1866481
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:11.713 2931 INFO nova.virt.libvirt.driver [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:12.623 25746 INFO nova.osapi_compute.wsgi.server [req-d86ac496-7a40-4c30-8075-1217fca754d3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2584841
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:12.922 25746 INFO nova.osapi_compute.wsgi.server [req-0350bc90-73e4-414f-8f4a-b2640e1cde69 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2935462
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:12.953 2931 INFO nova.compute.manager [-] [instance: bf8c824d-f099-4433-a41e-e3da7578262e] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:14.188 25746 INFO nova.osapi_compute.wsgi.server [req-8a3543f5-d7a2-4715-ba54-1b9515b1bf6e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2618670
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:14.448 25746 INFO nova.osapi_compute.wsgi.server [req-ae9fd2b5-f3e8-416a-a7e9-901b07b280e3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2564912
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:15.706 25746 INFO nova.osapi_compute.wsgi.server [req-077c3c87-b7ff-46f8-8320-8fed200a0b94 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2517929
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:15.976 25746 INFO nova.osapi_compute.wsgi.server [req-f2df3c93-dded-4eed-9f76-6554b6f0ad80 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2661891
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:17.242 25746 INFO nova.osapi_compute.wsgi.server [req-e9256d28-3bba-4704-b96c-9257d8a94f9a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2605171
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:17.503 25746 INFO nova.osapi_compute.wsgi.server [req-4b4147f6-e5d1-469c-bae0-41d505cc6a89 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2561631
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:18.778 25746 INFO nova.osapi_compute.wsgi.server [req-64997958-5681-4d81-9842-7761830aba34 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2691131
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:19.051 25746 INFO nova.osapi_compute.wsgi.server [req-815218d2-3f52-467c-86bb-ca2bf4e19001 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2683980
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:20.321 25746 INFO nova.osapi_compute.wsgi.server [req-945804d9-c273-4d68-b179-ddda131082d0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2642100
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:20.581 25746 INFO nova.osapi_compute.wsgi.server [req-5068a20e-3eff-480e-aa0f-11d004908fe4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2559950
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:21.237 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:21.670 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 1
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:21.671 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=2560MB phys_disk=15GB used_disk=20GB total_vcpus=16 used_vcpus=1 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:21.731 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:21.858 25746 INFO nova.osapi_compute.wsgi.server [req-edcdf7cd-ba7f-4d48-bac6-d382209d037e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2706959
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:22.134 25746 INFO nova.osapi_compute.wsgi.server [req-7ab32aeb-f03d-4e16-9e0e-b06217f81a83 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2729750
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:23.420 25746 INFO nova.osapi_compute.wsgi.server [req-cd52b7e1-5ad8-4c2b-babc-3250dc367121 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2790480
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:23.677 25746 INFO nova.osapi_compute.wsgi.server [req-98851fab-6d07-49e3-a4de-b2fd5162bcba 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2529030
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:24.241 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:24.304 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:24.423 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:24.956 25746 INFO nova.osapi_compute.wsgi.server [req-0674a244-9379-44c7-b63e-24ca4623fd72 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2741919
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:25.181 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:25.182 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:25.225 25746 INFO nova.osapi_compute.wsgi.server [req-6e866b4c-bd67-4e7b-8beb-a450edda2fa5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2659979
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:25.366 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:26.486 25746 INFO nova.osapi_compute.wsgi.server [req-b2a03290-0331-430f-93f8-b3ead868b0d5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2558751
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:26.856 25746 INFO nova.osapi_compute.wsgi.server [req-3ce8c6ef-a314-44fc-84ba-752e2f6cadde 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3656721
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:28.293 25746 INFO nova.osapi_compute.wsgi.server [req-62d76f3f-ce7b-4d81-babe-cb80cafb0c46 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4316020
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:28.546 25746 INFO nova.osapi_compute.wsgi.server [req-23157160-b741-469c-a6c0-9d3a1548c852 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2495072
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:29.818 25746 INFO nova.osapi_compute.wsgi.server [req-d4f8d0c2-4f21-4b38-9f85-d82f8da75c23 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2663860
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:30.088 25746 INFO nova.osapi_compute.wsgi.server [req-deb750c5-cf92-4e47-b828-298d46ee7b78 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2659440
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:30.420 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:30.421 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:30.615 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:30.844 25743 INFO nova.api.openstack.compute.server_external_events [req-8c36e637-8e89-4cbe-813d-c5f46cc79818 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:418c107f-95d6-4655-ae6b-90bca57d9a71 for instance be793e89-2cc3-4f99-9884-9c6a624a84bc
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:30.848 25743 INFO nova.osapi_compute.wsgi.server [req-8c36e637-8e89-4cbe-813d-c5f46cc79818 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0919530
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:30.861 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:30.873 2931 INFO nova.virt.libvirt.driver [-] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:30.874 2931 INFO nova.compute.manager [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Took 19.16 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:30.993 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:30.994 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:31.027 2931 INFO nova.compute.manager [req-9118475d-6e72-48fa-9dee-e6b7d957dcd6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Took 19.97 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:31.358 25746 INFO nova.osapi_compute.wsgi.server [req-15ed46bc-c234-4dd1-ba31-6132a5a21007 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2641079
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:31.639 25746 INFO nova.osapi_compute.wsgi.server [req-95cb0030-6b56-44a8-8208-d68412f2eb43 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2763309
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:35.137 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:35.138 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:35.319 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:37.220 25793 INFO nova.metadata.wsgi.server [req-3169ebb5-1055-4017-a172-e8da7500e084 - - - - -] 10.11.21.137,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2397351
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:37.231 25793 INFO nova.metadata.wsgi.server [-] 10.11.21.137,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0005910
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:37.239 25793 INFO nova.metadata.wsgi.server [-] 10.11.21.137,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0005460
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:37.568 25790 INFO nova.metadata.wsgi.server [req-9cf301ad-363b-4559-82af-0c80f0e615bb - - - - -] 10.11.21.137,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2337520
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:37.792 25778 INFO nova.metadata.wsgi.server [req-24f7495b-14ca-4513-87d2-a413dd760f5c - - - - -] 10.11.21.137,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.2108901
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:37.806 25793 INFO nova.metadata.wsgi.server [-] 10.11.21.137,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0013261
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:37.817 25793 INFO nova.metadata.wsgi.server [-] 10.11.21.137,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0008872
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:37.898 25746 INFO nova.osapi_compute.wsgi.server [req-d3ff0250-98e8-4ed4-945b-3a67bfe78507 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/be793e89-2cc3-4f99-9884-9c6a624a84bc HTTP/1.1" status: 204 len: 203 time: 0.2519062
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:37.936 2931 INFO nova.compute.manager [req-d3ff0250-98e8-4ed4-945b-3a67bfe78507 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:38.056 25775 INFO nova.metadata.wsgi.server [req-3521b636-65e4-4a20-8eab-d4cc01f20eff - - - - -] 10.11.21.137,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.2276061
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:38.153 2931 INFO nova.virt.libvirt.driver [-] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:38.172 25746 INFO nova.osapi_compute.wsgi.server [req-0ed561a5-fdf2-4b23-ab40-0f283f3da54e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2693460
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:38.835 2931 INFO nova.virt.libvirt.driver [req-d3ff0250-98e8-4ed4-945b-3a67bfe78507 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Deleting instance files /var/lib/nova/instances/be793e89-2cc3-4f99-9884-9c6a624a84bc_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:38.836 2931 INFO nova.virt.libvirt.driver [req-d3ff0250-98e8-4ed4-945b-3a67bfe78507 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Deletion of /var/lib/nova/instances/be793e89-2cc3-4f99-9884-9c6a624a84bc_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:38.954 2931 INFO nova.compute.manager [req-d3ff0250-98e8-4ed4-945b-3a67bfe78507 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Took 1.01 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:39.366 25746 INFO nova.osapi_compute.wsgi.server [req-93ec4346-3e60-4997-8a32-420bab0a390f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.1886330
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:39.417 2931 INFO nova.compute.manager [req-d3ff0250-98e8-4ed4-945b-3a67bfe78507 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] Took 0.46 seconds to deallocate network for instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:40.350 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:40.351 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:40.352 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:40.464 25746 INFO nova.osapi_compute.wsgi.server [req-e9d2e77a-a98d-4605-9681-3dcad21d7ec7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0931380
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:41.452 25746 INFO nova.api.openstack.wsgi [req-d0d1ead8-1065-40d2-a364-2dfde517832d f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:41.453 25746 INFO nova.osapi_compute.wsgi.server [req-d0d1ead8-1065-40d2-a364-2dfde517832d f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0970299
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:50.935 25746 INFO nova.osapi_compute.wsgi.server [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.4657719
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:51.134 25746 INFO nova.osapi_compute.wsgi.server [req-4ca1aca1-840a-4512-b431-5533a9e02497 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1969190
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:51.245 2931 INFO nova.compute.claims [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:51.246 2931 INFO nova.compute.claims [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:51.247 2931 INFO nova.compute.claims [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:51.248 2931 INFO nova.compute.claims [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:51.249 2931 INFO nova.compute.claims [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:51.249 2931 INFO nova.compute.claims [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:51.250 2931 INFO nova.compute.claims [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:51.280 2931 INFO nova.compute.claims [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:51.321 25746 INFO nova.osapi_compute.wsgi.server [req-92a8a068-8372-4c81-8008-6b8111e2dcb1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.1819348
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:51.513 25746 INFO nova.osapi_compute.wsgi.server [req-ddbf9283-3fb6-4ea4-b19f-8d50d72ad8a1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/a015cf14-84bb-4156-a48d-7c4824ac7a9d HTTP/1.1" status: 200 len: 1708 time: 0.1869872
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:51.923 2931 INFO nova.virt.libvirt.driver [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:52.772 25746 INFO nova.osapi_compute.wsgi.server [req-278d9036-452a-45bc-be2b-ef72034db638 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2531900
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:53.049 25746 INFO nova.osapi_compute.wsgi.server [req-5619cf42-78f9-4d40-aca9-4224cd0bc357 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2722960
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:10:53.150 2931 INFO nova.compute.manager [-] [instance: be793e89-2cc3-4f99-9884-9c6a624a84bc] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:54.325 25746 INFO nova.osapi_compute.wsgi.server [req-edac629f-a08b-4501-9863-97a32c6c9baf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2707181
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:54.599 25746 INFO nova.osapi_compute.wsgi.server [req-6e610a81-5877-45d7-ae19-1516790a3030 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2691090
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:55.873 25746 INFO nova.osapi_compute.wsgi.server [req-694b3a2f-adf3-4093-9f3e-e97b12fc6bd3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2678900
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:56.138 25746 INFO nova.osapi_compute.wsgi.server [req-ca1fce2f-adef-45b3-89b2-c03051136e9f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2597768
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:57.415 25746 INFO nova.osapi_compute.wsgi.server [req-3c4808c7-7693-4cff-9e9f-1bf449ae9a56 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2719259
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:57.678 25746 INFO nova.osapi_compute.wsgi.server [req-405a1c42-ae3c-45ec-abaf-eac55b56f73e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2570741
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:58.957 25746 INFO nova.osapi_compute.wsgi.server [req-31e71c53-2e15-4300-aeb6-1d77caf1d13a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2742469
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:10:59.230 25746 INFO nova.osapi_compute.wsgi.server [req-5a32f601-7181-47a1-8edc-eb734fc58645 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2691431
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:00.487 25746 INFO nova.osapi_compute.wsgi.server [req-f34c1523-f03e-46b7-a24d-af00c94cf879 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2507980
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:00.745 25746 INFO nova.osapi_compute.wsgi.server [req-ac21e2d3-1f54-4f11-a629-11fcde3d69a8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2522721
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:02.020 25746 INFO nova.osapi_compute.wsgi.server [req-b6883fec-dc0e-4f54-9df7-394c2491e52a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2690120
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:02.282 25746 INFO nova.osapi_compute.wsgi.server [req-77cba855-be33-419f-96b0-11a437e1063a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2581029
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:03.553 25746 INFO nova.osapi_compute.wsgi.server [req-a7234b0b-8e5a-4abd-9da1-6855ec3130d6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2648492
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:03.934 25746 INFO nova.osapi_compute.wsgi.server [req-c802891c-19ae-487c-bb5b-f9f65e8523ea 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3769510
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:04.923 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:04.991 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:05.111 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] During sync_power_state the instance has a pending task (spawning). Skip.
nova-scheduler.log.1.2017-05-16_13:53:08 2017-05-16 00:11:05.153 25998 INFO nova.scheduler.host_manager [req-5ad797e1-635e-4e99-b0c4-a46cd51c6ef9 - - - - -] The instance sync for host 'cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us' did not match. Re-created its InstanceList.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:05.170 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:05.172 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:05.215 25746 INFO nova.osapi_compute.wsgi.server [req-392e4a88-f4cb-41f2-98e7-44d87bd159d7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2756960
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:05.360 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:05.482 25746 INFO nova.osapi_compute.wsgi.server [req-5274c7d6-718d-4d14-99bc-3a6a3c6a8987 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2626600
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:06.731 25746 INFO nova.osapi_compute.wsgi.server [req-6c3203d9-d215-4722-af23-4a81f95f15fc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2438371
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:07.166 25746 INFO nova.osapi_compute.wsgi.server [req-fe088b1d-091d-4469-b624-642a43cf8789 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4303629
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:08.430 25746 INFO nova.osapi_compute.wsgi.server [req-9bfa48e0-464a-476d-81d5-72787269d268 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2597730
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:08.685 25746 INFO nova.osapi_compute.wsgi.server [req-a2cd93b2-1e15-4277-91c3-f32e5949210b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2514729
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:09.957 25746 INFO nova.osapi_compute.wsgi.server [req-fb899231-69fa-4ab6-ac40-93bb982b208e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2669530
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:10.144 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:10.145 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:10.238 25746 INFO nova.osapi_compute.wsgi.server [req-75353906-801d-43f3-a38b-1c1fbab380a1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2760520
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:10.339 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:11.506 25746 INFO nova.osapi_compute.wsgi.server [req-5fad98fe-c83a-4293-8b50-5fdf9c6b9127 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2613258
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:11.760 25746 INFO nova.osapi_compute.wsgi.server [req-6cbb36b7-bd0e-4336-8e06-af3b2951e446 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2498651
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:12.144 25743 INFO nova.api.openstack.compute.server_external_events [req-47c1f81c-87e8-42b0-8986-5f705cc2ce6b f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:72791ba0-33ef-44f2-9d88-d7c8f313cb46 for instance a015cf14-84bb-4156-a48d-7c4824ac7a9d
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:12.149 25743 INFO nova.osapi_compute.wsgi.server [req-47c1f81c-87e8-42b0-8986-5f705cc2ce6b f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0926321
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:12.162 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:12.170 2931 INFO nova.virt.libvirt.driver [-] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:12.171 2931 INFO nova.compute.manager [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Took 20.25 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:12.315 2931 INFO nova.compute.manager [req-4b4dd551-26d8-48e2-bd52-91793d2157bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Took 21.08 seconds to build instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:12.402 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] VM Resumed (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:13.042 25746 INFO nova.osapi_compute.wsgi.server [req-a3fe9ed3-7c8e-418d-8cd6-c4a2f013e31d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2771690
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:13.303 25746 INFO nova.osapi_compute.wsgi.server [req-65523c61-535f-4cb1-8c34-2dfe526b6ea2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2569480
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:15.392 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:15.393 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:15.571 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:18.602 25778 INFO nova.metadata.wsgi.server [req-895531e0-a48b-49a0-b190-1ce9cd0b7a66 - - - - -] 10.11.21.138,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2088590
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:18.689 25778 INFO nova.metadata.wsgi.server [-] 10.11.21.138,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0010941
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:18.937 25775 INFO nova.metadata.wsgi.server [req-3af3383b-9776-4b43-8cdf-c4983651f4bd - - - - -] 10.11.21.138,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2357898
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:19.171 25777 INFO nova.metadata.wsgi.server [req-c56d6984-5128-4925-aa24-2009c409e9e1 - - - - -] 10.11.21.138,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2222931
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:19.182 25777 INFO nova.metadata.wsgi.server [-] 10.11.21.138,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0009220
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:19.526 25790 INFO nova.metadata.wsgi.server [req-79c8b1ef-a852-415b-8c7c-8be53574cd17 - - - - -] 10.11.21.138,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2491531
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:19.616 25746 INFO nova.osapi_compute.wsgi.server [req-5e0f9d3c-be64-4960-a107-d406900e0ea8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/a015cf14-84bb-4156-a48d-7c4824ac7a9d HTTP/1.1" status: 204 len: 203 time: 0.3042688
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:19.656 2931 INFO nova.compute.manager [req-5e0f9d3c-be64-4960-a107-d406900e0ea8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:19.786 25776 INFO nova.metadata.wsgi.server [req-f98a7db8-25c1-495e-935d-413e4bc76c8f - - - - -] 10.11.21.138,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2451930
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:19.872 2931 INFO nova.virt.libvirt.driver [-] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:19.878 25746 INFO nova.osapi_compute.wsgi.server [req-6cf1a1d4-92bd-457c-8fc5-7dc005447ea2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2582610
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:20.569 2931 INFO nova.virt.libvirt.driver [req-5e0f9d3c-be64-4960-a107-d406900e0ea8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Deleting instance files /var/lib/nova/instances/a015cf14-84bb-4156-a48d-7c4824ac7a9d_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:20.571 2931 INFO nova.virt.libvirt.driver [req-5e0f9d3c-be64-4960-a107-d406900e0ea8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Deletion of /var/lib/nova/instances/a015cf14-84bb-4156-a48d-7c4824ac7a9d_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:20.629 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:20.630 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:20.697 2931 INFO nova.compute.manager [req-5e0f9d3c-be64-4960-a107-d406900e0ea8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Took 1.04 seconds to destroy the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:20.732 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:21.179 25746 INFO nova.osapi_compute.wsgi.server [req-8e6482cd-ad01-4a71-92f9-2d218d014a8f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2944360
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:21.269 2931 INFO nova.compute.manager [req-5e0f9d3c-be64-4960-a107-d406900e0ea8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] Took 0.57 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:22.280 25746 INFO nova.osapi_compute.wsgi.server [req-31b99b97-cf99-4845-9481-b8b3719a11e4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0940671
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:23.127 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:23.292 25746 INFO nova.api.openstack.wsgi [req-033d97b9-69e4-4acd-9029-f0d7b9370645 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:23.294 25746 INFO nova.osapi_compute.wsgi.server [req-033d97b9-69e4-4acd-9029-f0d7b9370645 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0847020
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:23.428 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 0
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:23.429 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=512MB phys_disk=15GB used_disk=0GB total_vcpus=16 used_vcpus=0 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:23.489 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:25.147 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:25.148 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:25.148 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Base or swap file too young to remove: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:30.178 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:30.179 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:30.181 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:32.777 25746 INFO nova.osapi_compute.wsgi.server [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.4846020
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:32.985 25746 INFO nova.osapi_compute.wsgi.server [req-1c34c9ba-7c24-486f-9aa1-914e6415fc66 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.2032580
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:33.089 2931 INFO nova.compute.claims [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:33.091 2931 INFO nova.compute.claims [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:33.092 2931 INFO nova.compute.claims [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:33.092 2931 INFO nova.compute.claims [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:33.093 2931 INFO nova.compute.claims [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:33.093 2931 INFO nova.compute.claims [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:33.093 2931 INFO nova.compute.claims [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:33.126 2931 INFO nova.compute.claims [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:33.180 25746 INFO nova.osapi_compute.wsgi.server [req-bb798c56-02c1-4910-afe1-cf37fdf05e3b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.1902869
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:33.376 25746 INFO nova.osapi_compute.wsgi.server [req-425db137-40f4-4163-acca-c43a54f135c7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/d96a117b-0193-4549-bdcc-63b917273d1d HTTP/1.1" status: 200 len: 1708 time: 0.1916969
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:33.745 2931 INFO nova.virt.libvirt.driver [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:34.649 25746 INFO nova.osapi_compute.wsgi.server [req-91265cb3-65b6-438e-91a9-a4e8ac3714d7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2671440
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:34.908 2931 INFO nova.compute.manager [-] [instance: a015cf14-84bb-4156-a48d-7c4824ac7a9d] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:34.927 25746 INFO nova.osapi_compute.wsgi.server [req-d09fddb3-8afa-4d8f-a400-494aad29e9c8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2735381
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:36.199 25746 INFO nova.osapi_compute.wsgi.server [req-d083d3b6-88a1-4af0-b0ff-8adc0dfea774 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2661309
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:36.470 25746 INFO nova.osapi_compute.wsgi.server [req-44728780-6d8a-4bdb-8440-fdce3253364e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2660699
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:37.744 25746 INFO nova.osapi_compute.wsgi.server [req-16da8660-30be-44ba-990e-63231063b762 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2677679
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:38.016 25746 INFO nova.osapi_compute.wsgi.server [req-0e06bc07-8dc9-459a-9796-81ec99d6d085 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2669740
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:39.283 25746 INFO nova.osapi_compute.wsgi.server [req-a40c78d6-9f83-4945-86c5-11d623eb41b7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2620499
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:39.550 25746 INFO nova.osapi_compute.wsgi.server [req-cb466503-b438-4e85-b0aa-c543398c97d2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2626660
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:40.828 25746 INFO nova.osapi_compute.wsgi.server [req-3520ffde-5536-40dd-ba9d-ef1581a27b18 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2727239
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:41.093 25746 INFO nova.osapi_compute.wsgi.server [req-88f886f0-8d8a-4a1c-9a81-bd954933df8f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2616799
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:42.371 25746 INFO nova.osapi_compute.wsgi.server [req-5109428d-2e28-4e45-bc21-e205311b0c37 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2707500
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:42.608 25746 INFO nova.osapi_compute.wsgi.server [req-f0b73f56-6769-4493-9510-f68c4f463312 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2313168
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:43.981 25746 INFO nova.osapi_compute.wsgi.server [req-96bb2855-71cc-4c4c-8ffb-03336d12219e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3673830
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:44.261 25746 INFO nova.osapi_compute.wsgi.server [req-54b8381a-ee9b-4e94-a9dc-53762e6f6f33 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2742689
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:45.170 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:45.171 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:45.359 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:45.532 25746 INFO nova.osapi_compute.wsgi.server [req-5d540c90-a504-4ae7-ac15-3f527a2a8b71 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2657590
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:45.806 25746 INFO nova.osapi_compute.wsgi.server [req-243b1f1f-768b-4eb9-a559-3887ce7544ca 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2688351
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:46.681 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:46.754 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:46.882 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:47.269 25746 INFO nova.osapi_compute.wsgi.server [req-39b88da5-d66a-4bd5-8c1c-3d9a86685f81 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4555459
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:47.543 25746 INFO nova.osapi_compute.wsgi.server [req-db9f16f5-a91f-421d-a4a9-ae0f7e563c6d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2685769
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:48.812 25746 INFO nova.osapi_compute.wsgi.server [req-37d41a3b-e45c-4972-b4a4-0812e63e3da0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2627749
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:49.076 25746 INFO nova.osapi_compute.wsgi.server [req-a2c60c94-be5e-451e-ba23-b3ba022b70e9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2605669
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:50.362 25746 INFO nova.osapi_compute.wsgi.server [req-5930f621-ff62-4c56-a7c4-04165c4b235a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2803490
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:50.413 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:50.414 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:50.595 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:50.635 25746 INFO nova.osapi_compute.wsgi.server [req-5af8f9a2-fcb3-47b3-b34e-5286b95b0b8b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2692139
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:51.914 25746 INFO nova.osapi_compute.wsgi.server [req-065efef6-3934-40b5-91b3-03db9f152a8e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2734950
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:52.192 25746 INFO nova.osapi_compute.wsgi.server [req-e0181115-d3d1-4f16-b1fc-845c62a17d36 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2748618
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:53.451 25743 INFO nova.api.openstack.compute.server_external_events [req-f1165387-31bd-47d0-8401-c94cf986171d f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:a444c98d-73cb-4ef8-b9c8-dbc1c4a7bd5c for instance d96a117b-0193-4549-bdcc-63b917273d1d
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:53.456 25743 INFO nova.osapi_compute.wsgi.server [req-f1165387-31bd-47d0-8401-c94cf986171d f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0897720
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:53.466 25746 INFO nova.osapi_compute.wsgi.server [req-96b6dd3d-8eb1-43f0-9bbb-19fd234cc6f5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2685661
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:53.469 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:53.482 2931 INFO nova.virt.libvirt.driver [-] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:53.482 2931 INFO nova.compute.manager [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Took 19.74 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:53.591 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:53.592 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:53.630 2931 INFO nova.compute.manager [req-5c8f52bd-8e3c-41f0-95a5-7861d247cafa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Took 20.55 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:53.761 25746 INFO nova.osapi_compute.wsgi.server [req-47727d10-5693-482b-8a10-ae94798bfa8e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2909820
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:55.039 25746 INFO nova.osapi_compute.wsgi.server [req-83ca2d84-6122-4f81-9183-523acb70934b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2722361
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:55.140 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:55.141 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:55.290 25746 INFO nova.osapi_compute.wsgi.server [req-538510d6-0945-4417-a8c9-1c377762514f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2472789
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:11:55.326 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:59.816 25788 INFO nova.metadata.wsgi.server [req-e0d4ce94-e0cb-41b5-9192-c6b9fdd49a94 - - - - -] 10.11.21.139,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2261620
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:59.826 25788 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0006461
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:11:59.914 25788 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0006561
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:00.222 25776 INFO nova.metadata.wsgi.server [req-5de0f721-212b-46b8-81df-fb28eaafb152 - - - - -] 10.11.21.139,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2188199
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:00.236 25776 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0010579
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:00.381 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:00.382 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:00.548 25799 INFO nova.metadata.wsgi.server [req-dd6dbb5e-9bf4-4e7f-95b7-4dfa7bee65bc - - - - -] 10.11.21.139,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2225499
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:00.563 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0007999
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:00.579 25788 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.0009460
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:00.581 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:00.900 25784 INFO nova.metadata.wsgi.server [req-7fd3da99-a9b0-4dec-8a14-37be960fd707 - - - - -] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ HTTP/1.1" status: 200 len: 124 time: 0.2223170
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.212 25795 INFO nova.metadata.wsgi.server [req-414101a6-d4d4-48ba-adf3-b2c15eb943e1 - - - - -] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ami HTTP/1.1" status: 200 len: 119 time: 0.2219629
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.442 25777 INFO nova.metadata.wsgi.server [req-7ef5f130-fbae-4370-ad20-07c0ce5d475e - - - - -] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/block-device-mapping/root HTTP/1.1" status: 200 len: 124 time: 0.2165668
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.458 25777 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/placement/ HTTP/1.1" status: 200 len: 134 time: 0.0007932
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.471 25777 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/placement/availability-zone HTTP/1.1" status: 200 len: 120 time: 0.0008299
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.483 25777 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/local-ipv4 HTTP/1.1" status: 200 len: 129 time: 0.0008519
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.495 25777 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/reservation-id HTTP/1.1" status: 200 len: 127 time: 0.0009091
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.507 25788 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/local-hostname HTTP/1.1" status: 200 len: 130 time: 0.0008819
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.519 25788 INFO nova.metadata.wsgi.server [-] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/security-groups HTTP/1.1" status: 200 len: 123 time: 0.0008359
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.550 25746 INFO nova.osapi_compute.wsgi.server [req-121ecfae-3fb1-49cc-9a78-8b046fe73a77 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/d96a117b-0193-4549-bdcc-63b917273d1d HTTP/1.1" status: 204 len: 203 time: 0.2549498
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:01.590 2931 INFO nova.compute.manager [req-121ecfae-3fb1-49cc-9a78-8b046fe73a77 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.797 25797 INFO nova.metadata.wsgi.server [req-09ad207c-d18b-465c-aaa6-ca8326453c26 - - - - -] 10.11.21.139,10.11.10.1 "GET /latest/meta-data/ami-launch-index HTTP/1.1" status: 200 len: 117 time: 0.2664881
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:01.806 2931 INFO nova.virt.libvirt.driver [-] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:01.866 25746 INFO nova.osapi_compute.wsgi.server [req-c8743542-0b14-4dcd-97a4-9e98303b58ba 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.3120630
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:02.514 2931 INFO nova.virt.libvirt.driver [req-121ecfae-3fb1-49cc-9a78-8b046fe73a77 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Deleting instance files /var/lib/nova/instances/d96a117b-0193-4549-bdcc-63b917273d1d_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:02.516 2931 INFO nova.virt.libvirt.driver [req-121ecfae-3fb1-49cc-9a78-8b046fe73a77 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Deletion of /var/lib/nova/instances/d96a117b-0193-4549-bdcc-63b917273d1d_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:02.629 2931 INFO nova.compute.manager [req-121ecfae-3fb1-49cc-9a78-8b046fe73a77 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Took 1.03 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:03.079 25746 INFO nova.osapi_compute.wsgi.server [req-5158941a-f199-4fb0-9e63-688d2ba433a9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.2061739
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:03.079 2931 INFO nova.compute.manager [req-121ecfae-3fb1-49cc-9a78-8b046fe73a77 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] Took 0.45 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:04.178 25746 INFO nova.osapi_compute.wsgi.server [req-cc4f8ca6-e412-41c5-b196-f1390dba7faf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0934021
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:05.112 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:05.112 25746 INFO nova.api.openstack.wsgi [req-d6e9cfb8-d914-48c3-b677-72bc73329c69 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:05.112 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:05.113 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Base or swap file too young to remove: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:05.114 25746 INFO nova.osapi_compute.wsgi.server [req-d6e9cfb8-d914-48c3-b677-72bc73329c69 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0934131
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:10.117 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:10.118 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:10.119 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:14.727 25746 INFO nova.osapi_compute.wsgi.server [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.5341210
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:14.925 25746 INFO nova.osapi_compute.wsgi.server [req-47b64d2e-e73c-4948-a1f9-17c873456407 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1934071
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:15.046 2931 INFO nova.compute.claims [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:15.047 2931 INFO nova.compute.claims [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:15.048 2931 INFO nova.compute.claims [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:15.048 2931 INFO nova.compute.claims [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:15.049 2931 INFO nova.compute.claims [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:15.049 2931 INFO nova.compute.claims [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:15.050 2931 INFO nova.compute.claims [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:15.080 2931 INFO nova.compute.claims [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:15.125 25746 INFO nova.osapi_compute.wsgi.server [req-bfce366e-9868-4f52-b245-271e9b3455ca 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1962459
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:15.330 25746 INFO nova.osapi_compute.wsgi.server [req-065015f7-ef1c-460d-be9e-68adf319b082 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/d6b7bd36-2943-4363-9235-fffdd89ea40e HTTP/1.1" status: 200 len: 1708 time: 0.2011390
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:15.751 2931 INFO nova.virt.libvirt.driver [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:16.614 25746 INFO nova.osapi_compute.wsgi.server [req-9eb36ce0-5cbd-44c3-9a07-06fea8c77326 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2800210
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:16.893 25746 INFO nova.osapi_compute.wsgi.server [req-40de51cd-b1d6-4350-9d46-57e0bc9489c1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2750871
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:16.898 2931 INFO nova.compute.manager [-] [instance: d96a117b-0193-4549-bdcc-63b917273d1d] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:18.167 25746 INFO nova.osapi_compute.wsgi.server [req-4872dec2-12a9-494a-a0c7-237f279bf011 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2676511
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:18.430 25746 INFO nova.osapi_compute.wsgi.server [req-b7cfcfab-18ec-4bde-a156-fdb1b2f47c00 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2587070
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:19.810 25746 INFO nova.osapi_compute.wsgi.server [req-f152f651-e6f7-445c-926d-3bfc957bb0f4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3740301
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:20.084 25746 INFO nova.osapi_compute.wsgi.server [req-eb2ffac6-1ce1-42fe-99a9-cd7b7c6cb78f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2694941
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:21.356 25746 INFO nova.osapi_compute.wsgi.server [req-3e488ee5-f0f3-41b6-b0b1-894d043b6f5e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2681561
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:21.621 25746 INFO nova.osapi_compute.wsgi.server [req-103f400e-2e15-4052-9d68-3c1f3be374f6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2600908
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:22.898 25746 INFO nova.osapi_compute.wsgi.server [req-9f9bea16-b6b9-45e6-8fd7-83cb41c5db6d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2717772
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:23.177 25746 INFO nova.osapi_compute.wsgi.server [req-eb575f6b-9fcc-48a5-b179-97b24561a888 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2747040
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:24.452 25746 INFO nova.osapi_compute.wsgi.server [req-ae99e5e4-fa5f-4574-8b38-e246c01b5d70 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2700670
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:24.724 25746 INFO nova.osapi_compute.wsgi.server [req-1b487345-4528-402e-a731-e4466158c607 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2672610
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:25.165 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:25.615 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 1
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:25.616 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=2560MB phys_disk=15GB used_disk=20GB total_vcpus=16 used_vcpus=1 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:25.695 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:25.745 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:25.745 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:25.927 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:25.994 25746 INFO nova.osapi_compute.wsgi.server [req-7e7cfc23-3ca0-4da6-905e-2ad3e38efe55 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2639382
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:26.252 25746 INFO nova.osapi_compute.wsgi.server [req-0b9ec1ca-9a89-44fc-8f9a-c6a7460afa4b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2556710
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:27.529 25746 INFO nova.osapi_compute.wsgi.server [req-2a1f7bb7-7587-41c9-9cbf-ecfa82136c36 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2704930
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:27.779 25746 INFO nova.osapi_compute.wsgi.server [req-4892d245-063a-4b1a-b6b6-9fb90b09c63d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2466712
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:28.718 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:28.783 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:28.906 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:29.054 25746 INFO nova.osapi_compute.wsgi.server [req-e4a5ec58-0304-4e5e-930c-57180ee7c1dd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2686100
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:29.322 25746 INFO nova.osapi_compute.wsgi.server [req-671e38f1-b681-4dfb-b2a3-b28096ffe443 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2640181
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:30.595 25746 INFO nova.osapi_compute.wsgi.server [req-2f3cc725-2a26-46b3-9b24-0df5baf1ebf2 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2660620
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:30.858 25746 INFO nova.osapi_compute.wsgi.server [req-afa4d0e3-8b30-49a5-a9b8-4048c7d3b6d9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2591708
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:30.980 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:30.981 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:31.174 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:32.302 25746 INFO nova.osapi_compute.wsgi.server [req-410230db-16e7-4909-9ce4-8147d7bb053f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4367440
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:32.569 25746 INFO nova.osapi_compute.wsgi.server [req-2fbc1e4d-d334-43bb-adb6-1b3a32c127a6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2634561
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:33.847 25746 INFO nova.osapi_compute.wsgi.server [req-67506f9d-a336-4418-893e-aa3b01d2dfba 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2727580
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:34.133 25746 INFO nova.osapi_compute.wsgi.server [req-f8409217-ad00-40c8-96ba-c5eea0f38b6d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2826021
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:34.914 25743 INFO nova.api.openstack.compute.server_external_events [req-1695526d-1332-4b35-932c-bb5df4d7e1bb f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:0b72aca0-102e-4421-8ff3-2902d8353ff0 for instance d6b7bd36-2943-4363-9235-fffdd89ea40e
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:34.921 25743 INFO nova.osapi_compute.wsgi.server [req-1695526d-1332-4b35-932c-bb5df4d7e1bb f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.1011631
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:34.930 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:34.936 2931 INFO nova.virt.libvirt.driver [-] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:34.937 2931 INFO nova.compute.manager [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Took 19.19 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:35.071 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:35.072 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:35.078 2931 INFO nova.compute.manager [req-beb938db-df6e-4611-8113-1a148a0224bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Took 20.05 seconds to build instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:35.134 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:35.135 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:35.306 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:35.416 25746 INFO nova.osapi_compute.wsgi.server [req-6984d94a-c865-4248-afbc-07e050bea526 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2781398
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:35.663 25746 INFO nova.osapi_compute.wsgi.server [req-537bc103-6c6d-4c9d-a801-52aa85533601 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2428820
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:40.144 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:40.145 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:40.325 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:41.227 25783 INFO nova.metadata.wsgi.server [req-fb8d5cd6-127f-4735-9c32-25567cf0d2d6 - - - - -] 10.11.21.140,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2175281
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:41.470 25777 INFO nova.metadata.wsgi.server [req-27525a89-b0a1-4d18-8263-f2bc3079f9dc - - - - -] 10.11.21.140,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.2332909
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:41.797 25786 INFO nova.metadata.wsgi.server [req-624c74d8-31e8-45d2-aa80-bb9b1821399c - - - - -] 10.11.21.140,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2250240
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:41.809 25786 INFO nova.metadata.wsgi.server [-] 10.11.21.140,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.0006189
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:41.818 25786 INFO nova.metadata.wsgi.server [-] 10.11.21.140,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0012619
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:41.834 25786 INFO nova.metadata.wsgi.server [-] 10.11.21.140,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0032220
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:41.923 25746 INFO nova.osapi_compute.wsgi.server [req-83a70d92-4077-4368-b3e4-b416cf0128aa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/d6b7bd36-2943-4363-9235-fffdd89ea40e HTTP/1.1" status: 204 len: 203 time: 0.2511380
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:41.962 2931 INFO nova.compute.manager [req-83a70d92-4077-4368-b3e4-b416cf0128aa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:42.150 25795 INFO nova.metadata.wsgi.server [req-3d7baa90-ec29-4386-a6c5-edd5ae651f45 - - - - -] 10.11.21.140,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2314420
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:42.178 2931 INFO nova.virt.libvirt.driver [-] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:42.292 25746 INFO nova.osapi_compute.wsgi.server [req-109ff6a8-131b-47fb-978f-fca442499379 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.3658569
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:42.843 2931 INFO nova.virt.libvirt.driver [req-83a70d92-4077-4368-b3e4-b416cf0128aa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Deleting instance files /var/lib/nova/instances/d6b7bd36-2943-4363-9235-fffdd89ea40e_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:42.844 2931 INFO nova.virt.libvirt.driver [req-83a70d92-4077-4368-b3e4-b416cf0128aa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Deletion of /var/lib/nova/instances/d6b7bd36-2943-4363-9235-fffdd89ea40e_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:42.975 2931 INFO nova.compute.manager [req-83a70d92-4077-4368-b3e4-b416cf0128aa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Took 1.01 seconds to destroy the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:43.429 2931 INFO nova.compute.manager [req-83a70d92-4077-4368-b3e4-b416cf0128aa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] Took 0.45 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:43.493 25746 INFO nova.osapi_compute.wsgi.server [req-36f6bb06-1b97-4d65-9794-e557cd23be7c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.1959360
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:44.602 25746 INFO nova.osapi_compute.wsgi.server [req-38319757-f509-49a2-ac28-becee3202f47 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.1033430
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:45.352 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:45.353 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:45.354 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:45.461 25746 INFO nova.api.openstack.wsgi [req-ab2e766f-eee9-4e84-8171-8ba713a28f9f f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:45.463 25746 INFO nova.osapi_compute.wsgi.server [req-ab2e766f-eee9-4e84-8171-8ba713a28f9f f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0927291
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:55.123 25746 INFO nova.osapi_compute.wsgi.server [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.5053148
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:55.324 25746 INFO nova.osapi_compute.wsgi.server [req-be711236-194c-40ba-afc0-2c1c4327c503 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1981771
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:55.418 2931 INFO nova.compute.claims [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:55.418 2931 INFO nova.compute.claims [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:55.419 2931 INFO nova.compute.claims [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:55.419 2931 INFO nova.compute.claims [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:55.420 2931 INFO nova.compute.claims [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:55.420 2931 INFO nova.compute.claims [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:55.421 2931 INFO nova.compute.claims [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:55.452 2931 INFO nova.compute.claims [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:55.495 25746 INFO nova.osapi_compute.wsgi.server [req-709c0ecb-ac6e-4b6b-80d1-cf6215b828d5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1575 time: 0.1659300
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:55.686 25746 INFO nova.osapi_compute.wsgi.server [req-01f43732-6b76-41c6-953f-a061d8fde6bc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/127e769a-4fe6-4548-93b1-513ac51e0452 HTTP/1.1" status: 200 len: 1708 time: 0.1873600
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:56.021 2931 INFO nova.virt.libvirt.driver [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:56.962 25746 INFO nova.osapi_compute.wsgi.server [req-5f93f093-6d8c-45cc-86cd-c8602e922d6a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2707939
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:57.247 25746 INFO nova.osapi_compute.wsgi.server [req-e2e335d1-99eb-4702-aa0a-111f1dc7cd93 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2809780
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:12:57.370 2931 INFO nova.compute.manager [-] [instance: d6b7bd36-2943-4363-9235-fffdd89ea40e] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:58.515 25746 INFO nova.osapi_compute.wsgi.server [req-412cbd21-a789-4318-8f25-acea4ff050d5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2608390
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:12:58.774 25746 INFO nova.osapi_compute.wsgi.server [req-d230ec37-1175-4546-bd8b-45715b8acf08 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2535899
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:00.051 25746 INFO nova.osapi_compute.wsgi.server [req-4f5d8a7d-8353-4f98-983f-bd0854bea276 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2724411
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:00.318 25746 INFO nova.osapi_compute.wsgi.server [req-db6932df-e3e8-4798-b123-2e0ee6a8999b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2623498
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:01.571 25746 INFO nova.osapi_compute.wsgi.server [req-3161b8c0-6350-466b-a9e8-600437e9783c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2472031
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:01.834 25746 INFO nova.osapi_compute.wsgi.server [req-4d8212f4-f29d-4448-a7b6-872fb05c703c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2590120
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:03.104 25746 INFO nova.osapi_compute.wsgi.server [req-b7a3a805-eee2-4539-b412-18d9d9b22c0f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2643571
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:03.349 25746 INFO nova.osapi_compute.wsgi.server [req-c7adc0ec-5ebd-40c7-9575-2cec6db128b4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2399089
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:04.621 25746 INFO nova.osapi_compute.wsgi.server [req-085df2be-ea00-4a37-b91a-1a9e1213782d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2659712
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:04.905 25746 INFO nova.osapi_compute.wsgi.server [req-ccffca62-8a09-4c29-b42e-d7500c235d6d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2796071
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:06.168 25746 INFO nova.osapi_compute.wsgi.server [req-781b0d07-ebc0-46b8-8998-4641f556ebeb 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2585039
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:06.424 25746 INFO nova.osapi_compute.wsgi.server [req-2e5c7471-1bfb-4634-8a87-aeef024c256d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2524278
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:07.682 25746 INFO nova.osapi_compute.wsgi.server [req-f791b1b3-df99-4b6f-9d52-1874ced07e4d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2522879
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:07.958 25746 INFO nova.osapi_compute.wsgi.server [req-ea4a360f-b853-46fe-af72-7db91c3346ad 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2724142
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:09.118 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] VM Started (Lifecycle Event)
nova-scheduler.log.1.2017-05-16_13:53:08 2017-05-16 00:13:09.162 25998 INFO nova.scheduler.host_manager [req-c34edf86-25ac-4c79-ab19-9ea9caa4eddf - - - - -] The instance sync for host 'cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us' did not match. Re-created its InstanceList.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:09.185 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] VM Paused (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:09.210 25746 INFO nova.osapi_compute.wsgi.server [req-fc3464cd-8dd6-4fe9-a825-bb3b395eff72 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2485340
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:09.307 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:09.578 25746 INFO nova.osapi_compute.wsgi.server [req-9fe605b6-b7ce-41bf-b16a-2be754c45c65 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3648129
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:10.137 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:10.138 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:10.327 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:10.853 25746 INFO nova.osapi_compute.wsgi.server [req-06221192-6331-4531-8e2a-f6f1041ea6a9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2682462
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:11.294 25746 INFO nova.osapi_compute.wsgi.server [req-298721cb-d01e-4dab-9475-518f5471932d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4362931
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:12.574 25746 INFO nova.osapi_compute.wsgi.server [req-3bcb41f9-500e-4b8b-b452-5fbedba613e8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2755351
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:12.834 25746 INFO nova.osapi_compute.wsgi.server [req-6a27e91d-9c9c-4b48-ade9-853fc2a05d1a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2558961
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:14.110 25746 INFO nova.osapi_compute.wsgi.server [req-a20a3b43-972a-487f-94a6-bff85ee735fc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2697771
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:14.372 25746 INFO nova.osapi_compute.wsgi.server [req-403846bd-dd0d-4472-a78d-6077e720094b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2580168
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:15.386 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:15.387 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:15.561 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:15.642 25746 INFO nova.osapi_compute.wsgi.server [req-f73e5f36-f74f-4335-831b-820d3996fbd5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2643099
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:15.917 25746 INFO nova.osapi_compute.wsgi.server [req-1f89dbfb-bbfe-45ca-8162-110e3c405711 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2716691
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:16.287 25743 INFO nova.api.openstack.compute.server_external_events [req-fc5a3419-08c8-4398-9b5c-1b14207e6129 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:b25f912d-d8bd-4332-b55f-8e244fe36c7c for instance 127e769a-4fe6-4548-93b1-513ac51e0452
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:16.292 25743 INFO nova.osapi_compute.wsgi.server [req-fc5a3419-08c8-4398-9b5c-1b14207e6129 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0975678
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:16.304 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:16.313 2931 INFO nova.virt.libvirt.driver [-] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:16.313 2931 INFO nova.compute.manager [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Took 20.29 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:16.424 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:16.425 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:16.456 2931 INFO nova.compute.manager [req-afb5ee70-39d1-435e-9bf9-2fc3339c5b5b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Took 21.05 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:17.185 25746 INFO nova.osapi_compute.wsgi.server [req-139b619d-f2c8-4877-9a0e-27e33f8dd917 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2617211
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:17.448 25746 INFO nova.osapi_compute.wsgi.server [req-5b273dce-4719-47ba-acc2-276214bd6c87 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2577119
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:20.613 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:20.614 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:20.778 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:22.680 25795 INFO nova.metadata.wsgi.server [req-94ad64f0-2983-4cec-821a-3a634edec172 - - - - -] 10.11.21.141,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2279789
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:22.695 25795 INFO nova.metadata.wsgi.server [-] 10.11.21.141,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0009151
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:22.930 25788 INFO nova.metadata.wsgi.server [req-659f79b6-9a06-48f2-89d6-84a1485015e7 - - - - -] 10.11.21.141,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2249861
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:23.250 25799 INFO nova.metadata.wsgi.server [req-db4ae65d-6aab-4be6-abbd-59fbb9dae569 - - - - -] 10.11.21.141,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2228522
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:23.262 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.141,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0009251
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:23.274 25799 INFO nova.metadata.wsgi.server [-] 10.11.21.141,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0009491
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:23.367 25788 INFO nova.metadata.wsgi.server [-] 10.11.21.141,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0013518
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:23.605 25784 INFO nova.metadata.wsgi.server [req-cf86bdeb-2809-46ac-a163-99450b2985aa - - - - -] 10.11.21.141,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.2267661
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:23.708 25746 INFO nova.osapi_compute.wsgi.server [req-2aa6387d-95fb-4fd7-b1be-f36ebc5c11a6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/127e769a-4fe6-4548-93b1-513ac51e0452 HTTP/1.1" status: 204 len: 203 time: 0.2509129
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:23.746 2931 INFO nova.compute.manager [req-2aa6387d-95fb-4fd7-b1be-f36ebc5c11a6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:23.852 25777 INFO nova.metadata.wsgi.server [req-ba29717b-249a-4b80-bceb-f95959dedc24 - - - - -] 10.11.21.141,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ HTTP/1.1" status: 200 len: 124 time: 0.2311139
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:23.967 2931 INFO nova.virt.libvirt.driver [-] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:23.987 25746 INFO nova.osapi_compute.wsgi.server [req-94d476af-a264-46bc-81cb-b42d00ee2d0f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2735779
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:24.634 2931 INFO nova.virt.libvirt.driver [req-2aa6387d-95fb-4fd7-b1be-f36ebc5c11a6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Deleting instance files /var/lib/nova/instances/127e769a-4fe6-4548-93b1-513ac51e0452_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:24.636 2931 INFO nova.virt.libvirt.driver [req-2aa6387d-95fb-4fd7-b1be-f36ebc5c11a6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Deletion of /var/lib/nova/instances/127e769a-4fe6-4548-93b1-513ac51e0452_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:24.760 2931 INFO nova.compute.manager [req-2aa6387d-95fb-4fd7-b1be-f36ebc5c11a6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Took 1.00 seconds to destroy the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:25.175 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:25.190 25746 INFO nova.osapi_compute.wsgi.server [req-04afc2da-442d-4a57-ac6e-c0f7a277825c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1874 time: 0.1984708
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:25.233 2931 INFO nova.compute.manager [req-2aa6387d-95fb-4fd7-b1be-f36ebc5c11a6 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] Took 0.47 seconds to deallocate network for instance.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:25.532 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 0
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:25.533 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=512MB phys_disk=15GB used_disk=0GB total_vcpus=16 used_vcpus=0 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:25.591 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:25.620 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:25.621 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:25.622 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Base or swap file too young to remove: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:26.290 25746 INFO nova.osapi_compute.wsgi.server [req-5addb7fc-f821-4418-85ee-18d453e4401e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0954659
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:27.251 25746 INFO nova.api.openstack.wsgi [req-4beefba4-a928-45a5-90f6-6246e77bc2ce f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:27.253 25746 INFO nova.osapi_compute.wsgi.server [req-4beefba4-a928-45a5-90f6-6246e77bc2ce f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0877421
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:30.649 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:30.650 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:30.652 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:36.797 25746 INFO nova.osapi_compute.wsgi.server [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.4923580
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:37.004 25746 INFO nova.osapi_compute.wsgi.server [req-dc39a957-533e-48ec-bdbe-e9483f37cb85 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.2042511
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:37.116 2931 INFO nova.compute.claims [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:37.117 2931 INFO nova.compute.claims [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:37.117 2931 INFO nova.compute.claims [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:37.118 2931 INFO nova.compute.claims [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:37.118 2931 INFO nova.compute.claims [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:37.119 2931 INFO nova.compute.claims [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:37.119 2931 INFO nova.compute.claims [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:37.154 2931 INFO nova.compute.claims [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:37.193 25746 INFO nova.osapi_compute.wsgi.server [req-aef59c8e-1f3c-4a10-8c3b-8e73c538f3ed 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1843870
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:37.397 25746 INFO nova.osapi_compute.wsgi.server [req-d45c30d9-d550-4192-ae3f-9921a31a99e8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/c62f4f25-982c-4ea2-b5e4-93000edfcfbf HTTP/1.1" status: 200 len: 1708 time: 0.1990912
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:37.725 2931 INFO nova.virt.libvirt.driver [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:38.664 25746 INFO nova.osapi_compute.wsgi.server [req-68b928fe-2e4c-41b1-b891-baef5468e91f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2619801
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:38.946 25746 INFO nova.osapi_compute.wsgi.server [req-01f26494-f0ef-4145-8d57-e2afa2aebbdf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2775590
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:38.964 2931 INFO nova.compute.manager [-] [instance: 127e769a-4fe6-4548-93b1-513ac51e0452] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:40.230 25746 INFO nova.osapi_compute.wsgi.server [req-505be128-84cc-4f5a-a280-38720ea84241 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2774730
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:40.505 25746 INFO nova.osapi_compute.wsgi.server [req-2bef14f0-3254-4098-8d7c-b319eebbd43e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2710969
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:41.782 25746 INFO nova.osapi_compute.wsgi.server [req-afb01936-1170-4119-9232-5e5ecdaede34 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2718439
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:42.048 25746 INFO nova.osapi_compute.wsgi.server [req-77753070-ea2f-46c8-8fa6-2a21b015c00f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2621701
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:43.316 25746 INFO nova.osapi_compute.wsgi.server [req-d30bba94-9266-4848-8388-87de60c4ee51 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2635791
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:43.578 25746 INFO nova.osapi_compute.wsgi.server [req-4f3253c1-cda0-44a7-b910-a1e816a985fe 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2570450
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:44.852 25746 INFO nova.osapi_compute.wsgi.server [req-90bee863-254f-4aaa-b399-2a7d15fed576 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2682898
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:45.134 25746 INFO nova.osapi_compute.wsgi.server [req-e101c9c1-2850-452f-be89-d5c81daba5b8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2776449
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:46.412 25746 INFO nova.osapi_compute.wsgi.server [req-3f4867ee-5402-4e7f-bf24-41c58b20e80b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2732630
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:46.676 25746 INFO nova.osapi_compute.wsgi.server [req-6d19b68e-0a24-4e59-8d02-5f3e1ca859f8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2597642
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:47.944 25746 INFO nova.osapi_compute.wsgi.server [req-b35243f5-f4cf-44c3-8085-9c3e6d636cd4 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2634368
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:48.312 25746 INFO nova.osapi_compute.wsgi.server [req-60ef3ade-e54b-47c2-ae3c-cdf307668e2e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3644130
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:49.563 25746 INFO nova.osapi_compute.wsgi.server [req-97a94302-7980-4793-9333-2254342e1d40 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2462721
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:49.818 25746 INFO nova.osapi_compute.wsgi.server [req-660ffeaf-e3fe-4f45-a960-2a207e61fcc7 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2509589
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:50.258 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:50.259 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:50.443 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:50.667 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:50.738 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] VM Paused (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:50.867 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:51.276 25746 INFO nova.osapi_compute.wsgi.server [req-f1f8bad1-1773-49bb-a7b5-782106415c71 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4522619
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:51.521 25746 INFO nova.osapi_compute.wsgi.server [req-6d61da96-503a-40e2-a90e-6429d34f0c00 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2413731
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:52.802 25746 INFO nova.osapi_compute.wsgi.server [req-7b894c1d-bfc0-4dca-afed-0b8ca7a93a72 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2758210
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:53.091 25746 INFO nova.osapi_compute.wsgi.server [req-f334d0f5-b683-4689-a574-ae0a7da19a69 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2840128
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:54.347 25746 INFO nova.osapi_compute.wsgi.server [req-cfb8d696-c59f-483f-be6a-06c7f011ac6c 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2508061
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:54.602 25746 INFO nova.osapi_compute.wsgi.server [req-f5ece0cf-7a39-41b3-84d0-833a63040442 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2500231
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:55.140 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:55.141 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:55.325 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:55.874 25746 INFO nova.osapi_compute.wsgi.server [req-bd44e0ae-45e8-4721-93b0-d14563ca6caf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2650018
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:56.142 25746 INFO nova.osapi_compute.wsgi.server [req-e965c8a8-1a85-4bb4-9103-fb8227b10886 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2633300
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:57.401 25746 INFO nova.osapi_compute.wsgi.server [req-402c6ee4-e191-4b57-9a7c-9063363b678e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2529092
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:57.673 25746 INFO nova.osapi_compute.wsgi.server [req-852aef96-b049-4e2e-8dc6-267474bbb155 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2669339
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:57.731 25743 INFO nova.api.openstack.compute.server_external_events [req-2fd26d35-366a-4fea-9452-0e22e3ff995b f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:b73a8d80-ce9e-49ff-bb0d-a1b8571f5e26 for instance c62f4f25-982c-4ea2-b5e4-93000edfcfbf
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:57.736 25743 INFO nova.osapi_compute.wsgi.server [req-2fd26d35-366a-4fea-9452-0e22e3ff995b f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0913880
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:57.747 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:57.756 2931 INFO nova.virt.libvirt.driver [-] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:57.757 2931 INFO nova.compute.manager [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Took 20.03 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:57.863 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:57.865 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:13:57.894 2931 INFO nova.compute.manager [req-6f9ecdfe-481c-4535-9bdc-45d86085d739 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Took 20.79 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:58.954 25746 INFO nova.osapi_compute.wsgi.server [req-c4fe12ee-96d2-455a-bd90-4456a7878d04 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2752299
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:13:59.219 25746 INFO nova.osapi_compute.wsgi.server [req-3029fcaa-2872-4bc1-be4d-59de67fbabe1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2587130
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:00.166 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:00.167 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:00.340 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:04.129 25797 INFO nova.metadata.wsgi.server [req-3d4a0060-5493-49ef-bcfe-3661c420a8dc - - - - -] 10.11.21.142,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2294021
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:04.140 25797 INFO nova.metadata.wsgi.server [-] 10.11.21.142,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0008900
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:04.450 25775 INFO nova.metadata.wsgi.server [req-b54879c8-254d-412e-a06b-81ee08235d46 - - - - -] 10.11.21.142,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2247450
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:04.689 25790 INFO nova.metadata.wsgi.server [req-5fc6b2b8-6fb2-4c4d-8b8a-310372462c25 - - - - -] 10.11.21.142,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2241619
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:04.701 25790 INFO nova.metadata.wsgi.server [-] 10.11.21.142,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.0009949
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:04.938 25778 INFO nova.metadata.wsgi.server [req-81a97e64-19c0-4fca-9d0a-4ebfefde7b11 - - - - -] 10.11.21.142,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2257652
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:05.322 25793 INFO nova.metadata.wsgi.server [req-02daae83-21bb-40d2-a6da-3cb3fdd661d0 - - - - -] 10.11.21.142,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2429230
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:05.394 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:05.395 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:05.517 25746 INFO nova.osapi_compute.wsgi.server [req-08d50ea8-a6d0-474a-aaea-560407ef2dec 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/c62f4f25-982c-4ea2-b5e4-93000edfcfbf HTTP/1.1" status: 204 len: 203 time: 0.2904921
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:05.562 2931 INFO nova.compute.manager [req-08d50ea8-a6d0-474a-aaea-560407ef2dec 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Terminating instance
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:05.577 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:05.678 25786 INFO nova.metadata.wsgi.server [req-778a3f3a-8eac-41f0-968b-5c8fb31e7bb1 - - - - -] 10.11.21.142,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.2538400
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:05.776 2931 INFO nova.virt.libvirt.driver [-] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:05.785 25746 INFO nova.osapi_compute.wsgi.server [req-554e3f99-2f55-4004-bda8-f9bed26f5ac9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2639148
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:06.453 2931 INFO nova.virt.libvirt.driver [req-08d50ea8-a6d0-474a-aaea-560407ef2dec 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Deleting instance files /var/lib/nova/instances/c62f4f25-982c-4ea2-b5e4-93000edfcfbf_del
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:06.455 2931 INFO nova.virt.libvirt.driver [req-08d50ea8-a6d0-474a-aaea-560407ef2dec 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Deletion of /var/lib/nova/instances/c62f4f25-982c-4ea2-b5e4-93000edfcfbf_del complete
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:06.582 2931 INFO nova.compute.manager [req-08d50ea8-a6d0-474a-aaea-560407ef2dec 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Took 1.02 seconds to destroy the instance on the hypervisor.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:07.095 25746 INFO nova.osapi_compute.wsgi.server [req-066e2f4a-435d-4939-84c5-f74916ccecb1 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.3046119
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:07.161 2931 INFO nova.compute.manager [req-08d50ea8-a6d0-474a-aaea-560407ef2dec 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] Took 0.58 seconds to deallocate network for instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:08.197 25746 INFO nova.osapi_compute.wsgi.server [req-fd70d0b5-7a2d-46ad-a0cb-93b6d93028f9 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 211 time: 0.0958090
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:09.186 25746 INFO nova.api.openstack.wsgi [req-8a5b19ff-20d8-40e7-94d3-29b89f9b6987 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] HTTP exception thrown: No instances found for any event
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:09.187 25746 INFO nova.osapi_compute.wsgi.server [req-8a5b19ff-20d8-40e7-94d3-29b89f9b6987 f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 404 len: 296 time: 0.0831139
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:10.137 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
[10.30 16:49:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 16:49:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 403 bytes sent, 426 bytes received, lifetime <1 sec
[10.30 16:49:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 18846 bytes (18.4 KB) received, lifetime <1 sec
[10.30 16:49:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 445 bytes sent, 5174 bytes (5.05 KB) received, lifetime <1 sec
[10.30 16:49:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1190 bytes (1.16 KB) sent, 1671 bytes (1.63 KB) received, lifetime 00:02
[10.30 16:49:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1165 bytes (1.13 KB) sent, 3098 bytes (3.02 KB) received, lifetime 00:01
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1165 bytes (1.13 KB) sent, 815 bytes received, lifetime <1 sec
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1165 bytes (1.13 KB) sent, 783 bytes received, lifetime <1 sec
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 850 bytes sent, 10547 bytes (10.2 KB) received, lifetime 00:02
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 408 bytes sent, 421 bytes received, lifetime 00:03
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1165 bytes (1.13 KB) sent, 0 bytes received, lifetime <1 sec
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 19904 bytes (19.4 KB) sent, 27629 bytes (26.9 KB) received, lifetime 02:19
[10.30 16:49:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1118 bytes (1.09 KB) sent, 340 bytes received, lifetime <1 sec
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1143 bytes (1.11 KB) sent, 365 bytes received, lifetime 00:01
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1093 bytes (1.06 KB) sent, 1006 bytes received, lifetime 00:01
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 428 bytes sent, 5365 bytes (5.23 KB) received, lifetime <1 sec
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1268 bytes (1.23 KB) sent, 6274 bytes (6.12 KB) received, lifetime <1 sec
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 845 bytes sent, 12076 bytes (11.7 KB) received, lifetime <1 sec
[10.30 16:49:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 431 bytes sent, 7896 bytes (7.71 KB) received, lifetime <1 sec
[10.30 16:49:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 431 bytes sent, 9780 bytes (9.55 KB) received, lifetime <1 sec
[10.30 16:49:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1590 bytes (1.55 KB) sent, 472 bytes received, lifetime 00:01
[10.30 16:49:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 416 bytes sent, 10670 bytes (10.4 KB) received, lifetime 00:01
[10.30 16:49:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:49:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1327 bytes (1.29 KB) sent, 3250 bytes (3.17 KB) received, lifetime <1 sec
[10.30 16:49:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1107 bytes (1.08 KB) sent, 29322 bytes (28.6 KB) received, lifetime <1 sec
[10.30 16:49:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:17
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1133 bytes (1.10 KB) sent, 11393 bytes (11.1 KB) received, lifetime <1 sec
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 483 bytes sent, 342 bytes received, lifetime <1 sec
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1210 bytes (1.18 KB) sent, 366 bytes received, lifetime <1 sec
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 697 bytes sent, 324 bytes received, lifetime <1 sec
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 704 bytes sent, 2476 bytes (2.41 KB) received, lifetime <1 sec
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 7829 bytes (7.64 KB) sent, 578548 bytes (564 KB) received, lifetime 00:18
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3425 bytes (3.34 KB) sent, 212164 bytes (207 KB) received, lifetime 00:18
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 934 bytes sent, 5869 bytes (5.73 KB) received, lifetime <1 sec
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1325 bytes (1.29 KB) sent, 514 bytes received, lifetime <1 sec
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1293 bytes (1.26 KB) sent, 2439 bytes (2.38 KB) received, lifetime <1 sec
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 508 bytes sent, 29786 bytes (29.0 KB) received, lifetime <1 sec
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 508 bytes sent, 47293 bytes (46.1 KB) received, lifetime <1 sec
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1190 bytes (1.16 KB) sent, 367 bytes received, lifetime <1 sec
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1293 bytes (1.26 KB) sent, 2247 bytes (2.19 KB) received, lifetime <1 sec
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1033 bytes (1.00 KB) sent, 46810 bytes (45.7 KB) received, lifetime <1 sec
[10.30 16:49:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2380 bytes (2.32 KB) sent, 678 bytes received, lifetime <1 sec
[10.30 16:49:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 974 bytes sent, 9624 bytes (9.39 KB) received, lifetime <1 sec
[10.30 16:49:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 4512 bytes (4.40 KB) sent, 82166 bytes (80.2 KB) received, lifetime 00:01
[10.30 16:49:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1119 bytes (1.09 KB) sent, 3210 bytes (3.13 KB) received, lifetime <1 sec
[10.30 16:49:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 458 bytes sent, 1781 bytes (1.73 KB) received, lifetime <1 sec
[10.30 16:49:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1188 bytes (1.16 KB) sent, 421 bytes received, lifetime <1 sec
[10.30 16:49:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1257 bytes (1.22 KB) sent, 421 bytes received, lifetime <1 sec
[10.30 16:49:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1257 bytes (1.22 KB) sent, 421 bytes received, lifetime <1 sec
[10.30 16:49:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:02
[10.30 16:49:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:02
[10.30 16:49:32] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1301 bytes (1.27 KB) sent, 434 bytes received, lifetime <1 sec
[10.30 16:49:32] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 16:49:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1034 bytes (1.00 KB) sent, 17383 bytes (16.9 KB) received, lifetime <1 sec
[10.30 16:49:35] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:04
[10.30 16:49:48] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:17
[10.30 16:49:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1248 bytes (1.21 KB) sent, 334 bytes received, lifetime 00:20
[10.30 16:49:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1682 bytes (1.64 KB) sent, 472 bytes received, lifetime <1 sec
[10.30 16:49:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2473 bytes (2.41 KB) sent, 1926 bytes (1.88 KB) received, lifetime 00:20
[10.30 16:49:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1293 bytes (1.26 KB) sent, 2459 bytes (2.40 KB) received, lifetime <1 sec
[10.30 16:49:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1293 bytes (1.26 KB) sent, 2460 bytes (2.40 KB) received, lifetime <1 sec
[10.30 16:49:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1293 bytes (1.26 KB) sent, 2440 bytes (2.38 KB) received, lifetime <1 sec
[10.30 16:49:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1133 bytes (1.10 KB) sent, 3274 bytes (3.19 KB) received, lifetime 00:01
[10.30 16:49:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 489 bytes sent, 566 bytes received, lifetime <1 sec
[10.30 16:49:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1372 bytes (1.33 KB) sent, 1224 bytes (1.19 KB) received, lifetime <1 sec
[10.30 16:49:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:49:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:49:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:50:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:50:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1198 bytes (1.16 KB) sent, 344 bytes received, lifetime <1 sec
[10.30 16:50:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:19
[10.30 16:51:56] svchost.exe *64 - proxy.cse.cuhk.edu.hk:5070 close, 303 bytes sent, 275 bytes received, lifetime <1 sec
[10.30 16:51:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1180 bytes (1.15 KB) sent, 421 bytes received, lifetime 02:01
[10.30 16:52:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2437 bytes (2.37 KB) sent, 842 bytes received, lifetime 02:04
[10.30 16:52:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 952 bytes sent, 782 bytes received, lifetime 00:10
[10.30 16:52:25] svchost.exe *64 - proxy.cse.cuhk.edu.hk:5070 close, 301 bytes sent, 278 bytes received, lifetime <1 sec
[10.30 16:52:30] Dropbox.exe - proxy.cse.cuhk.edu.hk:5070 close, 1853 bytes (1.80 KB) sent, 4179 bytes (4.08 KB) received, lifetime 02:00
[10.30 16:53:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1166 bytes (1.13 KB) sent, 336 bytes received, lifetime <1 sec
[10.30 16:53:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1180 bytes (1.15 KB) sent, 337 bytes received, lifetime <1 sec
[10.30 16:53:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2035 bytes (1.98 KB) sent, 469 bytes received, lifetime <1 sec
[10.30 16:53:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3690 bytes (3.60 KB) sent, 1306 bytes (1.27 KB) received, lifetime 02:03
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1325 bytes (1.29 KB) sent, 511 bytes received, lifetime <1 sec
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 896 bytes sent, 375 bytes received, lifetime <1 sec
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:53:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:26] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1248 bytes (1.21 KB) sent, 331 bytes received, lifetime 00:02
[10.30 16:53:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:53:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:17
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1166 bytes (1.13 KB) sent, 363 bytes received, lifetime <1 sec
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2455 bytes (2.39 KB) sent, 727 bytes received, lifetime 00:43
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1292 bytes (1.26 KB) sent, 2457 bytes (2.39 KB) received, lifetime <1 sec
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1292 bytes (1.26 KB) sent, 1500 bytes (1.46 KB) received, lifetime <1 sec
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1292 bytes (1.26 KB) sent, 1490 bytes (1.45 KB) received, lifetime <1 sec
[10.30 16:54:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1292 bytes (1.26 KB) sent, 429 bytes received, lifetime 00:01
[10.30 16:54:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1285 bytes (1.25 KB) sent, 1490 bytes (1.45 KB) received, lifetime 00:01
[10.30 16:54:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3637 bytes (3.55 KB) sent, 1432 bytes (1.39 KB) received, lifetime 00:01
[10.30 16:54:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:05
[10.30 16:54:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:54:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 16:56:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 939 bytes sent, 4231 bytes (4.13 KB) received, lifetime 02:00
[10.30 16:56:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 469 bytes sent, 1617 bytes (1.57 KB) received, lifetime 02:00
[10.30 16:58:25] Wiz.exe - proxy.cse.cuhk.edu.hk:5070 close, 2636 bytes (2.57 KB) sent, 2279 bytes (2.22 KB) received, lifetime 01:05
[10.30 17:01:35] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 435 bytes sent, 353 bytes received, lifetime <1 sec
[10.30 17:02:09] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 353 bytes received, lifetime <1 sec
[10.30 17:02:17] putty.exe - 183.62.156.108:22 open through proxy socks.cse.cuhk.edu.hk:5070 SOCKS5
[10.30 17:05:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:05:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:07:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3684 bytes (3.59 KB) sent, 8009 bytes (7.82 KB) received, lifetime 02:02
[10.30 17:07:51] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 946 bytes sent, 783 bytes received, lifetime <1 sec
[10.30 17:07:51] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 946 bytes sent, 783 bytes received, lifetime <1 sec
[10.30 17:07:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:07:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:07:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:08:02] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2818 bytes (2.75 KB) sent, 1460 bytes (1.42 KB) received, lifetime 01:52
[10.30 17:08:02] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 16493 bytes (16.1 KB) sent, 315731 bytes (308 KB) received, lifetime 02:37
[10.30 17:08:02] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1769 bytes (1.72 KB) sent, 61234 bytes (59.7 KB) received, lifetime 00:02
[10.30 17:08:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 791 bytes sent, 778 bytes received, lifetime 00:10
[10.30 17:09:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:10:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1578 bytes (1.54 KB) sent, 392726 bytes (383 KB) received, lifetime 00:31
[10.30 17:10:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:10:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 809 bytes sent, 186 bytes received, lifetime 00:06
[10.30 17:10:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:10:38] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:10:38] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:10:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2933 bytes (2.86 KB) sent, 11721005 bytes (11.1 MB) received, lifetime 02:48
[10.30 17:11:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1432 bytes (1.39 KB) sent, 44376 bytes (43.3 KB) received, lifetime 00:31
[10.30 17:11:12] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:11:17] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 353 bytes received, lifetime <1 sec
[10.30 17:11:43] SogouCloud.exe - get.sogou.com:80 close, 859 bytes sent, 316 bytes received, lifetime <1 sec
[10.30 17:12:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 35850 bytes (35.0 KB) sent, 13903 bytes (13.5 KB) received, lifetime 04:12
[10.30 17:14:20] Dropbox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:15:10] QQ.exe - tcpconn.tencent.com:80 close, 133 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 17:15:42] QQ.exe - tcpconn6.tencent.com:443 error : A connection request was canceled before the completion.
[10.30 17:16:10] QQ.exe - tcpconn4.tencent.com:80 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy closed the connection unexpectedly. 
[10.30 17:17:09] SogouCloud.exe - get.sogou.com:80 close, 651 bytes sent, 346 bytes received, lifetime <1 sec
[10.30 17:17:16] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:17:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:17:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:17:17] SogouCloud.exe - get.sogou.com:80 close, 651 bytes sent, 346 bytes received, lifetime <1 sec
[10.30 17:17:18] SogouCloud.exe - proxy.cse.cuhk.edu.hk:5070 close, 1237 bytes (1.20 KB) sent, 676064 bytes (660 KB) received, lifetime 00:07
[10.30 17:17:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:17:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:18:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1222 bytes (1.19 KB) sent, 604 bytes received, lifetime 00:38
[10.30 17:18:51] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3553 bytes (3.46 KB) sent, 71424 bytes (69.7 KB) received, lifetime 01:34
[10.30 17:19:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3585 bytes (3.50 KB) sent, 56498 bytes (55.1 KB) received, lifetime 01:57
[10.30 17:19:30] Wiz.exe - proxy.cse.cuhk.edu.hk:5070 close, 2636 bytes (2.57 KB) sent, 2279 bytes (2.22 KB) received, lifetime 01:05
[10.30 17:20:20] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:20:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 745 bytes sent, 4661 bytes (4.55 KB) received, lifetime <1 sec
[10.30 17:20:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:20:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1840 bytes (1.79 KB) sent, 1212 bytes (1.18 KB) received, lifetime 03:05
[10.30 17:20:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 18242 bytes (17.8 KB) sent, 10525 bytes (10.2 KB) received, lifetime 02:58
[10.30 17:20:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:20:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1396 bytes (1.36 KB) sent, 5669 bytes (5.53 KB) received, lifetime 00:05
[10.30 17:20:43] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2087 bytes (2.03 KB) sent, 4643 bytes (4.53 KB) received, lifetime 00:20
[10.30 17:21:16] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2123 bytes (2.07 KB) sent, 11355 bytes (11.0 KB) received, lifetime 04:00
[10.30 17:21:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2034 bytes (1.98 KB) sent, 4206 bytes (4.10 KB) received, lifetime 00:39
[10.30 17:22:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:22:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 18965 bytes (18.5 KB) sent, 9771 bytes (9.54 KB) received, lifetime 02:05
[10.30 17:23:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 14623 bytes (14.2 KB) sent, 124877 bytes (121 KB) received, lifetime 02:41
[10.30 17:25:09] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:26:13] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2717 bytes (2.65 KB) sent, 1911 bytes (1.86 KB) received, lifetime 04:50
[10.30 17:26:51] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 353 bytes received, lifetime <1 sec
[10.30 17:26:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:34:02] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 353 bytes received, lifetime <1 sec
[10.30 17:34:48] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:35:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 397 bytes sent, 3558 bytes (3.47 KB) received, lifetime 00:01
[10.30 17:35:13] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 519 bytes sent, 313 bytes received, lifetime 00:01
[10.30 17:35:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:35:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:35:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 415 bytes sent, 599 bytes received, lifetime 00:04
[10.30 17:35:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 5248 bytes (5.12 KB) sent, 1114 bytes (1.08 KB) received, lifetime <1 sec
[10.30 17:35:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:35:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:35:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:35:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1006 bytes sent, 6198 bytes (6.05 KB) received, lifetime 00:01
[10.30 17:35:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 811 bytes sent, 186 bytes received, lifetime 00:01
[10.30 17:35:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:36:00] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 17:36:00] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 811 bytes sent, 186 bytes received, lifetime 00:02
[10.30 17:36:00] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:36:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:36:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 860 bytes sent, 3314 bytes (3.23 KB) received, lifetime 00:01
[10.30 17:36:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:36:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1986 bytes (1.93 KB) sent, 1180 bytes (1.15 KB) received, lifetime <1 sec
[10.30 17:36:02] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:36:02] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:36:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 463 bytes sent, 426 bytes received, lifetime 00:09
[10.30 17:36:13] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1060 bytes (1.03 KB) sent, 358 bytes received, lifetime 00:11
[10.30 17:36:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:36:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 592 bytes sent, 4760 bytes (4.64 KB) received, lifetime 00:16
[10.30 17:36:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 592 bytes sent, 4760 bytes (4.64 KB) received, lifetime 00:16
[10.30 17:36:23] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 435 bytes sent, 353 bytes received, lifetime <1 sec
[10.30 17:37:05] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2800 bytes (2.73 KB) sent, 17757 bytes (17.3 KB) received, lifetime 01:07
[10.30 17:37:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1457 bytes (1.42 KB) sent, 5146 bytes (5.02 KB) received, lifetime 00:10
[10.30 17:37:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 494 bytes sent, 2218 bytes (2.16 KB) received, lifetime 00:01
[10.30 17:37:26] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 4567 bytes (4.45 KB) sent, 1432 bytes (1.39 KB) received, lifetime 00:29
[10.30 17:37:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:51] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 492 bytes sent, 2898 bytes (2.83 KB) received, lifetime 00:01
[10.30 17:37:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 17:37:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 17:37:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:03
[10.30 17:37:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:03
[10.30 17:37:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:37:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:38:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:38:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1736 bytes (1.69 KB) sent, 48536 bytes (47.3 KB) received, lifetime 00:01
[10.30 17:38:13] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 793 bytes sent, 180 bytes received, lifetime 00:20
[10.30 17:38:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:38:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:38:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2017 bytes (1.96 KB) sent, 52284 bytes (51.0 KB) received, lifetime 00:02
[10.30 17:39:00] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 595 bytes sent, 269 bytes received, lifetime 00:01
[10.30 17:39:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 17:39:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1987 bytes (1.94 KB) sent, 51801 bytes (50.5 KB) received, lifetime 00:01
[10.30 17:39:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:39:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1990 bytes (1.94 KB) sent, 23862 bytes (23.3 KB) received, lifetime 00:03
[10.30 17:39:19] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:39:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:39:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 5065 bytes (4.94 KB) sent, 2368 bytes (2.31 KB) received, lifetime 00:48
[10.30 17:40:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 877 bytes sent, 507 bytes received, lifetime 00:48
[10.30 17:40:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1714 bytes (1.67 KB) sent, 865 bytes received, lifetime 02:07
[10.30 17:40:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2697 bytes (2.63 KB) sent, 14638 bytes (14.2 KB) received, lifetime 02:09
[10.30 17:40:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:05] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 17:40:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 574 bytes sent, 4489 bytes (4.38 KB) received, lifetime 00:04
[10.30 17:40:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 17:40:08] putty.exe - 183.62.156.108:22 close, 89652 bytes (87.5 KB) sent, 599249 bytes (585 KB) received, lifetime 37:51
[10.30 17:40:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:02
[10.30 17:40:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1015 bytes sent, 590 bytes received, lifetime 00:01
[10.30 17:40:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:40:36] Wiz.exe - proxy.cse.cuhk.edu.hk:5070 close, 1417 bytes (1.38 KB) sent, 4631 bytes (4.52 KB) received, lifetime 01:05
[10.30 17:42:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:42:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:42:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:42:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 715 bytes sent, 4417 bytes (4.31 KB) received, lifetime 00:05
[10.30 17:43:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 8673 bytes (8.46 KB) sent, 116640 bytes (113 KB) received, lifetime 02:55
[10.30 17:43:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1731 bytes (1.69 KB) sent, 864 bytes received, lifetime 02:53
[10.30 17:45:58] WeChat.exe - 182.254.114.110:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:46:23] QQ.exe - tcpconn.tencent.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:46:23] QQ.exe - tcpconn3.tencent.com:80 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 17:46:23] QQ.exe - 183.60.49.182:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:46:38] QQ.exe - tcpconn6.tencent.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 17:46:44] QQ.exe - tcpconn3.tencent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:46:44] QQ.exe - tcpconn6.tencent.com:80 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 17:46:44] QQ.exe - tcpconn6.tencent.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:46:44] QQ.exe - tcpconn6.tencent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:46:44] QQ.exe - tcpconn3.tencent.com:443 close, 149 bytes sent, 121 bytes received, lifetime <1 sec
[10.30 17:47:01] SGPicFaceTool.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:47:02] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:47:23] QQ.exe - cgi.qqweb.qq.com:80 close, 477 bytes sent, 448 bytes received, lifetime <1 sec
[10.30 17:47:23] QQ.exe - cgi.qqweb.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:47:45] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:48:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:48:45] QQ.exe - qqmail.tencent.com:80 close, 336 bytes sent, 2854 bytes (2.78 KB) received, lifetime 00:01
[10.30 17:48:52] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:48:52] SogouCloud.exe - get.sogou.com:80 close, 724 bytes sent, 516 bytes received, lifetime <1 sec
[10.30 17:51:05] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:51:13] QQ.exe - showxml.qq.com:80 close, 600 bytes sent, 1716 bytes (1.67 KB) received, lifetime <1 sec
[10.30 17:51:21] QQ.exe - 2052.flash2-http.qq.com:80 close, 466 bytes sent, 125682 bytes (122 KB) received, lifetime <1 sec
[10.30 17:52:06] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:52:06] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:52:09] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 close, 493 bytes sent, 523 bytes received, lifetime 00:01
[10.30 17:52:09] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 close, 1002 bytes sent, 12309 bytes (12.0 KB) received, lifetime 00:01
[10.30 17:52:18] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 close, 493 bytes sent, 523 bytes received, lifetime <1 sec
[10.30 17:52:19] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:52:25] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:52:25] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:52:38] Wiz.exe - proxy.cse.cuhk.edu.hk:5070 close, 1417 bytes (1.38 KB) sent, 4631 bytes (4.52 KB) received, lifetime 01:05
[10.30 17:52:44] YodaoDict.exe - cidian.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:53:05] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2553 bytes (2.49 KB) sent, 2221 bytes (2.16 KB) received, lifetime 04:00
[10.30 17:53:43] YodaoDict.exe - impservice.dictapp.youdao.com:80 close, 666 bytes sent, 709 bytes received, lifetime 00:10
[10.30 17:53:51] Wiz.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:53:58] Wiz.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:54:00] Wiz.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:54:27] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:54:29] QQExternal.exe - proxy.cse.cuhk.edu.hk:5070 close, 1644 bytes (1.60 KB) sent, 388 bytes received, lifetime 01:06
[10.30 17:55:05] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3319 bytes (3.24 KB) sent, 2402 bytes (2.34 KB) received, lifetime 04:00
[10.30 17:55:51] Wiz.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:56:05] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 353 bytes received, lifetime <1 sec
[10.30 17:56:23] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:56:23] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 353 bytes received, lifetime <1 sec
[10.30 17:57:12] YodaoDict.exe - oimagec7.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 17:59:03] svchost.exe *64 - proxy.cse.cuhk.edu.hk:5070 close, 293 bytes sent, 514 bytes received, lifetime <1 sec
[10.30 17:59:09] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:59:10] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:59:13] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:59:13] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:59:13] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:59:13] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:59:13] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:59:17] git-remote-https.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:59:21] git-remote-https.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 17:59:58] firefox.exe - proxy.cse.cuhk.edu.hk:5070 close, 1555 bytes (1.51 KB) sent, 457464 bytes (446 KB) received, lifetime 00:46
[10.30 18:00:10] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:00:10] firefox.exe - proxy.cse.cuhk.edu.hk:5070 close, 950 bytes sent, 3559 bytes (3.47 KB) received, lifetime 01:01
[10.30 18:00:12] FlashPlayerPlugin_18_0_0_209.exe - formi.baidu.com:843 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[10.30 18:00:13] firefox.exe - proxy.cse.cuhk.edu.hk:5070 close, 1212 bytes (1.18 KB) sent, 11440 bytes (11.1 KB) received, lifetime 00:59
[10.30 18:00:15] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:00:15] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:00:15] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:00:24] firefox.exe - proxy.cse.cuhk.edu.hk:5070 close, 47856 bytes (46.7 KB) sent, 4090387 bytes (3.90 MB) received, lifetime 01:01
[10.30 18:00:24] GitHub.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:00:35] git-remote-https.exe - proxy.cse.cuhk.edu.hk:5070 close, 877 bytes sent, 3806 bytes (3.71 KB) received, lifetime 00:01
[10.30 18:00:41] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 353 bytes received, lifetime <1 sec
[10.30 18:00:55] SogouCloud.exe - get.sogou.com:80 close, 967 bytes sent, 316 bytes received, lifetime <1 sec
[10.30 18:01:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 101 bytes sent, 3449 bytes (3.36 KB) received, lifetime <1 sec
[10.30 18:01:08] firefox.exe - proxy.cse.cuhk.edu.hk:5070 close, 2928 bytes (2.85 KB) sent, 567520 bytes (554 KB) received, lifetime 01:55
[10.30 18:01:34] Skype.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:01:34] Skype.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:01:41] Skype.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:01:42] Skype.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:02:05] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 388 bytes sent, 2501 bytes (2.44 KB) received, lifetime 00:01
[10.30 18:02:12] firefox.exe - proxy.cse.cuhk.edu.hk:5070 close, 1309 bytes (1.27 KB) sent, 34052 bytes (33.2 KB) received, lifetime 01:57
[10.30 18:02:12] firefox.exe - proxy.cse.cuhk.edu.hk:5070 close, 2531 bytes (2.47 KB) sent, 59451 bytes (58.0 KB) received, lifetime 02:00
[10.30 18:02:12] firefox.exe - proxy.cse.cuhk.edu.hk:5070 close, 2813 bytes (2.74 KB) sent, 11743 bytes (11.4 KB) received, lifetime 02:02
[10.30 18:02:25] YodaoDict.exe - oimagec7.ydstatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:02:58] Skype.exe - 91.190.218.40:443 close, 497 bytes sent, 302 bytes received, lifetime 00:12
[10.30 18:03:13] Skype.exe - proxy.cse.cuhk.edu.hk:5070 close, 1318 bytes (1.28 KB) sent, 5380 bytes (5.25 KB) received, lifetime 00:02
[10.30 18:03:15] Skype.exe - 91.190.218.125:80 close, 5 bytes sent, 0 bytes received, lifetime 00:05
[10.30 18:03:15] Skype.exe - 91.190.216.125:443 close, 5 bytes sent, 0 bytes received, lifetime 00:04
[10.30 18:03:16] Skype.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:03:16] Skype.exe - proxy.cse.cuhk.edu.hk:5070 close, 287 bytes sent, 2229 bytes (2.17 KB) received, lifetime <1 sec
[10.30 18:03:17] Skype.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:03:19] Skype.exe - proxy.cse.cuhk.edu.hk:5070 close, 429 bytes sent, 318 bytes received, lifetime <1 sec
[10.30 18:03:21] Skype.exe - proxy.cse.cuhk.edu.hk:5070 close, 1028 bytes (1.00 KB) sent, 4559 bytes (4.45 KB) received, lifetime 00:08
[10.30 18:03:21] Skype.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:03:35] Skype.exe - 24.135.95.248:443 close, 131 bytes sent, 153 bytes received, lifetime 00:12
[10.30 18:03:37] Skype.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:03:41] Skype.exe - BAYMSGR2012806.gateway.messenger.live.com:443 close, 13155 bytes (12.8 KB) sent, 21737 bytes (21.2 KB) received, lifetime 00:35
[10.30 18:03:43] Skype.exe - 91.190.216.212:443 close, 5 bytes sent, 5 bytes received, lifetime 00:04
[10.30 18:03:47] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:03:47] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:03:47] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:03:47] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:03:47] firefox.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:04:14] Skype.exe - 86.99.222.235:443 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy closed the connection unexpectedly.
[10.30 18:05:44] firefox.exe - proxy.cse.cuhk.edu.hk:5070 close, 983 bytes sent, 308385 bytes (301 KB) received, lifetime 01:57
[10.30 18:05:44] firefox.exe - proxy.cse.cuhk.edu.hk:5070 close, 983 bytes sent, 268665 bytes (262 KB) received, lifetime 01:57
[10.30 18:06:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1535 bytes (1.49 KB) sent, 972 bytes received, lifetime 00:07
[10.30 18:07:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:07:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:07:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:07:36] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:07:36] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:07:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 715 bytes sent, 4417 bytes (4.31 KB) received, lifetime 00:10
[10.30 18:08:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1368 bytes (1.33 KB) sent, 20811 bytes (20.3 KB) received, lifetime 01:00
[10.30 18:08:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:08:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1277 bytes (1.24 KB) sent, 15408 bytes (15.0 KB) received, lifetime 00:01
[10.30 18:08:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:08:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:08:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1312 bytes (1.28 KB) sent, 2783 bytes (2.71 KB) received, lifetime 00:01
[10.30 18:09:06] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:10:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:10:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:10:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 705 bytes sent, 492 bytes received, lifetime 00:01
[10.30 18:10:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1398 bytes (1.36 KB) sent, 11654 bytes (11.3 KB) received, lifetime 00:01
[10.30 18:10:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:10:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:10:47] Dropbox.exe - proxy.cse.cuhk.edu.hk:5070 close, 2501 bytes (2.44 KB) sent, 4848 bytes (4.73 KB) received, lifetime 01:01
[10.30 18:11:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 18:11:26] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 326 bytes sent, 103039 bytes (100 KB) received, lifetime 00:01
[10.30 18:13:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 849 bytes sent, 9333 bytes (9.11 KB) received, lifetime 02:01
[10.30 18:13:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 952 bytes sent, 13595 bytes (13.2 KB) received, lifetime 03:00
[10.30 18:13:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 960 bytes sent, 8386 bytes (8.18 KB) received, lifetime 03:01
[10.30 20:39:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:39:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:39:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:43] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:43] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:13
[10.30 20:39:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:03
[10.30 20:39:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 576 bytes sent, 649 bytes received, lifetime 00:01
[10.30 20:39:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2279 bytes (2.22 KB) sent, 279 bytes received, lifetime 00:02
[10.30 20:39:47] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:39:47] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3115 bytes (3.04 KB) sent, 1252 bytes (1.22 KB) received, lifetime 00:01
[10.30 20:39:47] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 close, 31257 bytes (30.5 KB) sent, 1846301 bytes (1.76 MB) received, lifetime 01:33
[10.30 20:40:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:40:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:05] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 468 bytes sent, 3515 bytes (3.43 KB) received, lifetime 00:01
[10.30 20:40:05] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 780 bytes sent, 1401 bytes (1.36 KB) received, lifetime <1 sec
[10.30 20:40:05] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 409 bytes sent, 52645 bytes (51.4 KB) received, lifetime 00:01
[10.30 20:40:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1119 bytes (1.09 KB) sent, 417 bytes received, lifetime <1 sec
[10.30 20:40:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 474 bytes sent, 1047 bytes (1.02 KB) received, lifetime <1 sec
[10.30 20:40:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 675 bytes sent, 7178 bytes (7.00 KB) received, lifetime <1 sec
[10.30 20:40:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 628 bytes sent, 466 bytes received, lifetime 00:01
[10.30 20:40:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 513 bytes sent, 22162 bytes (21.6 KB) received, lifetime 00:01
[10.30 20:40:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 530 bytes sent, 546 bytes received, lifetime 00:02
[10.30 20:40:11] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2177 bytes (2.12 KB) sent, 279 bytes received, lifetime 00:01
[10.30 20:40:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 534 bytes sent, 388 bytes received, lifetime 00:05
[10.30 20:40:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1069 bytes (1.04 KB) sent, 776 bytes received, lifetime 00:05
[10.30 20:40:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 542 bytes sent, 388 bytes received, lifetime 00:05
[10.30 20:40:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:40:16] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1010 bytes sent, 5219 bytes (5.09 KB) received, lifetime 04:00
[10.30 20:40:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1127 bytes (1.10 KB) sent, 4317 bytes (4.21 KB) received, lifetime 00:20
[10.30 20:40:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:14
[10.30 20:41:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1477 bytes (1.44 KB) sent, 1431 bytes (1.39 KB) received, lifetime 02:01
[10.30 20:42:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2413 bytes (2.35 KB) sent, 324 bytes received, lifetime 02:01
[10.30 20:42:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1400 bytes (1.36 KB) sent, 3900 bytes (3.80 KB) received, lifetime 02:00
[10.30 20:42:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2464 bytes (2.40 KB) sent, 1010 bytes received, lifetime 02:01
[10.30 20:42:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:33] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 20:42:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1299 bytes (1.26 KB) sent, 1029 bytes (1.00 KB) received, lifetime <1 sec
[10.30 20:42:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1007 bytes sent, 355 bytes received, lifetime 00:08
[10.30 20:42:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1354 bytes (1.32 KB) sent, 902 bytes received, lifetime 00:15
[10.30 20:42:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1481 bytes (1.44 KB) sent, 1804 bytes (1.76 KB) received, lifetime 00:32
[10.30 20:42:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 453 bytes sent, 352 bytes received, lifetime 00:02
[10.30 20:42:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 4186 bytes (4.08 KB) sent, 2888 bytes (2.82 KB) received, lifetime 00:06
[10.30 20:42:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1318 bytes (1.28 KB) sent, 427 bytes received, lifetime 00:01
[10.30 20:42:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:43] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:02
[10.30 20:42:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:03
[10.30 20:42:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:03
[10.30 20:42:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:47] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 555 bytes sent, 400 bytes received, lifetime 00:04
[10.30 20:42:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 935 bytes sent, 6675 bytes (6.51 KB) received, lifetime <1 sec
[10.30 20:42:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1205 bytes (1.17 KB) sent, 391 bytes received, lifetime 00:01
[10.30 20:42:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:42:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 7437 bytes (7.26 KB) sent, 2235596 bytes (2.13 MB) received, lifetime 00:07
[10.30 20:42:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 634 bytes sent, 538 bytes received, lifetime 00:01
[10.30 20:43:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:43] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3254 bytes (3.17 KB) sent, 5295 bytes (5.17 KB) received, lifetime 00:04
[10.30 20:43:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 553 bytes sent, 1424 bytes (1.39 KB) received, lifetime <1 sec
[10.30 20:43:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3205 bytes (3.12 KB) sent, 15704 bytes (15.3 KB) received, lifetime <1 sec
[10.30 20:43:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1341 bytes (1.30 KB) sent, 290 bytes received, lifetime 00:01
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1023 bytes sent, 4076 bytes (3.98 KB) received, lifetime 00:01
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 553 bytes sent, 28794 bytes (28.1 KB) received, lifetime <1 sec
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 996 bytes sent, 367 bytes received, lifetime 00:01
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1536 bytes (1.50 KB) sent, 5325 bytes (5.20 KB) received, lifetime 00:01
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:47] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:47] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:47] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:47] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:43:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:02
[10.30 20:43:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2735 bytes (2.67 KB) sent, 79708 bytes (77.8 KB) received, lifetime 00:55
[10.30 20:44:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 6996 bytes (6.83 KB) sent, 2055 bytes (2.00 KB) received, lifetime 00:57
[10.30 20:44:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1522 bytes (1.48 KB) sent, 1765 bytes (1.72 KB) received, lifetime 00:53
[10.30 20:44:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 5992 bytes (5.85 KB) sent, 74512 bytes (72.7 KB) received, lifetime 00:51
[10.30 20:44:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:42] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:44:48] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1236 bytes (1.20 KB) sent, 408 bytes received, lifetime 00:06
[10.30 20:44:48] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:48] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:44:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1680 bytes (1.64 KB) sent, 935 bytes received, lifetime <1 sec
[10.30 20:44:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1730 bytes (1.68 KB) sent, 4796 bytes (4.68 KB) received, lifetime 00:01
[10.30 20:44:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2737 bytes (2.67 KB) sent, 1375 bytes (1.34 KB) received, lifetime 00:01
[10.30 20:44:51] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 902 bytes sent, 15242 bytes (14.8 KB) received, lifetime 00:03
[10.30 20:44:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1845 bytes (1.80 KB) sent, 402 bytes received, lifetime 00:02
[10.30 20:44:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:10
[10.30 20:44:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2018 bytes (1.97 KB) sent, 1298 bytes (1.26 KB) received, lifetime 00:04
[10.30 20:44:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 806 bytes sent, 1407 bytes (1.37 KB) received, lifetime 00:01
[10.30 20:44:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 383 bytes sent, 1303 bytes (1.27 KB) received, lifetime 00:01
[10.30 20:44:53] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1584 bytes (1.54 KB) sent, 256548 bytes (250 KB) received, lifetime 00:01
[10.30 20:44:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1616 bytes (1.57 KB) sent, 42950 bytes (41.9 KB) received, lifetime 00:01
[10.30 20:44:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:44:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:44:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:44:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:44:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 848 bytes sent, 310 bytes received, lifetime 00:01
[10.30 20:44:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1920 bytes (1.87 KB) sent, 338 bytes received, lifetime <1 sec
[10.30 20:44:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:02
[10.30 20:44:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1501 bytes (1.46 KB) sent, 601 bytes received, lifetime 00:01
[10.30 20:44:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1808 bytes (1.76 KB) sent, 346 bytes received, lifetime <1 sec
[10.30 20:44:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:44:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:02] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1672 bytes (1.63 KB) sent, 771 bytes received, lifetime 00:05
[10.30 20:45:02] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 4272 bytes (4.17 KB) sent, 20283 bytes (19.8 KB) received, lifetime 00:20
[10.30 20:45:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1624 bytes (1.58 KB) sent, 375 bytes received, lifetime 00:03
[10.30 20:45:06] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1104 bytes (1.07 KB) sent, 406 bytes received, lifetime <1 sec
[10.30 20:45:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 726 bytes sent, 310 bytes received, lifetime 00:01
[10.30 20:45:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:45:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:07] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:45:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2442 bytes (2.38 KB) sent, 9401 bytes (9.18 KB) received, lifetime 00:01
[10.30 20:45:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2587 bytes (2.52 KB) sent, 317 bytes received, lifetime <1 sec
[10.30 20:45:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:45:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 939 bytes sent, 398 bytes received, lifetime <1 sec
[10.30 20:45:10] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:45:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:07
[10.30 20:45:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2126 bytes (2.07 KB) sent, 816 bytes received, lifetime 00:08
[10.30 20:45:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:08
[10.30 20:45:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 786 bytes sent, 348 bytes received, lifetime 00:03
[10.30 20:45:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:45:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 968 bytes sent, 272 bytes received, lifetime <1 sec
[10.30 20:45:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:45:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:45:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2224 bytes (2.17 KB) sent, 659 bytes received, lifetime 00:03
[10.30 20:45:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 787 bytes sent, 348 bytes received, lifetime 00:03
[10.30 20:45:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:03
[10.30 20:45:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:24] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:25] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:45:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:36] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1448 bytes (1.41 KB) sent, 1794 bytes (1.75 KB) received, lifetime <1 sec
[10.30 20:45:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 722 bytes sent, 310 bytes received, lifetime 00:01
[10.30 20:45:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1173 bytes (1.14 KB) sent, 375 bytes received, lifetime 00:01
[10.30 20:45:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:13
[10.30 20:45:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:13
[10.30 20:45:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:13
[10.30 20:45:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 10069 bytes (9.83 KB) sent, 4062 bytes (3.96 KB) received, lifetime 00:45
[10.30 20:45:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:51] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 20:45:55] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 694 bytes sent, 892 bytes received, lifetime <1 sec
[10.30 20:45:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:15
[10.30 20:45:56] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:16
[10.30 20:45:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:45:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:45:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:45:58] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 941 bytes sent, 401 bytes received, lifetime 00:01
[10.30 20:45:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:46:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2842 bytes (2.77 KB) sent, 282 bytes received, lifetime 00:01
[10.30 20:46:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:46:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:20
[10.30 20:46:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:20
[10.30 20:46:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:20
[10.30 20:46:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:20
[10.30 20:46:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1008 bytes sent, 21970 bytes (21.4 KB) received, lifetime 00:03
[10.30 20:46:17] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:46:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:46:19] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1572 bytes (1.53 KB) sent, 696 bytes received, lifetime 00:42
[10.30 20:46:19] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:46:19] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:46:20] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1155 bytes (1.12 KB) sent, 378 bytes received, lifetime 00:01
[10.30 20:46:20] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:46:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1359 bytes (1.32 KB) sent, 378 bytes received, lifetime <1 sec
[10.30 20:46:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:46:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2842 bytes (2.77 KB) sent, 282 bytes received, lifetime 00:02
[10.30 20:46:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2842 bytes (2.77 KB) sent, 282 bytes received, lifetime 00:02
[10.30 20:46:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:46:25] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:46:26] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2862 bytes (2.79 KB) sent, 282 bytes received, lifetime 00:01
[10.30 20:46:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:17
[10.30 20:46:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:17
[10.30 20:46:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:18
[10.30 20:46:37] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:17
[10.30 20:46:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1173 bytes (1.14 KB) sent, 375 bytes received, lifetime 00:01
[10.30 20:46:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2132 bytes (2.08 KB) sent, 822 bytes received, lifetime 00:29
[10.30 20:46:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 722 bytes sent, 310 bytes received, lifetime <1 sec
[10.30 20:46:50] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:46:51] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2842 bytes (2.77 KB) sent, 279 bytes received, lifetime 00:01
[10.30 20:46:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 588 bytes sent, 215882 bytes (210 KB) received, lifetime 00:03
[10.30 20:46:52] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 941 bytes sent, 398 bytes received, lifetime <1 sec
[10.30 20:46:54] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 5280 bytes (5.15 KB) sent, 6808 bytes (6.64 KB) received, lifetime 00:04
[10.30 20:46:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2842 bytes (2.77 KB) sent, 279 bytes received, lifetime 00:05
[10.30 20:47:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 4126 bytes (4.02 KB) sent, 43096 bytes (42.0 KB) received, lifetime 00:11
[10.30 20:47:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 696 bytes sent, 889 bytes received, lifetime <1 sec
[10.30 20:47:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2844 bytes (2.77 KB) sent, 279 bytes received, lifetime 00:02
[10.30 20:47:39] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:05
[10.30 20:47:39] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3029 bytes (2.95 KB) sent, 65994 bytes (64.4 KB) received, lifetime 00:27
[10.30 20:47:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 5510 bytes (5.38 KB) sent, 2436 bytes (2.37 KB) received, lifetime 02:03
[10.30 20:47:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:08
[10.30 20:47:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:08
[10.30 20:47:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1376 bytes (1.34 KB) sent, 13653 bytes (13.3 KB) received, lifetime 00:05
[10.30 20:47:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2756 bytes (2.69 KB) sent, 39055 bytes (38.1 KB) received, lifetime 00:05
[10.30 20:47:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:05
[10.30 20:47:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:05
[10.30 20:47:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:47:55] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 20:47:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:12
[10.30 20:48:26] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1107 bytes (1.08 KB) sent, 406 bytes received, lifetime 00:01
[10.30 20:48:26] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2532 bytes (2.47 KB) sent, 750 bytes received, lifetime 00:41
[10.30 20:48:26] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 618 bytes sent, 3235 bytes (3.15 KB) received, lifetime 00:41
[10.30 20:48:26] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:26] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 725 bytes sent, 310 bytes received, lifetime <1 sec
[10.30 20:48:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:48:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:28] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:03
[10.30 20:48:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 942 bytes sent, 398 bytes received, lifetime <1 sec
[10.30 20:48:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2844 bytes (2.77 KB) sent, 279 bytes received, lifetime 00:04
[10.30 20:48:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2844 bytes (2.77 KB) sent, 279 bytes received, lifetime <1 sec
[10.30 20:48:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:43] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1010 bytes sent, 21976 bytes (21.4 KB) received, lifetime 00:02
[10.30 20:48:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1455 bytes (1.42 KB) sent, 1794 bytes (1.75 KB) received, lifetime 00:01
[10.30 20:48:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2535 bytes (2.47 KB) sent, 750 bytes received, lifetime 00:15
[10.30 20:48:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 4528 bytes (4.42 KB) sent, 648 bytes received, lifetime 01:03
[10.30 20:48:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:15
[10.30 20:48:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 20:48:44] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:02
[10.30 20:48:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:48:47] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:49:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:49:01] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 697 bytes sent, 889 bytes received, lifetime <1 sec
[10.30 20:49:02] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:49:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2230 bytes (2.17 KB) sent, 659 bytes received, lifetime 00:01
[10.30 20:49:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:49:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:49:03] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 4496 bytes (4.39 KB) sent, 648 bytes received, lifetime 00:18
[10.30 20:49:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:49:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 20:49:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:18
[10.30 20:49:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:20
[10.30 20:49:13] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:49:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[10.30 20:49:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:49:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:49:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:19
[10.30 20:49:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:19
[10.30 20:49:35] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:49:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:49:59] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 952 bytes sent, 782 bytes received, lifetime 00:10
[10.30 20:51:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1576 bytes (1.53 KB) sent, 702 bytes received, lifetime 02:19
[10.30 20:51:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1578 bytes (1.54 KB) sent, 702 bytes received, lifetime 02:19
[10.30 20:51:35] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 4498 bytes (4.39 KB) sent, 654 bytes received, lifetime 02:17
[10.30 20:51:48] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 6409 bytes (6.25 KB) sent, 2466 bytes (2.40 KB) received, lifetime 02:44
[10.30 20:51:49] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 4763 bytes (4.65 KB) sent, 1421 bytes (1.38 KB) received, lifetime 02:15
[10.30 20:52:14] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:53:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:53:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:53:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:54:01] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:55:49] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 20:57:23] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2798 bytes (2.73 KB) sent, 15599 bytes (15.2 KB) received, lifetime 04:05
[10.30 20:57:25] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:57:43] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 435 bytes sent, 356 bytes received, lifetime <1 sec
[10.30 20:57:44] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 20:59:47] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 20:59:47] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:03:16] WeChat.exe - proxy.cse.cuhk.edu.hk:5070 close, 259 bytes sent, 5197 bytes (5.07 KB) received, lifetime <1 sec
[10.30 21:03:34] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:03:57] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3090 bytes (3.01 KB) sent, 1093 bytes (1.06 KB) received, lifetime 04:00
[10.30 21:05:25] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:32] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 21:05:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1408 bytes (1.37 KB) sent, 0 bytes received, lifetime <1 sec
[10.30 21:05:35] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:04
[10.30 21:05:35] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:38] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:39] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:39] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1065 bytes (1.04 KB) sent, 11966 bytes (11.6 KB) received, lifetime <1 sec
[10.30 21:05:39] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1905 bytes (1.86 KB) sent, 11087 bytes (10.8 KB) received, lifetime 00:08
[10.30 21:05:39] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:39] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1080 bytes (1.05 KB) sent, 4111 bytes (4.01 KB) received, lifetime <1 sec
[10.30 21:05:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1036 bytes (1.01 KB) sent, 4343 bytes (4.24 KB) received, lifetime 00:02
[10.30 21:05:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1054 bytes (1.02 KB) sent, 366 bytes received, lifetime <1 sec
[10.30 21:05:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:41] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1077 bytes (1.05 KB) sent, 2610 bytes (2.54 KB) received, lifetime <1 sec
[10.30 21:05:43] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:04
[10.30 21:05:45] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:05:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1081 bytes (1.05 KB) sent, 401 bytes received, lifetime 00:01
[10.30 21:06:04] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:06:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:06:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 643 bytes sent, 442 bytes received, lifetime 00:45
[10.30 21:06:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:06:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:06:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3094 bytes (3.02 KB) sent, 1014 bytes received, lifetime 00:05
[10.30 21:06:35] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:06:38] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2813 bytes (2.74 KB) sent, 1241 bytes (1.21 KB) received, lifetime 00:05
[10.30 21:06:38] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2301 bytes (2.24 KB) sent, 9442 bytes (9.22 KB) received, lifetime 00:07
[10.30 21:06:38] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 948 bytes sent, 782 bytes received, lifetime <1 sec
[10.30 21:06:46] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:07:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:07:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:07:18] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:07:20] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:08:03] spoolsv.exe *64 - 127.0.0.1:135 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[10.30 21:08:23] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:08:23] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 close, 441 bytes sent, 687 bytes received, lifetime <1 sec
[10.30 21:10:12] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1237 bytes (1.20 KB) sent, 1164 bytes (1.13 KB) received, lifetime 04:00
[10.30 21:11:09] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 2001 bytes (1.95 KB) sent, 18018 bytes (17.5 KB) received, lifetime 04:00
[10.30 21:11:51] YodaoDict.exe - oimageb3.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:11:55] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:11:55] YodaoDict.exe - oimageb1.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:12:32] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:12:32] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:12:56] YodaoDict.exe - oimagea3.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:12:56] YodaoDict.exe - oimagec1.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:12:56] YodaoDict.exe - oimageb5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:13:09] AcroRd32.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:13:23] spoolsv.exe *64 - 127.0.0.1:135 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[10.30 21:14:03] YodaoDict.exe - oimageb7.ydstatic.com:80 close, 357 bytes sent, 24275 bytes (23.7 KB) received, lifetime 00:30
[10.30 21:14:05] YodaoDict.exe - oimagea5.ydstatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:14:05] YodaoDict.exe - oimageb2.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:14:05] YodaoDict.exe - oimageb4.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:14:05] YodaoDict.exe - oimagec7.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:14:05] YodaoDict.exe - oimagec4.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:14:05] YodaoDict.exe - oimageb8.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:14:16] YodaoDict.exe - oimageb6.ydstatic.com:80 close, 358 bytes sent, 38960 bytes (38.0 KB) received, lifetime 00:30
[10.30 21:14:57] YodaoDict.exe - oimagec4.ydstatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:14:59] YodaoDict.exe - oimagea7.ydstatic.com:80 close, 414 bytes sent, 41182 bytes (40.2 KB) received, lifetime 00:30
[10.30 21:15:27] YodaoDict.exe - oimagec3.ydstatic.com:80 close, 358 bytes sent, 48647 bytes (47.5 KB) received, lifetime 00:30
[10.30 21:16:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 7685 bytes (7.50 KB) sent, 51839 bytes (50.6 KB) received, lifetime 09:54
[10.30 21:16:40] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:16:41] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:16:41] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:16:41] YodaoDict.exe - oimageb8.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:16:41] YodaoDict.exe - oimagec1.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:17:51] YodaoDict.exe - oimagea5.ydstatic.com:80 error : A connection request was canceled before the completion. 
[10.30 21:17:57] YodaoDict.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:18:27] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:19:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:19:15] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:19:16] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1225 bytes (1.19 KB) sent, 1093 bytes (1.06 KB) received, lifetime 00:01
[10.30 21:19:19] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:19:19] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 21:19:19] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:19:19] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:19:20] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 964 bytes sent, 848 bytes received, lifetime <1 sec
[10.30 21:19:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:19:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 948 bytes sent, 782 bytes received, lifetime <1 sec
[10.30 21:19:21] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 5349 bytes (5.22 KB) sent, 1767 bytes (1.72 KB) received, lifetime <1 sec
[10.30 21:19:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:19:22] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:19:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:19:29] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1066 bytes (1.04 KB) sent, 3042 bytes (2.97 KB) received, lifetime 00:10
[10.30 21:19:32] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:13
[10.30 21:19:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 3757 bytes (3.66 KB) sent, 1307 bytes (1.27 KB) received, lifetime 00:15
[10.30 21:19:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:20:30] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01
[10.30 21:20:31] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1978 bytes (1.93 KB) sent, 29237 bytes (28.5 KB) received, lifetime 00:01
[10.30 21:21:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:21:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1217 bytes (1.18 KB) sent, 592 bytes received, lifetime 01:02
[10.30 21:21:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[10.30 21:21:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1770 bytes (1.72 KB) sent, 1297 bytes (1.26 KB) received, lifetime 02:12
[10.30 21:21:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:21:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:21:33] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:21:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:21:34] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:21:35] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:02
[10.30 21:21:35] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:21:36] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 564 bytes sent, 28902 bytes (28.2 KB) received, lifetime <1 sec
[10.30 21:21:36] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[10.30 21:21:48] chrome.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:34] chrome.exe *64 - www.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:34] chrome.exe *64 - c.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:34] chrome.exe *64 - sclick.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:39] chrome.exe *64 - www.baidu.com:80 close, 6719 bytes (6.56 KB) sent, 644545 bytes (629 KB) received, lifetime 00:05
[07.26 13:30:43] chrome.exe *64 - timg.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:44] SogouCloud.exe - get.sogou.com:80 close, 899 bytes sent, 348 bytes received, lifetime 00:03
[07.26 13:30:45] chrome.exe *64 - ss.bdimg.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 13:30:45] chrome.exe *64 - t11.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 13:30:45] chrome.exe *64 - sestat.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 13:30:45] chrome.exe *64 - ss.bdimg.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 13:30:48] chrome.exe *64 - comment.5054399.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:48] chrome.exe *64 - www.4399.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:48] chrome.exe *64 - cbjs.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:49] chrome.exe *64 - ubmcmm.baidustatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:49] chrome.exe *64 - dup.baidustatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:49] chrome.exe *64 - eclick.baidu.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:50] chrome.exe *64 - dup.baidustatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:50] chrome.exe *64 - dup.baidustatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:30:57] chrome.exe *64 - b1.bdstatic.com:80 close, 389 bytes sent, 409 bytes received, lifetime 00:21
[07.26 13:30:58] chrome.exe *64 - pos.baidu.com:80 close, 1822 bytes (1.77 KB) sent, 1370 bytes (1.33 KB) received, lifetime 00:10
[07.26 13:31:09] chrome.exe *64 - i9.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:27
[07.26 13:31:09] chrome.exe *64 - xin.4399.cn:80 close, 0 bytes sent, 0 bytes received, lifetime 00:21
[07.26 13:31:09] chrome.exe *64 - newsimg.5054399.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:19
[07.26 13:31:09] chrome.exe *64 - comment.5054399.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:21
[07.26 13:31:09] chrome.exe *64 - i9.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:27
[07.26 13:31:09] chrome.exe *64 - m.news.4399.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:19
[07.26 13:31:09] chrome.exe *64 - www.4399.cn:80 close, 0 bytes sent, 0 bytes received, lifetime 00:21
[07.26 13:31:12] chrome.exe *64 - api.share.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:31:13] chrome.exe *64 - timg.baidu.com:80 close, 1299 bytes (1.26 KB) sent, 4993 bytes (4.87 KB) received, lifetime 00:30
[07.26 13:31:21] chrome.exe *64 - www.googletagservices.com:443 close, 1257 bytes (1.22 KB) sent, 540 bytes received, lifetime 04:00
[07.26 13:31:23] chrome.exe *64 - yt3.ggpht.com:443 close, 10542 bytes (10.2 KB) sent, 356545 bytes (348 KB) received, lifetime 09:54
[07.26 13:31:28] chrome.exe *64 - www.google.com:443 close, 1740 bytes (1.69 KB) sent, 21911 bytes (21.3 KB) received, lifetime 04:01
[07.26 13:31:49] chrome.exe *64 - dup.baidustatic.com:443 close, 814 bytes sent, 4145 bytes (4.04 KB) received, lifetime 01:00
[07.26 13:36:07] chrome.exe *64 - pubads.g.doubleclick.net:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:36:11] chrome.exe *64 - d.agkn.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:36:22] chrome.exe *64 - odr.mookie1.com:443 close, 361 bytes sent, 4839 bytes (4.72 KB) received, lifetime 00:11
[07.26 13:37:15] chrome.exe *64 - safebrowsing.googleapis.com:443 close, 1344 bytes (1.31 KB) sent, 1170 bytes (1.14 KB) received, lifetime 04:00
[07.26 13:39:07] chrome.exe *64 - clients6.google.com:443 close, 1891 bytes (1.84 KB) sent, 811 bytes received, lifetime 04:00
[07.26 13:39:39] Dropbox.exe - d.dropbox.com:443 close, 1021 bytes sent, 4906 bytes (4.79 KB) received, lifetime 01:01
[07.26 13:40:11] chrome.exe *64 - clients6.google.com:443 close, 3216 bytes (3.14 KB) sent, 2391 bytes (2.33 KB) received, lifetime 05:04
[07.26 13:40:22] chrome.exe *64 - i.ytimg.com:443 close, 2836 bytes (2.76 KB) sent, 125622 bytes (122 KB) received, lifetime 06:33
[07.26 13:41:04] chrome.exe *64 - d.agkn.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:42:05] chrome.exe *64 - r6---sn-i3b7kn7d.googlevideo.com:443 close, 12789 bytes (12.4 KB) sent, 13833013 bytes (13.1 MB) received, lifetime 01:01
[07.26 13:42:10] chrome.exe *64 - odr.mookie1.com:443 close, 1045 bytes (1.02 KB) sent, 5662 bytes (5.52 KB) received, lifetime 01:06
[07.26 13:43:17] chrome.exe *64 - odr.mookie1.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:43:18] chrome.exe *64 - static.doubleclick.net:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:44:26] Dropbox.exe - d.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:44:28] chrome.exe *64 - fonts.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:44:29] chrome.exe *64 - static.doubleclick.net:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:45:00] chrome.exe *64 - r6---sn-i3b7knez.googlevideo.com:443 close, 13118 bytes (12.8 KB) sent, 3242984 bytes (3.09 MB) received, lifetime 00:31
[07.26 13:46:41] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:46:41] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:46:41] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:46:41] chrome.exe *64 - timg.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:46:41] chrome.exe *64 - sclick.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:46:41] chrome.exe *64 - sclick.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:46:43] chrome.exe *64 - ecmb.bdimg.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:46:48] chrome.exe *64 - c.baidu.com:80 close, 1052 bytes (1.02 KB) sent, 113 bytes received, lifetime <1 sec
[07.26 13:46:51] chrome.exe *64 - t12.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:10
[07.26 13:46:51] chrome.exe *64 - t12.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:10
[07.26 13:46:51] chrome.exe *64 - timg.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:10
[07.26 13:46:51] chrome.exe *64 - cpro.baidustatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:46:52] chrome.exe *64 - ubmcmm.baidustatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:46:52] chrome.exe *64 - dup.baidustatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:46:52] chrome.exe *64 - ss.bdimg.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:10
[07.26 13:47:02] chrome.exe *64 - pos.baidu.com:80 close, 3783 bytes (3.69 KB) sent, 38448 bytes (37.5 KB) received, lifetime 00:11
[07.26 13:47:07] chrome.exe *64 - i9.baidu.com:80 close, 1915 bytes (1.87 KB) sent, 12764 bytes (12.4 KB) received, lifetime 00:21
[07.26 13:47:11] chrome.exe *64 - z13.cnzz.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:19
[07.26 13:47:11] chrome.exe *64 - eclick.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:47:12] chrome.exe *64 - f10.baidu.com:80 close, 6956 bytes (6.79 KB) sent, 101059 bytes (98.6 KB) received, lifetime 00:20
[07.26 13:47:16] chrome.exe *64 - suggestion.baidu.com:80 close, 1829 bytes (1.78 KB) sent, 1039 bytes (1.01 KB) received, lifetime 00:35
[07.26 13:47:16] chrome.exe *64 - eclick.baidu.com:80 close, 1037 bytes (1.01 KB) sent, 311 bytes received, lifetime 00:05
[07.26 13:47:17] chrome.exe *64 - cm.g.doubleclick.net:443 close, 2197 bytes (2.14 KB) sent, 1527 bytes (1.49 KB) received, lifetime 06:13
[07.26 13:47:37] chrome.exe *64 - news.4399.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:47:37] chrome.exe *64 - app.4399.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:47:37] chrome.exe *64 - app.4399.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:47:37] chrome.exe *64 - video.5054399.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:47:37] chrome.exe *64 - video.5054399.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:47:38] chrome.exe *64 - comment.5054399.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:47:38] chrome.exe *64 - a.img4399.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:47:38] chrome.exe *64 - w.cnzz.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:47:38] chrome.exe *64 - www.4399.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:47:39] chrome.exe *64 - cpro.baidustatic.com:443 close, 334 bytes sent, 3713 bytes (3.62 KB) received, lifetime 00:48
[07.26 13:47:59] chrome.exe *64 - www.4399.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:21
[07.26 13:47:59] chrome.exe *64 - cnzz.mmstat.com:80 close, 686 bytes sent, 579 bytes received, lifetime <1 sec
[07.26 13:48:21] chrome.exe *64 - hm.baidu.com:80 error : A connection request was canceled before the completion. 
[07.26 13:48:29] chrome.exe *64 - app.4399.cn:80 close, 1381 bytes (1.34 KB) sent, 38524 bytes (37.6 KB) received, lifetime 00:52
[07.26 13:48:30] chrome.exe *64 - www.google.com:443 close, 9034 bytes (8.82 KB) sent, 3696 bytes (3.60 KB) received, lifetime 07:26
[07.26 13:48:30] chrome.exe *64 - www.youtube.com:443 close, 30789 bytes (30.0 KB) sent, 164940 bytes (161 KB) received, lifetime 07:44
[07.26 13:48:51] chrome.exe *64 - www.qulishi.com:80 close, 1581 bytes (1.54 KB) sent, 79427 bytes (77.5 KB) received, lifetime 02:01
[07.26 13:48:59] chrome.exe *64 - q7.cnzz.com:80 close, 0 bytes sent, 0 bytes received, lifetime 01:00
[07.26 13:49:38] chrome.exe *64 - w.cnzz.com:80 close, 570 bytes sent, 11519 bytes (11.2 KB) received, lifetime 02:00
[07.26 13:50:06] Dropbox.exe - client-lb.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:50:06] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.26 13:53:48] WeChat.exe - short.weixin.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 13:55:07] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:33
[07.26 13:59:44] chrome.exe *64 - www.google.com.hk:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:03:57] chrome.exe *64 - notifications.google.com:443 close, 470 bytes sent, 4856 bytes (4.74 KB) received, lifetime 04:00
[07.26 14:04:50] chrome.exe *64 - apis.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:05:04] chrome.exe *64 - csi.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:05:04] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:05:07] Dropbox.exe - block.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:05:23] SogouCloud.exe - get.sogou.com:80 close, 759 bytes sent, 51462 bytes (50.2 KB) received, lifetime 00:05
[07.26 14:05:25] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:05:28] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:07:33] YodaoDict.exe - cidian.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:08:54] chrome.exe *64 - clientservices.googleapis.com:443 close, 1051 bytes (1.02 KB) sent, 592 bytes received, lifetime 04:01
[07.26 14:10:07] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:32
[07.26 14:10:59] WeChat.exe - short.weixin.qq.com:80 close, 425 bytes sent, 161 bytes received, lifetime <1 sec
[07.26 14:11:09] SGTool.exe - info.pinyin.sogou.com:80 close, 1020 bytes sent, 607 bytes received, lifetime 00:30
[07.26 14:11:25] Dropbox.exe - client-cf.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:11:50] WeChat.exe - 203.205.129.102:8080 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.26 14:11:52] WeChat.exe - 203.205.146.15:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:12:28] Dropbox.exe - client-cf.dropbox.com:443 close, 3934 bytes (3.84 KB) sent, 5647 bytes (5.51 KB) received, lifetime 01:03
[07.26 14:12:29] Dropbox.exe - client-cf.dropbox.com:443 close, 4478 bytes (4.37 KB) sent, 16913 bytes (16.5 KB) received, lifetime 01:01
[07.26 14:13:05] chrome.exe *64 - people-pa.clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:13:10] chrome.exe *64 - f-log-extension.grammarly.io:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:13:17] chrome.exe *64 - lh6.googleusercontent.com:443 close, 471 bytes sent, 4694 bytes (4.58 KB) received, lifetime 00:11
[07.26 14:15:07] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:32
[07.26 14:16:11] Dropbox.exe - d.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:17:01] chrome.exe *64 - www.google.com.hk:443 close, 2411 bytes (2.35 KB) sent, 2093 bytes (2.04 KB) received, lifetime 04:01
[07.26 14:17:45] Acrobat.exe - ocsp.digicert.com:80 close, 942 bytes sent, 3184 bytes (3.10 KB) received, lifetime 00:07
[07.26 14:21:40] chrome.exe *64 - mail-attachment.googleusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:21:41] chrome.exe *64 - ssl.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:22:35] Acrobat.exe - static.adobelogin.com:443 close, 356 bytes sent, 3523 bytes (3.44 KB) received, lifetime 00:02
[07.26 14:23:31] Dropbox.exe - block-edge.dropbox.com:443 close, 861480 bytes (841 KB) sent, 5464 bytes (5.33 KB) received, lifetime 01:45
[07.26 14:24:32] YodaoDict.exe - cidian.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:24:43] YodaoDict.exe - dict.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:24:58] YodaoDict.exe - impservice.dictapp.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:25:41] chrome.exe *64 - capi.grammarly.com:443 close, 4974 bytes (4.85 KB) sent, 8661 bytes (8.45 KB) received, lifetime 02:13
[07.26 14:25:47] chrome.exe *64 - ogs.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:25:47] chrome.exe *64 - ssl.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:25:58] chrome.exe *64 - ssl.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:27:34] YodaoDict.exe - cidian.youdao.com:80 close, 2369 bytes (2.31 KB) sent, 1583 bytes (1.54 KB) received, lifetime 02:59
[07.26 14:28:55] chrome.exe *64 - 3.client-channel.google.com:443 close, 27694 bytes (27.0 KB) sent, 165991 bytes (162 KB) received, lifetime 01:07:37
[07.26 14:30:30] chrome.exe *64 - lh3.googleusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:31] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:32] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:32] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:32] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:32] chrome.exe *64 - www.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:32] chrome.exe *64 - sestat.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:32] chrome.exe *64 - sestat.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:32] chrome.exe *64 - suggestion.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:32] chrome.exe *64 - d3cv4a9a9wh0bt.cloudfront.net:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:35] chrome.exe *64 - i9.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:30:43] chrome.exe *64 - s1.bdstatic.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 14:30:43] chrome.exe *64 - www.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 14:30:44] chrome.exe *64 - ss.bdimg.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 14:30:45] chrome.exe *64 - news.4399.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:31:01] chrome.exe *64 - i9.baidu.com:80 close, 1671 bytes (1.63 KB) sent, 4659 bytes (4.54 KB) received, lifetime 00:26
[07.26 14:31:08] chrome.exe *64 - i9.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:27
[07.26 14:31:08] chrome.exe *64 - i9.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:27
[07.26 14:31:10] chrome.exe *64 - comment.5054399.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:31:11] chrome.exe *64 - xin.4399.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:31:12] chrome.exe *64 - newsapp.5054399.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:31:31] chrome.exe *64 - cbjs.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:19
[07.26 14:31:31] chrome.exe *64 - bdimg.share.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:19
[07.26 14:31:31] chrome.exe *64 - ubmcmm.baidustatic.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:19
[07.26 14:31:31] chrome.exe *64 - s1.img4399.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:21
[07.26 14:31:31] chrome.exe *64 - ubmcmm.baidustatic.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:19
[07.26 14:32:04] chrome.exe *64 - c.cnzz.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:30
[07.26 14:34:45] chrome.exe *64 - www.evernote.com:443 close, 4733 bytes (4.62 KB) sent, 8179 bytes (7.98 KB) received, lifetime 04:12
[07.26 14:35:08] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:32
[07.26 14:35:08] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.26 14:39:29] chrome.exe *64 - clients4.google.com:443 close, 11736 bytes (11.4 KB) sent, 8496 bytes (8.29 KB) received, lifetime 08:46
[07.26 14:40:28] chrome.exe *64 - clients6.google.com:443 close, 3025 bytes (2.95 KB) sent, 6726 bytes (6.56 KB) received, lifetime 05:20
[07.26 14:43:52] chrome.exe *64 - mtalk.google.com:5228 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.26 14:44:17] chrome.exe *64 - clients2.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:44:23] chrome.exe *64 - mtalk.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:44:36] WeChat.exe - qbwup.imtt.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:35] chrome.exe *64 - www.google.com.hk:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:38] chrome.exe *64 - sestat.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:38] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:38] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:38] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:38] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:39] chrome.exe *64 - suggestion.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:39] chrome.exe *64 - suggestion.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:39] chrome.exe *64 - sclick.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:40] chrome.exe *64 - ss.bdimg.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:41] chrome.exe *64 - www.baidu.com:80 close, 9987 bytes (9.75 KB) sent, 148343 bytes (144 KB) received, lifetime 00:03
[07.26 14:45:43] chrome.exe *64 - dj1.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:49] chrome.exe *64 - t12.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 14:45:49] chrome.exe *64 - t12.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 14:45:50] chrome.exe *64 - ss0.baidu.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:50] chrome.exe *64 - hpd.baidu.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:52] chrome.exe *64 - ubmcmm.baidustatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:45:52] chrome.exe *64 - eclick.baidu.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:46:05] chrome.exe *64 - dup.baidustatic.com:443 close, 333 bytes sent, 3697 bytes (3.61 KB) received, lifetime 00:14
[07.26 14:46:13] chrome.exe *64 - dj1.baidu.com:80 close, 1369 bytes (1.33 KB) sent, 289 bytes received, lifetime 00:30
[07.26 14:50:09] Dropbox.exe - client-lb.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.26 14:50:44] WeChat.exe - short.weixin.qq.com:80 close, 785 bytes sent, 145 bytes received, lifetime <1 sec
[07.26 14:51:20] chrome.exe *64 - clients5.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:21] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:22] chrome.exe *64 - s1.bdstatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:22] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:24] chrome.exe *64 - ss.bdimg.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:25] chrome.exe *64 - i7.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:25] chrome.exe *64 - i7.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:26] chrome.exe *64 - www.evernote.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:31] chrome.exe *64 - dj1.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:31] chrome.exe *64 - sestat.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:31] chrome.exe *64 - sclick.baidu.com:80 close, 1971 bytes (1.92 KB) sent, 401 bytes received, lifetime 00:08
[07.26 14:51:31] chrome.exe *64 - clients4.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:35] chrome.exe *64 - s1.bdstatic.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:13
[07.26 14:51:35] chrome.exe *64 - sclick.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:37] chrome.exe *64 - www.3367.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:37] chrome.exe *64 - img.3367.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:38] chrome.exe *64 - assets.changyan.sohu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:39] chrome.exe *64 - bdimg.share.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:40] chrome.exe *64 - clients1.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:40] chrome.exe *64 - s.360.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:42] chrome.exe *64 - long.open.weixin.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:51:46] chrome.exe *64 - changyan.sohu.com:80 close, 6361 bytes (6.21 KB) sent, 6197 bytes (6.05 KB) received, lifetime 00:08
[07.26 14:51:49] chrome.exe *64 - t12.baidu.com:80 close, 837 bytes sent, 4059 bytes (3.96 KB) received, lifetime 00:27
[07.26 14:51:51] chrome.exe *64 - i7.baidu.com:80 close, 2813 bytes (2.74 KB) sent, 23975 bytes (23.4 KB) received, lifetime 00:26
[07.26 14:51:55] chrome.exe *64 - js.passport.qihucdn.com:80 close, 496 bytes sent, 426 bytes received, lifetime 00:16
[07.26 14:52:00] chrome.exe *64 - js.passport.qihucdn.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:21
[07.26 14:52:30] chrome.exe *64 - www.baidu.com:80 close, 2590 bytes (2.52 KB) sent, 621 bytes received, lifetime 01:06
[07.26 14:52:38] chrome.exe *64 - hm.baidu.com:443 error : A connection request was canceled before the completion. 
[07.26 14:52:40] chrome.exe *64 - upload.3367.com:80 close, 2331 bytes (2.27 KB) sent, 440133 bytes (429 KB) received, lifetime 01:01
[07.26 14:53:43] chrome.exe *64 - long.open.weixin.qq.com:443 close, 1910 bytes (1.86 KB) sent, 276 bytes received, lifetime 00:29
[07.26 14:54:41] chrome.exe *64 - res.wx.qq.com:443 close, 1582 bytes (1.54 KB) sent, 40194 bytes (39.2 KB) received, lifetime 03:01
[07.26 14:55:22] chrome.exe *64 - www.google.com:443 close, 2020 bytes (1.97 KB) sent, 17986 bytes (17.5 KB) received, lifetime 04:01
[07.26 14:56:52] chrome.exe *64 - long.open.weixin.qq.com:443 close, 1910 bytes (1.86 KB) sent, 276 bytes received, lifetime 00:27
[07.26 14:57:27] chrome.exe *64 - long.open.weixin.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 14:57:54] chrome.exe *64 - long.open.weixin.qq.com:443 close, 1910 bytes (1.86 KB) sent, 276 bytes received, lifetime 00:27
[07.26 14:59:22] chrome.exe *64 - mtalk.google.com:443 close, 985 bytes sent, 447 bytes received, lifetime 14:59
[07.26 14:59:58] chrome.exe *64 - long.open.weixin.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:04:20] chrome.exe *64 - long.open.weixin.qq.com:443 close, 1910 bytes (1.86 KB) sent, 276 bytes received, lifetime 00:27
[07.26 15:04:37] WeChat.exe - qbwup.imtt.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:04:54] chrome.exe *64 - update.googleapis.com:443 close, 298 bytes sent, 4250 bytes (4.15 KB) received, lifetime <1 sec
[07.26 15:05:25] chrome.exe *64 - long.open.weixin.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:06:12] Dropbox.exe - dl-debug.dropbox.com:443 close, 163539 bytes (159 KB) sent, 4712 bytes (4.60 KB) received, lifetime 01:02
[07.26 15:07:00] chrome.exe *64 - long.open.weixin.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:07:25] chrome.exe *64 - long.open.weixin.qq.com:443 close, 1910 bytes (1.86 KB) sent, 276 bytes received, lifetime 00:28
[07.26 15:07:30] chrome.exe *64 - long.open.weixin.qq.com:443 close, 1985 bytes (1.93 KB) sent, 4832 bytes (4.71 KB) received, lifetime 00:02
[07.26 15:08:38] chrome.exe *64 - www.google.com.hk:443 close, 734 bytes sent, 160 bytes received, lifetime <1 sec
[07.26 15:08:40] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:08:55] chrome.exe *64 - fonts.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:08:56] chrome.exe *64 - lh4.googleusercontent.com:443 close, 471 bytes sent, 4692 bytes (4.58 KB) received, lifetime 00:11
[07.26 15:08:57] chrome.exe *64 - csi.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:09:28] chrome.exe *64 - lh4.googleusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:09:29] chrome.exe *64 - avatars3.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:09:29] chrome.exe *64 - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:09:29] chrome.exe *64 - collector.githubapp.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:09:31] chrome.exe *64 - clients1.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:10:38] SGTool.exe - config.pinyin.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:11:08] SGTool.exe - info.pinyin.sogou.com:80 close, 1020 bytes sent, 607 bytes received, lifetime 00:30
[07.26 15:11:10] SGTool.exe - ping.pinyin.sogou.com:80 close, 1109 bytes (1.08 KB) sent, 332 bytes received, lifetime 00:32
[07.26 15:11:31] chrome.exe *64 - api.github.com:443 close, 0 bytes sent, 0 bytes received, lifetime 02:02
[07.26 15:12:46] chrome.exe *64 - clients6.google.com:443 close, 4318 bytes (4.21 KB) sent, 1951 bytes (1.90 KB) received, lifetime 09:43
[07.26 15:13:17] chrome.exe *64 - www.google.com:443 close, 2564 bytes (2.50 KB) sent, 2035 bytes (1.98 KB) received, lifetime 04:37
[07.26 15:13:19] chrome.exe *64 - apis.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:13:19] chrome.exe *64 - www.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:13:31] chrome.exe *64 - www.google-analytics.com:443 close, 1770 bytes (1.72 KB) sent, 5146 bytes (5.02 KB) received, lifetime 04:02
[07.26 15:13:33] chrome.exe *64 - www.google-analytics.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:13:33] chrome.exe *64 - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:13:35] chrome.exe *64 - notifications.google.com:443 close, 470 bytes sent, 4794 bytes (4.68 KB) received, lifetime 00:10
[07.26 15:13:42] chrome.exe *64 - notifications.google.com:443 close, 733 bytes sent, 229 bytes received, lifetime 00:13
[07.26 15:13:45] chrome.exe *64 - live.github.com:443 close, 1207 bytes (1.17 KB) sent, 452 bytes received, lifetime 00:01
[07.26 15:14:03] chrome.exe *64 - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:14:29] chrome.exe *64 - github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:17:19] chrome.exe *64 - www.googleapis.com:443 close, 1733 bytes (1.69 KB) sent, 1288 bytes (1.25 KB) received, lifetime 04:00
[07.26 15:17:34] chrome.exe *64 - www.google.com.hk:443 close, 24256 bytes (23.6 KB) sent, 434160 bytes (423 KB) received, lifetime 08:56
[07.26 15:17:36] YodaoDict.exe - dict.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:18:05] chrome.exe *64 - play.google.com:443 close, 33864 bytes (33.0 KB) sent, 22082 bytes (21.5 KB) received, lifetime 19:00
[07.26 15:20:09] chrome.exe *64 - collector.githubapp.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:20:49] chrome.exe *64 - www.gmail.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:20:54] chrome.exe *64 - people-pa.clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:20:54] chrome.exe *64 - 11.client-channel.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:21:06] chrome.exe *64 - lh6.googleusercontent.com:443 close, 471 bytes sent, 4693 bytes (4.58 KB) received, lifetime 00:13
[07.26 15:21:06] chrome.exe *64 - www.gmail.com:443 close, 459 bytes sent, 4187 bytes (4.08 KB) received, lifetime 00:17
[07.26 15:21:06] chrome.exe *64 - lh6.googleusercontent.com:443 close, 471 bytes sent, 4694 bytes (4.58 KB) received, lifetime 00:13
[07.26 15:21:06] chrome.exe *64 - plus.google.com:443 close, 470 bytes sent, 4793 bytes (4.68 KB) received, lifetime 00:16
[07.26 15:21:06] chrome.exe *64 - csi.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:21:30] Dropbox.exe - client-cf.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:22:58] Dropbox.exe - block-edge.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:23:40] chrome.exe *64 - 3.client-channel.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:23:47] GitHub.exe - avatars.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:23:48] GitHub.exe - avatars3.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:23:48] GitHub.exe - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:23:48] GitHub.exe - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:23:51] GitHub.exe - github-windows.s3.amazonaws.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:23:59] GitHub.exe - api.github.com:443 close, 674 bytes sent, 2132 bytes (2.08 KB) received, lifetime 00:11
[07.26 15:24:55] chrome.exe *64 - apis.google.com:443 close, 2498 bytes (2.43 KB) sent, 5835 bytes (5.69 KB) received, lifetime 04:02
[07.26 15:25:08] chrome.exe *64 - www.zhihu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:25:08] chrome.exe *64 - pic4.zhimg.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:25:09] chrome.exe *64 - clients1.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:25:19] chrome.exe *64 - pic4.zhimg.com:443 close, 330 bytes sent, 3763 bytes (3.67 KB) received, lifetime 00:11
[07.26 15:25:19] chrome.exe *64 - zhihu-web-analytics.zhihu.com:443 close, 343 bytes sent, 3122 bytes (3.04 KB) received, lifetime 00:11
[07.26 15:25:19] chrome.exe *64 - content.googleapis.com:443 close, 2094 bytes (2.04 KB) sent, 623 bytes received, lifetime 04:00
[07.26 15:25:29] chrome.exe *64 - pubads.g.doubleclick.net:443 close, 476 bytes sent, 4417 bytes (4.31 KB) received, lifetime 00:14
[07.26 15:25:32] WeChat.exe - short.weixin.qq.com:80 close, 425 bytes sent, 161 bytes received, lifetime <1 sec
[07.26 15:25:49] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:25:55] chrome.exe *64 - notifications.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:25:56] chrome.exe *64 - avatars3.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:25:56] chrome.exe *64 - www.google-analytics.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:26:11] chrome.exe *64 - user-images.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:26:14] chrome.exe *64 - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:26:27] chrome.exe *64 - api.github.com:443 close, 328 bytes sent, 3642 bytes (3.55 KB) received, lifetime 00:16
[07.26 15:29:09] chrome.exe *64 - clients1.google.com:443 close, 939 bytes sent, 5532 bytes (5.40 KB) received, lifetime 04:00
[07.26 15:29:26] chrome.exe *64 - user-images.githubusercontent.com:443 close, 335 bytes sent, 4369 bytes (4.26 KB) received, lifetime 03:15
[07.26 15:29:26] chrome.exe *64 - user-images.githubusercontent.com:443 close, 335 bytes sent, 4369 bytes (4.26 KB) received, lifetime 03:15
[07.26 15:29:30] chrome.exe *64 - lh6.googleusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:29:30] chrome.exe *64 - clients6.google.com:443 close, 568 bytes sent, 156 bytes received, lifetime <1 sec
[07.26 15:29:31] chrome.exe *64 - 12.client-channel.google.com:443 close, 474 bytes sent, 4042 bytes (3.94 KB) received, lifetime <1 sec
[07.26 15:29:43] chrome.exe *64 - csi.gstatic.com:443 close, 733 bytes sent, 229 bytes received, lifetime 00:13
[07.26 15:29:43] chrome.exe *64 - lh6.googleusercontent.com:443 close, 471 bytes sent, 4693 bytes (4.58 KB) received, lifetime 00:12
[07.26 15:29:43] chrome.exe *64 - csi.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:29:54] chrome.exe *64 - csi.gstatic.com:443 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 15:29:57] chrome.exe *64 - fonts.gstatic.com:443 close, 463 bytes sent, 4786 bytes (4.67 KB) received, lifetime 00:23
[07.26 15:30:02] chrome.exe *64 - tvax1.sinaimg.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:02] chrome.exe *64 - tvax1.sinaimg.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:07] chrome.exe *64 - js.t.sinajs.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:08] chrome.exe *64 - dslb.cdn.krcom.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:08] chrome.exe *64 - wx3.sinaimg.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:08] chrome.exe *64 - wx1.sinaimg.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:08] chrome.exe *64 - wx2.sinaimg.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:08] chrome.exe *64 - tva1.sinaimg.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:08] chrome.exe *64 - mu1.sinaimg.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:10] chrome.exe *64 - s.weibo.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:11] chrome.exe *64 - d0.sina.com.cn:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:11] chrome.exe *64 - api.weibo.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:11] chrome.exe *64 - www.zhihu.com:443 close, 1685 bytes (1.64 KB) sent, 10151 bytes (9.91 KB) received, lifetime 05:03
[07.26 15:30:11] chrome.exe *64 - d0.sina.com.cn:443 close, 328 bytes sent, 3654 bytes (3.56 KB) received, lifetime <1 sec
[07.26 15:30:12] chrome.exe *64 - contentrecommend-out.uve.weibo.com:443 close, 0 bytes sent, 0 bytes received, lifetime 00:10
[07.26 15:30:14] chrome.exe *64 - strip.alicdn.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:15] chrome.exe *64 - strip.alicdn.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:15] chrome.exe *64 - show.re.taobao.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:18] chrome.exe *64 - tvax1.sinaimg.cn:80 close, 909 bytes sent, 3467 bytes (3.38 KB) received, lifetime 00:16
[07.26 15:30:20] chrome.exe *64 - rs.sinajs.cn:80 close, 1187 bytes (1.15 KB) sent, 472 bytes received, lifetime 00:12
[07.26 15:30:26] chrome.exe *64 - gtms03.alicdn.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:32] chrome.exe *64 - news.sina.com.cn:80 close, 0 bytes sent, 0 bytes received, lifetime 00:21
[07.26 15:30:40] chrome.exe *64 - m.simba.taobao.com:443 close, 0 bytes sent, 0 bytes received, lifetime 00:26
[07.26 15:30:41] chrome.exe *64 - rm.api.weibo.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:30:44] chrome.exe *64 - strip.alicdn.com:443 close, 330 bytes sent, 3332 bytes (3.25 KB) received, lifetime 00:30
[07.26 15:31:07] chrome.exe *64 - dslb.cdn.krcom.cn:80 close, 446 bytes sent, 195160 bytes (190 KB) received, lifetime 00:59
[07.26 15:32:15] chrome.exe *64 - strip.alicdn.com:80 close, 1596 bytes (1.55 KB) sent, 29034 bytes (28.3 KB) received, lifetime 02:00
[07.26 15:32:23] chrome.exe *64 - gtms03.alicdn.com:80 close, 399 bytes sent, 498 bytes received, lifetime 02:06
[07.26 15:32:47] chrome.exe *64 - rm.api.weibo.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:05
[07.26 15:32:55] chrome.exe *64 - www.weibo.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:33:02] chrome.exe *64 - www.weibo.com:80 close, 11058 bytes (10.7 KB) sent, 451 bytes received, lifetime 00:07
[07.26 15:33:20] chrome.exe *64 - www.weibo.com:80 close, 2741 bytes (2.67 KB) sent, 4016 bytes (3.92 KB) received, lifetime 00:08
[07.26 15:33:29] chrome.exe *64 - www.google.com:443 close, 2285 bytes (2.23 KB) sent, 1184 bytes (1.15 KB) received, lifetime 04:03
[07.26 15:33:31] chrome.exe *64 - people-pa.clients6.google.com:443 close, 4301 bytes (4.20 KB) sent, 15401 bytes (15.0 KB) received, lifetime 04:01
[07.26 15:33:41] chrome.exe *64 - rm.api.weibo.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:34:40] GitHub.exe - api.github.com:443 close, 797 bytes sent, 4670 bytes (4.56 KB) received, lifetime 00:11
[07.26 15:34:54] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:34:55] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:34:59] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:35:16] chrome.exe *64 - notifications.google.com:443 close, 19334 bytes (18.8 KB) sent, 8031 bytes (7.84 KB) received, lifetime 09:21
[07.26 15:35:27] chrome.exe *64 - collector.githubapp.com:443 close, 0 bytes sent, 0 bytes received, lifetime 00:16
[07.26 15:35:28] chrome.exe *64 - live.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:35:42] chrome.exe *64 - data.grammarly.com:443 close, 1277 bytes (1.24 KB) sent, 3833 bytes (3.74 KB) received, lifetime 00:11
[07.26 15:35:43] chrome.exe *64 - github.com:443 close, 9626 bytes (9.40 KB) sent, 287127 bytes (280 KB) received, lifetime 00:34
[07.26 15:36:25] chrome.exe *64 - github.com:443 close, 0 bytes sent, 0 bytes received, lifetime 00:30
[07.26 15:36:28] chrome.exe *64 - ssl.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:38:02] chrome.exe *64 - 13.client-channel.google.com:443 close, 474 bytes sent, 4464 bytes (4.35 KB) received, lifetime 01:30
[07.26 15:38:33] MobaXterm.exe - 183.62.156.108:22 open through proxy socks.cse.cuhk.edu.hk:5070 SOCKS5
[07.26 15:39:37] git-remote-https.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:40:32] chrome.exe *64 - people-pa.clients6.google.com:443 close, 5267 bytes (5.14 KB) sent, 17802 bytes (17.3 KB) received, lifetime 04:01
[07.26 15:41:36] chrome.exe *64 - assets-cdn.github.com:443 close, 1227 bytes (1.19 KB) sent, 4890 bytes (4.77 KB) received, lifetime 06:19
[07.26 15:42:02] chrome.exe *64 - lh3.googleusercontent.com:443 close, 2694 bytes (2.63 KB) sent, 8050 bytes (7.86 KB) received, lifetime 07:03
[07.26 15:43:25] YodaoDict.exe - icauh.youdao.com:80 close, 413 bytes sent, 534 bytes received, lifetime 00:05
[07.26 15:44:44] GitHub.exe - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:49:55] GitHub.exe - api.github.com:443 close, 797 bytes sent, 4814 bytes (4.70 KB) received, lifetime 00:11
[07.26 15:52:01] WeChat.exe - short.weixin.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:53:17] WeChat.exe - mmbiz.qpic.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 15:55:21] GitHub.exe - api.github.com:443 close, 797 bytes sent, 4814 bytes (4.70 KB) received, lifetime 00:11
[07.26 15:55:32] chrome.exe *64 - www.evernote.com:443 close, 2487 bytes (2.42 KB) sent, 2361 bytes (2.30 KB) received, lifetime 04:00
[07.26 15:56:57] chrome.exe *64 - safebrowsing.googleapis.com:443 close, 1848 bytes (1.80 KB) sent, 3191 bytes (3.11 KB) received, lifetime 04:00
[07.26 16:00:23] git-remote-https.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:00:34] GitHub.exe - api.github.com:443 close, 781 bytes sent, 6147 bytes (6.00 KB) received, lifetime 00:10
[07.26 16:01:10] GoogleUpdate.exe - tools.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:03:33] WeChat.exe - short.weixin.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:04:59] SogouCloud.exe - security.ie.sogou.com:80 close, 691 bytes sent, 184 bytes received, lifetime <1 sec
[07.26 16:05:13] Dropbox.exe - block.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:06:32] chrome.exe *64 - www.evernote.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:10:17] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:38
[07.26 16:10:32] chrome.exe *64 - www.evernote.com:443 close, 2486 bytes (2.42 KB) sent, 2336 bytes (2.28 KB) received, lifetime 04:00
[07.26 16:10:36] GitHub.exe - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:10:47] GitHub.exe - api.github.com:443 close, 781 bytes sent, 6147 bytes (6.00 KB) received, lifetime 00:11
[07.26 16:11:46] YodaoDict.exe - dict.youdao.com:80 close, 447 bytes sent, 2122 bytes (2.07 KB) received, lifetime 10:01
[07.26 16:12:17] SGTool.exe - p3p.sogou.com:80 close, 441 bytes sent, 139 bytes received, lifetime 01:05
[07.26 16:12:27] chrome.exe *64 - avatars3.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:12:28] chrome.exe *64 - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:13:28] chrome.exe *64 - collector.githubapp.com:443 close, 0 bytes sent, 0 bytes received, lifetime 01:01
[07.26 16:14:56] chrome.exe *64 - assets-cdn.github.com:443 close, 335 bytes sent, 4369 bytes (4.26 KB) received, lifetime 02:24
[07.26 16:14:56] chrome.exe *64 - notifications.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:16:15] GitHub.exe - api.github.com:443 close, 781 bytes sent, 6147 bytes (6.00 KB) received, lifetime 00:11
[07.26 16:16:27] chrome.exe *64 - www.google-analytics.com:443 close, 1430 bytes (1.39 KB) sent, 4938 bytes (4.82 KB) received, lifetime 04:00
[07.26 16:19:26] WeChat.exe - short.weixin.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:21:04] chrome.exe *64 - clients2.google.com:443 close, 154863 bytes (151 KB) sent, 321609 bytes (314 KB) received, lifetime 01:00:12
[07.26 16:21:06] chrome.exe *64 - apis.google.com:443 close, 461 bytes sent, 4217 bytes (4.11 KB) received, lifetime <1 sec
[07.26 16:21:06] chrome.exe *64 - gm1.ggpht.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:21:31] GitHub.exe - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:21:31] GitHub.exe - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:23:13] chrome.exe *64 - live.github.com:443 close, 2931 bytes (2.86 KB) sent, 2089 bytes (2.04 KB) received, lifetime 47:52
[07.26 16:24:03] chrome.exe *64 - assets-cdn.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:24:51] chrome.exe *64 - notifications.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:25:08] chrome.exe *64 - drive-thirdparty.googleusercontent.com:443 close, 484 bytes sent, 4707 bytes (4.59 KB) received, lifetime 00:17
[07.26 16:25:20] chrome.exe *64 - csi.gstatic.com:443 close, 1397 bytes (1.36 KB) sent, 5154 bytes (5.03 KB) received, lifetime 04:01
[07.26 16:26:58] GitHub.exe - api.github.com:443 close, 781 bytes sent, 6147 bytes (6.00 KB) received, lifetime 00:11
[07.26 16:27:18] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:27:18] chrome.exe *64 - www.google.com.hk:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:27:19] chrome.exe *64 - f-log-extension.grammarly.io:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:27:19] chrome.exe *64 - f-log-extension.grammarly.io:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:27:20] chrome.exe *64 - video-hkg3-2.xx.fbcdn.net:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:27:22] chrome.exe *64 - pixel.facebook.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:28:19] chrome.exe *64 - f-log-extension.grammarly.io:443 close, 1364 bytes (1.33 KB) sent, 3626 bytes (3.54 KB) received, lifetime 01:00
[07.26 16:28:27] chrome.exe *64 - pixel.facebook.com:443 close, 1491 bytes (1.45 KB) sent, 5027 bytes (4.90 KB) received, lifetime 01:05
[07.26 16:29:56] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:31:18] chrome.exe *64 - www.google.com:443 close, 1227 bytes (1.19 KB) sent, 2561 bytes (2.50 KB) received, lifetime 04:00
[07.26 16:31:19] chrome.exe *64 - www.google.com.hk:443 close, 3443 bytes (3.36 KB) sent, 67897 bytes (66.3 KB) received, lifetime 04:01
[07.26 16:31:48] git-remote-https.exe - proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:31:50] git-remote-https.exe - proxy.cse.cuhk.edu.hk:5070 close, 999 bytes sent, 4582 bytes (4.47 KB) received, lifetime 00:02
[07.26 16:33:31] chrome.exe *64 - clients6.google.com:443 close, 1915 bytes (1.87 KB) sent, 842 bytes received, lifetime 04:00
[07.26 16:34:31] chrome.exe *64 - github.com:443 close, 2774 bytes (2.70 KB) sent, 27608 bytes (26.9 KB) received, lifetime 00:12
[07.26 16:34:32] chrome.exe *64 - auth.grammarly.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:34:56] chrome.exe *64 - live.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:36:49] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:37:02] Dropbox.exe - bolt.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:37:15] GitHub.exe - api.github.com:443 close, 781 bytes sent, 6147 bytes (6.00 KB) received, lifetime 00:11
[07.26 16:38:13] chrome.exe *64 - github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:38:14] chrome.exe *64 - collector.githubapp.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:40:18] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:38
[07.26 16:40:55] chrome.exe *64 - trello.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:40:55] chrome.exe *64 - apis.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:40:55] chrome.exe *64 - csi.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:43:17] chrome.exe *64 - avatars1.githubusercontent.com:443 close, 1145 bytes (1.11 KB) sent, 666 bytes received, lifetime 05:03
[07.26 16:43:22] chrome.exe *64 - avatars2.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:43:23] chrome.exe *64 - collector.githubapp.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:43:27] chrome.exe *64 - ssl.gstatic.com:443 close, 461 bytes sent, 4784 bytes (4.67 KB) received, lifetime 00:10
[07.26 16:43:45] chrome.exe *64 - user-images.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:43:52] chrome.exe *64 - www.evernote.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:44:14] chrome.exe *64 - clients6.google.com:443 close, 2242 bytes (2.18 KB) sent, 1014 bytes received, lifetime 06:52
[07.26 16:44:14] chrome.exe *64 - clients6.google.com:443 close, 3504 bytes (3.42 KB) sent, 7037 bytes (6.87 KB) received, lifetime 06:52
[07.26 16:44:25] chrome.exe *64 - user-images.githubusercontent.com:443 close, 335 bytes sent, 4369 bytes (4.26 KB) received, lifetime 00:40
[07.26 16:44:25] chrome.exe *64 - user-images.githubusercontent.com:443 close, 335 bytes sent, 4369 bytes (4.26 KB) received, lifetime 00:40
[07.26 16:44:37] chrome.exe *64 - www.ieeeconfpublishing.org:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:44:58] chrome.exe *64 - dblp.uni-trier.de:80 close, 0 bytes sent, 0 bytes received, lifetime 00:15
[07.26 16:45:27] Dropbox.exe - block-edge.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:47:51] chrome.exe *64 - www.googleapis.com:443 close, 1792 bytes (1.75 KB) sent, 6591 bytes (6.43 KB) received, lifetime 04:00
[07.26 16:48:51] chrome.exe *64 - avatars3.githubusercontent.com:443 close, 2194 bytes (2.14 KB) sent, 11695 bytes (11.4 KB) received, lifetime 05:29
[07.26 16:52:56] chrome.exe *64 - accounts.google.com:443 close, 2834 bytes (2.76 KB) sent, 1881 bytes (1.83 KB) received, lifetime 04:05
[07.26 16:54:40] WeChat.exe - qbwup.imtt.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 16:56:52] chrome.exe *64 - clients4.google.com:443 close, 3547 bytes (3.46 KB) sent, 918 bytes received, lifetime 06:12
[07.26 16:57:25] chrome.exe *64 - safebrowsing.googleapis.com:443 close, 1848 bytes (1.80 KB) sent, 2419 bytes (2.36 KB) received, lifetime 04:00
[07.26 16:57:40] YodaoDict.exe - dict.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:01:03] Dropbox.exe - d.dropbox.com:443 close, 1337 bytes (1.30 KB) sent, 4946 bytes (4.83 KB) received, lifetime 01:00
[07.26 17:03:31] chrome.exe *64 - clients4.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:04:33] Dropbox.exe - log.getdropbox.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:04:53] SogouCloud.exe - security.ie.sogou.com:80 close, 735 bytes sent, 184 bytes received, lifetime <1 sec
[07.26 17:05:00] SogouCloud.exe - security.ie.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:05:06] chrome.exe *64 - s.youtube.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:05:06] chrome.exe *64 - yt3.ggpht.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:05:16] Dropbox.exe - client-cf.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:05:21] GitHub.exe - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:05:22] GitHub.exe - avatars.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:05:22] GitHub.exe - avatars.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:05:22] GitHub.exe - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:05:27] GitHub.exe - api.github.com:443 close, 845 bytes sent, 4814 bytes (4.70 KB) received, lifetime 00:11
[07.26 17:05:32] GitHub.exe - api.github.com:443 close, 754 bytes sent, 3108 bytes (3.03 KB) received, lifetime 00:10
[07.26 17:06:07] chrome.exe *64 - f-log-extension.grammarly.io:443 close, 1364 bytes (1.33 KB) sent, 3597 bytes (3.51 KB) received, lifetime 01:00
[07.26 17:07:02] GitHub.exe - avatars.githubusercontent.com:443 close, 620 bytes sent, 33814 bytes (33.0 KB) received, lifetime 01:40
[07.26 17:09:10] chrome.exe *64 - www.google.com:443 close, 4012 bytes (3.91 KB) sent, 6761 bytes (6.60 KB) received, lifetime 04:07
[07.26 17:09:41] WeChat.exe - qbwup.imtt.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:10:23] GitHub.exe - api.github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:10:34] GitHub.exe - api.github.com:443 close, 781 bytes sent, 6728 bytes (6.57 KB) received, lifetime 00:11
[07.26 17:14:08] chrome.exe *64 - clients4.google.com:443 close, 3548 bytes (3.46 KB) sent, 919 bytes received, lifetime 06:56
[07.26 17:15:14] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:33
[07.26 17:15:34] SGTool.exe - config.pinyin.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:15:52] WeChat.exe - short.weixin.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:15:58] SGTool.exe - p3p.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:17:40] YodaoDict.exe - dict.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:18:07] chrome.exe *64 - trello.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:18:08] chrome.exe *64 - secure.quantserve.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:18:09] chrome.exe *64 - c.trello.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:18:09] chrome.exe *64 - www.googletagmanager.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:18:09] chrome.exe *64 - trello.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:18:10] chrome.exe *64 - ssl.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:18:19] chrome.exe *64 - secure.quantserve.com:443 close, 643 bytes sent, 3323 bytes (3.24 KB) received, lifetime 00:11
[07.26 17:18:20] chrome.exe *64 - c.trello.com:443 close, 568 bytes sent, 156 bytes received, lifetime 00:12
[07.26 17:18:20] chrome.exe *64 - c.trello.com:443 close, 568 bytes sent, 156 bytes received, lifetime 00:11
[07.26 17:18:20] chrome.exe *64 - clients4.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:18:20] TeamViewer_Service.exe - server26401.teamviewer.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:20:16] chrome.exe *64 - c.trello.com:443 close, 18252 bytes (17.8 KB) sent, 4768 bytes (4.65 KB) received, lifetime 02:08
[07.26 17:22:05] chrome.exe *64 - clients6.google.com:443 close, 16089 bytes (15.7 KB) sent, 12420 bytes (12.1 KB) received, lifetime 20:10
[07.26 17:23:03] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:23:03] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:23:04] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:23:04] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:23:04] chrome.exe *64 - s1.bdstatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:23:04] chrome.exe *64 - c.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:23:04] chrome.exe *64 - c.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:23:04] chrome.exe *64 - suggestion.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:23:04] chrome.exe *64 - ss.bdimg.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:23:51] chrome.exe *64 - t12.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:48
[07.26 17:23:51] chrome.exe *64 - t12.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:48
[07.26 17:23:51] chrome.exe *64 - play.google.com:443 close, 35212 bytes (34.3 KB) sent, 22125 bytes (21.6 KB) received, lifetime 32:07
[07.26 17:23:51] chrome.exe *64 - clients4.google.com:443 close, 8395 bytes (8.19 KB) sent, 4455 bytes (4.35 KB) received, lifetime 05:31
[07.26 17:23:51] WeChat.exe - 223.167.104.147:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:23:53] chrome.exe *64 - mtalk.google.com:443 close, 696 bytes sent, 5005 bytes (4.88 KB) received, lifetime 00:07
[07.26 17:23:54] chrome.exe *64 - mtalk.google.com:5228 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:23:55] chrome.exe *64 - www.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:23:55] chrome.exe *64 - awqiizxequsixgr:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:23:59] chrome.exe *64 - www.baidu.com:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:23:59] chrome.exe *64 - c.baidu.com:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:23:59] chrome.exe *64 - s1.bdstatic.com:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:23:59] chrome.exe *64 - s1.bdstatic.com:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:23:59] chrome.exe *64 - ss.bdimg.com:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:23:59] chrome.exe *64 - ss.bdimg.com:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:23:59] chrome.exe *64 - ss.bdimg.com:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:24:02] chrome.exe *64 - clients4.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:24:02] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:24:02] chrome.exe *64 - wpad:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:24:08] WeChat.exe - www.qq.com:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 17:24:09] WeChat.exe - short.weixin.qq.com:80 close, 357 bytes sent, 468 bytes received, lifetime <1 sec
[07.26 17:27:41] YodaoDict.exe - dict.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:27:47] chrome.exe *64 - dj1.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:33
[07.26 17:27:58] chrome.exe *64 - t11.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:27:58] chrome.exe *64 - b1.bdstatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:28:12] chrome.exe *64 - www.google.com:443 close, 1747 bytes (1.70 KB) sent, 1218 bytes (1.18 KB) received, lifetime 04:00
[07.26 17:28:21] chrome.exe *64 - b1.bdstatic.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:23
[07.26 17:28:22] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:28:52] chrome.exe *64 - www.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:28:55] chrome.exe *64 - www.baidu.com:80 close, 11662 bytes (11.3 KB) sent, 7986 bytes (7.79 KB) received, lifetime 00:02
[07.26 17:28:56] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:28:57] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:28:57] chrome.exe *64 - sclick.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:29:08] chrome.exe *64 - t12.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 17:29:11] chrome.exe *64 - cpro.baidustatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:29:11] chrome.exe *64 - cpro.baidustatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:29:11] chrome.exe *64 - cpro.baidustatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:29:12] chrome.exe *64 - bdimg.share.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:29:17] chrome.exe *64 - t12.baidu.com:80 close, 1915 bytes (1.87 KB) sent, 4198 bytes (4.09 KB) received, lifetime 00:24
[07.26 17:29:21] HiSuiteDownLoader.exe - update.hicloud.com:8180 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.26 17:29:26] chrome.exe *64 - suggestion.baidu.com:80 close, 3808 bytes (3.71 KB) sent, 2035 bytes (1.98 KB) received, lifetime 00:35
[07.26 17:29:32] chrome.exe *64 - f12.baidu.com:80 close, 1634 bytes (1.59 KB) sent, 15284 bytes (14.9 KB) received, lifetime 00:21
[07.26 17:29:33] chrome.exe *64 - f12.baidu.com:80 close, 1640 bytes (1.60 KB) sent, 17802 bytes (17.3 KB) received, lifetime 00:22
[07.26 17:29:33] chrome.exe *64 - f12.baidu.com:80 close, 3281 bytes (3.20 KB) sent, 102173 bytes (99.7 KB) received, lifetime 00:22
[07.26 17:29:42] chrome.exe *64 - c.csdnimg.cn:80 close, 0 bytes sent, 0 bytes received, lifetime 00:30
[07.26 17:29:42] chrome.exe *64 - avatar.csdn.net:80 close, 0 bytes sent, 0 bytes received, lifetime 00:30
[07.26 17:30:02] chrome.exe *64 - csdnimg.cn:80 error : A connection request was canceled before the completion. 
[07.26 17:30:02] chrome.exe *64 - hm.baidu.com:443 error : A connection request was canceled before the completion. 
[07.26 17:30:11] chrome.exe *64 - cpro.baidustatic.com:80 close, 0 bytes sent, 0 bytes received, lifetime 01:00
[07.26 17:30:16] 360AP.exe - intf.zsall.mobilem.360.cn:80 close, 314 bytes sent, 1035 bytes (1.01 KB) received, lifetime 00:30
[07.26 17:30:19] chrome.exe *64 - csdnimg.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:30:21] chrome.exe *64 - dc.csdn.net:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:30:21] chrome.exe *64 - dc.csdn.net:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:30:21] chrome.exe *64 - dc.csdn.net:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:30:31] chrome.exe *64 - csdnimg.cn:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 17:30:31] chrome.exe *64 - csdnimg.cn:80 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 17:30:31] chrome.exe *64 - dc.csdn.net:80 close, 0 bytes sent, 0 bytes received, lifetime 00:10
[07.26 17:30:32] chrome.exe *64 - csdnimg.cn:80 error : A connection request was canceled before the completion. 
[07.26 17:30:33] chrome.exe *64 - ubmcmm.baidustatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:30:33] chrome.exe *64 - cpro.baidustatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:30:35] chrome.exe *64 - beacon.tingyun.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:30:49] chrome.exe *64 - blog.csdn.net:80 close, 670 bytes sent, 421 bytes received, lifetime 00:29
[07.26 17:31:13] chrome.exe *64 - static.blog.csdn.net:80 close, 2188 bytes (2.13 KB) sent, 41402 bytes (40.4 KB) received, lifetime 02:03
[07.26 17:31:22] chrome.exe *64 - dc.csdn.net:80 close, 1822 bytes (1.77 KB) sent, 754 bytes received, lifetime 01:01
[07.26 17:31:23] chrome.exe *64 - eclick.baidu.com:443 close, 338 bytes sent, 3597 bytes (3.51 KB) received, lifetime 00:50
[07.26 17:31:23] chrome.exe *64 - wn.pos.baidu.com:443 close, 0 bytes sent, 0 bytes received, lifetime 00:48
[07.26 17:31:35] chrome.exe *64 - beacon.tingyun.com:80 close, 42307 bytes (41.3 KB) sent, 279 bytes received, lifetime 01:00
[07.26 17:32:20] chrome.exe *64 - c.csdnimg.cn:80 close, 1314 bytes (1.28 KB) sent, 27289 bytes (26.6 KB) received, lifetime 03:09
[07.26 17:32:21] chrome.exe *64 - csdnimg.cn:80 close, 1278 bytes (1.24 KB) sent, 6854 bytes (6.69 KB) received, lifetime 02:02
[07.26 17:34:25] Dropbox.exe - block-edge.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:34:46] chrome.exe *64 - clients4.google.com:443 close, 20249 bytes (19.7 KB) sent, 20243 bytes (19.7 KB) received, lifetime 10:34
[07.26 17:36:32] chrome.exe *64 - mail.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:39:34] chrome.exe *64 - mtalk.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:40:10] chrome.exe *64 - lh3.googleusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:40:22] chrome.exe *64 - www.gstatic.com:443 close, 461 bytes sent, 4784 bytes (4.67 KB) received, lifetime 00:12
[07.26 17:40:22] chrome.exe *64 - www.gstatic.com:443 close, 461 bytes sent, 4784 bytes (4.67 KB) received, lifetime 00:12
[07.26 17:41:54] chrome.exe *64 - fonts.gstatic.com:443 close, 461 bytes sent, 4786 bytes (4.67 KB) received, lifetime 01:03
[07.26 17:41:54] chrome.exe *64 - engine.adzerk.net:443 close, 0 bytes sent, 0 bytes received, lifetime 01:00
[07.26 17:41:57] chrome.exe *64 - engine.adzerk.net:443 close, 2508 bytes (2.44 KB) sent, 7073 bytes (6.90 KB) received, lifetime 01:03
[07.26 17:44:25] chrome.exe *64 - clients6.google.com:443 close, 6703 bytes (6.54 KB) sent, 5167 bytes (5.04 KB) received, lifetime 10:52
[07.26 17:45:21] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:39
[07.26 17:45:58] chrome.exe *64 - www.evernote.com:443 close, 7238 bytes (7.06 KB) sent, 12813 bytes (12.5 KB) received, lifetime 05:46
[07.26 17:46:17] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:46:30] chrome.exe *64 - googleads.g.doubleclick.net:443 close, 749 bytes sent, 229 bytes received, lifetime 00:11
[07.26 17:46:44] chrome.exe *64 - pubads.g.doubleclick.net:443 close, 476 bytes sent, 4417 bytes (4.31 KB) received, lifetime 00:23
[07.26 17:46:45] chrome.exe *64 - www.evernote.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:47:56] chrome.exe *64 - engine.adzerk.net:443 close, 2419 bytes (2.36 KB) sent, 4742 bytes (4.63 KB) received, lifetime 01:07
[07.26 17:49:26] WeChat.exe - short.weixin.qq.com:80 close, 357 bytes sent, 484 bytes received, lifetime <1 sec
[07.26 17:50:46] chrome.exe *64 - ogs.google.com:443 close, 2476 bytes (2.41 KB) sent, 2643 bytes (2.58 KB) received, lifetime 04:28
[07.26 17:51:56] chrome.exe *64 - sb.scorecardresearch.com:443 close, 3818 bytes (3.72 KB) sent, 6276 bytes (6.12 KB) received, lifetime 11:02
[07.26 17:52:05] Dropbox.exe - client-cf.dropbox.com:443 close, 3460 bytes (3.37 KB) sent, 5211 bytes (5.08 KB) received, lifetime 01:04
[07.26 17:55:33] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 17:57:54] WeChat.exe - short.weixin.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:02:20] WeChat.exe - short.weixin.qq.com:80 close, 409 bytes sent, 161 bytes received, lifetime <1 sec
[07.26 18:04:49] chrome.exe *64 - c.trello.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:04:49] chrome.exe *64 - secure.quantserve.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:04:50] chrome.exe *64 - trello.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:04:54] SogouCloud.exe - security.ie.sogou.com:80 close, 735 bytes sent, 184 bytes received, lifetime <1 sec
[07.26 18:05:01] chrome.exe *64 - c.trello.com:443 close, 326 bytes sent, 5710 bytes (5.57 KB) received, lifetime 00:12
[07.26 18:05:01] chrome.exe *64 - www.google.com.hk:443 close, 734 bytes sent, 229 bytes received, lifetime 00:12
[07.26 18:05:01] SogouCloud.exe - security.ie.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:05:22] chrome.exe *64 - play.google.com:443 close, 2130 bytes (2.08 KB) sent, 1009 bytes received, lifetime 03:24
[07.26 18:05:22] chrome.exe *64 - f-log-extension.grammarly.io:443 close, 1363 bytes (1.33 KB) sent, 3549 bytes (3.46 KB) received, lifetime 00:33
[07.26 18:05:22] chrome.exe *64 - clients4.google.com:443 close, 7063 bytes (6.89 KB) sent, 15399 bytes (15.0 KB) received, lifetime 00:21
[07.26 18:05:24] chrome.exe *64 - trello.com:443 close, 2168 bytes (2.11 KB) sent, 605 bytes received, lifetime 00:34
[07.26 18:05:24] chrome.exe *64 - qa.sockets.stackexchange.com:443 close, 1603 bytes (1.56 KB) sent, 527 bytes received, lifetime 22:32
[07.26 18:05:25] WeChat.exe - 203.205.151.204:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 18:55:22] SogouCloud.exe - get.sogou.com:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 18:55:22] WeChat.exe - 120.204.0.139:8080 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 18:55:22] WeChat.exe - 101.226.211.46:8080 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.26 18:55:23] WeChat.exe - short.weixin.qq.com:80 close, 484 bytes sent, 30152 bytes (29.4 KB) received, lifetime <1 sec
[07.26 18:55:23] SGTool.exe - info.pinyin.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:55:36] chrome.exe *64 - s.youtube.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:55:37] chrome.exe *64 - f-log-extension.grammarly.io:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:55:37] chrome.exe *64 - lh6.googleusercontent.com:443 close, 471 bytes sent, 4693 bytes (4.58 KB) received, lifetime 00:10
[07.26 18:55:37] chrome.exe *64 - ssl.gstatic.com:443 close, 461 bytes sent, 4786 bytes (4.67 KB) received, lifetime 00:10
[07.26 18:55:48] chrome.exe *64 - www.gstatic.com:443 close, 461 bytes sent, 4785 bytes (4.67 KB) received, lifetime 00:12
[07.26 18:55:54] SGTool.exe - ping.pinyin.sogou.com:80 close, 561 bytes sent, 166 bytes received, lifetime 00:30
[07.26 18:56:11] chrome.exe *64 - fonts.gstatic.com:443 close, 463 bytes sent, 4786 bytes (4.67 KB) received, lifetime 00:33
[07.26 18:56:11] chrome.exe *64 - securepubads.g.doubleclick.net:443 close, 476 bytes sent, 4417 bytes (4.31 KB) received, lifetime 00:33
[07.26 18:56:29] chrome.exe *64 - r1---sn-i3beln7k.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:56:29] chrome.exe *64 - fonts.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:57:01] chrome.exe *64 - r3---sn-i3b7kn7r.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:57:03] chrome.exe *64 - odr.mookie1.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:57:15] chrome.exe *64 - d.agkn.com:443 close, 324 bytes sent, 2764 bytes (2.69 KB) received, lifetime 00:12
[07.26 18:57:23] Dropbox.exe - client-cf.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:58:17] chrome.exe *64 - play.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:58:45] Dropbox.exe - log.getdropbox.com:80 close, 739 bytes sent, 404 bytes received, lifetime <1 sec
[07.26 18:59:41] WeChat.exe - 203.205.151.204:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:59:43] WeChat.exe - short.weixin.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 18:59:43] chrome.exe *64 - hangouts.google.com:443 close, 13281 bytes (12.9 KB) sent, 19539 bytes (19.0 KB) received, lifetime 04:17
[07.26 19:00:18] WeChat.exe - qbwup.imtt.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:32] WeChat.exe - 203.205.150.89:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:47] tencentdl.exe - xf-com-update-doctor.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:47] tencentdl.exe - pdlxf-doctor.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:52] tencentdl.exe - puui.qpic.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:52] tencentdl.exe - fs-conn-doctor.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:52] tencentdl.exe - puui.qpic.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:52] tencentdl.exe - fs-conn-doctor.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:52] tencentdl.exe - fs-conn-doctor.qq.com:443 close, 184 bytes sent, 134 bytes received, lifetime <1 sec
[07.26 19:00:52] tencentdl.exe - puui.qpic.cn:80 close, 379 bytes sent, 7852 bytes (7.66 KB) received, lifetime <1 sec
[07.26 19:00:52] tencentdl.exe - fs-conn-doctor.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:52] tencentdl.exe - puui.qpic.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:52] tencentdl.exe - fs-conn-doctor.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:52] tencentdl.exe - fs-conn-doctor.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:52] tencentdl.exe - puui.qpic.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:52] tencentdl.exe - puui.qpic.cn:80 close, 384 bytes sent, 8537 bytes (8.33 KB) received, lifetime <1 sec
[07.26 19:00:52] tencentdl.exe - puui.qpic.cn:80 close, 379 bytes sent, 9409 bytes (9.18 KB) received, lifetime <1 sec
[07.26 19:00:52] tencentdl.exe - puui.qpic.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:00:53] tencentdl.exe - puui.qpic.cn:80 close, 379 bytes sent, 6983 bytes (6.81 KB) received, lifetime 00:01
[07.26 19:00:53] tencentdl.exe - puui.qpic.cn:80 close, 384 bytes sent, 8601 bytes (8.39 KB) received, lifetime 00:01
[07.26 19:00:53] tencentdl.exe - fs-conn-doctor.qq.com:443 close, 184 bytes sent, 134 bytes received, lifetime 00:01
[07.26 19:01:00] QQPlayer.exe - btrace.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:01:00] QQPlayer.exe - btrace.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:01:02] chrome.exe *64 - pubads.g.doubleclick.net:443 close, 6044 bytes (5.90 KB) sent, 22964 bytes (22.4 KB) received, lifetime 05:24
[07.26 19:01:05] chrome.exe *64 - fonts.gstatic.com:443 close, 909 bytes sent, 12593 bytes (12.2 KB) received, lifetime 04:00
[07.26 19:01:46] tencentdl.exe - local-p2p.qq.com:443 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 503
[07.26 19:01:46] tencentdl.exe - local-p2p.qq.com:443 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 503
[07.26 19:02:12] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:03:02] chrome.exe *64 - cm.g.doubleclick.net:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:04:11] chrome.exe *64 - suggestion.baidu.com:80 close, 1383 bytes (1.35 KB) sent, 392 bytes received, lifetime 00:31
[07.26 19:04:52] SogouCloud.exe - security.ie.sogou.com:80 close, 735 bytes sent, 184 bytes received, lifetime 00:01
[07.26 19:08:04] chrome.exe *64 - eclick.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:08:09] chrome.exe *64 - eclick.baidu.com:80 close, 1065 bytes (1.04 KB) sent, 311 bytes received, lifetime 00:05
[07.26 19:10:16] chrome.exe *64 - safebrowsing.googleapis.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:14:16] chrome.exe *64 - safebrowsing.googleapis.com:443 close, 1571 bytes (1.53 KB) sent, 7822 bytes (7.63 KB) received, lifetime 04:00
[07.26 19:20:40] SGTool.exe - input.shouji.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:21:04] WeChat.exe - 203.205.150.89:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:22:13] WeChat.exe - short.weixin.qq.com:80 close, 425 bytes sent, 161 bytes received, lifetime <1 sec
[07.26 19:22:20] chrome.exe *64 - www.googleadservices.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:22:23] chrome.exe *64 - fonts.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:22:32] chrome.exe *64 - r1---sn-i3beln7k.googlevideo.com:443 close, 733 bytes sent, 151 bytes received, lifetime 00:12
[07.26 19:22:32] chrome.exe *64 - f-log-extension.grammarly.io:443 close, 0 bytes sent, 0 bytes received, lifetime 00:11
[07.26 19:22:54] chrome.exe *64 - r16---sn-i3b7knez.googlevideo.com:443 close, 6014 bytes (5.87 KB) sent, 904350 bytes (883 KB) received, lifetime 00:30
[07.26 19:22:54] chrome.exe *64 - fonts.gstatic.com:443 close, 463 bytes sent, 4787 bytes (4.67 KB) received, lifetime 00:31
[07.26 19:30:56] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:37
[07.26 19:31:35] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:40:21] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.26 19:44:17] chrome.exe *64 - www.evernote.com:443 close, 2487 bytes (2.42 KB) sent, 2361 bytes (2.30 KB) received, lifetime 04:00
[07.26 19:46:22] Dropbox.exe - client-cf.dropbox.com:443 close, 1573 bytes (1.53 KB) sent, 16743 bytes (16.3 KB) received, lifetime 01:01
[07.26 19:52:45] Dropbox.exe - client-cf.dropbox.com:443 close, 5129 bytes (5.00 KB) sent, 17057 bytes (16.6 KB) received, lifetime 01:01
[07.26 19:53:44] chrome.exe *64 - s.youtube.com:443 close, 733 bytes sent, 292 bytes received, lifetime 04:00
[07.26 19:55:32] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.26 19:55:54] SGTool.exe - tc.dl.pinyin.sogoucdn.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 19:56:54] SGTool.exe - tc.dl.pinyin.sogoucdn.com:80 close, 1537 bytes (1.50 KB) sent, 7856 bytes (7.67 KB) received, lifetime 01:00
[07.26 19:56:54] SGTool.exe - tc.dl.pinyin.sogoucdn.com:80 close, 799 bytes sent, 346 bytes received, lifetime 01:00
[07.26 20:05:18] WeChat.exe - short.weixin.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 20:06:15] SGTool.exe - p3p.sogou.com:80 close, 441 bytes sent, 139 bytes received, lifetime <1 sec
[07.26 20:10:33] Dropbox.exe - client-lb.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.26 20:12:05] chrome.exe *64 - mtalk.google.com:443 close, 696 bytes sent, 5004 bytes (4.88 KB) received, lifetime 14:59
[07.26 20:12:29] chrome.exe *64 - mtalk.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 20:15:24] BSvcProcessor.exe - g.msn.com:80 close, 170 bytes sent, 370 bytes received, lifetime 00:01
[07.26 20:19:32] chrome.exe *64 - play.google.com:443 close, 4837 bytes (4.72 KB) sent, 7412 bytes (7.23 KB) received, lifetime 14:02
[07.26 20:25:16] WeChat.exe - 203.205.151.164:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 20:25:21] WeChat.exe - qbwup.imtt.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 20:25:37] WeChat.exe - short.weixin.qq.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:16:24] WeChat.exe - 203.205.148.84:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:18:02] chrome.exe *64 - clients5.google.com:443 close, 465 bytes sent, 4790 bytes (4.67 KB) received, lifetime 01:08
[07.26 21:20:40] chrome.exe *64 - www.google.com:443 close, 3218 bytes (3.14 KB) sent, 24541 bytes (23.9 KB) received, lifetime 10:42
[07.26 21:20:55] chrome.exe *64 - www.googleapis.com:443 close, 1736 bytes (1.69 KB) sent, 1052 bytes (1.02 KB) received, lifetime 04:00
[07.26 21:20:56] chrome.exe *64 - www.googleapis.com:443 close, 1106 bytes (1.08 KB) sent, 1167 bytes (1.13 KB) received, lifetime 04:00
[07.26 21:20:57] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:34
[07.26 21:25:37] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.26 21:29:23] chrome.exe *64 - s.youtube.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:29:25] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:29:27] chrome.exe *64 - mtalk.google.com:5228 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.26 21:29:49] chrome.exe *64 - mtalk.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:29:51] chrome.exe *64 - ogs.google.com:443 close, 461 bytes sent, 4217 bytes (4.11 KB) received, lifetime 00:29
[07.26 21:29:51] chrome.exe *64 - lh3.googleusercontent.com:443 close, 471 bytes sent, 4694 bytes (4.58 KB) received, lifetime 00:29
[07.26 21:29:51] chrome.exe *64 - r2---sn-i3b7kne6.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:30:59] chrome.exe *64 - r2---sn-i3b7kne6.googlevideo.com:443 close, 17742 bytes (17.3 KB) sent, 11581393 bytes (11.0 MB) received, lifetime 01:08
[07.26 21:31:57] chrome.exe *64 - www.evernote.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:32:02] chrome.exe *64 - clients1.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:32:05] chrome.exe *64 - r3---sn-i3beln7k.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:32:33] chrome.exe *64 - r17---sn-p5qlsn67.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:32:34] chrome.exe *64 - r17---sn-p5qlsn67.googlevideo.com:443 close, 1173 bytes (1.14 KB) sent, 11041 bytes (10.7 KB) received, lifetime 00:01
[07.26 21:33:52] chrome.exe *64 - www.googletagservices.com:443 close, 983 bytes sent, 4752 bytes (4.64 KB) received, lifetime 04:00
[07.26 21:34:51] Dropbox.exe - d.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:35:27] chrome.exe *64 - f-log-extension.grammarly.io:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:35:52] Dropbox.exe - d.dropbox.com:443 close, 1021 bytes sent, 4906 bytes (4.79 KB) received, lifetime 01:01
[07.26 21:35:59] WeChat.exe - qbwup.imtt.qq.com:80 close, 494 bytes sent, 208 bytes received, lifetime 00:35
[07.26 21:36:05] chrome.exe *64 - static.doubleclick.net:443 close, 2553 bytes (2.49 KB) sent, 641 bytes received, lifetime 06:39
[07.26 21:36:28] chrome.exe *64 - yt3.ggpht.com:443 close, 7886 bytes (7.70 KB) sent, 69275 bytes (67.6 KB) received, lifetime 07:05
[07.26 21:37:26] chrome.exe *64 - r3---sn-i3belnez.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:37:35] chrome.exe *64 - pagead2.googlesyndication.com:443 close, 8439 bytes (8.24 KB) sent, 41643 bytes (40.6 KB) received, lifetime 07:44
[07.26 21:39:33] chrome.exe *64 - securepubads.g.doubleclick.net:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:39:37] chrome.exe *64 - tpc.googlesyndication.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:39:37] chrome.exe *64 - i1.ytimg.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:41:27] chrome.exe *64 - plus.google.com:443 close, 2130 bytes (2.08 KB) sent, 1035 bytes (1.01 KB) received, lifetime 05:41
[07.26 21:42:16] chrome.exe *64 - r3---sn-i3b7kn7s.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:42:16] chrome.exe *64 - r3---sn-i3b7kn7s.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:45:14] chrome.exe *64 - mtalk.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:45:51] chrome.exe *64 - static.zhihu.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:45:53] chrome.exe *64 - 0-edge-chat.facebook.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:46:09] chrome.exe *64 - fonts.gstatic.com:443 close, 733 bytes sent, 229 bytes received, lifetime 00:17
[07.26 21:46:09] chrome.exe *64 - www.zhihu.com:443 close, 327 bytes sent, 3699 bytes (3.61 KB) received, lifetime 00:18
[07.26 21:46:09] chrome.exe *64 - zhihu-web-analytics.zhihu.com:443 close, 343 bytes sent, 3122 bytes (3.04 KB) received, lifetime 00:17
[07.26 21:46:57] chrome.exe *64 - www.evernote.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:47:00] chrome.exe *64 - www.google.com:443 close, 13303 bytes (12.9 KB) sent, 2325 bytes (2.27 KB) received, lifetime 04:53
[07.26 21:47:02] chrome.exe *64 - scontent-hkg3-2.xx.fbcdn.net:443 close, 3732 bytes (3.64 KB) sent, 804155 bytes (785 KB) received, lifetime 01:10
[07.26 21:47:23] chrome.exe *64 - github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:47:24] chrome.exe *64 - avatars0.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:47:25] chrome.exe *64 - collector.githubapp.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:47:29] chrome.exe *64 - pagead2.googlesyndication.com:443 close, 16398 bytes (16.0 KB) sent, 93642 bytes (91.4 KB) received, lifetime 07:56
[07.26 21:48:19] chrome.exe *64 - video-hkg3-2.xx.fbcdn.net:443 close, 58373 bytes (57.0 KB) sent, 8896991 bytes (8.48 MB) received, lifetime 02:25
[07.26 21:49:53] chrome.exe *64 - ssl.google-analytics.com:443 close, 974 bytes sent, 4680 bytes (4.57 KB) received, lifetime 04:02
[07.26 21:50:06] Dropbox.exe - client-cf.dropbox.com:443 close, 9985 bytes (9.75 KB) sent, 9719 bytes (9.49 KB) received, lifetime 01:27
[07.26 21:50:07] Dropbox.exe - client-cf.dropbox.com:443 close, 22083 bytes (21.5 KB) sent, 71944 bytes (70.2 KB) received, lifetime 01:28
[07.26 21:51:36] chrome.exe *64 - github.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:52:45] chrome.exe *64 - avatars0.githubusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:52:46] chrome.exe *64 - collector.githubapp.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 21:53:01] chrome.exe *64 - api.github.com:443 close, 328 bytes sent, 3642 bytes (3.55 KB) received, lifetime 00:15
[07.26 21:53:24] chrome.exe *64 - avatars0.githubusercontent.com:443 close, 335 bytes sent, 4369 bytes (4.26 KB) received, lifetime 00:39
[07.26 21:53:24] chrome.exe *64 - avatars0.githubusercontent.com:443 close, 568 bytes sent, 160 bytes received, lifetime 00:39
[07.26 21:55:16] WeChat.exe - 203.205.151.204:80 close, 357 bytes sent, 484 bytes received, lifetime <1 sec
[07.26 22:01:22] Dropbox.exe - d.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 22:01:31] msfeedssync.exe *64 - go.microsoft.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 22:01:31] msfeedssync.exe *64 - rssgov.windows.microsoft.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 22:01:32] msfeedssync.exe *64 - go.microsoft.com:80 close, 717 bytes sent, 357 bytes received, lifetime 00:01
[07.26 22:02:23] Dropbox.exe - d.dropbox.com:443 close, 6579 bytes (6.42 KB) sent, 5944 bytes (5.80 KB) received, lifetime 01:01
[07.26 22:02:49] chrome.exe *64 - www.google.com.hk:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 22:02:53] chrome.exe *64 - clients5.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 22:02:54] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 22:02:56] chrome.exe *64 - clients4.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 22:03:11] chrome.exe *64 - 15.client-channel.google.com:443 close, 474 bytes sent, 4464 bytes (4.35 KB) received, lifetime 00:11
[07.26 22:05:04] QQProtectUpd.exe - qd-update.qq.com:8080 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.26 22:05:05] SogouCloud.exe - security.ie.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.26 22:05:59] chrome.exe *64 - play.google.com:443 close, 40925 bytes (39.9 KB) sent, 33398 bytes (32.6 KB) received, lifetime 30:55
[07.26 22:05:59] chrome.exe *64 - people-pa.clients6.google.com:443 close, 1836 bytes (1.79 KB) sent, 4868 bytes (4.75 KB) received, lifetime 03:00
[07.26 22:05:59] chrome.exe *64 - www.google.com.hk:443 close, 4710 bytes (4.59 KB) sent, 71939 bytes (70.2 KB) received, lifetime 03:10
[07.26 22:06:02] SGTool.exe - info.pinyin.sogou.com:80 close, 1041 bytes (1.01 KB) sent, 607 bytes received, lifetime 00:04
[07.26 22:06:02] SGTool.exe - config.pinyin.sogou.com:80 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy closed the connection unexpectedly.
[07.27 03:00:17] Dropbox.exe - client.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001
[07.27 03:00:18] Dropbox.exe - client-cf.dropbox.com:443 error : A connection request was canceled before the completion. 
[07.27 03:00:19] Dropbox.exe - bolt.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:20] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:21] chrome.exe *64 - clients5.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:21] chrome.exe *64 - clients5.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:21] chrome.exe *64 - clients5.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:21] chrome.exe *64 - hangouts.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:21] chrome.exe *64 - clientservices.googleapis.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:21] chrome.exe *64 - clients5.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:21] chrome.exe *64 - clients5.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:21] chrome.exe *64 - play.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:27] Dropbox.exe - bolt.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 03:00:29] chrome.exe *64 - ssl.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:00:34] SogouCloud.exe - get.sogou.com:80 close, 651 bytes sent, 328 bytes received, lifetime <1 sec
[07.27 03:01:09] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:01:10] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:01:10] SogouCloud.exe - get.sogou.com:80 close, 671 bytes sent, 328 bytes received, lifetime <1 sec
[07.27 03:01:20] Dropbox.exe - client-lb.dropbox.com:443 close, 1531 bytes (1.49 KB) sent, 16765 bytes (16.3 KB) received, lifetime 01:01
[07.27 03:05:27] YodaoDict.exe - dict.youdao.com:80 close, 941 bytes sent, 163 bytes received, lifetime 05:00
[07.27 03:05:29] chrome.exe *64 - clients2.google.com:443 close, 733 bytes sent, 292 bytes received, lifetime 04:00
[07.27 03:12:24] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:15:43] chrome.exe *64 - mtalk.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:25:29] chrome.exe *64 - clients2.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:35:18] chrome.exe *64 - play.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:36:32] chrome.exe *64 - mail.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 03:47:56] Dropbox.exe - client-cf.dropbox.com:443 close, 5129 bytes (5.00 KB) sent, 17049 bytes (16.6 KB) received, lifetime 01:01
[07.27 04:00:12] SogouCloud.exe - security.ie.sogou.com:80 close, 735 bytes sent, 184 bytes received, lifetime 00:01
[07.27 04:00:18] Dropbox.exe - client-cf.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 04:00:27] Dropbox.exe - d.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 04:01:40] SGTool.exe - config.pinyin.sogou.com:80 close, 545 bytes sent, 278 bytes received, lifetime 01:05
[07.27 04:05:01] QQProtectUpd.exe - qdun-data.qq.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 04:09:24] chrome.exe *64 - play.google.com:443 close, 8399 bytes (8.20 KB) sent, 9521 bytes (9.29 KB) received, lifetime 09:01
[07.27 04:12:38] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 04:17:03] chrome.exe *64 - mtalk.google.com:5228 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.27 04:19:13] chrome.exe *64 - www.evernote.com:443 close, 2214 bytes (2.16 KB) sent, 5451 bytes (5.32 KB) received, lifetime 04:00
[07.27 04:19:15] Dropbox.exe - d.dropbox.com:443 close, 1821 bytes (1.77 KB) sent, 4970 bytes (4.85 KB) received, lifetime 01:01
[07.27 04:19:49] chrome.exe *64 - play.google.com:443 close, 6339 bytes (6.19 KB) sent, 3969 bytes (3.87 KB) received, lifetime 04:22
[07.27 04:19:50] chrome.exe *64 - www.googleadservices.com:443 close, 1188 bytes (1.16 KB) sent, 5007 bytes (4.88 KB) received, lifetime 04:00
[07.27 04:38:15] chrome.exe *64 - mtalk.google.com:5228 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.27 04:44:20] chrome.exe *64 - clients4.google.com:443 close, 1738 bytes (1.69 KB) sent, 487 bytes received, lifetime 04:00
[07.27 04:51:30] chrome.exe *64 - play.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 04:53:56] chrome.exe *64 - clients6.google.com:443 close, 2867 bytes (2.79 KB) sent, 1014 bytes received, lifetime 05:12
[07.27 05:00:13] SogouCloud.exe - security.ie.sogou.com:80 close, 691 bytes sent, 184 bytes received, lifetime <1 sec
[07.27 05:00:22] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 05:00:49] SGTool.exe - info.pinyin.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:00:50] SGTool.exe - tc.dl.pinyin.sogoucdn.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:00:52] SGTool.exe - tc.dl.pinyin.sogoucdn.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:00:53] SGTool.exe - tc.dl.pinyin.sogoucdn.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:00:53] SGTool.exe - tc.dl.pinyin.sogoucdn.com:80 close, 158 bytes sent, 33899 bytes (33.1 KB) received, lifetime <1 sec
[07.27 05:00:55] SGTool.exe - tc.dl.pinyin.sogoucdn.com:80 close, 158 bytes sent, 46624 bytes (45.5 KB) received, lifetime <1 sec
[07.27 05:01:12] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:02:32] chrome.exe *64 - qa.sockets.stackexchange.com:443 close, 4755 bytes (4.64 KB) sent, 4917 bytes (4.80 KB) received, lifetime 02:02:11
[07.27 05:04:15] chrome.exe *64 - www.evernote.com:443 close, 2485 bytes (2.42 KB) sent, 2359 bytes (2.30 KB) received, lifetime 04:01
[07.27 05:04:38] chrome.exe *64 - 13.client-channel.google.com:443 close, 14267 bytes (13.9 KB) sent, 36698 bytes (35.8 KB) received, lifetime 48:06
[07.27 05:05:50] chrome.exe *64 - qa.sockets.stackexchange.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:05:51] chrome.exe *64 - 13.client-channel.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:05:52] chrome.exe *64 - 13.client-channel.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:05:52] chrome.exe *64 - 13.client-channel.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:05:53] chrome.exe *64 - www.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:05:53] Dropbox.exe - client-cf.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:05:54] chrome.exe *64 - qa.sockets.stackexchange.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:05:54] chrome.exe *64 - 16.client-channel.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:05:56] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:06:01] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:06:08] chrome.exe *64 - www.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:06:09] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:06:13] chrome.exe *64 - 16.client-channel.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:06:15] chrome.exe *64 - www.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:06:22] chrome.exe *64 - mtalk.google.com:5228 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:06:32] chrome.exe *64 - www.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:06:40] Dropbox.exe - client-lb.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:07:02] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:07:06] chrome.exe *64 - 16.client-channel.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:08:27] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:08:29] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:09:52] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:10:59] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:11:29] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:11:34] chrome.exe *64 - 16.client-channel.google.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:12:56] Dropbox.exe - bolt.dropbox.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061
[07.27 05:14:50] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:18:06] chrome.exe *64 - clients4.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:20:57] chrome.exe *64 - clients6.google.com:443 close, 3215 bytes (3.13 KB) sent, 1174 bytes (1.14 KB) received, lifetime 06:07
[07.27 05:21:11] SGTool.exe - p3p.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:26:50] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:29:51] Dropbox.exe - bolt.dropbox.com:443 close, 15427 bytes (15.0 KB) sent, 8268 bytes (8.07 KB) received, lifetime 15:31
[07.27 05:30:23] Dropbox.exe - client-cf.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:30:23] Dropbox.exe - client-cf.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 05:30:41] chrome.exe *64 - clients6.google.com:443 close, 6895 bytes (6.73 KB) sent, 3475 bytes (3.39 KB) received, lifetime 01:39
[07.27 05:32:52] chrome.exe *64 - clients4.google.com:443 close, 2302 bytes (2.24 KB) sent, 5187 bytes (5.06 KB) received, lifetime 02:02
[07.27 05:45:16] chrome.exe *64 - www.evernote.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 05:45:24] Dropbox.exe - block.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 06:00:11] YodaoDict.exe - cidian.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 06:00:14] SogouCloud.exe - security.ie.sogou.com:80 close, 691 bytes sent, 184 bytes received, lifetime <1 sec
[07.27 06:00:25] Dropbox.exe - client-cf.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 06:01:12] SGTool.exe - info.pinyin.sogou.com:80 close, 1035 bytes (1.01 KB) sent, 40820 bytes (39.8 KB) received, lifetime 00:30
[07.27 06:01:20] Dropbox.exe - client-cf.dropbox.com:443 close, 913 bytes sent, 3881 bytes (3.79 KB) received, lifetime 01:00
[07.27 06:15:26] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 06:19:08] chrome.exe *64 - mtalk.google.com:5228 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.27 06:34:38] chrome.exe *64 - mtalk.google.com:5228 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.27 06:45:28] Dropbox.exe - client-lb.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 06:48:40] chrome.exe *64 - clients6.google.com:443 close, 5304 bytes (5.17 KB) sent, 4396 bytes (4.29 KB) received, lifetime 09:38
[07.27 07:00:16] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:01:13] SogouCloud.exe - get.sogou.com:80 close, 651 bytes sent, 328 bytes received, lifetime 00:01
[07.27 07:01:13] SogouCloud.exe - get.sogou.com:80 close, 651 bytes sent, 328 bytes received, lifetime <1 sec
[07.27 07:01:17] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:01:17] 360AP.exe - p5.ssl.qhimg.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:01:19] SogouCloud.exe - get.sogou.com:80 close, 651 bytes sent, 328 bytes received, lifetime <1 sec
[07.27 07:01:19] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:01:28] 360AP.exe - p2.qhimg.com:80 close, 230 bytes sent, 1125 bytes (1.09 KB) received, lifetime 00:09
[07.27 07:01:54] SogouCloud.exe - get.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:01:55] SogouCloud.exe - get.sogou.com:80 close, 651 bytes sent, 328 bytes received, lifetime <1 sec
[07.27 07:02:32] chrome.exe *64 - safebrowsing.googleapis.com:443 close, 1848 bytes (1.80 KB) sent, 2305 bytes (2.25 KB) received, lifetime 04:00
[07.27 07:04:19] chrome.exe *64 - www.evernote.com:443 close, 2486 bytes (2.42 KB) sent, 2337 bytes (2.28 KB) received, lifetime 04:00
[07.27 07:04:29] chrome.exe *64 - play.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:05:35] SGTool.exe - info.pinyin.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:05:36] SGTool.exe - info.pinyin.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:06:06] SGTool.exe - info.pinyin.sogou.com:80 close, 561 bytes sent, 166 bytes received, lifetime 00:30
[07.27 07:12:37] Dropbox.exe - d.dropbox.com:443 close, 2560 bytes (2.50 KB) sent, 4976 bytes (4.85 KB) received, lifetime 01:01
[07.27 07:16:03] chrome.exe *64 - play.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:24:38] chrome.exe *64 - 13.client-channel.google.com:443 close, 16473 bytes (16.0 KB) sent, 42641 bytes (41.6 KB) received, lifetime 01:07:47
[07.27 07:27:08] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:29:00] chrome.exe *64 - www.googleapis.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:45:31] Dropbox.exe - client-lb.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:45:55] chrome.exe *64 - clients6.google.com:443 close, 5015 bytes (4.89 KB) sent, 2599 bytes (2.53 KB) received, lifetime 06:47
[07.27 07:46:00] Dropbox.exe - client-cf.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 07:47:02] Dropbox.exe - client-cf.dropbox.com:443 close, 1573 bytes (1.53 KB) sent, 16750 bytes (16.3 KB) received, lifetime 01:02
[07.27 08:00:32] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 08:04:21] chrome.exe *64 - www.evernote.com:443 close, 2486 bytes (2.42 KB) sent, 2337 bytes (2.28 KB) received, lifetime 04:00
[07.27 08:09:35] chrome.exe *64 - clients4.google.com:443 close, 2570 bytes (2.50 KB) sent, 626 bytes received, lifetime 04:00
[07.27 08:15:33] Dropbox.exe - block.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 08:22:45] chrome.exe *64 - clients4.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 08:27:47] chrome.exe *64 - safebrowsing.googleapis.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 08:30:33] Dropbox.exe - client-cf.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 08:30:34] Dropbox.exe - block.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 08:30:34] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 08:34:30] chrome.exe *64 - play.google.com:443 close, 3874 bytes (3.78 KB) sent, 2186 bytes (2.13 KB) received, lifetime 08:13
[07.27 08:34:34] chrome.exe *64 - clients6.google.com:443 close, 7738 bytes (7.55 KB) sent, 8620 bytes (8.41 KB) received, lifetime 07:17
[07.27 08:38:54] svchost.exe *64 - pki.google.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 08:40:55] Dropbox.exe - client-cf.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 08:41:56] Dropbox.exe - client-cf.dropbox.com:443 close, 5129 bytes (5.00 KB) sent, 17049 bytes (16.6 KB) received, lifetime 01:01
[07.27 08:43:37] Dropbox.exe - client-cf.dropbox.com:443 close, 1573 bytes (1.53 KB) sent, 16743 bytes (16.3 KB) received, lifetime 01:00
[07.27 08:53:22] chrome.exe *64 - mtalk.google.com:443 close, 985 bytes sent, 463 bytes received, lifetime 15:00
[07.27 08:57:48] chrome.exe *64 - safebrowsing.googleapis.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:00:11] YodaoDict.exe - cidian.youdao.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:00:14] chrome.exe *64 - clientservices.googleapis.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:00:35] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 09:05:44] SGTool.exe - info.pinyin.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:06:14] SGTool.exe - info.pinyin.sogou.com:80 close, 548 bytes sent, 166 bytes received, lifetime 00:30
[07.27 09:15:23] chrome.exe *64 - www.evernote.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:15:26] chrome.exe *64 - clients6.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:15:33] chrome.exe *64 - www.evernote.com:443 close, 0 bytes sent, 0 bytes received, lifetime 00:10
[07.27 09:21:05] chrome.exe *64 - clients6.google.com:443 close, 2867 bytes (2.79 KB) sent, 1014 bytes received, lifetime 05:39
[07.27 09:25:17] chrome.exe *64 - clients4.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:30:37] Dropbox.exe - block.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:30:37] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 09:30:54] Dropbox.exe - log.getdropbox.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:34:34] chrome.exe *64 - clients6.google.com:443 close, 2987 bytes (2.91 KB) sent, 5913 bytes (5.77 KB) received, lifetime 07:04
[07.27 09:45:38] Dropbox.exe - block.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 09:48:46] SohuNews.exe - css.sohu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:48:46] SohuNews.exe - xy.brand.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:48:49] SohuNews.exe - pv.mini.sohu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:48:49] SohuNews.exe - src.wei.focus.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:48:50] SohuNews.exe - mini.cpc.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:48:50] SohuNews.exe - mini.cpc.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:48:53] SohuNews.exe - bx.optimix.asia:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:48:53] SohuNews.exe - txt.go.sohu.com:80 close, 606 bytes sent, 857 bytes received, lifetime 00:07
[07.27 09:48:54] SohuNews.exe - pv.mini.sohu.com:80 close, 1941 bytes (1.89 KB) sent, 1363 bytes (1.33 KB) received, lifetime 00:05
[07.27 09:48:56] SohuNews.exe - pv.mini.sohu.com:80 close, 512 bytes sent, 15495 bytes (15.1 KB) received, lifetime 00:05
[07.27 09:48:56] SohuNews.exe - pv.mini.sohu.com:80 close, 816 bytes sent, 264 bytes received, lifetime 00:05
[07.27 09:48:56] SohuNews.exe - pv.mini.sohu.com:80 close, 1510 bytes (1.47 KB) sent, 1293 bytes (1.26 KB) received, lifetime 00:06
[07.27 09:49:07] SohuNews.exe - css.sohu.com:80 close, 3549 bytes (3.46 KB) sent, 108371 bytes (105 KB) received, lifetime 00:21
[07.27 09:49:08] SohuNews.exe - p.inte.sogou.com:80 close, 5061 bytes (4.94 KB) sent, 1248 bytes (1.21 KB) received, lifetime 00:15
[07.27 09:49:19] SohuNews.exe - i2.itc.cn:80 close, 1330 bytes (1.29 KB) sent, 62141 bytes (60.6 KB) received, lifetime 00:33
[07.27 09:49:20] svchost.exe *64 - crl.microsoft.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:49:22] SohuNews.exe - cdn.e9377f.com:80 close, 575 bytes sent, 243 bytes received, lifetime 00:30
[07.27 09:49:37] SohuNews.exe - pv.mini.sohu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:49:47] SohuNews.exe - v.admaster.com.cn:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:49:48] SohuNews.exe - i2.itc.cn:80 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 503
[07.27 09:49:49] SohuNews.exe - v.admaster.com.cn:80 close, 4121 bytes (4.02 KB) sent, 4104 bytes (4.00 KB) received, lifetime 00:02
[07.27 09:49:55] SohuNews.exe - at.mct01.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:50:03] SohuNews.exe - ping.pinyin.sogou.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:50:05] SohuNews.exe - ping.pinyin.sogou.com:80 close, 634 bytes sent, 166 bytes received, lifetime 00:01
[07.27 09:50:06] chrome.exe *64 - mail.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:50:30] chrome.exe *64 - s2.googleusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:50:32] chrome.exe *64 - www.youtube.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:50:35] chrome.exe *64 - r6---sn-i3b7knl6.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:50:44] chrome.exe *64 - s2.googleusercontent.com:443 close, 470 bytes sent, 4691 bytes (4.58 KB) received, lifetime 00:14
[07.27 09:50:44] chrome.exe *64 - r18---sn-vgqsenes.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:50:55] chrome.exe *64 - r5---sn-i3b7kn7z.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:51:04] chrome.exe *64 - r13---sn-vgqsenee.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:52:32] chrome.exe *64 - d3cv4a9a9wh0bt.cloudfront.net:443 close, 820 bytes sent, 5619 bytes (5.48 KB) received, lifetime 02:00
[07.27 09:52:33] chrome.exe *64 - r1---sn-i3belnez.googlevideo.com:443 close, 10046 bytes (9.81 KB) sent, 5657896 bytes (5.39 MB) received, lifetime 00:54
[07.27 09:53:24] chrome.exe *64 - r5---sn-i3b7kn7r.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:53:32] chrome.exe *64 - r2---sn-i3b7kn7r.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:53:38] chrome.exe *64 - r6---sn-p5qs7n7s.googlevideo.com:443 close, 1175 bytes (1.14 KB) sent, 11041 bytes (10.7 KB) received, lifetime 00:01
[07.27 09:54:02] chrome.exe *64 - r2---sn-i3b7kn7r.googlevideo.com:443 close, 2883 bytes (2.81 KB) sent, 90785 bytes (88.6 KB) received, lifetime 00:30
[07.27 09:55:09] chrome.exe *64 - mtalk.google.com:443 close, 985 bytes sent, 463 bytes received, lifetime 14:59
[07.27 09:56:35] Dropbox.exe - bolt.dropbox.com:443 close, 0 bytes sent, 0 bytes received, lifetime <1 sec
[07.27 09:56:36] Dropbox.exe - bolt.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 09:57:32] chrome.exe *64 - www.googleapis.com:443 close, 1046 bytes (1.02 KB) sent, 5463 bytes (5.33 KB) received, lifetime 04:00
[07.27 10:00:11] YodaoDict.exe - cidian.youdao.com:80 close, 923 bytes sent, 456 bytes received, lifetime 09:53
[07.27 10:00:39] Dropbox.exe - block.dropbox.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:02:34] chrome.exe *64 - notifications.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:02:48] chrome.exe *64 - lh3.googleusercontent.com:443 close, 745 bytes sent, 229 bytes received, lifetime 00:14
[07.27 10:05:03] QQProtectUpd.exe - qdun-data.qq.com:443 close, 261 bytes sent, 70 bytes received, lifetime <1 sec
[07.27 10:05:05] QQProtectUpd.exe - qd-update.qq.com:8080 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.27 10:05:06] QQProtectUpd.exe - qd-update.qq.com:8080 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection with the target, status code 403
[07.27 10:06:38] chrome.exe *64 - www.gstatic.com:443 close, 1239 bytes (1.20 KB) sent, 20185 bytes (19.7 KB) received, lifetime 04:00
[07.27 10:08:00] chrome.exe *64 - pubads.g.doubleclick.net:443 close, 22150 bytes (21.6 KB) sent, 60543 bytes (59.1 KB) received, lifetime 17:27
[07.27 10:09:05] chrome.exe *64 - r8---sn-i3b7knez.googlevideo.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:13:12] chrome.exe *64 - clients4.google.com:443 close, 4552 bytes (4.44 KB) sent, 1616 bytes (1.57 KB) received, lifetime 04:00
[07.27 10:20:38] chrome.exe *64 - yt3.ggpht.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:20:38] chrome.exe *64 - yt3.ggpht.com:443 close, 733 bytes sent, 229 bytes received, lifetime <1 sec
[07.27 10:20:53] chrome.exe *64 - www.gstatic.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:21:07] chrome.exe *64 - r6---sn-i3b7knl6.googlevideo.com:443 close, 733 bytes sent, 151 bytes received, lifetime 00:15
[07.27 10:21:13] chrome.exe *64 - clients1.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:15] chrome.exe *64 - clients5.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:16] chrome.exe *64 - www.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:17] chrome.exe *64 - f-log-extension.grammarly.io:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:17] chrome.exe *64 - f-log-extension.grammarly.io:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:18] chrome.exe *64 - d3cv4a9a9wh0bt.cloudfront.net:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:19] chrome.exe *64 - drive-thirdparty.googleusercontent.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:23] chrome.exe *64 - www.evernote.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:27] chrome.exe *64 - drive.google.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:36] chrome.exe *64 - f-log-extension.grammarly.io:443 close, 643 bytes sent, 3276 bytes (3.19 KB) received, lifetime 00:19
[07.27 10:22:36] chrome.exe *64 - clients4.google.com:443 close, 733 bytes sent, 229 bytes received, lifetime 00:13
[07.27 10:22:36] chrome.exe *64 - www.zhihu.com:443 close, 568 bytes sent, 152 bytes received, lifetime 00:18
[07.27 10:22:36] chrome.exe *64 - zhstatic.zhihu.com:443 close, 332 bytes sent, 3763 bytes (3.67 KB) received, lifetime 00:18
[07.27 10:22:39] chrome.exe *64 - www.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:39] chrome.exe *64 - s1.bdstatic.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:39] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:39] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:45] chrome.exe *64 - www.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:06
[07.27 10:22:55] chrome.exe *64 - suggestion.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:55] chrome.exe *64 - b1.bdstatic.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:14
[07.27 10:22:55] chrome.exe *64 - xueshu.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:22:56] chrome.exe *64 - imgsrc.baidu.com:80 close, 867 bytes sent, 23546 bytes (22.9 KB) received, lifetime 00:10
[07.27 10:23:03] chrome.exe *64 - t12.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:05] chrome.exe *64 - pagead2.googlesyndication.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:07] chrome.exe *64 - pic01.ishuhui.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:07] chrome.exe *64 - pic01.ishuhui.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:07] chrome.exe *64 - tpc.googlesyndication.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:08] chrome.exe *64 - eclick.baidu.com:443 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:19] chrome.exe *64 - googleads.g.doubleclick.net:80 close, 0 bytes sent, 0 bytes received, lifetime 00:12
[07.27 10:23:21] chrome.exe *64 - xueshu.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:26] chrome.exe *64 - fclick.baidu.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:29] chrome.exe *64 - mhfm5.us.cdndm5.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:29] chrome.exe *64 - mhfm7.us.cdndm5.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:30] chrome.exe *64 - wn.pos.baidu.com:443 close, 330 bytes sent, 4169 bytes (4.07 KB) received, lifetime 00:22
[07.27 10:23:30] chrome.exe *64 - cpro.baidustatic.com:443 close, 334 bytes sent, 3713 bytes (3.62 KB) received, lifetime 00:22
[07.27 10:23:30] chrome.exe *64 - q1.cnzz.com:80 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS
[07.27 10:23:42] chrome.exe *64 - t12.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:17
[07.27 10:23:42] chrome.exe *64 - mhfm9.us.cdndm5.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:12
[07.27 10:23:42] chrome.exe *64 - t12.baidu.com:80 close, 0 bytes sent, 0 bytes received, lifetime 00:17
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:10.138 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:10.139 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Base or swap file too young to remove: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:15.167 2931 WARNING nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Unknown base file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:15.168 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removable base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:15.169 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Removing base or swap file: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:18.689 25746 INFO nova.osapi_compute.wsgi.server [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "POST /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers HTTP/1.1" status: 202 len: 733 time: 0.4759691
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:18.874 25746 INFO nova.osapi_compute.wsgi.server [req-874c0a78-48c8-43df-a9f9-9764103e03fa 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1820800
17/06/09 20:10:40 INFO executor.CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
17/06/09 20:10:40 INFO spark.SecurityManager: Changing view acls to: yarn,curi
17/06/09 20:10:40 INFO spark.SecurityManager: Changing modify acls to: yarn,curi
17/06/09 20:10:40 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, curi); users with modify permissions: Set(yarn, curi)
17/06/09 20:10:41 INFO spark.SecurityManager: Changing view acls to: yarn,curi
17/06/09 20:10:41 INFO spark.SecurityManager: Changing modify acls to: yarn,curi
17/06/09 20:10:41 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, curi); users with modify permissions: Set(yarn, curi)
17/06/09 20:10:41 INFO slf4j.Slf4jLogger: Slf4jLogger started
17/06/09 20:10:41 INFO Remoting: Starting remoting
17/06/09 20:10:41 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@mesos-slave-07:55904]
17/06/09 20:10:41 INFO util.Utils: Successfully started service 'sparkExecutorActorSystem' on port 55904.
17/06/09 20:10:41 INFO storage.DiskBlockManager: Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0147/blockmgr-70293f72-844a-4b39-9ad6-fb0ad7e364e4
17/06/09 20:10:41 INFO storage.MemoryStore: MemoryStore started with capacity 17.7 GB
17/06/09 20:10:42 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.10.34.11:48069
17/06/09 20:10:42 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
17/06/09 20:10:42 INFO executor.Executor: Starting executor ID 5 on host mesos-slave-07
17/06/09 20:10:42 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40984.
17/06/09 20:10:42 INFO netty.NettyBlockTransferService: Server created on 40984
17/06/09 20:10:42 INFO storage.BlockManagerMaster: Trying to register BlockManager
17/06/09 20:10:42 INFO storage.BlockManagerMaster: Registered BlockManager
17/06/09 20:10:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
17/06/09 20:10:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
17/06/09 20:10:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
17/06/09 20:10:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
17/06/09 20:10:45 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
17/06/09 20:10:45 INFO executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
17/06/09 20:10:45 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
17/06/09 20:10:45 INFO executor.Executor: Running task 3.0 in stage 0.0 (TID 3)
17/06/09 20:10:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
17/06/09 20:10:45 INFO executor.Executor: Running task 4.0 in stage 0.0 (TID 4)
17/06/09 20:10:45 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
17/06/09 20:10:45 INFO storage.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.2 KB, free 5.2 KB)
17/06/09 20:10:45 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 160 ms
17/06/09 20:10:46 INFO storage.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 8.8 KB, free 14.0 KB)
17/06/09 20:10:46 INFO spark.CacheManager: Partition rdd_2_1 not found, computing it
17/06/09 20:10:46 INFO spark.CacheManager: Partition rdd_2_3 not found, computing it
17/06/09 20:10:46 INFO spark.CacheManager: Partition rdd_2_0 not found, computing it
17/06/09 20:10:46 INFO spark.CacheManager: Partition rdd_2_2 not found, computing it
17/06/09 20:10:46 INFO spark.CacheManager: Partition rdd_2_4 not found, computing it
17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:21876+7292
17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:14584+7292
17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:0+7292
17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:7292+7292
17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:29168+7292
17/06/09 20:10:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
17/06/09 20:10:46 INFO storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 21.4 KB, free 35.4 KB)
17/06/09 20:10:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 19 ms
17/06/09 20:10:46 INFO storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 281.6 KB, free 317.0 KB)
17/06/09 20:10:47 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/06/09 20:10:47 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/06/09 20:10:47 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/06/09 20:10:47 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/06/09 20:10:47 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/06/09 20:10:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
17/06/09 20:10:48 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 93.0 B, free 317.1 KB)
17/06/09 20:10:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 21 ms
17/06/09 20:10:48 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 384.0 B, free 317.5 KB)
17/06/09 20:10:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
17/06/09 20:10:48 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 95.0 B, free 317.6 KB)
17/06/09 20:10:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 18 ms
17/06/09 20:10:48 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 384.0 B, free 318.0 KB)
17/06/09 20:10:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
17/06/09 20:10:48 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 152.0 B, free 318.1 KB)
17/06/09 20:10:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 17 ms
17/06/09 20:10:48 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 384.0 B, free 318.5 KB)
17/06/09 20:10:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
17/06/09 20:10:48 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 107.0 B, free 318.6 KB)
17/06/09 20:10:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 16 ms
17/06/09 20:10:48 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 384.0 B, free 319.0 KB)
17/06/09 20:10:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
17/06/09 20:10:48 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 93.0 B, free 319.0 KB)
17/06/09 20:10:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 16 ms
17/06/09 20:10:48 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 384.0 B, free 319.4 KB)
17/06/09 20:10:48 INFO python.PythonRunner: Times: total = 1072, boot = 856, init = 210, finish = 6
17/06/09 20:10:48 INFO python.PythonRunner: Times: total = 1114, boot = 885, init = 223, finish = 6
17/06/09 20:10:48 INFO python.PythonRunner: Times: total = 1074, boot = 869, init = 199, finish = 6
17/06/09 20:10:48 INFO python.PythonRunner: Times: total = 1078, boot = 851, init = 219, finish = 8
17/06/09 20:10:48 INFO python.PythonRunner: Times: total = 1077, boot = 865, init = 206, finish = 6
17/06/09 20:10:48 INFO storage.MemoryStore: Block rdd_2_2 stored as bytes in memory (estimated size 850.0 B, free 320.3 KB)
17/06/09 20:10:48 INFO storage.MemoryStore: Block rdd_2_3 stored as bytes in memory (estimated size 930.0 B, free 321.2 KB)
17/06/09 20:10:48 INFO storage.MemoryStore: Block rdd_2_1 stored as bytes in memory (estimated size 935.0 B, free 322.1 KB)
17/06/09 20:10:48 INFO storage.MemoryStore: Block rdd_2_0 stored as bytes in memory (estimated size 913.0 B, free 323.0 KB)
17/06/09 20:10:48 INFO storage.MemoryStore: Block rdd_2_4 stored as bytes in memory (estimated size 890.0 B, free 323.8 KB)
17/06/09 20:10:48 INFO python.PythonRunner: Times: total = 38, boot = 11, init = 27, finish = 0
17/06/09 20:10:48 INFO python.PythonRunner: Times: total = 42, boot = 12, init = 30, finish = 0
17/06/09 20:10:48 INFO python.PythonRunner: Times: total = 41, boot = 15, init = 26, finish = 0
17/06/09 20:10:48 INFO python.PythonRunner: Times: total = 40, boot = 7, init = 33, finish = 0
17/06/09 20:10:48 INFO python.PythonRunner: Times: total = 42, boot = 13, init = 28, finish = 1
17/06/09 20:10:48 INFO executor.Executor: Finished task 3.0 in stage 0.0 (TID 3). 2703 bytes result sent to driver
17/06/09 20:10:48 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2703 bytes result sent to driver
17/06/09 20:10:48 INFO executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 2703 bytes result sent to driver
17/06/09 20:10:48 INFO executor.Executor: Finished task 4.0 in stage 0.0 (TID 4). 2703 bytes result sent to driver
17/06/09 20:10:48 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2703 bytes result sent to driver
17/06/09 20:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42
17/06/09 20:10:52 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 42)
17/06/09 20:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56
17/06/09 20:10:52 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 56)
17/06/09 20:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69
17/06/09 20:10:52 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
17/06/09 20:10:52 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 69)
17/06/09 20:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78
17/06/09 20:10:52 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 78)
17/06/09 20:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79
17/06/09 20:10:52 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 79)
17/06/09 20:10:52 INFO storage.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.8 KB, free 329.6 KB)
17/06/09 20:10:52 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 17 ms
17/06/09 20:10:52 INFO storage.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 9.6 KB, free 339.2 KB)
17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 42, boot = -4131, init = 4172, finish = 1
17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 41, boot = -4122, init = 4162, finish = 1
17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 42, boot = -4121, init = 4162, finish = 1
17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 42, boot = -4138, init = 4179, finish = 1
17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 42, boot = -4134, init = 4175, finish = 1
17/06/09 20:10:52 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 79). 2685 bytes result sent to driver
17/06/09 20:10:52 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 56). 2428 bytes result sent to driver
17/06/09 20:10:52 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 42). 2474 bytes result sent to driver
17/06/09 20:10:52 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 78). 2651 bytes result sent to driver
17/06/09 20:10:52 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 69). 2523 bytes result sent to driver
17/06/09 20:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 87
17/06/09 20:10:52 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 87)
17/06/09 20:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 101
17/06/09 20:10:52 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 101)
17/06/09 20:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 112
17/06/09 20:10:52 INFO executor.Executor: Running task 2.0 in stage 2.0 (TID 112)
17/06/09 20:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 118
17/06/09 20:10:52 INFO executor.Executor: Running task 3.0 in stage 2.0 (TID 118)
17/06/09 20:10:52 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11
17/06/09 20:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 119
17/06/09 20:10:52 INFO executor.Executor: Running task 4.0 in stage 2.0 (TID 119)
17/06/09 20:10:52 INFO storage.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.4 KB, free 344.7 KB)
17/06/09 20:10:52 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 15 ms
17/06/09 20:10:52 INFO storage.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 9.2 KB, free 353.9 KB)
17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 39, boot = -147, init = 186, finish = 0
17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 39, boot = -147, init = 186, finish = 0
17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 41, boot = -139, init = 179, finish = 1
17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 38, boot = -156, init = 194, finish = 0
17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 39, boot = -159, init = 198, finish = 0
17/06/09 20:10:52 INFO executor.Executor: Finished task 4.0 in stage 2.0 (TID 119). 2087 bytes result sent to driver
17/06/09 20:10:52 INFO executor.Executor: Finished task 2.0 in stage 2.0 (TID 112). 2087 bytes result sent to driver
17/06/09 20:10:52 INFO executor.Executor: Finished task 3.0 in stage 2.0 (TID 118). 2087 bytes result sent to driver
17/06/09 20:10:52 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 101). 2087 bytes result sent to driver
17/06/09 20:10:52 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 87). 2087 bytes result sent to driver
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 122
17/06/09 20:10:53 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 122)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 136
17/06/09 20:10:53 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 136)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 150
17/06/09 20:10:53 INFO executor.Executor: Running task 2.0 in stage 3.0 (TID 150)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 158
17/06/09 20:10:53 INFO executor.Executor: Running task 3.0 in stage 3.0 (TID 158)
17/06/09 20:10:53 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 159
17/06/09 20:10:53 INFO executor.Executor: Running task 4.0 in stage 3.0 (TID 159)
17/06/09 20:10:53 INFO storage.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.6 KB, free 359.4 KB)
17/06/09 20:10:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 30 ms
17/06/09 20:10:53 INFO storage.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.8 KB, free 369.3 KB)
17/06/09 20:10:53 INFO spark.CacheManager: Partition rdd_6_4 not found, computing it
17/06/09 20:10:53 INFO spark.CacheManager: Partition rdd_6_3 not found, computing it
17/06/09 20:10:53 INFO spark.CacheManager: Partition rdd_6_0 not found, computing it
17/06/09 20:10:53 INFO spark.CacheManager: Partition rdd_6_1 not found, computing it
17/06/09 20:10:53 INFO spark.CacheManager: Partition rdd_6_2 not found, computing it
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 38, boot = -170, init = 208, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 41, boot = -171, init = 212, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 41, boot = -160, init = 200, finish = 1
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 38, boot = -172, init = 210, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 40, boot = -173, init = 213, finish = 0
17/06/09 20:10:53 INFO storage.MemoryStore: Block rdd_6_4 stored as bytes in memory (estimated size 287.0 B, free 369.5 KB)
17/06/09 20:10:53 INFO storage.MemoryStore: Block rdd_6_3 stored as bytes in memory (estimated size 344.0 B, free 369.9 KB)
17/06/09 20:10:53 INFO storage.MemoryStore: Block rdd_6_1 stored as bytes in memory (estimated size 330.0 B, free 370.2 KB)
17/06/09 20:10:53 INFO storage.MemoryStore: Block rdd_6_2 stored as bytes in memory (estimated size 252.0 B, free 370.4 KB)
17/06/09 20:10:53 INFO storage.MemoryStore: Block rdd_6_0 stored as bytes in memory (estimated size 265.0 B, free 370.7 KB)
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 38, boot = 14, init = 24, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 38, boot = 12, init = 26, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 41, boot = 19, init = 22, finish = 0
17/06/09 20:10:53 INFO executor.Executor: Finished task 4.0 in stage 3.0 (TID 159). 2667 bytes result sent to driver
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 41, boot = 16, init = 25, finish = 0
17/06/09 20:10:53 INFO executor.Executor: Finished task 3.0 in stage 3.0 (TID 158). 2667 bytes result sent to driver
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 40, boot = 5, init = 35, finish = 0
17/06/09 20:10:53 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 136). 2667 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 2.0 in stage 3.0 (TID 150). 2667 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 122). 2667 bytes result sent to driver
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 171
17/06/09 20:10:53 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 171)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 185
17/06/09 20:10:53 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 185)
17/06/09 20:10:53 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 195
17/06/09 20:10:53 INFO executor.Executor: Running task 2.0 in stage 4.0 (TID 195)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 198
17/06/09 20:10:53 INFO executor.Executor: Running task 3.0 in stage 4.0 (TID 198)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 199
17/06/09 20:10:53 INFO executor.Executor: Running task 4.0 in stage 4.0 (TID 199)
17/06/09 20:10:53 INFO storage.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.7 KB, free 376.4 KB)
17/06/09 20:10:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 41 ms
17/06/09 20:10:53 INFO storage.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 10.1 KB, free 386.5 KB)
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_6_0 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_6_4 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_6_3 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_6_2 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_6_1 locally
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 39, boot = -141, init = 180, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 40, boot = -129, init = 169, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 41, boot = -134, init = 175, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 39, boot = -123, init = 162, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 42, boot = -125, init = 166, finish = 1
17/06/09 20:10:53 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 171). 2099 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 185). 2099 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 2.0 in stage 4.0 (TID 195). 2138 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 3.0 in stage 4.0 (TID 198). 2099 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 4.0 in stage 4.0 (TID 199). 2138 bytes result sent to driver
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 206
17/06/09 20:10:53 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 206)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 220
17/06/09 20:10:53 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 220)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 231
17/06/09 20:10:53 INFO executor.Executor: Running task 2.0 in stage 5.0 (TID 231)
17/06/09 20:10:53 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 238
17/06/09 20:10:53 INFO executor.Executor: Running task 3.0 in stage 5.0 (TID 238)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 239
17/06/09 20:10:53 INFO executor.Executor: Running task 4.0 in stage 5.0 (TID 239)
17/06/09 20:10:53 INFO storage.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.8 KB, free 392.3 KB)
17/06/09 20:10:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 12 ms
17/06/09 20:10:53 INFO storage.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 9.6 KB, free 401.9 KB)
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 39, boot = -85, init = 123, finish = 1
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 40, boot = -77, init = 117, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 39, boot = -81, init = 119, finish = 1
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 40, boot = -78, init = 117, finish = 1
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 42, boot = -79, init = 120, finish = 1
17/06/09 20:10:53 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 206). 2198 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 3.0 in stage 5.0 (TID 238). 2198 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 2.0 in stage 5.0 (TID 231). 2198 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 4.0 in stage 5.0 (TID 239). 2198 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 220). 2272 bytes result sent to driver
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 247
17/06/09 20:10:53 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 247)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 261
17/06/09 20:10:53 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 261)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 273
17/06/09 20:10:53 INFO executor.Executor: Running task 2.0 in stage 6.0 (TID 273)
17/06/09 20:10:53 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 278
17/06/09 20:10:53 INFO executor.Executor: Running task 3.0 in stage 6.0 (TID 278)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 279
17/06/09 20:10:53 INFO executor.Executor: Running task 4.0 in stage 6.0 (TID 279)
17/06/09 20:10:53 INFO storage.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.4 KB, free 407.4 KB)
17/06/09 20:10:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 15 ms
17/06/09 20:10:53 INFO storage.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 9.2 KB, free 416.6 KB)
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 39, boot = -107, init = 146, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 41, boot = -100, init = 141, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 42, boot = -99, init = 141, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 39, boot = -103, init = 142, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 42, boot = -101, init = 143, finish = 0
17/06/09 20:10:53 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 247). 2087 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 3.0 in stage 6.0 (TID 278). 2087 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 2.0 in stage 6.0 (TID 273). 2087 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 4.0 in stage 6.0 (TID 279). 2087 bytes result sent to driver
17/06/09 20:10:53 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 261). 2087 bytes result sent to driver
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 285
17/06/09 20:10:53 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 285)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 299
17/06/09 20:10:53 INFO executor.Executor: Running task 1.0 in stage 7.0 (TID 299)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 312
17/06/09 20:10:53 INFO executor.Executor: Running task 2.0 in stage 7.0 (TID 312)
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 318
17/06/09 20:10:53 INFO executor.Executor: Running task 3.0 in stage 7.0 (TID 318)
17/06/09 20:10:53 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16
17/06/09 20:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 319
17/06/09 20:10:53 INFO executor.Executor: Running task 4.0 in stage 7.0 (TID 319)
17/06/09 20:10:53 INFO storage.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.6 KB, free 422.1 KB)
17/06/09 20:10:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 13 ms
17/06/09 20:10:53 INFO storage.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 9.8 KB, free 432.0 KB)
17/06/09 20:10:53 INFO spark.CacheManager: Partition rdd_11_0 not found, computing it
17/06/09 20:10:53 INFO spark.CacheManager: Partition rdd_11_4 not found, computing it
17/06/09 20:10:53 INFO spark.CacheManager: Partition rdd_11_3 not found, computing it
17/06/09 20:10:53 INFO spark.CacheManager: Partition rdd_11_2 not found, computing it
17/06/09 20:10:53 INFO spark.CacheManager: Partition rdd_11_1 not found, computing it
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:53 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 39, boot = -110, init = 149, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 39, boot = -105, init = 144, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 42, boot = -103, init = 144, finish = 1
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 41, boot = -107, init = 148, finish = 0
17/06/09 20:10:53 INFO python.PythonRunner: Times: total = 42, boot = -107, init = 148, finish = 1
17/06/09 20:10:53 INFO storage.MemoryStore: Block rdd_11_0 stored as bytes in memory (estimated size 152.0 B, free 432.1 KB)
17/06/09 20:10:53 INFO storage.MemoryStore: Block rdd_11_3 stored as bytes in memory (estimated size 157.0 B, free 432.3 KB)
17/06/09 20:10:53 INFO storage.MemoryStore: Block rdd_11_4 stored as bytes in memory (estimated size 145.0 B, free 432.4 KB)
17/06/09 20:10:53 INFO storage.MemoryStore: Block rdd_11_2 stored as bytes in memory (estimated size 199.0 B, free 432.6 KB)
17/06/09 20:10:53 INFO storage.MemoryStore: Block rdd_11_1 stored as bytes in memory (estimated size 160.0 B, free 432.7 KB)
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 38, boot = 17, init = 21, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 41, boot = 28, init = 13, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 38, boot = 18, init = 20, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 40, boot = 19, init = 21, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 41, boot = 17, init = 24, finish = 0
17/06/09 20:10:54 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 285). 2667 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 3.0 in stage 7.0 (TID 318). 2667 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 4.0 in stage 7.0 (TID 319). 2667 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 1.0 in stage 7.0 (TID 299). 2667 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 2.0 in stage 7.0 (TID 312). 2667 bytes result sent to driver
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 321
17/06/09 20:10:54 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 321)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 335
17/06/09 20:10:54 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 335)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 349
17/06/09 20:10:54 INFO executor.Executor: Running task 2.0 in stage 8.0 (TID 349)
17/06/09 20:10:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 358
17/06/09 20:10:54 INFO executor.Executor: Running task 3.0 in stage 8.0 (TID 358)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 359
17/06/09 20:10:54 INFO executor.Executor: Running task 4.0 in stage 8.0 (TID 359)
17/06/09 20:10:54 INFO storage.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.7 KB, free 438.5 KB)
17/06/09 20:10:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 12 ms
17/06/09 20:10:54 INFO storage.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 10.1 KB, free 448.6 KB)
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_11_0 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_11_4 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_11_3 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_11_2 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_11_1 locally
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 39, boot = -109, init = 148, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 41, boot = -107, init = 148, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 39, boot = -103, init = 142, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 41, boot = -104, init = 144, finish = 1
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 42, boot = -107, init = 149, finish = 0
17/06/09 20:10:54 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 321). 2141 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 2.0 in stage 8.0 (TID 349). 2141 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 4.0 in stage 8.0 (TID 359). 2141 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 3.0 in stage 8.0 (TID 358). 2141 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 1.0 in stage 8.0 (TID 335). 2111 bytes result sent to driver
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 360
17/06/09 20:10:54 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 360)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 374
17/06/09 20:10:54 INFO executor.Executor: Running task 1.0 in stage 9.0 (TID 374)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 388
17/06/09 20:10:54 INFO executor.Executor: Running task 2.0 in stage 9.0 (TID 388)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 398
17/06/09 20:10:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18
17/06/09 20:10:54 INFO executor.Executor: Running task 3.0 in stage 9.0 (TID 398)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 399
17/06/09 20:10:54 INFO executor.Executor: Running task 4.0 in stage 9.0 (TID 399)
17/06/09 20:10:54 INFO storage.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 5.8 KB, free 454.4 KB)
17/06/09 20:10:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 12 ms
17/06/09 20:10:54 INFO storage.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 9.6 KB, free 464.0 KB)
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 39, boot = -73, init = 111, finish = 1
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 40, boot = -75, init = 114, finish = 1
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 41, boot = -75, init = 115, finish = 1
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 40, boot = -75, init = 114, finish = 1
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 42, boot = -67, init = 108, finish = 1
17/06/09 20:10:54 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 360). 2239 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 2.0 in stage 9.0 (TID 388). 2239 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 3.0 in stage 9.0 (TID 398). 2239 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 1.0 in stage 9.0 (TID 374). 2298 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 4.0 in stage 9.0 (TID 399). 2239 bytes result sent to driver
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 402
17/06/09 20:10:54 INFO executor.Executor: Running task 0.0 in stage 10.0 (TID 402)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 416
17/06/09 20:10:54 INFO executor.Executor: Running task 1.0 in stage 10.0 (TID 416)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 430
17/06/09 20:10:54 INFO executor.Executor: Running task 2.0 in stage 10.0 (TID 430)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 438
17/06/09 20:10:54 INFO executor.Executor: Running task 3.0 in stage 10.0 (TID 438)
17/06/09 20:10:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 439
17/06/09 20:10:54 INFO executor.Executor: Running task 4.0 in stage 10.0 (TID 439)
17/06/09 20:10:54 INFO storage.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.4 KB, free 469.4 KB)
17/06/09 20:10:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 12 ms
17/06/09 20:10:54 INFO storage.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.2 KB, free 478.6 KB)
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 39, boot = -124, init = 163, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 38, boot = -123, init = 161, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 41, boot = -126, init = 167, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 40, boot = -127, init = 167, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 41, boot = -115, init = 155, finish = 1
17/06/09 20:10:54 INFO executor.Executor: Finished task 3.0 in stage 10.0 (TID 438). 2087 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 0.0 in stage 10.0 (TID 402). 2087 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 4.0 in stage 10.0 (TID 439). 2087 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 2.0 in stage 10.0 (TID 430). 2087 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 1.0 in stage 10.0 (TID 416). 2087 bytes result sent to driver
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 450
17/06/09 20:10:54 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 450)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 464
17/06/09 20:10:54 INFO executor.Executor: Running task 1.0 in stage 11.0 (TID 464)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 475
17/06/09 20:10:54 INFO executor.Executor: Running task 2.0 in stage 11.0 (TID 475)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 478
17/06/09 20:10:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20
17/06/09 20:10:54 INFO executor.Executor: Running task 3.0 in stage 11.0 (TID 478)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 479
17/06/09 20:10:54 INFO executor.Executor: Running task 4.0 in stage 11.0 (TID 479)
17/06/09 20:10:54 INFO storage.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 5.5 KB, free 416.7 KB)
17/06/09 20:10:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 15 ms
17/06/09 20:10:54 INFO storage.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 9.8 KB, free 411.6 KB)
17/06/09 20:10:54 INFO spark.CacheManager: Partition rdd_16_0 not found, computing it
17/06/09 20:10:54 INFO spark.CacheManager: Partition rdd_16_4 not found, computing it
17/06/09 20:10:54 INFO spark.CacheManager: Partition rdd_16_3 not found, computing it
17/06/09 20:10:54 INFO spark.CacheManager: Partition rdd_16_2 not found, computing it
17/06/09 20:10:54 INFO spark.CacheManager: Partition rdd_16_1 not found, computing it
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 40, boot = -95, init = 135, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 39, boot = -96, init = 135, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 42, boot = -95, init = 137, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 42, boot = -97, init = 139, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 43, boot = -85, init = 127, finish = 1
17/06/09 20:10:54 INFO storage.MemoryStore: Block rdd_16_4 stored as bytes in memory (estimated size 263.0 B, free 386.4 KB)
17/06/09 20:10:54 INFO storage.MemoryStore: Block rdd_16_0 stored as bytes in memory (estimated size 191.0 B, free 386.6 KB)
17/06/09 20:10:54 INFO storage.MemoryStore: Block rdd_16_3 stored as bytes in memory (estimated size 232.0 B, free 386.8 KB)
17/06/09 20:10:54 INFO storage.MemoryStore: Block rdd_16_2 stored as bytes in memory (estimated size 230.0 B, free 387.0 KB)
17/06/09 20:10:54 INFO storage.MemoryStore: Block rdd_16_1 stored as bytes in memory (estimated size 351.0 B, free 387.4 KB)
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 39, boot = 22, init = 17, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 41, boot = 20, init = 21, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 40, boot = 20, init = 20, finish = 0
17/06/09 20:10:54 INFO executor.Executor: Finished task 4.0 in stage 11.0 (TID 479). 2667 bytes result sent to driver
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 41, boot = 16, init = 25, finish = 0
17/06/09 20:10:54 INFO python.PythonRunner: Times: total = 40, boot = 26, init = 14, finish = 0
17/06/09 20:10:54 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 450). 2667 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 3.0 in stage 11.0 (TID 478). 2667 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 2.0 in stage 11.0 (TID 475). 2667 bytes result sent to driver
17/06/09 20:10:54 INFO executor.Executor: Finished task 1.0 in stage 11.0 (TID 464). 2667 bytes result sent to driver
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 480
17/06/09 20:10:54 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 480)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 494
17/06/09 20:10:54 INFO executor.Executor: Running task 1.0 in stage 12.0 (TID 494)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 508
17/06/09 20:10:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21
17/06/09 20:10:54 INFO executor.Executor: Running task 2.0 in stage 12.0 (TID 508)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 518
17/06/09 20:10:54 INFO executor.Executor: Running task 3.0 in stage 12.0 (TID 518)
17/06/09 20:10:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 519
17/06/09 20:10:54 INFO executor.Executor: Running task 4.0 in stage 12.0 (TID 519)
17/06/09 20:10:54 INFO storage.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.7 KB, free 393.1 KB)
17/06/09 20:10:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 11 ms
17/06/09 20:10:54 INFO storage.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 10.1 KB, free 403.2 KB)
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_16_0 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_16_4 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_16_3 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_16_2 locally
17/06/09 20:10:54 INFO storage.BlockManager: Found block rdd_16_1 locally
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 37, boot = -162, init = 199, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 40, boot = -157, init = 197, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 39, boot = -157, init = 196, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 38, boot = -146, init = 184, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 40, boot = -156, init = 196, finish = 0
17/06/09 20:10:55 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 480). 2155 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 3.0 in stage 12.0 (TID 518). 2155 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 4.0 in stage 12.0 (TID 519). 2155 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 1.0 in stage 12.0 (TID 494). 2105 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 2.0 in stage 12.0 (TID 508). 2155 bytes result sent to driver
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 528
17/06/09 20:10:55 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 528)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 542
17/06/09 20:10:55 INFO executor.Executor: Running task 1.0 in stage 13.0 (TID 542)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 553
17/06/09 20:10:55 INFO executor.Executor: Running task 2.0 in stage 13.0 (TID 553)
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 558
17/06/09 20:10:55 INFO executor.Executor: Running task 3.0 in stage 13.0 (TID 558)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 559
17/06/09 20:10:55 INFO executor.Executor: Running task 4.0 in stage 13.0 (TID 559)
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 5.8 KB, free 409.0 KB)
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 11 ms
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 9.6 KB, free 418.6 KB)
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 39, boot = -76, init = 114, finish = 1
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 42, boot = -73, init = 114, finish = 1
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 42, boot = -73, init = 114, finish = 1
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 41, boot = -67, init = 107, finish = 1
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 40, boot = -74, init = 113, finish = 1
17/06/09 20:10:55 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 528). 2256 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 4.0 in stage 13.0 (TID 559). 2241 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 2.0 in stage 13.0 (TID 553). 2241 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 1.0 in stage 13.0 (TID 542). 2241 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 3.0 in stage 13.0 (TID 558). 2257 bytes result sent to driver
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 562
17/06/09 20:10:55 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 562)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 576
17/06/09 20:10:55 INFO executor.Executor: Running task 1.0 in stage 14.0 (TID 576)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 590
17/06/09 20:10:55 INFO executor.Executor: Running task 2.0 in stage 14.0 (TID 590)
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 598
17/06/09 20:10:55 INFO executor.Executor: Running task 3.0 in stage 14.0 (TID 598)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 599
17/06/09 20:10:55 INFO executor.Executor: Running task 4.0 in stage 14.0 (TID 599)
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 5.4 KB, free 378.2 KB)
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 11 ms
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 9.2 KB, free 387.4 KB)
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 39, boot = -90, init = 129, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 40, boot = -93, init = 133, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 39, boot = -90, init = 129, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 38, boot = -82, init = 120, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 42, boot = -92, init = 133, finish = 1
17/06/09 20:10:55 INFO executor.Executor: Finished task 3.0 in stage 14.0 (TID 598). 2087 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 2.0 in stage 14.0 (TID 590). 2087 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 562). 2087 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 4.0 in stage 14.0 (TID 599). 2087 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 1.0 in stage 14.0 (TID 576). 2087 bytes result sent to driver
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 612
17/06/09 20:10:55 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 612)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 626
17/06/09 20:10:55 INFO executor.Executor: Running task 1.0 in stage 15.0 (TID 626)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 636
17/06/09 20:10:55 INFO executor.Executor: Running task 2.0 in stage 15.0 (TID 636)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 638
17/06/09 20:10:55 INFO executor.Executor: Running task 3.0 in stage 15.0 (TID 638)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 639
17/06/09 20:10:55 INFO executor.Executor: Running task 4.0 in stage 15.0 (TID 639)
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 5.6 KB, free 393.0 KB)
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 10 ms
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 9.8 KB, free 402.8 KB)
17/06/09 20:10:55 INFO spark.CacheManager: Partition rdd_21_1 not found, computing it
17/06/09 20:10:55 INFO spark.CacheManager: Partition rdd_21_4 not found, computing it
17/06/09 20:10:55 INFO spark.CacheManager: Partition rdd_21_3 not found, computing it
17/06/09 20:10:55 INFO spark.CacheManager: Partition rdd_21_2 not found, computing it
17/06/09 20:10:55 INFO spark.CacheManager: Partition rdd_21_0 not found, computing it
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 40, boot = -95, init = 135, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 39, boot = -96, init = 135, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 41, boot = -98, init = 139, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 40, boot = -99, init = 139, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 40, boot = -95, init = 135, finish = 0
17/06/09 20:10:55 INFO storage.MemoryStore: Block rdd_21_1 stored as bytes in memory (estimated size 162.0 B, free 403.0 KB)
17/06/09 20:10:55 INFO storage.MemoryStore: Block rdd_21_4 stored as bytes in memory (estimated size 200.0 B, free 403.2 KB)
17/06/09 20:10:55 INFO storage.MemoryStore: Block rdd_21_3 stored as bytes in memory (estimated size 194.0 B, free 403.4 KB)
17/06/09 20:10:55 INFO storage.MemoryStore: Block rdd_21_2 stored as bytes in memory (estimated size 163.0 B, free 403.5 KB)
17/06/09 20:10:55 INFO storage.MemoryStore: Block rdd_21_0 stored as bytes in memory (estimated size 244.0 B, free 403.8 KB)
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 38, boot = 19, init = 19, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 39, boot = 20, init = 19, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 38, boot = 19, init = 19, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 40, boot = 17, init = 23, finish = 0
17/06/09 20:10:55 INFO executor.Executor: Finished task 1.0 in stage 15.0 (TID 626). 2667 bytes result sent to driver
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 42, boot = 18, init = 24, finish = 0
17/06/09 20:10:55 INFO executor.Executor: Finished task 3.0 in stage 15.0 (TID 638). 2667 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 4.0 in stage 15.0 (TID 639). 2667 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 2.0 in stage 15.0 (TID 636). 2667 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 612). 2667 bytes result sent to driver
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 643
17/06/09 20:10:55 INFO executor.Executor: Running task 0.0 in stage 16.0 (TID 643)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 657
17/06/09 20:10:55 INFO executor.Executor: Running task 1.0 in stage 16.0 (TID 657)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 671
17/06/09 20:10:55 INFO executor.Executor: Running task 2.0 in stage 16.0 (TID 671)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 678
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25
17/06/09 20:10:55 INFO executor.Executor: Running task 3.0 in stage 16.0 (TID 678)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 679
17/06/09 20:10:55 INFO executor.Executor: Running task 4.0 in stage 16.0 (TID 679)
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 5.7 KB, free 409.5 KB)
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 11 ms
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 10.1 KB, free 419.6 KB)
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_21_0 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_21_4 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_21_3 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_21_2 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_21_1 locally
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 39, boot = -104, init = 143, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 41, boot = -104, init = 145, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 39, boot = -106, init = 145, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 39, boot = -106, init = 145, finish = 0
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 43, boot = -104, init = 146, finish = 1
17/06/09 20:10:55 INFO executor.Executor: Finished task 2.0 in stage 16.0 (TID 671). 2163 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 3.0 in stage 16.0 (TID 678). 2151 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 4.0 in stage 16.0 (TID 679). 2163 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 0.0 in stage 16.0 (TID 643). 2151 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 1.0 in stage 16.0 (TID 657). 2163 bytes result sent to driver
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 692
17/06/09 20:10:55 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 692)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 706
17/06/09 20:10:55 INFO executor.Executor: Running task 1.0 in stage 17.0 (TID 706)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 716
17/06/09 20:10:55 INFO executor.Executor: Running task 2.0 in stage 17.0 (TID 716)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 718
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26
17/06/09 20:10:55 INFO executor.Executor: Running task 3.0 in stage 17.0 (TID 718)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 719
17/06/09 20:10:55 INFO executor.Executor: Running task 4.0 in stage 17.0 (TID 719)
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.8 KB, free 379.9 KB)
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 10 ms
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 9.7 KB, free 389.6 KB)
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 42, boot = -69, init = 110, finish = 1
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 40, boot = -71, init = 110, finish = 1
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 39, boot = -73, init = 111, finish = 1
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 44, boot = -72, init = 115, finish = 1
17/06/09 20:10:55 INFO python.PythonRunner: Times: total = 42, boot = -71, init = 112, finish = 1
17/06/09 20:10:55 INFO executor.Executor: Finished task 4.0 in stage 17.0 (TID 719). 2156 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 692). 2156 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 2.0 in stage 17.0 (TID 716). 2156 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 3.0 in stage 17.0 (TID 718). 2156 bytes result sent to driver
17/06/09 20:10:55 INFO executor.Executor: Finished task 1.0 in stage 17.0 (TID 706). 2156 bytes result sent to driver
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 722
17/06/09 20:10:55 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 722)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 736
17/06/09 20:10:55 INFO executor.Executor: Running task 1.0 in stage 18.0 (TID 736)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 750
17/06/09 20:10:55 INFO executor.Executor: Running task 2.0 in stage 18.0 (TID 750)
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 758
17/06/09 20:10:55 INFO executor.Executor: Running task 3.0 in stage 18.0 (TID 758)
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27
17/06/09 20:10:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 759
17/06/09 20:10:55 INFO executor.Executor: Running task 4.0 in stage 18.0 (TID 759)
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 5.4 KB, free 395.0 KB)
17/06/09 20:10:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 10 ms
17/06/09 20:10:55 INFO storage.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 9.2 KB, free 404.2 KB)
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:55 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 38, boot = -77, init = 115, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = -76, init = 115, finish = 1
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = -75, init = 115, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 42, boot = -74, init = 115, finish = 1
17/06/09 20:10:56 INFO executor.Executor: Finished task 4.0 in stage 18.0 (TID 759). 2087 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 722). 2087 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 3.0 in stage 18.0 (TID 758). 2087 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 1.0 in stage 18.0 (TID 736). 2087 bytes result sent to driver
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 42, boot = -74, init = 115, finish = 1
17/06/09 20:10:56 INFO executor.Executor: Finished task 2.0 in stage 18.0 (TID 750). 2087 bytes result sent to driver
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 762
17/06/09 20:10:56 INFO executor.Executor: Running task 0.0 in stage 19.0 (TID 762)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 776
17/06/09 20:10:56 INFO executor.Executor: Running task 1.0 in stage 19.0 (TID 776)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 789
17/06/09 20:10:56 INFO executor.Executor: Running task 2.0 in stage 19.0 (TID 789)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 798
17/06/09 20:10:56 INFO executor.Executor: Running task 3.0 in stage 19.0 (TID 798)
17/06/09 20:10:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 799
17/06/09 20:10:56 INFO executor.Executor: Running task 4.0 in stage 19.0 (TID 799)
17/06/09 20:10:56 INFO storage.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 5.6 KB, free 409.8 KB)
17/06/09 20:10:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 9 ms
17/06/09 20:10:56 INFO storage.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 9.8 KB, free 419.6 KB)
17/06/09 20:10:56 INFO spark.CacheManager: Partition rdd_26_0 not found, computing it
17/06/09 20:10:56 INFO spark.CacheManager: Partition rdd_26_4 not found, computing it
17/06/09 20:10:56 INFO spark.CacheManager: Partition rdd_26_3 not found, computing it
17/06/09 20:10:56 INFO spark.CacheManager: Partition rdd_26_2 not found, computing it
17/06/09 20:10:56 INFO spark.CacheManager: Partition rdd_26_1 not found, computing it
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = -79, init = 119, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 41, boot = -82, init = 123, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 41, boot = -82, init = 123, finish = 0
17/06/09 20:10:56 INFO storage.MemoryStore: Block rdd_26_0 stored as bytes in memory (estimated size 16.0 B, free 419.6 KB)
17/06/09 20:10:56 INFO storage.MemoryStore: Block rdd_26_3 stored as bytes in memory (estimated size 16.0 B, free 419.6 KB)
17/06/09 20:10:56 INFO storage.MemoryStore: Block rdd_26_4 stored as bytes in memory (estimated size 16.0 B, free 419.6 KB)
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 41, boot = -67, init = 107, finish = 1
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 42, boot = -66, init = 107, finish = 1
17/06/09 20:10:56 INFO storage.MemoryStore: Block rdd_26_2 stored as bytes in memory (estimated size 16.0 B, free 419.7 KB)
17/06/09 20:10:56 INFO storage.MemoryStore: Block rdd_26_1 stored as bytes in memory (estimated size 16.0 B, free 419.7 KB)
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 38, boot = 23, init = 15, finish = 0
17/06/09 20:10:56 INFO executor.Executor: Finished task 0.0 in stage 19.0 (TID 762). 2667 bytes result sent to driver
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = 20, init = 20, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 41, boot = 23, init = 18, finish = 0
17/06/09 20:10:56 INFO executor.Executor: Finished task 3.0 in stage 19.0 (TID 798). 2667 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 4.0 in stage 19.0 (TID 799). 2667 bytes result sent to driver
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 39, boot = 18, init = 21, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 39, boot = 18, init = 21, finish = 0
17/06/09 20:10:56 INFO executor.Executor: Finished task 1.0 in stage 19.0 (TID 776). 2667 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 2.0 in stage 19.0 (TID 789). 2667 bytes result sent to driver
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 807
17/06/09 20:10:56 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 807)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 821
17/06/09 20:10:56 INFO executor.Executor: Running task 1.0 in stage 20.0 (TID 821)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 833
17/06/09 20:10:56 INFO executor.Executor: Running task 2.0 in stage 20.0 (TID 833)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 838
17/06/09 20:10:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29
17/06/09 20:10:56 INFO executor.Executor: Running task 3.0 in stage 20.0 (TID 838)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 839
17/06/09 20:10:56 INFO executor.Executor: Running task 4.0 in stage 20.0 (TID 839)
17/06/09 20:10:56 INFO storage.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 5.7 KB, free 379.5 KB)
17/06/09 20:10:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 11 ms
17/06/09 20:10:56 INFO storage.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 10.1 KB, free 374.2 KB)
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_26_0 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_26_4 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_26_3 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_26_2 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_26_1 locally
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 38, boot = -119, init = 157, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 38, boot = -119, init = 157, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 39, boot = -126, init = 165, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = -111, init = 151, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 41, boot = -110, init = 151, finish = 0
17/06/09 20:10:56 INFO executor.Executor: Finished task 4.0 in stage 20.0 (TID 839). 2076 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 3.0 in stage 20.0 (TID 838). 2076 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 0.0 in stage 20.0 (TID 807). 2076 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 1.0 in stage 20.0 (TID 821). 2076 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 2.0 in stage 20.0 (TID 833). 2076 bytes result sent to driver
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 853
17/06/09 20:10:56 INFO executor.Executor: Running task 0.0 in stage 21.0 (TID 853)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 867
17/06/09 20:10:56 INFO executor.Executor: Running task 1.0 in stage 21.0 (TID 867)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 877
17/06/09 20:10:56 INFO executor.Executor: Running task 2.0 in stage 21.0 (TID 877)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 878
17/06/09 20:10:56 INFO executor.Executor: Running task 3.0 in stage 21.0 (TID 878)
17/06/09 20:10:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 879
17/06/09 20:10:56 INFO executor.Executor: Running task 4.0 in stage 21.0 (TID 879)
17/06/09 20:10:56 INFO storage.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 5.8 KB, free 380.0 KB)
17/06/09 20:10:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 8 ms
17/06/09 20:10:56 INFO storage.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 9.7 KB, free 389.7 KB)
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 38, boot = -59, init = 96, finish = 1
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 39, boot = -59, init = 98, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 41, boot = -59, init = 100, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 41, boot = -59, init = 99, finish = 1
17/06/09 20:10:56 INFO executor.Executor: Finished task 0.0 in stage 21.0 (TID 853). 2348 bytes result sent to driver
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 41, boot = -58, init = 98, finish = 1
17/06/09 20:10:56 INFO executor.Executor: Finished task 1.0 in stage 21.0 (TID 867). 2333 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 3.0 in stage 21.0 (TID 878). 2333 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 4.0 in stage 21.0 (TID 879). 2333 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 2.0 in stage 21.0 (TID 877). 2333 bytes result sent to driver
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 886
17/06/09 20:10:56 INFO executor.Executor: Running task 0.0 in stage 22.0 (TID 886)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 900
17/06/09 20:10:56 INFO executor.Executor: Running task 1.0 in stage 22.0 (TID 900)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 912
17/06/09 20:10:56 INFO executor.Executor: Running task 2.0 in stage 22.0 (TID 912)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 918
17/06/09 20:10:56 INFO executor.Executor: Running task 3.0 in stage 22.0 (TID 918)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 919
17/06/09 20:10:56 INFO executor.Executor: Running task 4.0 in stage 22.0 (TID 919)
17/06/09 20:10:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31
17/06/09 20:10:56 INFO storage.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 5.6 KB, free 395.3 KB)
17/06/09 20:10:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 8 ms
17/06/09 20:10:56 INFO storage.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 9.8 KB, free 405.1 KB)
17/06/09 20:10:56 INFO spark.CacheManager: Partition rdd_30_0 not found, computing it
17/06/09 20:10:56 INFO spark.CacheManager: Partition rdd_30_4 not found, computing it
17/06/09 20:10:56 INFO spark.CacheManager: Partition rdd_30_3 not found, computing it
17/06/09 20:10:56 INFO spark.CacheManager: Partition rdd_30_2 not found, computing it
17/06/09 20:10:56 INFO spark.CacheManager: Partition rdd_30_1 not found, computing it
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_0 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_4 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_3 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_2 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_2_1 locally
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 37, boot = -88, init = 125, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 39, boot = -86, init = 125, finish = 0
17/06/09 20:10:56 INFO storage.MemoryStore: Block rdd_30_0 stored as bytes in memory (estimated size 324.0 B, free 405.4 KB)
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 39, boot = -86, init = 125, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 41, boot = -84, init = 125, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = -85, init = 125, finish = 0
17/06/09 20:10:56 INFO storage.MemoryStore: Block rdd_30_1 stored as bytes in memory (estimated size 165.0 B, free 405.6 KB)
17/06/09 20:10:56 INFO storage.MemoryStore: Block rdd_30_2 stored as bytes in memory (estimated size 250.0 B, free 405.8 KB)
17/06/09 20:10:56 INFO storage.MemoryStore: Block rdd_30_4 stored as bytes in memory (estimated size 226.0 B, free 406.0 KB)
17/06/09 20:10:56 INFO storage.MemoryStore: Block rdd_30_3 stored as bytes in memory (estimated size 251.0 B, free 406.3 KB)
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = 19, init = 21, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 39, boot = 20, init = 19, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 39, boot = 21, init = 18, finish = 0
17/06/09 20:10:56 INFO executor.Executor: Finished task 0.0 in stage 22.0 (TID 886). 2667 bytes result sent to driver
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 41, boot = 18, init = 23, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = 18, init = 22, finish = 0
17/06/09 20:10:56 INFO executor.Executor: Finished task 1.0 in stage 22.0 (TID 900). 2667 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 2.0 in stage 22.0 (TID 912). 2667 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 4.0 in stage 22.0 (TID 919). 2667 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 3.0 in stage 22.0 (TID 918). 2667 bytes result sent to driver
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 926
17/06/09 20:10:56 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 926)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 940
17/06/09 20:10:56 INFO executor.Executor: Running task 1.0 in stage 23.0 (TID 940)
17/06/09 20:10:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 951
17/06/09 20:10:56 INFO executor.Executor: Running task 2.0 in stage 23.0 (TID 951)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 958
17/06/09 20:10:56 INFO executor.Executor: Running task 3.0 in stage 23.0 (TID 958)
17/06/09 20:10:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 959
17/06/09 20:10:56 INFO executor.Executor: Running task 4.0 in stage 23.0 (TID 959)
17/06/09 20:10:56 INFO storage.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.7 KB, free 406.3 KB)
17/06/09 20:10:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 16 ms
17/06/09 20:10:56 INFO storage.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 10.1 KB, free 400.4 KB)
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_30_0 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_30_3 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_30_2 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_30_1 locally
17/06/09 20:10:56 INFO storage.BlockManager: Found block rdd_30_4 locally
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 39, boot = -134, init = 173, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = -130, init = 170, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = -132, init = 172, finish = 0
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 42, boot = -132, init = 173, finish = 1
17/06/09 20:10:56 INFO python.PythonRunner: Times: total = 40, boot = -129, init = 168, finish = 1
17/06/09 20:10:56 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 926). 2211 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 2.0 in stage 23.0 (TID 951). 2223 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 3.0 in stage 23.0 (TID 958). 2223 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 4.0 in stage 23.0 (TID 959). 2223 bytes result sent to driver
17/06/09 20:10:56 INFO executor.Executor: Finished task 1.0 in stage 23.0 (TID 940). 2223 bytes result sent to driver
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 970
17/06/09 20:10:57 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 970)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 984
17/06/09 20:10:57 INFO executor.Executor: Running task 1.0 in stage 24.0 (TID 984)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 998
17/06/09 20:10:57 INFO executor.Executor: Running task 2.0 in stage 24.0 (TID 998)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1012
17/06/09 20:10:57 INFO executor.Executor: Running task 3.0 in stage 24.0 (TID 1012)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1026
17/06/09 20:10:57 INFO executor.Executor: Running task 4.0 in stage 24.0 (TID 1026)
17/06/09 20:10:57 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 33
17/06/09 20:10:57 INFO storage.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 29.5 KB, free 404.8 KB)
17/06/09 20:10:57 INFO broadcast.TorrentBroadcast: Reading broadcast variable 33 took 9 ms
17/06/09 20:10:57 INFO storage.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 82.1 KB, free 486.9 KB)
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_6_4 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_6_3 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_6_2 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_6_1 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_6_0 locally
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 40, boot = -340, init = 380, finish = 0
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 40, boot = -337, init = 376, finish = 1
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 42, boot = -343, init = 384, finish = 1
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 39, boot = -342, init = 381, finish = 0
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 41, boot = -343, init = 384, finish = 0
17/06/09 20:10:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000003_1012' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000003
17/06/09 20:10:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000004_1026' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000004
17/06/09 20:10:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000002_998' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000002
17/06/09 20:10:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000000_970' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000000
17/06/09 20:10:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000001_984' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000001
17/06/09 20:10:57 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000004_1026: Committed
17/06/09 20:10:57 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000002_998: Committed
17/06/09 20:10:57 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000003_1012: Committed
17/06/09 20:10:57 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000001_984: Committed
17/06/09 20:10:57 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000000_970: Committed
17/06/09 20:10:57 INFO executor.Executor: Finished task 3.0 in stage 24.0 (TID 1012). 2364 bytes result sent to driver
17/06/09 20:10:57 INFO executor.Executor: Finished task 4.0 in stage 24.0 (TID 1026). 2364 bytes result sent to driver
17/06/09 20:10:57 INFO executor.Executor: Finished task 1.0 in stage 24.0 (TID 984). 2364 bytes result sent to driver
17/06/09 20:10:57 INFO executor.Executor: Finished task 2.0 in stage 24.0 (TID 998). 2364 bytes result sent to driver
17/06/09 20:10:57 INFO executor.Executor: Finished task 0.0 in stage 24.0 (TID 970). 2364 bytes result sent to driver
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1125
17/06/09 20:10:57 INFO executor.Executor: Running task 40.0 in stage 24.0 (TID 1125)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1126
17/06/09 20:10:57 INFO executor.Executor: Running task 41.0 in stage 24.0 (TID 1126)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1127
17/06/09 20:10:57 INFO executor.Executor: Running task 42.0 in stage 24.0 (TID 1127)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1128
17/06/09 20:10:57 INFO executor.Executor: Running task 43.0 in stage 24.0 (TID 1128)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1129
17/06/09 20:10:57 INFO executor.Executor: Running task 44.0 in stage 24.0 (TID 1129)
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_11_0 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_11_1 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_11_2 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_11_3 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_11_4 locally
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 39, boot = -397, init = 436, finish = 0
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 38, boot = -405, init = 443, finish = 0
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 40, boot = -400, init = 440, finish = 0
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 38, boot = -390, init = 428, finish = 0
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 41, boot = -398, init = 439, finish = 0
17/06/09 20:10:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000044_1129' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000044
17/06/09 20:10:57 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000044_1129: Committed
17/06/09 20:10:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000043_1128' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000043
17/06/09 20:10:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000040_1125' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000040
17/06/09 20:10:57 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000043_1128: Committed
17/06/09 20:10:57 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000040_1125: Committed
17/06/09 20:10:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000042_1127' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000042
17/06/09 20:10:57 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000042_1127: Committed
17/06/09 20:10:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000041_1126' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000041
17/06/09 20:10:57 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000041_1126: Committed
17/06/09 20:10:57 INFO executor.Executor: Finished task 44.0 in stage 24.0 (TID 1129). 2364 bytes result sent to driver
17/06/09 20:10:57 INFO executor.Executor: Finished task 43.0 in stage 24.0 (TID 1128). 2364 bytes result sent to driver
17/06/09 20:10:57 INFO executor.Executor: Finished task 40.0 in stage 24.0 (TID 1125). 2364 bytes result sent to driver
17/06/09 20:10:57 INFO executor.Executor: Finished task 42.0 in stage 24.0 (TID 1127). 2364 bytes result sent to driver
17/06/09 20:10:57 INFO executor.Executor: Finished task 41.0 in stage 24.0 (TID 1126). 2364 bytes result sent to driver
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1135
17/06/09 20:10:57 INFO executor.Executor: Running task 80.0 in stage 24.0 (TID 1135)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1136
17/06/09 20:10:57 INFO executor.Executor: Running task 81.0 in stage 24.0 (TID 1136)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1137
17/06/09 20:10:57 INFO executor.Executor: Running task 82.0 in stage 24.0 (TID 1137)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1138
17/06/09 20:10:57 INFO executor.Executor: Running task 83.0 in stage 24.0 (TID 1138)
17/06/09 20:10:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1141
17/06/09 20:10:57 INFO executor.Executor: Running task 84.0 in stage 24.0 (TID 1141)
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_16_0 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_16_1 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_16_2 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_16_3 locally
17/06/09 20:10:57 INFO storage.BlockManager: Found block rdd_16_4 locally
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 37, boot = -87, init = 124, finish = 0
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 39, boot = -79, init = 118, finish = 0
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 39, boot = -86, init = 125, finish = 0
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 42, boot = -81, init = 122, finish = 1
17/06/09 20:10:57 INFO python.PythonRunner: Times: total = 41, boot = -83, init = 123, finish = 1
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000083_1138' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000083
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000083_1138: Committed
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000084_1141' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000084
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000080_1135' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000080
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000084_1141: Committed
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000080_1135: Committed
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000082_1137' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000082
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000082_1137: Committed
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000081_1136' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000081
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000081_1136: Committed
17/06/09 20:10:58 INFO executor.Executor: Finished task 83.0 in stage 24.0 (TID 1138). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.Executor: Finished task 84.0 in stage 24.0 (TID 1141). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.Executor: Finished task 80.0 in stage 24.0 (TID 1135). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.Executor: Finished task 82.0 in stage 24.0 (TID 1137). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.Executor: Finished task 81.0 in stage 24.0 (TID 1136). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1145
17/06/09 20:10:58 INFO executor.Executor: Running task 120.0 in stage 24.0 (TID 1145)
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1146
17/06/09 20:10:58 INFO executor.Executor: Running task 121.0 in stage 24.0 (TID 1146)
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1147
17/06/09 20:10:58 INFO executor.Executor: Running task 122.0 in stage 24.0 (TID 1147)
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1150
17/06/09 20:10:58 INFO executor.Executor: Running task 123.0 in stage 24.0 (TID 1150)
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1151
17/06/09 20:10:58 INFO executor.Executor: Running task 124.0 in stage 24.0 (TID 1151)
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_21_4 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_21_1 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_21_0 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_21_2 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_21_3 locally
17/06/09 20:10:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:58 INFO python.PythonRunner: Times: total = 40, boot = -82, init = 122, finish = 0
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000124_1151' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000124
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000124_1151: Committed
17/06/09 20:10:58 INFO executor.Executor: Finished task 124.0 in stage 24.0 (TID 1151). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1155
17/06/09 20:10:58 INFO executor.Executor: Running task 160.0 in stage 24.0 (TID 1155)
17/06/09 20:10:58 INFO python.PythonRunner: Times: total = 39, boot = -102, init = 141, finish = 0
17/06/09 20:10:58 INFO python.PythonRunner: Times: total = 38, boot = -112, init = 150, finish = 0
17/06/09 20:10:58 INFO python.PythonRunner: Times: total = 40, boot = -107, init = 147, finish = 0
17/06/09 20:10:58 INFO python.PythonRunner: Times: total = 40, boot = -116, init = 156, finish = 0
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000120_1145' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000120
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000120_1145: Committed
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000122_1147' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000122
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000121_1146' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000121
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000122_1147: Committed
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000123_1150' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000123
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000121_1146: Committed
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000123_1150: Committed
17/06/09 20:10:58 INFO executor.Executor: Finished task 120.0 in stage 24.0 (TID 1145). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.Executor: Finished task 121.0 in stage 24.0 (TID 1146). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.Executor: Finished task 122.0 in stage 24.0 (TID 1147). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.Executor: Finished task 123.0 in stage 24.0 (TID 1150). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1156
17/06/09 20:10:58 INFO executor.Executor: Running task 161.0 in stage 24.0 (TID 1156)
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1157
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1158
17/06/09 20:10:58 INFO executor.Executor: Running task 162.0 in stage 24.0 (TID 1157)
17/06/09 20:10:58 INFO executor.Executor: Running task 163.0 in stage 24.0 (TID 1158)
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1159
17/06/09 20:10:58 INFO executor.Executor: Running task 164.0 in stage 24.0 (TID 1159)
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_26_0 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_26_3 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_26_4 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_26_1 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_26_2 locally
17/06/09 20:10:58 INFO python.PythonRunner: Times: total = 38, boot = -62, init = 100, finish = 0
17/06/09 20:10:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000160_1155' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000160
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000160_1155: Committed
17/06/09 20:10:58 INFO executor.Executor: Finished task 160.0 in stage 24.0 (TID 1155). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO python.PythonRunner: Times: total = 38, boot = -45, init = 83, finish = 0
17/06/09 20:10:58 INFO python.PythonRunner: Times: total = 38, boot = -37, init = 75, finish = 0
17/06/09 20:10:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000163_1158' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000163
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000163_1158: Committed
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000164_1159' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000164
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000164_1159: Committed
17/06/09 20:10:58 INFO executor.Executor: Finished task 163.0 in stage 24.0 (TID 1158). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.Executor: Finished task 164.0 in stage 24.0 (TID 1159). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO python.PythonRunner: Times: total = 37, boot = -54, init = 91, finish = 0
17/06/09 20:10:58 INFO python.PythonRunner: Times: total = 37, boot = -62, init = 99, finish = 0
17/06/09 20:10:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000162_1157' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000162
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000162_1157: Committed
17/06/09 20:10:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0024_m_000161_1156' to hdfs://10.10.34.11:9000/pjhe/test/1/_temporary/0/task_201706092018_0024_m_000161
17/06/09 20:10:58 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0024_m_000161_1156: Committed
17/06/09 20:10:58 INFO executor.Executor: Finished task 162.0 in stage 24.0 (TID 1157). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.Executor: Finished task 161.0 in stage 24.0 (TID 1156). 2364 bytes result sent to driver
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1166
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1180
17/06/09 20:10:58 INFO executor.Executor: Running task 0.0 in stage 25.0 (TID 1166)
17/06/09 20:10:58 INFO executor.Executor: Running task 1.0 in stage 25.0 (TID 1180)
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1194
17/06/09 20:10:58 INFO executor.Executor: Running task 2.0 in stage 25.0 (TID 1194)
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1198
17/06/09 20:10:58 INFO executor.Executor: Running task 3.0 in stage 25.0 (TID 1198)
17/06/09 20:10:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1199
17/06/09 20:10:58 INFO executor.Executor: Running task 4.0 in stage 25.0 (TID 1199)
17/06/09 20:10:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 34
17/06/09 20:10:58 INFO storage.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 28.8 KB, free 515.7 KB)
17/06/09 20:10:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 34 took 9 ms
17/06/09 20:10:58 INFO storage.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 77.4 KB, free 593.1 KB)
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_30_4 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_30_3 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_30_2 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_30_1 locally
17/06/09 20:10:58 INFO storage.BlockManager: Found block rdd_30_0 locally
17/06/09 20:10:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:59 INFO python.PythonRunner: Times: total = 38, boot = -727, init = 765, finish = 0
17/06/09 20:10:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/06/09 20:10:59 INFO python.PythonRunner: Times: total = 41, boot = -707, init = 748, finish = 0
17/06/09 20:10:59 INFO python.PythonRunner: Times: total = 39, boot = -700, init = 739, finish = 0
17/06/09 20:10:59 INFO python.PythonRunner: Times: total = 39, boot = -690, init = 729, finish = 0
17/06/09 20:10:59 INFO python.PythonRunner: Times: total = 41, boot = -699, init = 740, finish = 0
17/06/09 20:10:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0025_m_000004_1199' to hdfs://10.10.34.11:9000/pjhe/test/2/_temporary/0/task_201706092018_0025_m_000004
17/06/09 20:10:59 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0025_m_000004_1199: Committed
17/06/09 20:10:59 INFO executor.Executor: Finished task 4.0 in stage 25.0 (TID 1199). 2364 bytes result sent to driver
17/06/09 20:10:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0025_m_000003_1198' to hdfs://10.10.34.11:9000/pjhe/test/2/_temporary/0/task_201706092018_0025_m_000003
17/06/09 20:10:59 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0025_m_000003_1198: Committed
17/06/09 20:10:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0025_m_000000_1166' to hdfs://10.10.34.11:9000/pjhe/test/2/_temporary/0/task_201706092018_0025_m_000000
17/06/09 20:10:59 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0025_m_000000_1166: Committed
17/06/09 20:10:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0025_m_000002_1194' to hdfs://10.10.34.11:9000/pjhe/test/2/_temporary/0/task_201706092018_0025_m_000002
17/06/09 20:10:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_201706092018_0025_m_000001_1180' to hdfs://10.10.34.11:9000/pjhe/test/2/_temporary/0/task_201706092018_0025_m_000001
17/06/09 20:10:59 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0025_m_000002_1194: Committed
17/06/09 20:10:59 INFO mapred.SparkHadoopMapRedUtil: attempt_201706092018_0025_m_000001_1180: Committed
17/06/09 20:10:59 INFO executor.Executor: Finished task 3.0 in stage 25.0 (TID 1198). 2364 bytes result sent to driver
17/06/09 20:10:59 INFO executor.Executor: Finished task 0.0 in stage 25.0 (TID 1166). 2364 bytes result sent to driver
17/06/09 20:10:59 INFO executor.Executor: Finished task 2.0 in stage 25.0 (TID 1194). 2364 bytes result sent to driver
17/06/09 20:10:59 INFO executor.Executor: Finished task 1.0 in stage 25.0 (TID 1180). 2364 bytes result sent to driver
17/06/09 20:11:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1200
17/06/09 20:11:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1201
17/06/09 20:11:07 INFO executor.Executor: Running task 0.0 in stage 26.0 (TID 1200)
17/06/09 20:11:07 INFO executor.Executor: Running task 1.0 in stage 26.0 (TID 1201)
17/06/09 20:11:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1202
17/06/09 20:11:07 INFO executor.Executor: Running task 2.0 in stage 26.0 (TID 1202)
17/06/09 20:11:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1203
17/06/09 20:11:07 INFO executor.Executor: Running task 3.0 in stage 26.0 (TID 1203)
17/06/09 20:11:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1204
17/06/09 20:11:07 INFO executor.Executor: Running task 4.0 in stage 26.0 (TID 1204)
17/06/09 20:11:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 37
17/06/09 20:11:07 INFO storage.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 5.2 KB, free 569.5 KB)
17/06/09 20:11:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 37 took 14 ms
17/06/09 20:11:07 INFO storage.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 8.8 KB, free 500.9 KB)
17/06/09 20:11:07 INFO spark.CacheManager: Partition rdd_42_0 not found, computing it
17/06/09 20:11:07 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:0+7292
17/06/09 20:11:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 36
17/06/09 20:11:07 INFO spark.CacheManager: Partition rdd_42_4 not found, computing it
17/06/09 20:11:07 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:29168+7292
17/06/09 20:11:07 INFO spark.CacheManager: Partition rdd_42_3 not found, computing it
17/06/09 20:11:07 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:21876+7292
17/06/09 20:11:07 INFO spark.CacheManager: Partition rdd_42_2 not found, computing it
17/06/09 20:11:07 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:14584+7292
17/06/09 20:11:07 INFO spark.CacheManager: Partition rdd_42_1 not found, computing it
17/06/09 20:11:07 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:7292+7292
17/06/09 20:11:07 INFO storage.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 21.4 KB, free 522.3 KB)
17/06/09 20:11:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 36 took 11 ms
17/06/09 20:11:07 INFO storage.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 281.6 KB, free 803.9 KB)
17/06/09 20:11:07 INFO python.PythonRunner: Times: total = 47, boot = -8758, init = 8800, finish = 5
17/06/09 20:11:07 INFO storage.MemoryStore: Block rdd_42_0 stored as bytes in memory (estimated size 913.0 B, free 804.8 KB)
17/06/09 20:11:07 INFO python.PythonRunner: Times: total = 55, boot = -8764, init = 8814, finish = 5
17/06/09 20:11:07 INFO python.PythonRunner: Times: total = 55, boot = -8768, init = 8818, finish = 5
17/06/09 20:11:07 INFO python.PythonRunner: Times: total = 59, boot = -8772, init = 8826, finish = 5
17/06/09 20:11:07 INFO python.PythonRunner: Times: total = 60, boot = -8772, init = 8827, finish = 5
17/06/09 20:11:07 INFO storage.MemoryStore: Block rdd_42_4 stored as bytes in memory (estimated size 890.0 B, free 805.7 KB)
17/06/09 20:11:07 INFO storage.MemoryStore: Block rdd_42_2 stored as bytes in memory (estimated size 850.0 B, free 806.5 KB)
17/06/09 20:11:07 INFO storage.MemoryStore: Block rdd_42_1 stored as bytes in memory (estimated size 935.0 B, free 807.4 KB)
17/06/09 20:11:07 INFO storage.MemoryStore: Block rdd_42_3 stored as bytes in memory (estimated size 930.0 B, free 808.4 KB)
17/06/09 20:11:07 INFO python.PythonRunner: Times: total = 42, boot = 25, init = 16, finish = 1
17/06/09 20:11:07 INFO executor.Executor: Finished task 0.0 in stage 26.0 (TID 1200). 2703 bytes result sent to driver
17/06/09 20:11:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1205
17/06/09 20:11:07 INFO executor.Executor: Running task 5.0 in stage 26.0 (TID 1205)
17/06/09 20:11:07 INFO python.PythonRunner: Times: total = 41, boot = 21, init = 20, finish = 0
17/06/09 20:11:07 INFO python.PythonRunner: Times: total = 41, boot = 20, init = 20, finish = 1
17/06/09 20:11:07 INFO spark.CacheManager: Partition rdd_42_5 not found, computing it
17/06/09 20:11:07 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:36460+7292
17/06/09 20:11:07 INFO python.PythonRunner: Times: total = 42, boot = 23, init = 19, finish = 0
17/06/09 20:11:07 INFO python.PythonRunner: Times: total = 42, boot = 20, init = 21, finish = 1
17/06/09 20:11:07 INFO executor.Executor: Finished task 4.0 in stage 26.0 (TID 1204). 2703 bytes result sent to driver
17/06/09 20:11:07 INFO executor.Executor: Finished task 2.0 in stage 26.0 (TID 1202). 2703 bytes result sent to driver
17/06/09 20:11:07 INFO executor.Executor: Finished task 1.0 in stage 26.0 (TID 1201). 2703 bytes result sent to driver
17/06/09 20:11:07 INFO executor.Executor: Finished task 3.0 in stage 26.0 (TID 1203). 2703 bytes result sent to driver
17/06/09 20:11:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1206
17/06/09 20:11:07 INFO executor.Executor: Running task 6.0 in stage 26.0 (TID 1206)
17/06/09 20:11:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1207
17/06/09 20:11:07 INFO executor.Executor: Running task 7.0 in stage 26.0 (TID 1207)
17/06/09 20:11:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1208
17/06/09 20:11:07 INFO executor.Executor: Running task 8.0 in stage 26.0 (TID 1208)
17/06/09 20:11:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1209
17/06/09 20:11:07 INFO executor.Executor: Running task 9.0 in stage 26.0 (TID 1209)
17/06/09 20:11:07 INFO spark.CacheManager: Partition rdd_42_6 not found, computing it
17/06/09 20:11:07 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:43752+7292
17/06/09 20:11:07 INFO spark.CacheManager: Partition rdd_42_7 not found, computing it
17/06/09 20:11:07 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:51044+7292
17/06/09 20:11:07 INFO spark.CacheManager: Partition rdd_42_8 not found, computing it
17/06/09 20:11:07 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:58336+7292
17/06/09 20:11:07 INFO spark.CacheManager: Partition rdd_42_9 not found, computing it
17/06/09 20:11:07 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:65628+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 54, boot = 21, init = 28, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_5 stored as bytes in memory (estimated size 930.0 B, free 809.3 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 57, boot = 11, init = 41, finish = 5
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 53, boot = 7, init = 41, finish = 5
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 58, boot = 7, init = 46, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_6 stored as bytes in memory (estimated size 559.0 B, free 809.8 KB)
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_9 stored as bytes in memory (estimated size 950.0 B, free 810.7 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 65, boot = 10, init = 45, finish = 10
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_8 stored as bytes in memory (estimated size 869.0 B, free 811.6 KB)
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_7 stored as bytes in memory (estimated size 640.0 B, free 812.2 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 45, boot = 19, init = 26, finish = 0
17/06/09 20:11:08 INFO executor.Executor: Finished task 5.0 in stage 26.0 (TID 1205). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1210
17/06/09 20:11:08 INFO executor.Executor: Running task 10.0 in stage 26.0 (TID 1210)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_10 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:72920+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 41, boot = 24, init = 17, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 38, boot = 18, init = 20, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 39, boot = 18, init = 21, finish = 0
17/06/09 20:11:08 INFO executor.Executor: Finished task 6.0 in stage 26.0 (TID 1206). 2697 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 9.0 in stage 26.0 (TID 1209). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 8.0 in stage 26.0 (TID 1208). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 42, boot = 22, init = 19, finish = 1
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1211
17/06/09 20:11:08 INFO executor.Executor: Running task 11.0 in stage 26.0 (TID 1211)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1212
17/06/09 20:11:08 INFO executor.Executor: Running task 12.0 in stage 26.0 (TID 1212)
17/06/09 20:11:08 INFO executor.Executor: Finished task 7.0 in stage 26.0 (TID 1207). 2699 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1213
17/06/09 20:11:08 INFO executor.Executor: Running task 13.0 in stage 26.0 (TID 1213)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_11 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:80212+7292
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1214
17/06/09 20:11:08 INFO executor.Executor: Running task 14.0 in stage 26.0 (TID 1214)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_12 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:87504+7292
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_13 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:94796+7292
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_14 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:102088+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 48, boot = 8, init = 35, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_10 stored as bytes in memory (estimated size 948.0 B, free 813.1 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 48, boot = 10, init = 33, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_11 stored as bytes in memory (estimated size 920.0 B, free 814.0 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 51, boot = 3, init = 43, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_13 stored as bytes in memory (estimated size 912.0 B, free 814.9 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 56, boot = 10, init = 40, finish = 6
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 58, boot = 13, init = 40, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_12 stored as bytes in memory (estimated size 852.0 B, free 815.8 KB)
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_14 stored as bytes in memory (estimated size 834.0 B, free 816.6 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 42, boot = 17, init = 24, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Finished task 10.0 in stage 26.0 (TID 1210). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1215
17/06/09 20:11:08 INFO executor.Executor: Running task 15.0 in stage 26.0 (TID 1215)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_15 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:109380+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 41, boot = 21, init = 20, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 39, boot = 20, init = 18, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Finished task 11.0 in stage 26.0 (TID 1211). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 13.0 in stage 26.0 (TID 1213). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1216
17/06/09 20:11:08 INFO executor.Executor: Running task 16.0 in stage 26.0 (TID 1216)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 40, boot = 22, init = 18, finish = 0
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1217
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 39, boot = 24, init = 14, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Running task 17.0 in stage 26.0 (TID 1217)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_16 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:116672+7292
17/06/09 20:11:08 INFO executor.Executor: Finished task 12.0 in stage 26.0 (TID 1212). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 14.0 in stage 26.0 (TID 1214). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_17 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:123964+7292
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1218
17/06/09 20:11:08 INFO executor.Executor: Running task 18.0 in stage 26.0 (TID 1218)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1219
17/06/09 20:11:08 INFO executor.Executor: Running task 19.0 in stage 26.0 (TID 1219)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_18 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:131256+7292
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_19 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:138548+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 49, boot = 16, init = 28, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_15 stored as bytes in memory (estimated size 913.0 B, free 817.5 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 53, boot = 14, init = 34, finish = 5
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 52, boot = 13, init = 34, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_16 stored as bytes in memory (estimated size 748.0 B, free 818.2 KB)
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_17 stored as bytes in memory (estimated size 836.0 B, free 819.0 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 50, boot = 7, init = 38, finish = 5
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 52, boot = 2, init = 45, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_19 stored as bytes in memory (estimated size 920.0 B, free 819.9 KB)
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_18 stored as bytes in memory (estimated size 955.0 B, free 820.8 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 38, boot = 16, init = 22, finish = 0
17/06/09 20:11:08 INFO executor.Executor: Finished task 15.0 in stage 26.0 (TID 1215). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1220
17/06/09 20:11:08 INFO executor.Executor: Running task 20.0 in stage 26.0 (TID 1220)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_20 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:145840+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 38, boot = 24, init = 14, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 43, boot = 23, init = 19, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Finished task 16.0 in stage 26.0 (TID 1216). 2701 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 17.0 in stage 26.0 (TID 1217). 2701 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1221
17/06/09 20:11:08 INFO executor.Executor: Running task 21.0 in stage 26.0 (TID 1221)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1222
17/06/09 20:11:08 INFO executor.Executor: Running task 22.0 in stage 26.0 (TID 1222)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_21 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:153132+7292
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_22 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:160424+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 38, boot = 20, init = 18, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 41, boot = 20, init = 21, finish = 0
17/06/09 20:11:08 INFO executor.Executor: Finished task 18.0 in stage 26.0 (TID 1218). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 19.0 in stage 26.0 (TID 1219). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1223
17/06/09 20:11:08 INFO executor.Executor: Running task 23.0 in stage 26.0 (TID 1223)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1224
17/06/09 20:11:08 INFO executor.Executor: Running task 24.0 in stage 26.0 (TID 1224)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_23 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:167716+7292
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_24 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:175008+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 47, boot = 5, init = 37, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_20 stored as bytes in memory (estimated size 917.0 B, free 821.7 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 53, boot = 15, init = 33, finish = 5
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 54, boot = 17, init = 32, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_22 stored as bytes in memory (estimated size 737.0 B, free 822.5 KB)
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_21 stored as bytes in memory (estimated size 922.0 B, free 823.4 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 50, boot = 4, init = 41, finish = 5
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 54, boot = 8, init = 41, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_24 stored as bytes in memory (estimated size 858.0 B, free 824.2 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 40, boot = 13, init = 26, finish = 1
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_23 stored as bytes in memory (estimated size 914.0 B, free 825.1 KB)
17/06/09 20:11:08 INFO executor.Executor: Finished task 20.0 in stage 26.0 (TID 1220). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1225
17/06/09 20:11:08 INFO executor.Executor: Running task 25.0 in stage 26.0 (TID 1225)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_25 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:182300+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 42, boot = 26, init = 16, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 39, boot = 23, init = 16, finish = 0
17/06/09 20:11:08 INFO executor.Executor: Finished task 22.0 in stage 26.0 (TID 1222). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 21.0 in stage 26.0 (TID 1221). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1226
17/06/09 20:11:08 INFO executor.Executor: Running task 26.0 in stage 26.0 (TID 1226)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1227
17/06/09 20:11:08 INFO executor.Executor: Running task 27.0 in stage 26.0 (TID 1227)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_26 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:189592+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 38, boot = 21, init = 17, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 42, boot = 20, init = 22, finish = 0
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_27 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:196884+7292
17/06/09 20:11:08 INFO executor.Executor: Finished task 23.0 in stage 26.0 (TID 1223). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 24.0 in stage 26.0 (TID 1224). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1228
17/06/09 20:11:08 INFO executor.Executor: Running task 28.0 in stage 26.0 (TID 1228)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1229
17/06/09 20:11:08 INFO executor.Executor: Running task 29.0 in stage 26.0 (TID 1229)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_28 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:204176+7292
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_29 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:211468+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 48, boot = 5, init = 38, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_25 stored as bytes in memory (estimated size 950.0 B, free 826.0 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 52, boot = 7, init = 40, finish = 5
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 56, boot = 7, init = 44, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_27 stored as bytes in memory (estimated size 790.0 B, free 826.8 KB)
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_26 stored as bytes in memory (estimated size 872.0 B, free 827.6 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 52, boot = 7, init = 40, finish = 5
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 41, boot = 16, init = 25, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 54, boot = 5, init = 41, finish = 8
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_28 stored as bytes in memory (estimated size 901.0 B, free 828.5 KB)
17/06/09 20:11:08 INFO executor.Executor: Finished task 25.0 in stage 26.0 (TID 1225). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_29 stored as bytes in memory (estimated size 901.0 B, free 829.4 KB)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1230
17/06/09 20:11:08 INFO executor.Executor: Running task 30.0 in stage 26.0 (TID 1230)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_30 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:218760+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 38, boot = 34, init = 4, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 42, boot = 24, init = 17, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Finished task 26.0 in stage 26.0 (TID 1226). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 27.0 in stage 26.0 (TID 1227). 2701 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1231
17/06/09 20:11:08 INFO executor.Executor: Running task 31.0 in stage 26.0 (TID 1231)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1232
17/06/09 20:11:08 INFO executor.Executor: Running task 32.0 in stage 26.0 (TID 1232)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 42, boot = 22, init = 19, finish = 1
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_31 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:226052+7292
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_32 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:233344+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 39, boot = 20, init = 18, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Finished task 28.0 in stage 26.0 (TID 1228). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 29.0 in stage 26.0 (TID 1229). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1233
17/06/09 20:11:08 INFO executor.Executor: Running task 33.0 in stage 26.0 (TID 1233)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1234
17/06/09 20:11:08 INFO executor.Executor: Running task 34.0 in stage 26.0 (TID 1234)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_34 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:247928+7292
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_33 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:240636+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 46, boot = 16, init = 25, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_30 stored as bytes in memory (estimated size 906.0 B, free 830.3 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 51, boot = 12, init = 34, finish = 5
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 50, boot = 3, init = 42, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_31 stored as bytes in memory (estimated size 827.0 B, free 831.1 KB)
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_32 stored as bytes in memory (estimated size 993.0 B, free 832.1 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 41, boot = 18, init = 23, finish = 0
17/06/09 20:11:08 INFO executor.Executor: Finished task 30.0 in stage 26.0 (TID 1230). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 50, boot = 6, init = 39, finish = 5
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1235
17/06/09 20:11:08 INFO executor.Executor: Running task 35.0 in stage 26.0 (TID 1235)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 50, boot = 2, init = 43, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_34 stored as bytes in memory (estimated size 842.0 B, free 832.9 KB)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_35 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:255220+7292
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_33 stored as bytes in memory (estimated size 869.0 B, free 833.7 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 42, boot = 24, init = 18, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 38, boot = 23, init = 15, finish = 0
17/06/09 20:11:08 INFO executor.Executor: Finished task 32.0 in stage 26.0 (TID 1232). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 31.0 in stage 26.0 (TID 1231). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1236
17/06/09 20:11:08 INFO executor.Executor: Running task 36.0 in stage 26.0 (TID 1236)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1237
17/06/09 20:11:08 INFO executor.Executor: Running task 37.0 in stage 26.0 (TID 1237)
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_36 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:262512+7292
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_37 not found, computing it
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 43, boot = 17, init = 25, finish = 1
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:269804+7292
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 38, boot = 12, init = 26, finish = 0
17/06/09 20:11:08 INFO executor.Executor: Finished task 34.0 in stage 26.0 (TID 1234). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 33.0 in stage 26.0 (TID 1233). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1238
17/06/09 20:11:08 INFO executor.Executor: Running task 38.0 in stage 26.0 (TID 1238)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1239
17/06/09 20:11:08 INFO executor.Executor: Running task 39.0 in stage 26.0 (TID 1239)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 47, boot = 17, init = 23, finish = 7
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_38 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:277096+7292
17/06/09 20:11:08 INFO spark.CacheManager: Partition rdd_42_39 not found, computing it
17/06/09 20:11:08 INFO rdd.HadoopRDD: Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:284388+7303
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_35 stored as bytes in memory (estimated size 917.0 B, free 834.6 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 53, boot = 8, init = 40, finish = 5
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 53, boot = 12, init = 36, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_37 stored as bytes in memory (estimated size 939.0 B, free 835.5 KB)
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_36 stored as bytes in memory (estimated size 912.0 B, free 836.4 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 41, boot = 9, init = 31, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Finished task 35.0 in stage 26.0 (TID 1235). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 48, boot = 17, init = 25, finish = 6
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 51, boot = 8, init = 38, finish = 5
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_38 stored as bytes in memory (estimated size 974.0 B, free 837.4 KB)
17/06/09 20:11:08 INFO storage.MemoryStore: Block rdd_42_39 stored as bytes in memory (estimated size 877.0 B, free 838.2 KB)
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 40, boot = 24, init = 16, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 42, boot = 22, init = 19, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Finished task 36.0 in stage 26.0 (TID 1236). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 37.0 in stage 26.0 (TID 1237). 2705 bytes result sent to driver
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 40, boot = 13, init = 27, finish = 0
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 38, boot = 18, init = 20, finish = 0
17/06/09 20:11:08 INFO executor.Executor: Finished task 38.0 in stage 26.0 (TID 1238). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Finished task 39.0 in stage 26.0 (TID 1239). 2703 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1240
17/06/09 20:11:08 INFO executor.Executor: Running task 0.0 in stage 27.0 (TID 1240)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1241
17/06/09 20:11:08 INFO executor.Executor: Running task 1.0 in stage 27.0 (TID 1241)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1242
17/06/09 20:11:08 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 38
17/06/09 20:11:08 INFO executor.Executor: Running task 2.0 in stage 27.0 (TID 1242)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1243
17/06/09 20:11:08 INFO executor.Executor: Running task 3.0 in stage 27.0 (TID 1243)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1244
17/06/09 20:11:08 INFO executor.Executor: Running task 4.0 in stage 27.0 (TID 1244)
17/06/09 20:11:08 INFO storage.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 5.8 KB, free 844.0 KB)
17/06/09 20:11:08 INFO broadcast.TorrentBroadcast: Reading broadcast variable 38 took 11 ms
17/06/09 20:11:08 INFO storage.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 9.6 KB, free 853.7 KB)
17/06/09 20:11:08 INFO storage.BlockManager: Found block rdd_42_0 locally
17/06/09 20:11:08 INFO storage.BlockManager: Found block rdd_42_4 locally
17/06/09 20:11:08 INFO storage.BlockManager: Found block rdd_42_2 locally
17/06/09 20:11:08 INFO storage.BlockManager: Found block rdd_42_3 locally
17/06/09 20:11:08 INFO storage.BlockManager: Found block rdd_42_1 locally
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 39, boot = -108, init = 146, finish = 1
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 42, boot = -77, init = 118, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Finished task 0.0 in stage 27.0 (TID 1240). 2474 bytes result sent to driver
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 41, boot = -71, init = 111, finish = 1
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 39, boot = -64, init = 102, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Finished task 4.0 in stage 27.0 (TID 1244). 2685 bytes result sent to driver
17/06/09 20:11:08 INFO python.PythonRunner: Times: total = 42, boot = -66, init = 107, finish = 1
17/06/09 20:11:08 INFO executor.Executor: Finished task 2.0 in stage 27.0 (TID 1242). 2523 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1245
17/06/09 20:11:08 INFO executor.Executor: Running task 5.0 in stage 27.0 (TID 1245)
17/06/09 20:11:08 INFO executor.Executor: Finished task 3.0 in stage 27.0 (TID 1243). 2651 bytes result sent to driver
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1246
17/06/09 20:11:08 INFO executor.Executor: Finished task 1.0 in stage 27.0 (TID 1241). 2428 bytes result sent to driver
17/06/09 20:11:08 INFO executor.Executor: Running task 6.0 in stage 27.0 (TID 1246)
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1247
17/06/09 20:11:08 INFO executor.Executor: Running task 7.0 in stage 27.0 (TID 1247)
17/06/09 20:11:08 INFO storage.BlockManager: Found block rdd_42_5 locally
17/06/09 20:11:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1248
17/06/09 20:11:09 INFO executor.Executor: Running task 8.0 in stage 27.0 (TID 1248)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1249
17/06/09 20:11:09 INFO executor.Executor: Running task 9.0 in stage 27.0 (TID 1249)
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_7 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_6 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_8 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_9 locally
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 40, boot = 26, init = 13, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 5.0 in stage 27.0 (TID 1245). 2721 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 42, boot = 13, init = 28, finish = 1
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 40, boot = 15, init = 25, finish = 0
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 42, boot = 24, init = 17, finish = 1
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1250
17/06/09 20:11:09 INFO executor.Executor: Running task 10.0 in stage 27.0 (TID 1250)
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 43, boot = 13, init = 29, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 8.0 in stage 27.0 (TID 1248). 2263 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 6.0 in stage 27.0 (TID 1246). 2220 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 7.0 in stage 27.0 (TID 1247). 2171 bytes result sent to driver
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_10 locally
17/06/09 20:11:09 INFO executor.Executor: Finished task 9.0 in stage 27.0 (TID 1249). 2427 bytes result sent to driver
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1251
17/06/09 20:11:09 INFO executor.Executor: Running task 11.0 in stage 27.0 (TID 1251)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1252
17/06/09 20:11:09 INFO executor.Executor: Running task 12.0 in stage 27.0 (TID 1252)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1253
17/06/09 20:11:09 INFO executor.Executor: Running task 13.0 in stage 27.0 (TID 1253)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1254
17/06/09 20:11:09 INFO executor.Executor: Running task 14.0 in stage 27.0 (TID 1254)
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_11 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_12 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_13 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_14 locally
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 40, boot = 14, init = 25, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 10.0 in stage 27.0 (TID 1250). 2667 bytes result sent to driver
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1255
17/06/09 20:11:09 INFO executor.Executor: Running task 15.0 in stage 27.0 (TID 1255)
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 39, boot = 12, init = 26, finish = 1
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 42, boot = 13, init = 28, finish = 1
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 40, boot = 10, init = 29, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 11.0 in stage 27.0 (TID 1251). 2402 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 12.0 in stage 27.0 (TID 1252). 2486 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = 11, init = 29, finish = 1
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_15 locally
17/06/09 20:11:09 INFO executor.Executor: Finished task 14.0 in stage 27.0 (TID 1254). 2404 bytes result sent to driver
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1256
17/06/09 20:11:09 INFO executor.Executor: Running task 16.0 in stage 27.0 (TID 1256)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1257
17/06/09 20:11:09 INFO executor.Executor: Finished task 13.0 in stage 27.0 (TID 1253). 2522 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Running task 17.0 in stage 27.0 (TID 1257)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1258
17/06/09 20:11:09 INFO executor.Executor: Running task 18.0 in stage 27.0 (TID 1258)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1259
17/06/09 20:11:09 INFO executor.Executor: Running task 19.0 in stage 27.0 (TID 1259)
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_16 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_17 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_19 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_18 locally
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 42, boot = 19, init = 22, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 15.0 in stage 27.0 (TID 1255). 2523 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 39, boot = 15, init = 24, finish = 0
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1260
17/06/09 20:11:09 INFO executor.Executor: Running task 20.0 in stage 27.0 (TID 1260)
17/06/09 20:11:09 INFO executor.Executor: Finished task 16.0 in stage 27.0 (TID 1256). 2171 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 40, boot = 14, init = 26, finish = 0
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 39, boot = 13, init = 25, finish = 1
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_20 locally
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1261
17/06/09 20:11:09 INFO executor.Executor: Running task 21.0 in stage 27.0 (TID 1261)
17/06/09 20:11:09 INFO executor.Executor: Finished task 19.0 in stage 27.0 (TID 1259). 2517 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 17.0 in stage 27.0 (TID 1257). 2128 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 42, boot = 16, init = 25, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 18.0 in stage 27.0 (TID 1258). 2349 bytes result sent to driver
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1262
17/06/09 20:11:09 INFO executor.Executor: Running task 22.0 in stage 27.0 (TID 1262)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1263
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_21 locally
17/06/09 20:11:09 INFO executor.Executor: Running task 23.0 in stage 27.0 (TID 1263)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1264
17/06/09 20:11:09 INFO executor.Executor: Running task 24.0 in stage 27.0 (TID 1264)
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_22 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_24 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_23 locally
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 40, boot = 17, init = 22, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 20.0 in stage 27.0 (TID 1260). 2355 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = 19, init = 21, finish = 1
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1265
17/06/09 20:11:09 INFO executor.Executor: Running task 25.0 in stage 27.0 (TID 1265)
17/06/09 20:11:09 INFO executor.Executor: Finished task 21.0 in stage 27.0 (TID 1261). 2486 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = 14, init = 25, finish = 2
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = 11, init = 29, finish = 1
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1266
17/06/09 20:11:09 INFO executor.Executor: Running task 26.0 in stage 27.0 (TID 1266)
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = 19, init = 21, finish = 1
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_25 locally
17/06/09 20:11:09 INFO executor.Executor: Finished task 22.0 in stage 27.0 (TID 1262). 2171 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 24.0 in stage 27.0 (TID 1264). 2592 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 23.0 in stage 27.0 (TID 1263). 2385 bytes result sent to driver
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_26 locally
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1267
17/06/09 20:11:09 INFO executor.Executor: Running task 27.0 in stage 27.0 (TID 1267)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1268
17/06/09 20:11:09 INFO executor.Executor: Running task 28.0 in stage 27.0 (TID 1268)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1269
17/06/09 20:11:09 INFO executor.Executor: Running task 29.0 in stage 27.0 (TID 1269)
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_27 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_28 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_29 locally
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 42, boot = 14, init = 27, finish = 1
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = 35, init = 5, finish = 1
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = 34, init = 6, finish = 1
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 42, boot = 34, init = 7, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 27.0 in stage 27.0 (TID 1267). 2182 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 28.0 in stage 27.0 (TID 1268). 2349 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 26.0 in stage 27.0 (TID 1266). 2267 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 25.0 in stage 27.0 (TID 1265). 2355 bytes result sent to driver
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1270
17/06/09 20:11:09 INFO executor.Executor: Running task 30.0 in stage 27.0 (TID 1270)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1271
17/06/09 20:11:09 INFO executor.Executor: Running task 31.0 in stage 27.0 (TID 1271)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1272
17/06/09 20:11:09 INFO executor.Executor: Running task 32.0 in stage 27.0 (TID 1272)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1273
17/06/09 20:11:09 INFO executor.Executor: Running task 33.0 in stage 27.0 (TID 1273)
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_31 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_32 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_33 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_30 locally
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 108, boot = 28, init = 79, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 29.0 in stage 27.0 (TID 1269). 2349 bytes result sent to driver
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1274
17/06/09 20:11:09 INFO executor.Executor: Running task 34.0 in stage 27.0 (TID 1274)
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_34 locally
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 40, boot = -27, init = 66, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 31.0 in stage 27.0 (TID 1271). 2128 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 42, boot = -28, init = 69, finish = 1
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = -24, init = 64, finish = 1
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = -15, init = 55, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 32.0 in stage 27.0 (TID 1272). 2801 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 33.0 in stage 27.0 (TID 1273). 2592 bytes result sent to driver
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1275
17/06/09 20:11:09 INFO executor.Executor: Running task 35.0 in stage 27.0 (TID 1275)
17/06/09 20:11:09 INFO executor.Executor: Finished task 30.0 in stage 27.0 (TID 1270). 2859 bytes result sent to driver
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1276
17/06/09 20:11:09 INFO executor.Executor: Running task 36.0 in stage 27.0 (TID 1276)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1277
17/06/09 20:11:09 INFO executor.Executor: Running task 37.0 in stage 27.0 (TID 1277)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1278
17/06/09 20:11:09 INFO executor.Executor: Running task 38.0 in stage 27.0 (TID 1278)
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_35 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_37 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_38 locally
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_36 locally
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = 11, init = 29, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 34.0 in stage 27.0 (TID 1274). 2128 bytes result sent to driver
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1279
17/06/09 20:11:09 INFO executor.Executor: Running task 39.0 in stage 27.0 (TID 1279)
17/06/09 20:11:09 INFO storage.BlockManager: Found block rdd_42_39 locally
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 41, boot = 31, init = 9, finish = 1
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 39, boot = 27, init = 11, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 35.0 in stage 27.0 (TID 1275). 2430 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 40, boot = 20, init = 19, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 38.0 in stage 27.0 (TID 1278). 2882 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 42, boot = 27, init = 14, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 37.0 in stage 27.0 (TID 1277). 2349 bytes result sent to driver
17/06/09 20:11:09 INFO executor.Executor: Finished task 36.0 in stage 27.0 (TID 1276). 2673 bytes result sent to driver
17/06/09 20:11:09 INFO python.PythonRunner: Times: total = 39, boot = 17, init = 21, finish = 1
17/06/09 20:11:09 INFO executor.Executor: Finished task 39.0 in stage 27.0 (TID 1279). 2267 bytes result sent to driver
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1280
17/06/09 20:11:09 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 1280)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1281
17/06/09 20:11:09 INFO executor.Executor: Running task 1.0 in stage 28.0 (TID 1281)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1282
17/06/09 20:11:09 INFO executor.Executor: Running task 2.0 in stage 28.0 (TID 1282)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1283
17/06/09 20:11:09 INFO executor.Executor: Running task 3.0 in stage 28.0 (TID 1283)
17/06/09 20:11:09 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1284
17/06/09 20:11:09 INFO executor.Executor: Running task 4.0 in stage 28.0 (TID 1284)
17/06/09 20:11:09 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 39
17/06/09 20:11:09 INFO storage.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 5.4 KB, free 859.1 KB)
17/06/09 20:11:10 INFO broadcast.TorrentBroadcast: Reading broadcast variable 39 took 716 ms
17/06/09 20:11:10 INFO storage.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 9.2 KB, free 868.3 KB)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_1 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_4 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_3 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_2 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_0 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 39, boot = -790, init = 829, finish = 0
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 38, boot = -784, init = 822, finish = 0
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 39, boot = -784, init = 823, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 4.0 in stage 28.0 (TID 1284). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = -768, init = 809, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 2.0 in stage 28.0 (TID 1282). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 3.0 in stage 28.0 (TID 1283). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 49, boot = -793, init = 841, finish = 1
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1285
17/06/09 20:11:10 INFO executor.Executor: Running task 5.0 in stage 28.0 (TID 1285)
17/06/09 20:11:10 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 1280). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1286
17/06/09 20:11:10 INFO executor.Executor: Running task 6.0 in stage 28.0 (TID 1286)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1287
17/06/09 20:11:10 INFO executor.Executor: Finished task 1.0 in stage 28.0 (TID 1281). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Running task 7.0 in stage 28.0 (TID 1287)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1288
17/06/09 20:11:10 INFO executor.Executor: Running task 8.0 in stage 28.0 (TID 1288)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_5 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_6 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1289
17/06/09 20:11:10 INFO executor.Executor: Running task 9.0 in stage 28.0 (TID 1289)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_7 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_8 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_9 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 38, boot = 23, init = 15, finish = 0
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 16, init = 24, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 6.0 in stage 28.0 (TID 1286). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 13, init = 28, finish = 1
17/06/09 20:11:10 INFO executor.Executor: Finished task 5.0 in stage 28.0 (TID 1285). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 39, boot = 23, init = 16, finish = 0
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 13, init = 29, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 7.0 in stage 28.0 (TID 1287). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1290
17/06/09 20:11:10 INFO executor.Executor: Running task 10.0 in stage 28.0 (TID 1290)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1291
17/06/09 20:11:10 INFO executor.Executor: Running task 11.0 in stage 28.0 (TID 1291)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1292
17/06/09 20:11:10 INFO executor.Executor: Finished task 9.0 in stage 28.0 (TID 1289). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 8.0 in stage 28.0 (TID 1288). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Running task 12.0 in stage 28.0 (TID 1292)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_10 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1293
17/06/09 20:11:10 INFO executor.Executor: Running task 13.0 in stage 28.0 (TID 1293)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_11 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1294
17/06/09 20:11:10 INFO executor.Executor: Running task 14.0 in stage 28.0 (TID 1294)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_12 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_13 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_14 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 21, init = 19, finish = 0
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 39, boot = 19, init = 20, finish = 0
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = 17, init = 24, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 10.0 in stage 28.0 (TID 1290). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 12.0 in stage 28.0 (TID 1292). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1295
17/06/09 20:11:10 INFO executor.Executor: Running task 15.0 in stage 28.0 (TID 1295)
17/06/09 20:11:10 INFO executor.Executor: Finished task 11.0 in stage 28.0 (TID 1291). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1296
17/06/09 20:11:10 INFO executor.Executor: Running task 16.0 in stage 28.0 (TID 1296)
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = 12, init = 28, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 17, init = 24, finish = 1
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1297
17/06/09 20:11:10 INFO executor.Executor: Running task 17.0 in stage 28.0 (TID 1297)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_15 locally
17/06/09 20:11:10 INFO executor.Executor: Finished task 14.0 in stage 28.0 (TID 1294). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 13.0 in stage 28.0 (TID 1293). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_17 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1298
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_16 locally
17/06/09 20:11:10 INFO executor.Executor: Running task 18.0 in stage 28.0 (TID 1298)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1299
17/06/09 20:11:10 INFO executor.Executor: Running task 19.0 in stage 28.0 (TID 1299)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_18 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_19 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 19, init = 21, finish = 0
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 20, init = 20, finish = 0
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 39, boot = 15, init = 24, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 15.0 in stage 28.0 (TID 1295). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 16.0 in stage 28.0 (TID 1296). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 17.0 in stage 28.0 (TID 1297). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1300
17/06/09 20:11:10 INFO executor.Executor: Running task 20.0 in stage 28.0 (TID 1300)
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 15, init = 25, finish = 0
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1301
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 16, init = 25, finish = 1
17/06/09 20:11:10 INFO executor.Executor: Running task 21.0 in stage 28.0 (TID 1301)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1302
17/06/09 20:11:10 INFO executor.Executor: Running task 22.0 in stage 28.0 (TID 1302)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_20 locally
17/06/09 20:11:10 INFO executor.Executor: Finished task 18.0 in stage 28.0 (TID 1298). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 19.0 in stage 28.0 (TID 1299). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1303
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_21 locally
17/06/09 20:11:10 INFO executor.Executor: Running task 23.0 in stage 28.0 (TID 1303)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_22 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1304
17/06/09 20:11:10 INFO executor.Executor: Running task 24.0 in stage 28.0 (TID 1304)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_24 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_23 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 22, init = 19, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 38, boot = 18, init = 20, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 20.0 in stage 28.0 (TID 1300). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 21.0 in stage 28.0 (TID 1301). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 14, init = 28, finish = 0
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1305
17/06/09 20:11:10 INFO executor.Executor: Running task 25.0 in stage 28.0 (TID 1305)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1306
17/06/09 20:11:10 INFO executor.Executor: Finished task 22.0 in stage 28.0 (TID 1302). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Running task 26.0 in stage 28.0 (TID 1306)
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 39, boot = 15, init = 23, finish = 1
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_25 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1307
17/06/09 20:11:10 INFO executor.Executor: Running task 27.0 in stage 28.0 (TID 1307)
17/06/09 20:11:10 INFO executor.Executor: Finished task 24.0 in stage 28.0 (TID 1304). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 43, boot = 15, init = 27, finish = 1
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_26 locally
17/06/09 20:11:10 INFO executor.Executor: Finished task 23.0 in stage 28.0 (TID 1303). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1308
17/06/09 20:11:10 INFO executor.Executor: Running task 28.0 in stage 28.0 (TID 1308)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_27 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1309
17/06/09 20:11:10 INFO executor.Executor: Running task 29.0 in stage 28.0 (TID 1309)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_28 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_29 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = 17, init = 23, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = 17, init = 24, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 25.0 in stage 28.0 (TID 1305). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 26.0 in stage 28.0 (TID 1306). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 27, init = 13, finish = 0
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1310
17/06/09 20:11:10 INFO executor.Executor: Running task 30.0 in stage 28.0 (TID 1310)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1311
17/06/09 20:11:10 INFO executor.Executor: Running task 31.0 in stage 28.0 (TID 1311)
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 26, init = 14, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 27.0 in stage 28.0 (TID 1307). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_30 locally
17/06/09 20:11:10 INFO executor.Executor: Finished task 28.0 in stage 28.0 (TID 1308). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1312
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 16, init = 25, finish = 1
17/06/09 20:11:10 INFO executor.Executor: Running task 32.0 in stage 28.0 (TID 1312)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_31 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1313
17/06/09 20:11:10 INFO executor.Executor: Running task 33.0 in stage 28.0 (TID 1313)
17/06/09 20:11:10 INFO executor.Executor: Finished task 29.0 in stage 28.0 (TID 1309). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_32 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1314
17/06/09 20:11:10 INFO executor.Executor: Running task 34.0 in stage 28.0 (TID 1314)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_33 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_34 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 18, init = 22, finish = 0
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = 19, init = 22, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 30.0 in stage 28.0 (TID 1310). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 31.0 in stage 28.0 (TID 1311). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 15, init = 25, finish = 0
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1315
17/06/09 20:11:10 INFO executor.Executor: Running task 35.0 in stage 28.0 (TID 1315)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1316
17/06/09 20:11:10 INFO executor.Executor: Running task 36.0 in stage 28.0 (TID 1316)
17/06/09 20:11:10 INFO executor.Executor: Finished task 32.0 in stage 28.0 (TID 1312). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 16, init = 25, finish = 1
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_35 locally
17/06/09 20:11:10 INFO executor.Executor: Finished task 33.0 in stage 28.0 (TID 1313). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1317
17/06/09 20:11:10 INFO executor.Executor: Running task 37.0 in stage 28.0 (TID 1317)
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 14, init = 25, finish = 1
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_36 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1318
17/06/09 20:11:10 INFO executor.Executor: Finished task 34.0 in stage 28.0 (TID 1314). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Running task 38.0 in stage 28.0 (TID 1318)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_37 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1319
17/06/09 20:11:10 INFO executor.Executor: Running task 39.0 in stage 28.0 (TID 1319)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_38 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_39 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = 23, init = 18, finish = 0
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = 19, init = 22, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 35.0 in stage 28.0 (TID 1315). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 36.0 in stage 28.0 (TID 1316). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 38, boot = 14, init = 24, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 37.0 in stage 28.0 (TID 1317). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 19, init = 22, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = 14, init = 26, finish = 0
17/06/09 20:11:10 INFO executor.Executor: Finished task 38.0 in stage 28.0 (TID 1318). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 39.0 in stage 28.0 (TID 1319). 2087 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1320
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1321
17/06/09 20:11:10 INFO executor.Executor: Running task 0.0 in stage 29.0 (TID 1320)
17/06/09 20:11:10 INFO executor.Executor: Running task 1.0 in stage 29.0 (TID 1321)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1322
17/06/09 20:11:10 INFO executor.Executor: Running task 2.0 in stage 29.0 (TID 1322)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1323
17/06/09 20:11:10 INFO executor.Executor: Running task 3.0 in stage 29.0 (TID 1323)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1324
17/06/09 20:11:10 INFO executor.Executor: Running task 4.0 in stage 29.0 (TID 1324)
17/06/09 20:11:10 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 40
17/06/09 20:11:10 INFO storage.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 6.0 KB, free 874.3 KB)
17/06/09 20:11:10 INFO broadcast.TorrentBroadcast: Reading broadcast variable 40 took 10 ms
17/06/09 20:11:10 INFO storage.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 9.9 KB, free 884.2 KB)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_0 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_4 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_3 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_2 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_1 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = -79, init = 118, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 39, boot = -75, init = 113, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 40, boot = -80, init = 119, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = -85, init = 125, finish = 1
17/06/09 20:11:10 INFO executor.Executor: Finished task 3.0 in stage 29.0 (TID 1323). 2128 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = -74, init = 115, finish = 1
17/06/09 20:11:10 INFO executor.Executor: Finished task 4.0 in stage 29.0 (TID 1324). 2128 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 0.0 in stage 29.0 (TID 1320). 2128 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 2.0 in stage 29.0 (TID 1322). 2128 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 1.0 in stage 29.0 (TID 1321). 2171 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1325
17/06/09 20:11:10 INFO executor.Executor: Running task 5.0 in stage 29.0 (TID 1325)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1326
17/06/09 20:11:10 INFO executor.Executor: Running task 6.0 in stage 29.0 (TID 1326)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1327
17/06/09 20:11:10 INFO executor.Executor: Running task 7.0 in stage 29.0 (TID 1327)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1328
17/06/09 20:11:10 INFO executor.Executor: Running task 8.0 in stage 29.0 (TID 1328)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1329
17/06/09 20:11:10 INFO executor.Executor: Running task 9.0 in stage 29.0 (TID 1329)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_5 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_7 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_8 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_6 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_9 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 39, boot = 18, init = 20, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = 16, init = 24, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 43, boot = 25, init = 17, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = 19, init = 21, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 24, init = 17, finish = 1
17/06/09 20:11:10 INFO executor.Executor: Finished task 7.0 in stage 29.0 (TID 1327). 2171 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 5.0 in stage 29.0 (TID 1325). 2171 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 8.0 in stage 29.0 (TID 1328). 2128 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 9.0 in stage 29.0 (TID 1329). 2128 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 6.0 in stage 29.0 (TID 1326). 2171 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1330
17/06/09 20:11:10 INFO executor.Executor: Running task 10.0 in stage 29.0 (TID 1330)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1331
17/06/09 20:11:10 INFO executor.Executor: Running task 11.0 in stage 29.0 (TID 1331)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1332
17/06/09 20:11:10 INFO executor.Executor: Running task 12.0 in stage 29.0 (TID 1332)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1333
17/06/09 20:11:10 INFO executor.Executor: Running task 13.0 in stage 29.0 (TID 1333)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_10 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1334
17/06/09 20:11:10 INFO executor.Executor: Running task 14.0 in stage 29.0 (TID 1334)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_11 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_12 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_14 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_13 locally
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 39, boot = 19, init = 19, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 41, boot = 21, init = 19, finish = 1
17/06/09 20:11:10 INFO executor.Executor: Finished task 11.0 in stage 29.0 (TID 1331). 2128 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 39, boot = 15, init = 23, finish = 1
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 42, boot = 18, init = 23, finish = 1
17/06/09 20:11:10 INFO executor.Executor: Finished task 10.0 in stage 29.0 (TID 1330). 2128 bytes result sent to driver
17/06/09 20:11:10 INFO python.PythonRunner: Times: total = 43, boot = 15, init = 27, finish = 1
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1335
17/06/09 20:11:10 INFO executor.Executor: Running task 15.0 in stage 29.0 (TID 1335)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1336
17/06/09 20:11:10 INFO executor.Executor: Finished task 14.0 in stage 29.0 (TID 1334). 2128 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Finished task 12.0 in stage 29.0 (TID 1332). 2128 bytes result sent to driver
17/06/09 20:11:10 INFO executor.Executor: Running task 16.0 in stage 29.0 (TID 1336)
17/06/09 20:11:10 INFO executor.Executor: Finished task 13.0 in stage 29.0 (TID 1333). 2171 bytes result sent to driver
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1337
17/06/09 20:11:10 INFO executor.Executor: Running task 17.0 in stage 29.0 (TID 1337)
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1338
17/06/09 20:11:10 INFO executor.Executor: Running task 18.0 in stage 29.0 (TID 1338)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_16 locally
17/06/09 20:11:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1339
17/06/09 20:11:10 INFO executor.Executor: Running task 19.0 in stage 29.0 (TID 1339)
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_15 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_17 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_18 locally
17/06/09 20:11:10 INFO storage.BlockManager: Found block rdd_42_19 locally
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 40, boot = 20, init = 19, finish = 1
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 40, boot = 20, init = 19, finish = 1
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 39, boot = 18, init = 20, finish = 1
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 41, boot = 27, init = 14, finish = 0
17/06/09 20:11:11 INFO executor.Executor: Finished task 15.0 in stage 29.0 (TID 1335). 2171 bytes result sent to driver
17/06/09 20:11:11 INFO executor.Executor: Finished task 16.0 in stage 29.0 (TID 1336). 2171 bytes result sent to driver
17/06/09 20:11:11 INFO executor.Executor: Finished task 18.0 in stage 29.0 (TID 1338). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 41, boot = 15, init = 25, finish = 1
17/06/09 20:11:11 INFO executor.Executor: Finished task 17.0 in stage 29.0 (TID 1337). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1340
17/06/09 20:11:11 INFO executor.Executor: Running task 20.0 in stage 29.0 (TID 1340)
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1341
17/06/09 20:11:11 INFO executor.Executor: Running task 21.0 in stage 29.0 (TID 1341)
17/06/09 20:11:11 INFO executor.Executor: Finished task 19.0 in stage 29.0 (TID 1339). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1342
17/06/09 20:11:11 INFO executor.Executor: Running task 22.0 in stage 29.0 (TID 1342)
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1343
17/06/09 20:11:11 INFO executor.Executor: Running task 23.0 in stage 29.0 (TID 1343)
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1344
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_21 locally
17/06/09 20:11:11 INFO executor.Executor: Running task 24.0 in stage 29.0 (TID 1344)
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_20 locally
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_22 locally
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_23 locally
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_24 locally
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 39, boot = 16, init = 22, finish = 1
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 41, boot = 18, init = 22, finish = 1
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 42, boot = 20, init = 21, finish = 1
17/06/09 20:11:11 INFO executor.Executor: Finished task 21.0 in stage 29.0 (TID 1341). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 43, boot = 15, init = 27, finish = 1
17/06/09 20:11:11 INFO executor.Executor: Finished task 22.0 in stage 29.0 (TID 1342). 2171 bytes result sent to driver
17/06/09 20:11:11 INFO executor.Executor: Finished task 23.0 in stage 29.0 (TID 1343). 2171 bytes result sent to driver
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 41, boot = 15, init = 25, finish = 1
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1345
17/06/09 20:11:11 INFO executor.Executor: Running task 25.0 in stage 29.0 (TID 1345)
17/06/09 20:11:11 INFO executor.Executor: Finished task 20.0 in stage 29.0 (TID 1340). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO executor.Executor: Finished task 24.0 in stage 29.0 (TID 1344). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1346
17/06/09 20:11:11 INFO executor.Executor: Running task 26.0 in stage 29.0 (TID 1346)
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1347
17/06/09 20:11:11 INFO executor.Executor: Running task 27.0 in stage 29.0 (TID 1347)
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1348
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_25 locally
17/06/09 20:11:11 INFO executor.Executor: Running task 28.0 in stage 29.0 (TID 1348)
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1349
17/06/09 20:11:11 INFO executor.Executor: Running task 29.0 in stage 29.0 (TID 1349)
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_26 locally
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_28 locally
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_27 locally
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_29 locally
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 41, boot = 23, init = 17, finish = 1
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 38, boot = 18, init = 20, finish = 0
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 42, boot = 18, init = 23, finish = 1
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 39, boot = 18, init = 20, finish = 1
17/06/09 20:11:11 INFO executor.Executor: Finished task 25.0 in stage 29.0 (TID 1345). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO executor.Executor: Finished task 28.0 in stage 29.0 (TID 1348). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO executor.Executor: Finished task 27.0 in stage 29.0 (TID 1347). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO executor.Executor: Finished task 26.0 in stage 29.0 (TID 1346). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1350
17/06/09 20:11:11 INFO executor.Executor: Running task 30.0 in stage 29.0 (TID 1350)
17/06/09 20:11:11 INFO python.PythonRunner: Times: total = 43, boot = 14, init = 28, finish = 1
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1351
17/06/09 20:11:11 INFO executor.Executor: Running task 31.0 in stage 29.0 (TID 1351)
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1352
17/06/09 20:11:11 INFO executor.Executor: Running task 32.0 in stage 29.0 (TID 1352)
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1353
17/06/09 20:11:11 INFO executor.Executor: Running task 33.0 in stage 29.0 (TID 1353)
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_30 locally
17/06/09 20:11:11 INFO executor.Executor: Finished task 29.0 in stage 29.0 (TID 1349). 2128 bytes result sent to driver
17/06/09 20:11:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1354
17/06/09 20:11:11 INFO executor.Executor: Running task 34.0 in stage 29.0 (TID 1354)
17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_32 locally
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:18.993 2931 INFO nova.compute.claims [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Attempting claim: memory 2048 MB, disk 20 GB, vcpus 1 CPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:18.995 2931 INFO nova.compute.claims [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Total memory: 64172 MB, used: 512.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:18.996 2931 INFO nova.compute.claims [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] memory limit: 96258.00 MB, free: 95746.00 MB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:18.996 2931 INFO nova.compute.claims [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Total disk: 15 GB, used: 0.00 GB
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:18.997 2931 INFO nova.compute.claims [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] disk limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:18.997 2931 INFO nova.compute.claims [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Total vcpu: 16 VCPU, used: 0.00 VCPU
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:18.998 2931 INFO nova.compute.claims [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] vcpu limit not specified, defaulting to unlimited
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:19.032 2931 INFO nova.compute.claims [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Claim successful
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:19.065 25746 INFO nova.osapi_compute.wsgi.server [req-78eed036-ab3e-4482-b5e2-e1e06c5f0951 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1583 time: 0.1864619
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:19.258 25746 INFO nova.osapi_compute.wsgi.server [req-42da7b86-cc4f-4a8d-b40a-2970b44b2293 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/faf974ea-cba5-4e1b-93f4-3a3bc606006f HTTP/1.1" status: 200 len: 1708 time: 0.1894400
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:19.600 2931 INFO nova.virt.libvirt.driver [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Creating image
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:20.538 25746 INFO nova.osapi_compute.wsgi.server [req-1a062c18-f82b-4f19-bcee-27ca6bf5d241 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2745810
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:20.825 25746 INFO nova.osapi_compute.wsgi.server [req-1239a305-a3f9-4451-8fd8-fc1da207fd05 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1759 time: 0.2832491
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:21.038 2931 INFO nova.compute.manager [-] [instance: c62f4f25-982c-4ea2-b5e4-93000edfcfbf] VM Stopped (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:22.100 25746 INFO nova.osapi_compute.wsgi.server [req-3a411e65-bde8-42a4-b222-4779a62e4316 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2697551
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:22.361 25746 INFO nova.osapi_compute.wsgi.server [req-195dc088-f47a-400c-88fc-84679675701e 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2558119
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:23.633 25746 INFO nova.osapi_compute.wsgi.server [req-ab6ff6af-451d-49d8-be95-0fcdfd749a0b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2665989
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:23.879 25746 INFO nova.osapi_compute.wsgi.server [req-31677284-946b-4d4f-bf44-736a41224f70 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2421730
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:25.130 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Auditing locally available compute resources for node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:25.166 25746 INFO nova.osapi_compute.wsgi.server [req-3f8eee6a-941b-44e5-af74-fb97a076217a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2812061
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:25.433 25746 INFO nova.osapi_compute.wsgi.server [req-9135d880-f631-446a-a1be-52240999a2a8 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2629149
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:25.456 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Total usable vcpus: 16, total allocated vcpus: 1
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:25.457 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Final resource view: name=cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us phys_ram=64172MB used_ram=2560MB phys_disk=15GB used_disk=20GB total_vcpus=16 used_vcpus=1 pci_stats=[]
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:25.518 2931 INFO nova.compute.resource_tracker [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:26.811 25746 INFO nova.osapi_compute.wsgi.server [req-70697504-27f7-40b2-a95b-ef2e60a299b0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.3723969
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:27.105 25746 INFO nova.osapi_compute.wsgi.server [req-b2fc192f-a41f-4ca6-ae6e-c737cc772580 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2913761
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:28.388 25746 INFO nova.osapi_compute.wsgi.server [req-33bf2413-1b09-4d0f-8465-91d928a7994a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2772589
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:28.646 25746 INFO nova.osapi_compute.wsgi.server [req-d703d08d-9f80-49a8-aa5e-ac59537fe21d 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2551649
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:29.942 25746 INFO nova.osapi_compute.wsgi.server [req-88b9956b-ebf6-49eb-9bee-cac3e642febd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2889762
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:30.211 25746 INFO nova.osapi_compute.wsgi.server [req-f0300fec-04dc-44c6-b410-5c7fbf5b2254 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2646649
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:30.676 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:30.678 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:30.866 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:31.484 25746 INFO nova.osapi_compute.wsgi.server [req-4ce4f8fb-f692-4d80-b150-96bc30db03f0 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2672279
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:31.748 25746 INFO nova.osapi_compute.wsgi.server [req-a72df015-19d7-49db-bbde-5b4163055d4f 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2596569
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:33.015 25746 INFO nova.osapi_compute.wsgi.server [req-2897f349-4f7a-48c5-b3b4-7df386f0436a 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2607100
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:33.197 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] VM Started (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:33.266 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] VM Paused (Lifecycle Event)
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:33.277 25746 INFO nova.osapi_compute.wsgi.server [req-bf118dbc-40ea-4813-bddd-8d9dee338ac5 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2576840
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:33.392 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] During sync_power_state the instance has a pending task (spawning). Skip.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:34.557 25746 INFO nova.osapi_compute.wsgi.server [req-0a83191f-13cf-4f95-a6bc-a55a554c37bf 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2735391
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:34.816 25746 INFO nova.osapi_compute.wsgi.server [req-a19a18d8-b7db-460c-b90a-4fa035362c3b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2555051
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:35.138 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:35.139 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:35.319 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:36.247 25746 INFO nova.osapi_compute.wsgi.server [req-e56c8db8-cbdd-4865-a4f9-4a47b276c118 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.4260600
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:36.508 25746 INFO nova.osapi_compute.wsgi.server [req-c632929d-1bf8-43a4-afb7-0f32e2d3cdf3 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2570438
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:37.777 25746 INFO nova.osapi_compute.wsgi.server [req-9568479e-0bb4-4c74-9a16-cf8bc204a371 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2639749
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:38.046 25746 INFO nova.osapi_compute.wsgi.server [req-b6241790-0b6d-4e98-84ca-36bbeb5e676b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2652829
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:39.044 25743 INFO nova.api.openstack.compute.server_external_events [req-dedb4b73-18c3-428b-8f65-56390838beef f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] Creating event network-vif-plugged:1771f0af-aae8-4b03-97c9-a7bc9c5e4bb7 for instance faf974ea-cba5-4e1b-93f4-3a3bc606006f
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:39.049 25743 INFO nova.osapi_compute.wsgi.server [req-dedb4b73-18c3-428b-8f65-56390838beef f7b8d1f1d4d44643b07fa10ca7d021fb e9746973ac574c6b8a9e8857f56a7608 - - -] 10.11.10.1 "POST /v2/e9746973ac574c6b8a9e8857f56a7608/os-server-external-events HTTP/1.1" status: 200 len: 380 time: 0.0901439
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:39.059 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:39.066 2931 INFO nova.virt.libvirt.driver [-] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Instance spawned successfully.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:39.067 2931 INFO nova.compute.manager [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Took 19.47 seconds to spawn the instance on the hypervisor.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:39.178 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] During sync_power_state the instance has a pending task (spawning). Skip.
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:39.179 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] VM Resumed (Lifecycle Event)
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:39.198 2931 INFO nova.compute.manager [req-3d26bc3d-373c-4269-b095-004aab7a9785 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Took 20.22 seconds to build instance.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:39.320 25746 INFO nova.osapi_compute.wsgi.server [req-465fd6e6-5eda-4b3f-b35e-5d48f8fa1156 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1893 time: 0.2692790
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:39.584 25746 INFO nova.osapi_compute.wsgi.server [req-b4340956-4635-4b87-96a5-fdc4a4bb19fd 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2587440
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:40.141 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:40.142 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:40.322 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:40.851 25746 INFO nova.osapi_compute.wsgi.server [req-a06fae15-859f-4608-a340-3185fa888bac 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2621021
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:41.129 25746 INFO nova.osapi_compute.wsgi.server [req-5daeb01c-10d3-4b9c-bf66-8a5171be5f2b 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1910 time: 0.2753000
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:45.369 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): checking
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:45.370 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] image 0673dd71-34c5-4fbb-86c4-40623fbe45b4 at (/var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742): in use: on this node 1 local, 0 on other nodes sharing this instance storage
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:45.452 25777 INFO nova.metadata.wsgi.server [req-a9f11ae2-996a-4e7e-acf5-453f7a2ff027 - - - - -] 10.11.21.143,10.11.10.1 "GET /openstack/2012-08-10/meta_data.json HTTP/1.1" status: 200 len: 264 time: 0.2402911
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:45.462 25777 INFO nova.metadata.wsgi.server [-] 10.11.21.143,10.11.10.1 "GET /openstack/2013-10-17 HTTP/1.1" status: 200 len: 157 time: 0.0009041
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:45.546 2931 INFO nova.virt.libvirt.imagecache [req-addc1839-2ed5-4778-b57e-5854eb7b8b09 - - - - -] Active base files: /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:45.772 25790 INFO nova.metadata.wsgi.server [req-189e6373-cac8-4dcd-9029-7cb80f2e114c - - - - -] 10.11.21.143,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2292829
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:45.997 25786 INFO nova.metadata.wsgi.server [req-bab7dcd0-d989-4e78-b6ba-7bd2f7658191 - - - - -] 10.11.21.143,10.11.10.1 "GET /openstack/2013-10-17/vendor_data.json HTTP/1.1" status: 200 len: 124 time: 0.2143800
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:46.305 25795 INFO nova.metadata.wsgi.server [req-c58fdf6b-7dad-448e-b06a-da2fd52415fc - - - - -] 10.11.21.143,10.11.10.1 "GET /openstack/2013-10-17/user_data HTTP/1.1" status: 404 len: 176 time: 0.2187860
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:46.317 25795 INFO nova.metadata.wsgi.server [-] 10.11.21.143,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.0014119
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:46.550 25774 INFO nova.metadata.wsgi.server [req-9adf5fbe-d988-4bac-9993-a4141b8fcadb - - - - -] 10.11.21.143,10.11.10.1 "GET /openstack/2013-10-17/meta_data.json HTTP/1.1" status: 200 len: 967 time: 0.2221360
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:46.859 25783 INFO nova.metadata.wsgi.server [req-ba4b4d89-3236-416c-999f-b42782557681 - - - - -] 10.11.21.143,10.11.10.1 "GET /latest/meta-data/ HTTP/1.1" status: 200 len: 328 time: 0.2230601
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:46.951 25795 INFO nova.metadata.wsgi.server [-] 10.11.21.143,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ HTTP/1.1" status: 200 len: 124 time: 0.0010331
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:47.199 25793 INFO nova.metadata.wsgi.server [req-667e1983-f313-4cac-9f50-5203fb4ac0be - - - - -] 10.11.21.143,10.11.10.1 "GET /latest/meta-data/block-device-mapping/ami HTTP/1.1" status: 200 len: 119 time: 0.2359550
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:47.215 25795 INFO nova.metadata.wsgi.server [-] 10.11.21.143,10.11.10.1 "GET /latest/meta-data/block-device-mapping/root HTTP/1.1" status: 200 len: 124 time: 0.0009151
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:47.410 25746 INFO nova.osapi_compute.wsgi.server [req-699eeadf-6db8-44a4-8521-1ab4e8a53b53 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "DELETE /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/faf974ea-cba5-4e1b-93f4-3a3bc606006f HTTP/1.1" status: 204 len: 203 time: 0.2727120
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:47.447 2931 INFO nova.compute.manager [req-699eeadf-6db8-44a4-8521-1ab4e8a53b53 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Terminating instance
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:47.652 25799 INFO nova.metadata.wsgi.server [req-e5e061d6-f2aa-464b-9512-2fd6c7c9a812 - - - - -] 10.11.21.143,10.11.10.1 "GET /latest/meta-data/placement/ HTTP/1.1" status: 200 len: 134 time: 0.4259689
nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:14:47.663 2931 INFO nova.virt.libvirt.driver [-] [instance: faf974ea-cba5-4e1b-93f4-3a3bc606006f] Instance destroyed successfully.
nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:14:47.687 25746 INFO nova.osapi_compute.wsgi.server [req-dd237280-5bc8-41cb-a035-26c8e64d49fc 113d3a99c3da401fbd62cc2caa5b96d2 54fadb412c4e40cdbaed9335e4c35a9e - - -] 10.11.10.1 "GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1" status: 200 len: 1916 time: 0.2717581